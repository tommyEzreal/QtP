[
    {
        "id": "1",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] DM id F",
                "[BOLD] DM ood F",
                "[BOLD] PAS id F",
                "[BOLD] PAS ood F",
                "[BOLD] PSD id F",
                "[BOLD] PSD ood F",
                "[BOLD] EDS Smatch F",
                "[BOLD] EDS EDM",
                "[BOLD] AMR 2015 Smatch F",
                "[BOLD] AMR 2017 Smatch F"
            ],
            "rows": [
                [
                    "Groschwitz et\u00a0al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "70.2",
                    "71.0"
                ],
                [
                    "Lyu and Titov ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "73.7",
                    "74.4 \u00b10.16"
                ],
                [
                    "Zhang et\u00a0al. ( 2019 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 76.3 \u00b10.1"
                ],
                [
                    "Peng et\u00a0al. ( 2017 ) Basic",
                    "89.4",
                    "84.5",
                    "92.2",
                    "88.3",
                    "77.6",
                    "75.3",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Dozat and Manning ( 2018 )",
                    "93.7",
                    "88.9",
                    "94.0",
                    "90.8",
                    "81.0",
                    "79.4",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Buys and Blunsom ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "85.5",
                    "85.9",
                    "60.1",
                    "-"
                ],
                [
                    "Chen et\u00a0al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 90.9,",
                    "[BOLD] 90.41",
                    "-",
                    "-"
                ],
                [
                    "This paper (GloVe)",
                    "90.4 \u00b10.2",
                    "84.3 \u00b10.2",
                    "91.4 \u00b10.1",
                    "86.6 \u00b10.1",
                    "78.1 \u00b10.2",
                    "74.5 \u00b10.2",
                    "87.6 \u00b10.1",
                    "82.5 \u00b10.1",
                    "69.2 \u00b10.4",
                    "70.7 \u00b10.2"
                ],
                [
                    "This paper (BERT)",
                    "[BOLD] 93.9 \u00b10.1",
                    "[BOLD] 90.3 \u00b10.1",
                    "[BOLD] 94.5 \u00b10.1",
                    "[BOLD] 92.5 \u00b10.1",
                    "[BOLD] 82.0 \u00b10.1",
                    "[BOLD] 81.5 \u00b10.3",
                    "90.1 \u00b10.1",
                    "84.9 \u00b10.1",
                    "[BOLD] 74.3 \u00b10.2",
                    "75.3 \u00b10.2"
                ],
                [
                    "Peng et\u00a0al. ( 2017 ) Freda1",
                    "90.0",
                    "84.9",
                    "92.3",
                    "88.3",
                    "78.1",
                    "75.8",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Peng et\u00a0al. ( 2017 ) Freda3",
                    "90.4",
                    "85.3",
                    "92.7",
                    "89.0",
                    "78.5",
                    "76.4",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "This paper, MTL (GloVe)",
                    "91.2 \u00b10.1",
                    "85.7 \u00b10.0",
                    "92.2 \u00b10.2",
                    "88.0 \u00b10.3",
                    "78.9 \u00b10.3",
                    "76.2 \u00b10.4",
                    "88.2 \u00b10.1",
                    "83.3 \u00b10.1",
                    "(70.4) \u00b10.2",
                    "71.2 \u00b10.2"
                ],
                [
                    "This paper, MTL (BERT)",
                    "[BOLD] 94.1 \u00b10.1",
                    "[BOLD] 90.5 \u00b10.1",
                    "[BOLD] 94.7 \u00b10.1",
                    "[BOLD] 92.8 \u00b10.1",
                    "[BOLD] 82.1 \u00b10.2",
                    "[BOLD] 81.6 \u00b10.1",
                    "90.4 \u00b10.1",
                    "85.2 \u00b10.1",
                    "(74.5)3 \u00b10.1",
                    "75.3 \u00b10.1"
                ]
            ],
            "title": "Table 1: Semantic parsing accuracies (id = in domain test set; ood = out of domain test set)."
        },
        "insight": "Table 1 (upper part) shows the results of our basic semantic parser (with GloVe embeddings) on all six graphbanks [CONTINUE] Our results are competitive across the board, and set a new state of the art for EDS Smatch scores (Cai and Knight, 2013) among EDS parsers which are not trained on gold syntax information. [CONTINUE] Our EDM score (Dridan and Oepen, 2011) on EDS is lower, [CONTINUE] The use of BERT embeddings is highly effective across the board. We set a new state of the art (without gold syntax) on all graphbanks except AMR 2017; [CONTINUE] The improvement is particularly pronounced in the out-of-domain evaluations, illustrating BERT's ability to transfer across domains. [CONTINUE] The results on the test dataset are shown in Table 1 (bottom). With GloVe, multi-task learning led to substantial improvements; with BERT the improvements are smaller but still noticeable."
    },
    {
        "id": "2",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] BERT dev",
                "[BOLD] BERT test",
                "[BOLD] BioBERT dev",
                "[BOLD] BioBERT test"
            ],
            "rows": [
                [
                    "MedNLI",
                    "79.56",
                    "77.49",
                    "82.15",
                    "79.04"
                ],
                [
                    "MNLI (M)",
                    "83.52",
                    "-",
                    "81.23",
                    "-"
                ],
                [
                    "SNLI (S)",
                    "90.39",
                    "-",
                    "89.10",
                    "-"
                ],
                [
                    "M \u2192 MedNLI",
                    "80.14",
                    "[BOLD] 78.62",
                    "82.72",
                    "80.80"
                ],
                [
                    "S \u2192 MedNLI",
                    "80.28",
                    "78.19",
                    "83.29",
                    "81.29"
                ],
                [
                    "M \u2192 S \u2192 MedNLI",
                    "80.43",
                    "78.12",
                    "83.29",
                    "80.30"
                ],
                [
                    "S \u2192 M \u2192 MedNLI",
                    "[BOLD] 81.72",
                    "77.98",
                    "[BOLD] 83.51",
                    "[BOLD] 82.63"
                ],
                [
                    "MedNLI (expanded)",
                    "79.13",
                    "77.07",
                    "[BOLD] 83.87",
                    "79.95"
                ],
                [
                    "S \u2192 M \u2192 MedNLI (expanded)",
                    "[BOLD] 82.15",
                    "[BOLD] 79.95",
                    "83.08",
                    "[BOLD] 81.85"
                ]
            ],
            "title": "Table 4: All experiment results of transfer learning and abbreviation expansion (top-2 scores marked as bold). MedNLI (expanded) denotes MedNLI with abbreviation expansion."
        },
        "insight": "We conduct transfer learning on four different combinations of MedNLI, SNLI, and MNLI as it shown in the table 4 (line 4 to 7) and also add the results of general domain tasks (MNLI, SNLI) for comparison. [CONTINUE] BERT performs better on tasks in the general domain while BioBERT performs better on MedNLI which is in the clinical domain. [CONTINUE] positive transfer occurs on MedNLI. [CONTINUE] even though BioBERT is finetuned on general domain tasks before MedNLI, transfer learning shows better results than that fine-tuned on MedNLI directly. [CONTINUE] the domain specific language representations from BioBERT are maintained while fine-tuning on general domain tasks by showing that the transfer learning results of MedNLI on BioBERT have better performance than the results on BERT (line 4 to 7). [CONTINUE] the accuracy of MNLI and SNLI on BioBERT is lower than the accuracy on BERT. [CONTINUE] The best combination is SNLI \u2192 MNLI \u2192 MedNLI on BioBERT. [CONTINUE] MedNLI (expanded) shows better performance than MedNLI on BioBERT while MedNLI works better on BERT (see table 4). [CONTINUE] the performance of MedNLI (expanded) with transfer learning is higher on BERT and lower on BioBERT than the performance of MedNLI with transfer learning."
    },
    {
        "id": "3",
        "table": {
            "header": [
                "Model no-distill",
                "Model no-project",
                "Test 85.98",
                "[ITALIC] but 78.69",
                "[ITALIC] but or  [ITALIC] neg 80.13"
            ],
            "rows": [
                [
                    "no-distill",
                    "project",
                    "86.54",
                    "83.40",
                    "-"
                ],
                [
                    "distill",
                    "no-project",
                    "86.11",
                    "79.04",
                    "-"
                ],
                [
                    "distill",
                    "project",
                    "86.62",
                    "83.32",
                    "-"
                ],
                [
                    "ELMo",
                    "no-project",
                    "88.89",
                    "86.51",
                    "87.24"
                ],
                [
                    "ELMo",
                    "project",
                    "88.96",
                    "87.20",
                    "-"
                ]
            ],
            "title": "Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (\u201cbut\u201d), negations (\u201cneg\u201d)."
        },
        "insight": "Switching to ELMo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improvement) correspond to A-but-B and negation style sentences, [CONTINUE] As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are A-but-B style or negations."
    },
    {
        "id": "4",
        "table": {
            "header": [
                "Threshold",
                "0.50",
                "0.66",
                "0.75",
                "0.90"
            ],
            "rows": [
                [
                    "Neutral Sentiment",
                    "10",
                    "70",
                    "95",
                    "234"
                ],
                [
                    "Flipped Sentiment",
                    "15",
                    "4",
                    "2",
                    "0"
                ],
                [
                    "Fleiss\u2019 Kappa ( [ITALIC] \u03ba)",
                    "0.38",
                    "0.42",
                    "0.44",
                    "0.58"
                ],
                [
                    "no-distill, no-project",
                    "81.32",
                    "83.54",
                    "84.54",
                    "87.55"
                ],
                [
                    "ELMo, no-project",
                    "87.56",
                    "90.00",
                    "91.31",
                    "93.14"
                ]
            ],
            "title": "Table 3: Number of sentences in the crowdsourced study (447 sentences) which got marked as neutral and which got the opposite of their labels in the SST2 dataset, using various thresholds. Inter-annotator agreement is computed using Fleiss\u2019 Kappa. Average accuracies of the baseline and ELMo (over 100 seeds) on non-neutral sentences are also shown."
        },
        "insight": "We present statistics of our dataset10 in Table 3. [CONTINUE] As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). According to Landis and Koch (1977), \u03ba \u2208 (0.2, 0.4] corresponds to \"fair agreement\", whereas \u03ba \u2208 (0.4, 0.6] corresponds to \"moderate agreement\". [CONTINUE] We next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences. Higher thresholds correspond to sets of less ambiguous sentences. Table 3 shows that ELMo's performance gains in Table 2 extends across all thresholds."
    },
    {
        "id": "5",
        "table": {
            "header": [
                "[BOLD] Concept Input \u2192  [BOLD] Embeddings",
                "[BOLD] Concept Input \u2192  [BOLD] TF",
                "[BOLD] Concept Input \u2192  [BOLD] IDF",
                "[BOLD] Label  [BOLD] T",
                "[BOLD] Label  [BOLD] P",
                "[BOLD] Label  [BOLD] R",
                "[BOLD] Label  [BOLD] F",
                "[BOLD] Description  [BOLD] T",
                "[BOLD] Description  [BOLD] P",
                "[BOLD] Description  [BOLD] R",
                "[BOLD] Description  [BOLD] F",
                "[BOLD] Both  [BOLD] T",
                "[BOLD] Both  [BOLD] P",
                "[BOLD] Both  [BOLD] R",
                "[BOLD] Both  [BOLD] F"
            ],
            "rows": [
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".635",
                    ".750",
                    ".818",
                    ".783",
                    ".720",
                    ".754",
                    ".891",
                    ".817",
                    ".735",
                    ".765",
                    ".945",
                    ".846"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".640",
                    ".891",
                    ".745",
                    ".812",
                    ".700",
                    ".831",
                    ".891",
                    ".860",
                    ".690",
                    ".813",
                    ".945",
                    ".874"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".600",
                    ".738",
                    ".873",
                    ".800",
                    ".670",
                    ".746",
                    ".909",
                    ".820",
                    ".755",
                    ".865",
                    ".818",
                    ".841"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".605",
                    ".904",
                    ".855",
                    ".879",
                    ".665",
                    ".857",
                    ".873",
                    ".865",
                    ".715",
                    ".923",
                    ".873",
                    ".897"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".440",
                    ".813",
                    ".945",
                    ".874",
                    ".515",
                    ".701",
                    ".982",
                    ".818",
                    ".635",
                    ".920",
                    ".836",
                    ".876"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".445",
                    ".943",
                    ".909",
                    "[BOLD] .926",
                    ".540",
                    ".873",
                    ".873",
                    ".873",
                    ".565",
                    ".927",
                    ".927",
                    ".927"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".435",
                    ".839",
                    ".945",
                    ".889",
                    ".520",
                    ".732",
                    ".945",
                    ".825",
                    ".590",
                    ".877",
                    ".909",
                    ".893"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".430",
                    ".943",
                    ".909",
                    "[BOLD] .926",
                    ".530",
                    ".889",
                    ".873",
                    "[BOLD] .881",
                    ".545",
                    ".945",
                    ".945",
                    "[BOLD] .945"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".440",
                    ".781",
                    ".909",
                    ".840",
                    ".555",
                    ".708",
                    ".927",
                    ".803",
                    ".615",
                    ".778",
                    ".891",
                    ".831"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".435",
                    ".850",
                    ".927",
                    ".887",
                    ".520",
                    ".781",
                    ".909",
                    ".840",
                    ".530",
                    ".803",
                    ".964",
                    ".876"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".435",
                    ".850",
                    ".927",
                    ".887",
                    ".525",
                    ".722",
                    ".945",
                    ".819",
                    ".600",
                    ".820",
                    ".909",
                    ".862"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".420",
                    ".895",
                    ".927",
                    ".911",
                    ".505",
                    ".803",
                    ".891",
                    ".845",
                    ".520",
                    ".833",
                    ".909",
                    ".870"
                ]
            ],
            "title": "Table 1: Tuning Data Results AVG_COS_SIM. Top F per Concept Input Type in Bold."
        },
        "insight": "The top tuning data scores for AVG COS SIM (Table 1) show that the Google embeddings with TF*IDF weighting yield the top F score for all three concept input types (.881 - .945). Somewhat expectedly, the best overall F score (.945) is produced in the setting Both, which provides the most information. Actually, this is true for all four weighting schemes for both GloVe and Google, while fastText consistently yields its top F scores (.840 - .911) in the Label setting, which provides the least information."
    },
    {
        "id": "6",
        "table": {
            "header": [
                "[BOLD] Concept Input \u2192  [BOLD] Embeddings",
                "[BOLD] Concept Input \u2192  [BOLD] TF",
                "[BOLD] Concept Input \u2192  [BOLD] IDF",
                "[BOLD] Label  [BOLD] T/n",
                "[BOLD] Label  [BOLD] P",
                "[BOLD] Label  [BOLD] R",
                "[BOLD] Label  [BOLD] F",
                "[BOLD] Description  [BOLD] T/n",
                "[BOLD] Description  [BOLD] P",
                "[BOLD] Description  [BOLD] R",
                "[BOLD] Description  [BOLD] F",
                "[BOLD] Both  [BOLD] T/n",
                "[BOLD] Both  [BOLD] P",
                "[BOLD] Both  [BOLD] R",
                "[BOLD] Both  [BOLD] F"
            ],
            "rows": [
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".365/6",
                    ".797",
                    ".927",
                    ".857",
                    ".690/14",
                    ".915",
                    ".782",
                    ".843",
                    ".675/16",
                    ".836",
                    ".927",
                    ".879"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".300/30",
                    ".929",
                    ".236",
                    ".377",
                    ".300/30",
                    ".806",
                    ".455",
                    ".581",
                    ".300/30",
                    ".778",
                    ".636",
                    ".700"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".330/6",
                    ".879",
                    ".927",
                    ".903",
                    ".345/6",
                    ".881",
                    ".945",
                    "[BOLD] .912",
                    ".345/6",
                    ".895",
                    ".927",
                    ".911"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".345/22",
                    ".981",
                    ".927",
                    "[BOLD] .953",
                    ".480/16",
                    ".895",
                    ".927",
                    ".911",
                    ".520/16",
                    ".912",
                    ".945",
                    ".929"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".300/30",
                    "1.00",
                    ".345",
                    ".514",
                    ".300/8",
                    "1.00",
                    ".345",
                    ".514",
                    ".300/30",
                    "1.00",
                    ".600",
                    ".750"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".300/10",
                    "1.00",
                    ".509",
                    ".675",
                    ".300/14",
                    ".972",
                    ".636",
                    ".769",
                    ".350/22",
                    "1.00",
                    ".836",
                    ".911"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".415/22",
                    ".980",
                    ".873",
                    ".923",
                    ".525/14",
                    ".887",
                    ".855",
                    ".870",
                    ".535/20",
                    ".869",
                    ".964",
                    ".914"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".350/24",
                    "1.00",
                    ".309",
                    ".472",
                    ".300/30",
                    "1.00",
                    ".382",
                    ".553",
                    ".300/28",
                    "1.00",
                    ".673",
                    ".804"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".300/20",
                    "1.00",
                    ".800",
                    ".889",
                    ".300/10",
                    ".953",
                    ".745",
                    ".837",
                    ".310/14",
                    ".963",
                    ".945",
                    "[BOLD] .954"
                ]
            ],
            "title": "Table 2: Tuning Data Results TOP_n_COS_SIM_AVG. Top F per Concept Input Type in Bold."
        },
        "insight": "For TOP n COS SIM AVG, the tuning data results (Table 2) are somewhat more varied: First, there is no single best performing set of embeddings: Google yields the best F score for the Label setting (.953), while GloVe (though only barely) leads in the Description setting (.912). This time, it is fastText which produces the best F score in the Both setting, which is also the best overall tuning data F score for TOP n COS SIM AVG (.954)."
    },
    {
        "id": "7",
        "table": {
            "header": [
                "Gong et\u00a0al.  [BOLD] ( 2018  [BOLD] )",
                "topic_science",
                "topic_science",
                "topic_science",
                "topic_science",
                "topic_science",
                "[BOLD] P .758",
                "[BOLD] P \u00b1.012",
                "[BOLD] R .885",
                "[BOLD] R \u00b1.071",
                "[BOLD] F .818",
                "[BOLD] F \u00b1.028"
            ],
            "rows": [
                [
                    "Gong et\u00a0al.  [BOLD] ( 2018  [BOLD] )",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    ".750",
                    "\u00b1.009",
                    ".842",
                    "\u00b1.010",
                    ".791",
                    "\u00b1.007"
                ],
                [
                    "[BOLD] Method",
                    "[BOLD] Embeddings",
                    "[BOLD] Settings",
                    "[BOLD] Settings",
                    "[BOLD] T/n",
                    "[BOLD] Conc.\u00a0Input",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".515",
                    "Label",
                    ".939",
                    "\u00b1.043",
                    ".839",
                    "\u00b1.067",
                    ".884",
                    "\u00b1.038"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".520",
                    "Description",
                    ".870",
                    "\u00b1.068",
                    ".834",
                    "\u00b1.048",
                    ".849",
                    "\u00b1.038"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".545",
                    "Both",
                    ".915",
                    "\u00b1.040",
                    ".938",
                    "\u00b1.047",
                    "[BOLD] .926",
                    "\u00b1.038"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "Google",
                    "+TF",
                    "-IDF",
                    ".345/22",
                    "Label",
                    ".854",
                    "\u00b1.077",
                    ".861",
                    "\u00b1.044",
                    ".856",
                    "\u00b1.054"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "GloVe",
                    "+TF",
                    "+IDF",
                    ".345/6",
                    "Description",
                    ".799",
                    "\u00b1.063",
                    ".766",
                    "\u00b1.094",
                    ".780",
                    "\u00b1.068"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "fastText",
                    "+TF",
                    "+IDF",
                    ".310/14",
                    "Both",
                    ".850",
                    "\u00b1.059",
                    ".918",
                    "\u00b1.049",
                    "[BOLD] .881",
                    "\u00b1.037"
                ]
            ],
            "title": "Table 3: Test Data Results"
        },
        "insight": "The results can be found in Table 3. For comparison, the two top rows provide the best results of Gong et al. (2018). The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926). Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018)."
    },
    {
        "id": "8",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] CNN  [ITALIC] full",
                "[BOLD] CNN  [ITALIC] dict",
                "[BOLD] CNN  [ITALIC] list",
                "[BOLD] CNN  [ITALIC] k",
                "[BOLD] CNN MRR",
                "[BOLD] LSTM  [ITALIC] full",
                "[BOLD] LSTM  [ITALIC] list",
                "[BOLD] LSTM k",
                "[BOLD] LSTM MRR"
            ],
            "rows": [
                [
                    "Baseline (BL)",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9"
                ],
                [
                    "BL+ Glove (Jeffrey:14)",
                    "22.0",
                    "62.5",
                    "75.8",
                    "7",
                    "44.5",
                    "19.1",
                    "75.3",
                    "4",
                    "78.8"
                ],
                [
                    "BL+C-LSTM (Chunting:15)",
                    "21.4",
                    "61.0",
                    "71.3",
                    "8",
                    "45.6",
                    "18.9",
                    "74.7",
                    "4",
                    "80.7"
                ],
                [
                    "BL+CNN-RNN (Xingyou:16)",
                    "21.7",
                    "61.8",
                    "73.3",
                    "8",
                    "44.5",
                    "19.5",
                    "77.1",
                    "4",
                    "80.9"
                ],
                [
                    "BL+MVCNN (Wenpeng:16)",
                    "21.3",
                    "60.6",
                    "71.9",
                    "8",
                    "44.2",
                    "19.2",
                    "75.8",
                    "4",
                    "78.8"
                ],
                [
                    "BL+Attentive LSTM (Ming:16)",
                    "21.9",
                    "62.4",
                    "74.0",
                    "8",
                    "45.7",
                    "19.1",
                    "71.4",
                    "5",
                    "80.2"
                ],
                [
                    "BL+fasttext (Armand:17)",
                    "21.9",
                    "62.2",
                    "75.4",
                    "7",
                    "44.6",
                    "19.4",
                    "76.1",
                    "4",
                    "80.3"
                ],
                [
                    "BL+InferSent (Alexis:17)",
                    "22.0",
                    "62.5",
                    "75.8",
                    "7",
                    "44.5",
                    "19.4",
                    "76.7",
                    "4",
                    "79.7"
                ],
                [
                    "BL+USE-T (Daniel:18)",
                    "22.0",
                    "62.5",
                    "[BOLD] 78.3",
                    "6",
                    "44.7",
                    "19.2",
                    "75.8",
                    "4",
                    "79.5"
                ],
                [
                    "BL+TWE (Ahmed:18)",
                    "22.2",
                    "63.0",
                    "76.3",
                    "7",
                    "44.7",
                    "19.5",
                    "76.7",
                    "4",
                    "80.2"
                ],
                [
                    "BL+FDCLSTM (ours)",
                    "22.3",
                    "63.3",
                    "75.1",
                    "8",
                    "45.0",
                    "[BOLD] 20.2",
                    "67.9",
                    "9",
                    "79.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] AT (ours)",
                    "[BOLD] 22.4",
                    "63.7",
                    "75.5",
                    "8",
                    "[BOLD] 45.9",
                    "20.1",
                    "67.6",
                    "9",
                    "[BOLD] 81.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] lexicon (ours)",
                    "22.6",
                    "64.3",
                    "76.3",
                    "8",
                    "45.1",
                    "19.4",
                    "76.4",
                    "4",
                    "78.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] AT+ [ITALIC] lexicon (ours)",
                    "22.6",
                    "64.3",
                    "76.3",
                    "8",
                    "45.1",
                    "19.7",
                    "[BOLD] 77.8",
                    "4",
                    "80.4"
                ]
            ],
            "title": "Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted from the baseline output (%). In addition, to evaluate our re-ranker with MRR we fixed k CNNk=8 LSTMk=4"
        },
        "insight": "We use two pre-trained deep models: a CNN (Jaderberg et al., 2016) and an LSTM (Ghosh et al., 2017) as baselines (BL) to extract the initial list of word hypotheses. [CONTINUE] We experimented extracting kbest hypotheses for k = 1 . . . 10. [CONTINUE] Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset. 2) dict columns correspond to the accuracy over the cases where the target word is among the 90K words of the CNN dictionary (which correspond to 43.3% of the whole dataset. 3) list columns report the accuracy over the cases where the right word was among the k-best produced by the baseline. 4) MRR Mean Reciprocal Rank (MRR), [CONTINUE] We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset. [CONTINUE] Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. [CONTINUE] Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words. [CONTINUE] We also compare our result with current state-of-the-art word embeddings trained on a large general text using glove and fasttext. The word model used only object and place information, and ignored the caption. Our proposed models achieve better performance than our TWE previous model (Sabir et al., 2018), that trained a word embedding (Mikolov et al., 2013) from scratch on the same task. [CONTINUE] As seen in Table 1, the introduction of this unigram lexicon produces the best results."
    },
    {
        "id": "9",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] WMT En-De BLEU",
                "[BOLD] WMT En-De Speedup",
                "[BOLD] WMT De-En BLEU",
                "[BOLD] WMT De-En Speedup",
                "[BOLD] IWSLT En-De BLEU",
                "[BOLD] IWSLT En-De Speedup",
                "[BOLD] WMT En-Fr BLEU",
                "[BOLD] WMT En-Fr Speedup"
            ],
            "rows": [
                [
                    "Baseline ( [ITALIC] b=1)",
                    "25.82",
                    "1.15\u00d7",
                    "29.83",
                    "1.14\u00d7",
                    "28.66",
                    "1.16\u00d7",
                    "39.41",
                    "1.18\u00d7"
                ],
                [
                    "Baseline ( [ITALIC] b=4)",
                    "26.87",
                    "1.00\u00d7",
                    "30.73",
                    "1.00\u00d7",
                    "30.00",
                    "1.00\u00d7",
                    "40.22",
                    "1.00\u00d7"
                ],
                [
                    "SAT ( [ITALIC] k=2)",
                    "22.81",
                    "2.05\u00d7",
                    "26.78",
                    "2.04\u00d7",
                    "25.48",
                    "2.03\u00d7",
                    "36.62",
                    "2.14\u00d7"
                ],
                [
                    "SAT ( [ITALIC] k=4)",
                    "16.44",
                    "3.61\u00d7",
                    "21.27",
                    "3.58\u00d7",
                    "20.25",
                    "3.45\u00d7",
                    "28.07",
                    "3.34\u00d7"
                ],
                [
                    "SAT ( [ITALIC] k=6)",
                    "12.55",
                    "4.86\u00d7",
                    "15.23",
                    "4.27\u00d7",
                    "14.02",
                    "4.39\u00d7",
                    "24.63",
                    "4.77\u00d7"
                ],
                [
                    "LT*",
                    "19.8",
                    "3.89\u00d7",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "SynST( [ITALIC] k=6)",
                    "20.74",
                    "4.86\u00d7",
                    "25.50",
                    "5.06\u00d7",
                    "23.82",
                    "3.78\u00d7",
                    "33.47",
                    "5.32\u00d7"
                ]
            ],
            "title": "Table 1: Controlled experiments comparing SynST\u00a0to a baseline Transformer, SAT, and LT\u00a0on four different datasets (two language pairs) demonstrate speed and BLEU improvements. Wall-clock speedup is measured on a single Nvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set, averaged over five runs. When beam width b is not specified, we perform greedy decoding (i.e., b=1). Note that the LT results are reported by\u00a0latentTransformer and not from our own implementation; as such, they are not directly comparable to the other results."
        },
        "insight": "Table 1 contains the results on all four datasets. SynST achieves speedups of \u223c 4 \u2212 5\u00d7 that of the vanilla Transformer, which is larger than nearly all [CONTINUE] of the SAT configurations. Quality-wise, SynST again significantly outperforms the SAT configurations at comparable speedups on all datasets. On WMT En-De, SynST improves by 1 BLEU over LT (20.74 vs LT's 19.8 without reranking)."
    },
    {
        "id": "10",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Time (s)",
                "[BOLD] Acc",
                "[BOLD] # Param"
            ],
            "rows": [
                [
                    "+0 dummy node",
                    "56",
                    "81.76",
                    "7,216K"
                ],
                [
                    "+1 dummy node",
                    "65",
                    "82.64",
                    "8,768K"
                ],
                [
                    "+2 dummy node",
                    "76",
                    "82.24",
                    "10,321K"
                ],
                [
                    "Hidden size 100",
                    "42",
                    "81.75",
                    "4,891K"
                ],
                [
                    "Hidden size 200",
                    "54",
                    "82.04",
                    "6,002K"
                ],
                [
                    "Hidden size 300",
                    "65",
                    "82.64",
                    "8,768K"
                ],
                [
                    "Hidden size 600",
                    "175",
                    "81.84",
                    "17,648K"
                ],
                [
                    "Hidden size 900",
                    "235",
                    "81.66",
                    "33,942K"
                ],
                [
                    "Without \u27e8s\u27e9, \u27e8/s\u27e9",
                    "63",
                    "82.36",
                    "8,768K"
                ],
                [
                    "With \u27e8s\u27e9, \u27e8/s\u27e9",
                    "65",
                    "82.64",
                    "8,768K"
                ]
            ],
            "title": "Table 2: Movie review Dev results of S-LSTM"
        },
        "insight": "Table 2 shows the development results of various S-LSTM settings, [CONTINUE] Adding one additional sentence-level node [CONTINUE] does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly. [CONTINUE] The accuracies of S-LSTM increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300. We fix the hidden size to 300 accordingly. Without using (cid:104)s(cid:105) and (cid:104)/s(cid:105), the performance of S-LSTM drops from 82.64% to 82.36%, showing the effectiveness of having these additional nodes."
    },
    {
        "id": "11",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Time (s)",
                "[BOLD] Acc",
                "[BOLD] # Param"
            ],
            "rows": [
                [
                    "LSTM",
                    "67",
                    "80.72",
                    "5,977K"
                ],
                [
                    "BiLSTM",
                    "106",
                    "81.73",
                    "7,059K"
                ],
                [
                    "2 stacked BiLSTM",
                    "207",
                    "81.97",
                    "9,221K"
                ],
                [
                    "3 stacked BiLSTM",
                    "310",
                    "81.53",
                    "11,383K"
                ],
                [
                    "4 stacked BiLSTM",
                    "411",
                    "81.37",
                    "13,546K"
                ],
                [
                    "S-LSTM",
                    "65",
                    "82.64*",
                    "8,768K"
                ],
                [
                    "CNN",
                    "34",
                    "80.35",
                    "5,637K"
                ],
                [
                    "2 stacked CNN",
                    "40",
                    "80.97",
                    "5,717K"
                ],
                [
                    "3 stacked CNN",
                    "47",
                    "81.46",
                    "5,808K"
                ],
                [
                    "4 stacked CNN",
                    "51",
                    "81.39",
                    "5,855K"
                ],
                [
                    "Transformer (N=6)",
                    "138",
                    "81.03",
                    "7,234K"
                ],
                [
                    "Transformer (N=8)",
                    "174",
                    "81.86",
                    "7,615K"
                ],
                [
                    "Transformer (N=10)",
                    "214",
                    "81.63",
                    "8,004K"
                ],
                [
                    "BiLSTM+Attention",
                    "126",
                    "82.37",
                    "7,419K"
                ],
                [
                    "S-LSTM+Attention",
                    "87",
                    "83.07*",
                    "8,858K"
                ]
            ],
            "title": "Table 3: Movie review development results"
        },
        "insight": "As shown in Table 3, BiLSTM gives significantly better accuracies compared to uni-directional LSTM2, with the training time per epoch growing from 67 seconds to 106 seconds. Stacking 2 layers of BiLSTM gives further improvements to development results, with a larger time of 207 seconds. 3 layers of stacked BiLSTM does not further improve the results. In contrast, S-LSTM gives a development result of 82.64%, which is significantly better compared to 2-layer stacked BiLSTM, with a smaller number of model parameters and a shorter time of 65 seconds. [CONTINUE] We additionally make comparisons with stacked CNNs and hierarchical attention (Vaswani et al., 2017), shown in Table 3 (the CNN and Transformer rows), [CONTINUE] CNN is the most efficient among all models compared, with the smallest model size. On the other hand, a 3-layer stacked CNN gives an accuracy of 81.46%, which is also [CONTINUE] the lowest compared with BiLSTM, hierarchical attention and S-LSTM. The best performance of hierarchical attention is between single-layer and two-layer BiLSTMs in terms of both accuracy and efficiency. S-LSTM gives significantly better accuracies compared with both CNN and hierarchical attention. [CONTINUE] Table 3 additionally shows the results of BiLSTM and S-LSTM when external attention is used [CONTINUE] Attention leads to improved accuracies for both BiLSTM and S-LSTM in classification, with S-LSTM still outperforming BiLSTM significantly."
    },
    {
        "id": "12",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Accuracy",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "rows": [
                [
                    "socher2011semi",
                    "77.70",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "socher2012semantic",
                    "79.00",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "kim2014convolutional",
                    "81.50",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "qian2016linguistically",
                    "81.50",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "BiLSTM",
                    "81.61",
                    "51",
                    "1.62"
                ],
                [
                    "2 stacked BiLSTM",
                    "81.94",
                    "98",
                    "3.18"
                ],
                [
                    "3 stacked BiLSTM",
                    "81.71",
                    "137",
                    "4.67"
                ],
                [
                    "3 stacked CNN",
                    "81.59",
                    "31",
                    "1.04"
                ],
                [
                    "Transformer (N=8)",
                    "81.97",
                    "89",
                    "2.75"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 82.45*",
                    "41",
                    "1.53"
                ]
            ],
            "title": "Table 4: Test set results on movie review dataset (* denotes significance in all tables)."
        },
        "insight": "The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively. [CONTINUE] As shown in Table 4, [CONTINUE] S-LSTM outperforms BiLSTM significantly, with a faster speed. [CONTINUE] S-LSTM also gives highly competitive results when compared with existing methods in the literature."
    },
    {
        "id": "13",
        "table": {
            "header": [
                "[BOLD] Dataset Camera",
                "[BOLD] SLSTM  [BOLD] 90.02*",
                "[BOLD] Time (s) 50 (2.85)",
                "[BOLD] BiLSTM 87.05",
                "[BOLD] Time (s) 115 (8.37)",
                "[BOLD] 2 BiLSTM 88.07",
                "[BOLD] Time (s) 221 (16.1)"
            ],
            "rows": [
                [
                    "Video",
                    "[BOLD] 86.75*",
                    "55 (3.95)",
                    "84.73",
                    "140 (12.59)",
                    "85.23",
                    "268 (25.86)"
                ],
                [
                    "Health",
                    "[BOLD] 86.5",
                    "37 (2.17)",
                    "85.52",
                    "118 (6.38)",
                    "85.89",
                    "227 (11.16)"
                ],
                [
                    "Music",
                    "[BOLD] 82.04*",
                    "52 (3.44)",
                    "78.74",
                    "185 (12.27)",
                    "80.45",
                    "268 (23.46)"
                ],
                [
                    "Kitchen",
                    "[BOLD] 84.54*",
                    "40 (2.50)",
                    "82.22",
                    "118 (10.18)",
                    "83.77",
                    "225 (19.77)"
                ],
                [
                    "DVD",
                    "[BOLD] 85.52*",
                    "63 (5.29)",
                    "83.71",
                    "166 (15.42)",
                    "84.77",
                    "217 (28.31)"
                ],
                [
                    "Toys",
                    "85.25",
                    "39 (2.42)",
                    "85.72",
                    "119 (7.58)",
                    "[BOLD] 85.82",
                    "231 (14.83)"
                ],
                [
                    "Baby",
                    "[BOLD] 86.25*",
                    "40 (2.63)",
                    "84.51",
                    "125 (8.50)",
                    "85.45",
                    "238 (17.73)"
                ],
                [
                    "Books",
                    "[BOLD] 83.44*",
                    "64 (3.64)",
                    "82.12",
                    "240 (13.59)",
                    "82.77",
                    "458 (28.82)"
                ],
                [
                    "IMDB",
                    "[BOLD] 87.15*",
                    "67 (3.69)",
                    "86.02",
                    "248 (13.33)",
                    "86.55",
                    "486 (26.22)"
                ],
                [
                    "MR",
                    "[BOLD] 76.2",
                    "27 (1.25)",
                    "75.73",
                    "39 (2.27)",
                    "75.98",
                    "72 (4.63)"
                ],
                [
                    "Appeal",
                    "85.75",
                    "35 (2.83)",
                    "86.05",
                    "119 (11.98)",
                    "[BOLD] 86.35*",
                    "229 (22.76)"
                ],
                [
                    "Magazines",
                    "[BOLD] 93.75*",
                    "51 (2.93)",
                    "92.52",
                    "214 (11.06)",
                    "92.89",
                    "417 (22.77)"
                ],
                [
                    "Electronics",
                    "[BOLD] 83.25*",
                    "47 (2.55)",
                    "82.51",
                    "195 (10.14)",
                    "82.33",
                    "356 (19.77)"
                ],
                [
                    "Sports",
                    "[BOLD] 85.75*",
                    "44 (2.64)",
                    "84.04",
                    "172 (8.64)",
                    "84.78",
                    "328 (16.34)"
                ],
                [
                    "Software",
                    "[BOLD] 87.75*",
                    "54 (2.98)",
                    "86.73",
                    "245 (12.38)",
                    "86.97",
                    "459 (24.68)"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] 85.38*",
                    "47.30 (2.98)",
                    "84.01",
                    "153.48 (10.29)",
                    "84.64",
                    "282.24 (20.2)"
                ]
            ],
            "title": "Table 5: Results on the 16 datasets of Liu et al. (2017). Time format: train (test)"
        },
        "insight": "The final results on the movie review and rich text classification datasets are shown in Tables 4 and 5, respectively. [CONTINUE] As shown in Table 5, among the 16 datasets [CONTINUE] S-LSTM gives the best results on 12, compared with BiLSTM and 2 layered BiLSTM models. The average accuracy of S-LSTM is 85.6%, significantly higher compared with 84.9% by 2-layer stacked BiLSTM. 3-layer stacked BiLSTM gives an average accuracy of 84.57%, which is lower compared to a 2-layer stacked BiLSTM, with a training time per epoch of 423.6 seconds."
    },
    {
        "id": "14",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Accuracy",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "rows": [
                [
                    "manning2011part",
                    "97.28",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "collobert2011natural",
                    "97.29",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "sun2014structure",
                    "97.36",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "sogaard2011semisupervised",
                    "97.50",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "huang2015bidirectional",
                    "[BOLD] 97.55",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "ma2016end",
                    "[BOLD] 97.55",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "yang2017transfer",
                    "[BOLD] 97.55",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "BiLSTM",
                    "97.35",
                    "254",
                    "22.50"
                ],
                [
                    "2 stacked BiLSTM",
                    "97.41",
                    "501",
                    "43.99"
                ],
                [
                    "3 stacked BiLSTM",
                    "97.40",
                    "746",
                    "64.96"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 97.55",
                    "237",
                    "22.16"
                ]
            ],
            "title": "Table 6: Results on PTB (POS tagging)"
        },
        "insight": "As can be seen in Table 6, S-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset. It also gives competitive accuracies as compared with existing methods in the literature. Stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve [CONTINUE] with three layers of stacked LSTMs."
    },
    {
        "id": "15",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] F1",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "rows": [
                [
                    "collobert2011natural",
                    "89.59",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "passos2014lexicon",
                    "90.90",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "luo2015joint",
                    "91.20",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "huang2015bidirectional",
                    "90.10",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "lample2016neural",
                    "90.94",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "ma2016end",
                    "91.21",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "yang2017transfer",
                    "91.26",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "rei:2017:Long",
                    "86.26",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "peters2017semi",
                    "[BOLD] 91.93",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "BiLSTM",
                    "90.96",
                    "82",
                    "9.89"
                ],
                [
                    "2 stacked BiLSTM",
                    "91.02",
                    "159",
                    "18.88"
                ],
                [
                    "3 stacked BiLSTM",
                    "91.06",
                    "235",
                    "30.97"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 91.57*",
                    "79",
                    "9.78"
                ]
            ],
            "title": "Table 7: Results on CoNLL03 (NER)"
        },
        "insight": "For NER (Table 7), S-LSTM gives an F1-score of 91.57% on the CoNLL test set, which is significantly better compared with BiLSTMs. Stacking more layers of BiLSTMs leads to slightly better F1-scores compared with a single-layer BiLSTM. Our BiLSTM results are comparable to the results reported by Ma and Hovy (2016) and Lample et al. (2016), [CONTINUE] In contrast, S-LSTM gives the best reported results under the same settings. In the second section of Table 7, Yang et al. (2017) obtain an Fscore of 91.26%"
    },
    {
        "id": "16",
        "table": {
            "header": [
                "system",
                "BLEU",
                "ROUGE-L"
            ],
            "rows": [
                [
                    "challenge",
                    "challenge",
                    "challenge"
                ],
                [
                    "baseline",
                    "65.9",
                    "68.5"
                ],
                [
                    "Thomson Reuters (np 3)",
                    "[BOLD] 68.1",
                    "69.3"
                ],
                [
                    "Thomson Reuters (np 4)",
                    "67.4",
                    "69.8"
                ],
                [
                    "HarvardNLP & H. Elder",
                    "67.4",
                    "[BOLD] 70.8"
                ],
                [
                    "own",
                    "own",
                    "own"
                ],
                [
                    "word",
                    "67.8\u00b10.8",
                    "70.4\u00b10.6"
                ],
                [
                    "character",
                    "64.6\u00b16.0",
                    "67.9\u00b14.7"
                ],
                [
                    "word (best on dev.)",
                    "67.8",
                    "70.2"
                ],
                [
                    "char. (best on dev.)",
                    "67.6",
                    "70.4"
                ]
            ],
            "title": "Table 2: E2E\u00a0test set results. Own results correspond to avg\u00b1SD of ten runs and single result of best models on the development set."
        },
        "insight": "Table 2 and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models [CONTINUE] On the E2E test set, our single best word- and character-based models reach comparable results to the best challenge submissions. The word-based models achieve significantly higher BLEU and ROUGE-L scores than the character-based models."
    },
    {
        "id": "17",
        "table": {
            "header": [
                "system",
                "BLEU",
                "ROUGE-L"
            ],
            "rows": [
                [
                    "challenge",
                    "challenge",
                    "challenge"
                ],
                [
                    "baseline",
                    "32.1",
                    "43.3"
                ],
                [
                    "Melbourne",
                    "43.4",
                    "[BOLD] 61.0"
                ],
                [
                    "Tilburg-SMT",
                    "43.1",
                    "58.0"
                ],
                [
                    "UPF-FORGe",
                    "37.5",
                    "58.8"
                ],
                [
                    "own",
                    "own",
                    "own"
                ],
                [
                    "word (best on dev.)",
                    "[BOLD] 44.2",
                    "60.9"
                ],
                [
                    "char. (best on dev.)",
                    "41.3",
                    "58.4"
                ],
                [
                    "word",
                    "37.0\u00b13.8",
                    "56.3\u00b12.6"
                ],
                [
                    "character",
                    "39.7\u00b11.7",
                    "58.4\u00b10.7"
                ]
            ],
            "title": "Table 3: WebNLG\u00a0test set results. Own results correspond to single best model on development set and avg\u00b1SD of ten runs."
        },
        "insight": "On the WebNLG test set, the BLEU score of our best word-based model outperforms the best challenge submission by a small margin. The character-based model achieves a significantly higher ROUGE-L score than the wordbased model, whereas the BLEU score difference is not significant."
    },
    {
        "id": "18",
        "table": {
            "header": [
                "# Layers",
                "Max Chunk Size",
                "Speedup",
                "BLEU"
            ],
            "rows": [
                [
                    "1",
                    "[ITALIC] k=6",
                    "3.8\u00d7",
                    "23.82"
                ],
                [
                    "2",
                    "[ITALIC] k=6",
                    "2.8\u00d7",
                    "23.98"
                ],
                [
                    "3",
                    "[ITALIC] k=6",
                    "2.2\u00d7",
                    "24.54"
                ],
                [
                    "4",
                    "[ITALIC] k=6",
                    "1.8\u00d7",
                    "24.04"
                ],
                [
                    "5",
                    "[ITALIC] k=6",
                    "1.4\u00d7",
                    "24.34"
                ],
                [
                    "1",
                    "[ITALIC] k\u2208{1\u20266}",
                    "3.1\u00d7",
                    "25.31"
                ]
            ],
            "title": "Table 4: Increasing the number of layers in SynST\u2019s parse decoder significantly lowers the speedup while marginally impacting BLEU. Randomly sampling k from {1\u20266} during training boosts BLEU significantly with minimal impact on speedup."
        },
        "insight": "Table 4 shows that increasing the number of layers from 1 to 5 results in a BLEU increase of only 0.5, while the speedup drops from 3.8\u00d7 to 1.4\u00d7. [CONTINUE] The final row of Table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving BLEU by 1.5 while decreasing the speedup from 3.8\u00d7 to 3.1\u00d7;"
    },
    {
        "id": "19",
        "table": {
            "header": [
                "metric",
                "human",
                "word",
                "char."
            ],
            "rows": [
                [
                    "E2E",
                    "E2E",
                    "E2E",
                    "E2E"
                ],
                [
                    "BLEU",
                    "55.5\u00b10.7",
                    "68.2\u00b11.4",
                    "65.8\u00b12.6"
                ],
                [
                    "ROUGE-L",
                    "62.0\u00b10.4",
                    "72.1\u00b10.7",
                    "69.8\u00b12.6"
                ],
                [
                    "WebNLG",
                    "WebNLG",
                    "WebNLG",
                    "WebNLG"
                ],
                [
                    "BLEU",
                    "48.3\u00b10.7",
                    "40.6\u00b14.2",
                    "43.7\u00b12.4"
                ],
                [
                    "ROUGE-L",
                    "62.4\u00b10.3",
                    "58.5\u00b13.0",
                    "63.1\u00b10.8"
                ]
            ],
            "title": "Table 4: E2E and WebNLG development set results in the format avg\u00b1SD. Human results are averaged over using each human reference as prediction once."
        },
        "insight": "Table 4 shows the BLEU and ROUGE-L development set scores when treating each human reference as prediction once and evaluating it against the remaining references, compared to the scores of the word-based and [CONTINUE] character-based models [CONTINUE] Strikingly, on the E2E development set, both model variants significantly outperform human texts by far with respect to both automatic evaluation measures. While the human BLEU score is significantly higher than those of both systems on the WebNLG development set, there is no statistical difference between human and system ROUGE-L scores."
    },
    {
        "id": "20",
        "table": {
            "header": [
                "[EMPTY]",
                "E2E word",
                "E2E char.",
                "WebNLG word",
                "WebNLG char."
            ],
            "rows": [
                [
                    "content errors",
                    "content errors",
                    "content errors",
                    "content errors",
                    "content errors"
                ],
                [
                    "info. dropped",
                    "40.0",
                    "30.0",
                    "42.9",
                    "66.7"
                ],
                [
                    "info. added",
                    "0.0",
                    "0.0",
                    "6.7",
                    "1.9"
                ],
                [
                    "info. modified",
                    "4.4",
                    "0.0",
                    "19.0",
                    "1.9"
                ],
                [
                    "info. repeated",
                    "0.0",
                    "0.0",
                    "15.2",
                    "28.6"
                ],
                [
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors"
                ],
                [
                    "punctuation errors",
                    "5.6",
                    "5.6",
                    "8.6",
                    "3.8"
                ],
                [
                    "grammatical errors",
                    "13.3",
                    "14.4",
                    "15.2",
                    "12.4"
                ],
                [
                    "spelling mistakes",
                    "0.0",
                    "0.0",
                    "9.5",
                    "5.7"
                ],
                [
                    "overall correctness",
                    "overall correctness",
                    "overall correctness",
                    "overall correctness",
                    "overall correctness"
                ],
                [
                    "content correct",
                    "55.6",
                    "70.0",
                    "46.7",
                    "31.4"
                ],
                [
                    "language correct",
                    "83.3",
                    "81.1",
                    "69.5",
                    "79.0"
                ],
                [
                    "all correct",
                    "48.9",
                    "61.1",
                    "33.3",
                    "26.7"
                ]
            ],
            "title": "Table 5: Percentage of affected instances in manual error analysis of 15\u00a0randomly selected development set instances for each input length."
        },
        "insight": "One annotator (one of the authors of this paper) manually assessed the outputs of the models that obtained the best development set BLEU score as summarized in Table 56. As we can see from the bottom part of the table, all models struggle more with getting the content right than with producing linguistically correct texts; 70-80% of the texts generated by all models are completely correct linguistically. [CONTINUE] Comparing the two datasets, we again observe that the WebNLG dataset is much more challenging than the E2E dataset, especially with respect to correctly verbalizing the content. [CONTINUE] Moreover, spelling mistakes only appeared in WebNLG texts, mainly concerning omissions of accents or umlauts. [CONTINUE] The most frequent content error in both datasets concerns omission of information. [CONTINUE] Information addition and repetition only occur in the WebNLG dataset. The latter is an especially frequent problem of the character-based model, affecting more than a quarter of all texts. [CONTINUE] In comparison, character-based models reproduce the content more faithfully on the E2E dataset while offering the same level of linguistic quality as word-based models, leading to more correct outputs overall. On the WebNLG dataset, the word-based model is more faithful to the inputs, probably because of the effective delexicalization strategy, whereas the character-based model errs less on the linguistic side. Overall, the word-based model yields more correct texts, stressing the importance of delexicalization and data normalization in low resource settings."
    },
    {
        "id": "21",
        "table": {
            "header": [
                "[EMPTY]",
                "E2E human",
                "E2E word",
                "E2E character",
                "WebNLG human",
                "WebNLG word",
                "WebNLG character"
            ],
            "rows": [
                [
                    "unique sents.",
                    "866.3\u00b116.5",
                    "203.5\u00b130.6",
                    "366.8\u00b160.0",
                    "1,185.0\u00b112.6",
                    "603.7\u00b1144.3",
                    "875.4\u00b130.2"
                ],
                [
                    "unique words",
                    "419.7\u00b116.7",
                    "64.4\u00b12.3",
                    "73.1\u00b17.2",
                    "1447.3\u00b17.4",
                    "620.3\u00b135.5",
                    "881.5\u00b126.0"
                ],
                [
                    "word E",
                    "6.5\u00b10.0",
                    "5.1\u00b10.0",
                    "5.5\u00b10.0",
                    "7.1\u00b10.0",
                    "6.3\u00b10.0",
                    "6.6\u00b10.0"
                ],
                [
                    "1-3-grams E",
                    "10.4\u00b10.0",
                    "7.7\u00b10.1",
                    "8.2\u00b10.1",
                    "11.6\u00b10.0",
                    "10.1\u00b10.1",
                    "10.5\u00b10.1"
                ],
                [
                    "% new texts",
                    "99.7\u00b10.2",
                    "98.2\u00b10.3",
                    "98.8\u00b10.2",
                    "91.1\u00b10.3",
                    "69.8\u00b14.8",
                    "87.5\u00b10.6"
                ],
                [
                    "% new sents.",
                    "85.1\u00b11.1",
                    "61.8\u00b16.4",
                    "71.4\u00b14.7",
                    "87.4\u00b10.4",
                    "57.2\u00b15.8",
                    "82.1\u00b11.2"
                ]
            ],
            "title": "Table 6: Linguistic diversity of development set references and generated texts as avg\u00b1SD. \u2018%\u00a0new\u2019\u00a0denotes the share of generated texts or sentences that do not appear in training references. Higher indicates more diversity for all measures."
        },
        "insight": "Table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the (generated) texts with the training set. [CONTINUE] On both datasets, our systems produce significantly less varied outputs and reproduce more [CONTINUE] texts and sentences from the training data than the human texts. Interestingly, however, the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word-based models, which copy about 40% of their generated sentences from the training data."
    },
    {
        "id": "22",
        "table": {
            "header": [
                "[EMPTY]",
                "c@1",
                "c@2",
                "c@5",
                "c@30"
            ],
            "rows": [
                [
                    "template\u00a01",
                    "0.8",
                    "0.8",
                    "0.9",
                    "1.7"
                ],
                [
                    "template\u00a02",
                    "1.0",
                    "1.2",
                    "1.3",
                    "1.9"
                ],
                [
                    "template\u00a01+2",
                    "0.9",
                    "1.6",
                    "2.2",
                    "3.3"
                ],
                [
                    "+ reranker",
                    "0.9",
                    "1.9",
                    "2.7",
                    "3.3"
                ]
            ],
            "title": "Table 7: Manual evaluation of generated texts for 10\u00a0random test instances of a word-based model trained with synthetic training data from two templates. c@n: avg. number of correct texts (with respect to content and language) among the top\u00a0n hypotheses."
        },
        "insight": "Table 7 shows our manual evaluation of the top 30 hypotheses for 10 random E2E test inputs generated by models trained with data synthesized from the two templates. As is evident from the first two rows, all models learned to generalize from the training data to produce correct texts for novel inputs consisting of unseen combinations of input attributes. [CONTINUE] Yet, the picture is a bit different for the model trained on data generated by both templates. While the top two hypotheses are equally distributed between adhering to Template 1 and Template 2, more than 5% among the lower-ranked hypotheses constitute a template combination such as the example shown in the bottom part of Figure 2. [CONTINUE] As can be seen in the final row of Table 7, this simple reranker successfully places correct hypotheses higher up in the ranking, improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average."
    },
    {
        "id": "23",
        "table": {
            "header": [
                "Model",
                "R1",
                "R2",
                "RL"
            ],
            "rows": [
                [
                    "Lead-75C",
                    "23.69",
                    "7.93",
                    "21.5"
                ],
                [
                    "Lead-8",
                    "21.30",
                    "7.34",
                    "19.94"
                ],
                [
                    "Schumann ( 2018 )",
                    "22.19",
                    "4.56",
                    "19.88"
                ],
                [
                    "Wang and Lee ( 2018 )",
                    "27.09",
                    "9.86",
                    "24.97"
                ],
                [
                    "Contextual Match",
                    "26.48",
                    "10.05",
                    "24.41"
                ],
                [
                    "Cao et\u00a0al. ( 2018 )",
                    "37.04",
                    "19.03",
                    "34.46"
                ],
                [
                    "seq2seq",
                    "33.50",
                    "15.85",
                    "31.44"
                ],
                [
                    "Contextual Oracle",
                    "37.03",
                    "15.46",
                    "33.23"
                ]
            ],
            "title": "Table 1: Experimental results of abstractive summarization on Gigaword test set with ROUGE metric. The top section is prefix baselines, the second section is recent unsupervised methods and ours, the third section is state-of-the-art supervised method along with our implementation of a seq-to-seq model with attention, and the bottom section is our model\u2019s oracle performance. Wang and Lee (2018) is by author correspondence (scores differ because of evaluation setup). For another unsupervised work Fevry and Phang (2018), we attempted to replicate on our test set, but were unable to obtain results better than the baselines."
        },
        "insight": "The automatic evaluation scores are presented in Table 1 and Table 2. [CONTINUE] Our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary. Our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training. [CONTINUE] In Table 1, we also list scores of the stateof-the-art supervised model, an attention based [CONTINUE] seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search. [CONTINUE] The oracle scores are much higher, indicating that our unsupervised method does allow summaries of better quality,"
    },
    {
        "id": "24",
        "table": {
            "header": [
                "Model",
                "F1",
                "CR"
            ],
            "rows": [
                [
                    "F&A Unsupervised",
                    "52.3",
                    "-"
                ],
                [
                    "Contextual Match",
                    "60.90",
                    "0.38"
                ],
                [
                    "Filippova et\u00a0al. ( 2015 )",
                    "82.0",
                    "0.38"
                ],
                [
                    "Zhao et\u00a0al. ( 2018 )",
                    "85.1",
                    "0.39"
                ],
                [
                    "Contextual Oracle",
                    "82.1",
                    "0.39"
                ]
            ],
            "title": "Table 2: Experimental results of extractive summarization on Google data set. F1 is the token overlapping score, and CR is the compression rate. F&A is an unsupervised baseline used in Filippova and Altun (2013), and the middle section is supervised results."
        },
        "insight": "For extractive sentence summarization, our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level F1 score."
    },
    {
        "id": "25",
        "table": {
            "header": [
                "Models",
                "abstractive R1",
                "abstractive R2",
                "abstractive RL",
                "extractive F1",
                "extractive CR"
            ],
            "rows": [
                [
                    "CS + cat",
                    "26.48",
                    "10.05",
                    "24.41",
                    "60.90",
                    "0.38"
                ],
                [
                    "CS + avg",
                    "26.34",
                    "9.79",
                    "24.23",
                    "60.09",
                    "0.38"
                ],
                [
                    "CS + top",
                    "26.21",
                    "9.69",
                    "24.14",
                    "62.18",
                    "0.34"
                ],
                [
                    "CS + mid",
                    "25.46",
                    "9.39",
                    "23.34",
                    "59.32",
                    "0.40"
                ],
                [
                    "CS + bot",
                    "15.29",
                    "3.95",
                    "14.06",
                    "21.14",
                    "0.23"
                ],
                [
                    "TEMP5 + cat",
                    "26.31",
                    "9.38",
                    "23.60",
                    "52.10",
                    "0.43"
                ],
                [
                    "TEMP10 + cat",
                    "25.63",
                    "8.82",
                    "22.86",
                    "42.33",
                    "0.47"
                ],
                [
                    "NA + cat",
                    "24.81",
                    "8.89",
                    "22.87",
                    "49.80",
                    "0.32"
                ]
            ],
            "title": "Table 3: Comparison of different model choices. The top section evaluates the effects of contextual representation in the matching model, and the bottom section evaluates the effects of different smoothing methods in the fluency model."
        },
        "insight": "Table 3 considers analysis of different aspects of the model. [CONTINUE] First, we look at the [CONTINUE] fluency model and compare the cluster smoothing (CS) approach with softmax temperature (TEMPx with x being the temperature) commonly used for generation in LM-integrated models (Chorowski and Jaitly, 2016) as well as no adjustment (NA). Second, we vary the 3-layer representation out of ELMo forward language model to do contextual matching (bot/mid/top: bottom/middle/top layer only, avg: average of 3 layers, cat: concatenation of all layers). [CONTINUE] Results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm, although temperature smoothing is an option for abstractive datasets. [CONTINUE] When using word embeddings (bottom layer only from ELMo language model) in our contextual matching model pcm, the summarization performance drops significantly to below simple baselines as demonstrated by score decrease."
    },
    {
        "id": "26",
        "table": {
            "header": [
                "[BOLD] Sub-task",
                "[BOLD] 1 Ext.",
                "[BOLD] 1 Class.",
                "[BOLD] 2 Ext.",
                "[BOLD] 2 Class.",
                "[ITALIC]  [BOLD] mv Ext.",
                "[ITALIC]  [BOLD] mv Class."
            ],
            "rows": [
                [
                    "1.1",
                    "-",
                    "72.1",
                    "-",
                    "74.7",
                    "-",
                    "[BOLD] 76.7"
                ],
                [
                    "1.2",
                    "-",
                    "[BOLD] 83.2",
                    "-",
                    "82.9",
                    "-",
                    "80.1"
                ],
                [
                    "2",
                    "[BOLD] 37.4",
                    "[BOLD] 33.6",
                    "36.5",
                    "28.8",
                    "35.6",
                    "28.3"
                ]
            ],
            "title": "Table 3: Official evaluation results of the submitted runs on the test set."
        },
        "insight": "We select the 1st and 2nd best performing models on the development datasets as well as the majority vote (mv) of 5 models for the final submission. The final results are shown in Table 3."
    },
    {
        "id": "27",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD]  Predicted parse vs. Gold parse (separate) ",
                "[BOLD]  Predicted parse vs. Gold parse (joint) ",
                "[BOLD]  Parsed prediction vs. Gold parse ",
                "[BOLD]  Parsed prediction vs. Predicted parse "
            ],
            "rows": [
                [
                    "F1",
                    "65.48",
                    "69.64",
                    "79.16",
                    "89.90"
                ],
                [
                    "Exact match",
                    "4.23%",
                    "5.24%",
                    "5.94%",
                    "43.10%"
                ]
            ],
            "title": "Table 3: F1 and exact match comparisons of predicted chunk sequences (from the parse decoder), ground-truth chunk sequences (from an external parser in the target language), and chunk sequences obtained after parsing the translation produced by the token decoder. First two columns show the improvement obtained by jointly training the two decoders. The third column shows that when the token decoder deviates from the predicted chunk sequence, it usually results in a translation that is closer to the ground-truth target syntax, while the fourth column shows that the token decoder closely follows the predicted chunk sequence."
        },
        "insight": "We evaluate two configurations of the parse decoder, one in which it is trained separately from the token decoder (first column of Table 3), and the other where both decoders are trained jointly (second column of Ta [CONTINUE] ble 3). We observe that joint training boosts the chunk F1 from 65.4 to 69.6, although, in both cases the F1 scores are relatively low, which matches our intuition as most source sentences can be translated into multiple target syntactic forms. [CONTINUE] To measure how often the token decoder follows the predicted chunk sequence, we parse the generated translation and compute the F1 between the resulting chunk sequence and the parse decoder's prediction (fourth column of Table 3). Strong results of 89.9 F1 and 43.1% exact match indicate that the token decoder is heavily reliant on the generated chunk sequences. [CONTINUE] The resulting F1 is indeed almost 10 points higher (third column of Table 3), indicating that the token decoder does have the ability to correct mistakes. [CONTINUE] In Section 5.3 (see the final row of Table 3) we consider the effect of randomly sampling the max chunk size k during training. This provides a considerable boost to BLEU with a minimal impact to speedup."
    },
    {
        "id": "28",
        "table": {
            "header": [
                "[BOLD] Decoder",
                "[BOLD] SICK-R",
                "[BOLD] SICK-E",
                "[BOLD] STS14",
                "[BOLD] MSRP (Acc/F1)",
                "[BOLD] SST",
                "[BOLD] TREC"
            ],
            "rows": [
                [
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder"
                ],
                [
                    "Teacher-Forcing",
                    "0.8530",
                    "82.6",
                    "0.51/0.50",
                    "74.1 / 81.7",
                    "82.5",
                    "88.2"
                ],
                [
                    "Always Sampling",
                    "0.8576",
                    "83.2",
                    "0.55/0.53",
                    "74.7 / 81.3",
                    "80.6",
                    "87.0"
                ],
                [
                    "Uniform Sampling",
                    "0.8559",
                    "82.9",
                    "0.54/0.53",
                    "74.0 / 81.8",
                    "81.0",
                    "87.4"
                ],
                [
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder"
                ],
                [
                    "Teacher-Forcing",
                    "0.8510",
                    "82.8",
                    "0.49/0.48",
                    "74.7 / 82.8",
                    "81.4",
                    "82.6"
                ],
                [
                    "Always Sampling",
                    "0.8535",
                    "83.3",
                    "0.53/0.52",
                    "75.0 / 81.7",
                    "81.4",
                    "87.6"
                ],
                [
                    "Uniform Sampling",
                    "0.8568",
                    "83.4",
                    "0.56/0.54",
                    "74.7 / 81.4",
                    "83.0",
                    "88.4"
                ],
                [
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder"
                ],
                [
                    "RNN",
                    "0.8508",
                    "82.8",
                    "0.58/0.55",
                    "74.2 / 82.8",
                    "81.6",
                    "88.8"
                ],
                [
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder"
                ],
                [
                    "CNN",
                    "0.8530",
                    "82.6",
                    "[BOLD] 0.58/ [BOLD] 0.56",
                    "[BOLD] 75.6 / 82.9",
                    "82.8",
                    "89.2"
                ],
                [
                    "CNN-MaxOnly",
                    "0.8465",
                    "82.6",
                    "0.50/0.47",
                    "73.3 / 81.5",
                    "79.1",
                    "82.2"
                ],
                [
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder"
                ],
                [
                    "CNN",
                    "[BOLD] 0.8631",
                    "[BOLD] 83.9",
                    "[BOLD] 0.58/0.55",
                    "74.7 /  [BOLD] 83.1",
                    "[BOLD] 83.4",
                    "[BOLD] 90.2"
                ],
                [
                    "CNN-MaxOnly",
                    "0.8485",
                    "83.2",
                    "0.47/0.44",
                    "72.9 / 80.8",
                    "82.2",
                    "86.6"
                ]
            ],
            "title": "Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in each direction). The default way of producing the representation is a concatenation of outputs from a global mean-pooling and a global max-pooling, while \u201c\u22c5-MaxOnly\u201d refers to the model with only global max-pooling. Bold numbers are the best results among all presented models. We found that 1) inputting correct words to an autoregressive decoder is not necessary; 2) predict-all-words decoders work roughly the same as autoregressive decoders; 3) mean+max pooling provides stronger transferability than the max-pooling alone does. The table supports our choice of the predict-all-words CNN decoder and the way of producing vector representations from the bi-directional RNN encoder."
        },
        "insight": "The results are presented in Table 1 (top two subparts). As we can see, the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks, with RNN or CNN as the decoder. [CONTINUE] The results are also presented in Table 1 (3rd and 4th subparts). The performance of the predict-allwords RNN decoder does not significantly differ from that of any one of the autoregressive RNN de [CONTINUE] coders, and the same situation can be also observed in CNN decoders. [CONTINUE] In our proposed RNN-CNN model, we empirically show that the mean+max pooling provides stronger transferability than the max pooling alone does, and the results are presented in the last two sections of Table 1."
    },
    {
        "id": "29",
        "table": {
            "header": [
                "[BOLD] Encoder type",
                "[BOLD] Encoder dim",
                "[BOLD] Decoder type",
                "[BOLD] Decoder dim",
                "[BOLD] Hrs",
                "[BOLD] SICK-R",
                "[BOLD] SICK-E",
                "[BOLD] STS14",
                "[BOLD] MSRP (Acc/F1)",
                "[BOLD] SST",
                "[BOLD] TREC"
            ],
            "rows": [
                [
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "600-1200-300",
                    "20",
                    "0.8530",
                    "82.6",
                    "0.58/0.56",
                    "[BOLD] 75.6/ [BOLD] 82.9",
                    "82.8",
                    "[BOLD] 89.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN\u2020",
                    "600-1200-300",
                    "21",
                    "0.8515",
                    "82.7",
                    "0.58/0.56",
                    "75.3/82.5",
                    "[BOLD] 82.9",
                    "85.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN(10)",
                    "600-1200-300",
                    "11",
                    "0.8474",
                    "82.9",
                    "0.57/0.55",
                    "74.2/81.6",
                    "82.8",
                    "88.0"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN(50)",
                    "600-1200-300",
                    "27",
                    "0.8533",
                    "82.5",
                    "0.57/0.55",
                    "74.7/82.2",
                    "81.5",
                    "86.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "RNN",
                    "600",
                    "26",
                    "0.8530",
                    "82.6",
                    "0.51/0.50",
                    "74.1/81.7",
                    "81.0",
                    "89.0"
                ],
                [
                    "CNN",
                    "4x300 \\lx @ [ITALIC] sectionsign",
                    "CNN",
                    "600-1200-300",
                    "8",
                    "0.8117",
                    "80.5",
                    "0.44/0.42",
                    "72.7/80.7",
                    "78.4",
                    "85.0"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "600-1200-2400-300",
                    "28",
                    "[BOLD] 0.8570",
                    "[BOLD] 84.0",
                    "0.58/0.56",
                    "74.3/81.5",
                    "82.8",
                    "88.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "1200-2400-300",
                    "27",
                    "0.8541",
                    "83.0",
                    "[BOLD] 0.59/ [BOLD] 0.57",
                    "74.3/82.2",
                    "[BOLD] 82.9",
                    "89.0"
                ],
                [
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400"
                ],
                [
                    "RNN",
                    "2x600",
                    "CNN",
                    "600-1200-300",
                    "25",
                    "0.8631",
                    "83.9",
                    "[BOLD] 0.58/ [BOLD] 0.55",
                    "[BOLD] 74.7/ [BOLD] 83.1",
                    "83.4",
                    "[BOLD] 90.2"
                ],
                [
                    "RNN",
                    "2x600",
                    "RNN",
                    "600",
                    "32",
                    "[BOLD] 0.8647",
                    "[BOLD] 84.2",
                    "0.52/0.51",
                    "74.0/81.2",
                    "[BOLD] 84.2",
                    "87.6"
                ],
                [
                    "CNN",
                    "3x800\u2021",
                    "RNN",
                    "600",
                    "8",
                    "0.8132",
                    "-",
                    "-",
                    "71.9/81.9",
                    "-",
                    "86.6"
                ],
                [
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800"
                ],
                [
                    "RNN",
                    "2x1200",
                    "CNN",
                    "600-1200-300",
                    "34",
                    "[BOLD] 0.8698",
                    "[BOLD] 85.2",
                    "[BOLD] 0.59/ [BOLD] 0.57",
                    "[BOLD] 75.1/ [BOLD] 83.2",
                    "[BOLD] 84.1",
                    "[BOLD] 92.2"
                ],
                [
                    "Skip-thought (Kiros et\u00a0al.,  2015 )",
                    "Skip-thought (Kiros et\u00a0al.,  2015 )",
                    "Skip-thought (Kiros et\u00a0al.,  2015 )",
                    "Skip-thought (Kiros et\u00a0al.,  2015 )",
                    "336",
                    "0.8584",
                    "82.3",
                    "0.29/0.35",
                    "73.0/82.0",
                    "82.0",
                    "[BOLD] 92.2"
                ],
                [
                    "Skip-thought+LN (Ba et\u00a0al.,  2016 )",
                    "Skip-thought+LN (Ba et\u00a0al.,  2016 )",
                    "Skip-thought+LN (Ba et\u00a0al.,  2016 )",
                    "Skip-thought+LN (Ba et\u00a0al.,  2016 )",
                    "720",
                    "0.8580",
                    "79.5",
                    "0.44/0.45",
                    "-",
                    "82.9",
                    "88.4"
                ]
            ],
            "title": "Table 4: Architecture Comparison. As shown in the table, our designed asymmetric RNN-CNN model (row 1,9, and 12) works better than other asymmetric models (CNN-LSTM, row 11), and models with symmetric structure (RNN-RNN, row 5 and 10)."
        },
        "insight": "We present the Table 4 in the supplementary material and we summarise it as follows: 1. Decoding the next sentence performed similarly to decoding the subsequent contiguous words. 2. Decoding the subsequent 30 words, which was adopted from the skip-thought training code3, gave reasonably good performance. More words for decoding didn't give us a significant performance gain, and took longer to train. 3. Adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed sightly improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain. 4. Increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder. We report results from both smallest and largest models in Table 2. [CONTINUE] As the transferability of the models trained in both cases perform similarly on the evaluation tasks (see rows 1 and 2 in Table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words. [CONTINUE] As stated in rows 1, 3, and 4 in Table 4, decoding short target sequences results in a slightly lower Pearson score on SICK, and decoding longer target sequences lead to a longer training time. [CONTINUE] We tweaked the CNN encoder, including different kernel size and activation function, and we report the best results of CNNCNN model at row 6 in Table 4. [CONTINUE] The future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in Table 4. Compared to our designed CNN-CNN model, their CNN-LSTM model contains more parameters than our model does, but they have similar performance on the evaluation tasks, which is also worse than our RNNCNN model. [CONTINUE] Clearly, we can tell from the comparison between rows 1, 9 and 12 in Table 4, increasing the dimensionality of the RNN encoder leads to better transferability of the model. [CONTINUE] Compared with the model with larger-size CNN decoder, apparently, we can see that larger encoder size helps more than larger decoder size does (rows 7,8, and 9 in Table 4)."
    },
    {
        "id": "30",
        "table": {
            "header": [
                "[EMPTY]",
                "WMT DE-EN",
                "WMT EN-FI",
                "WMT RO-EN",
                "IWSLT EN-FR",
                "IWSLT CS-EN"
            ],
            "rows": [
                [
                    "Words 50K",
                    "31.6",
                    "12.6",
                    "27.1",
                    "33.6",
                    "21.0"
                ],
                [
                    "BPE 32K",
                    "[BOLD] 33.5",
                    "[BOLD] 14.7",
                    "[BOLD] 27.8",
                    "34.5",
                    "22.6"
                ],
                [
                    "BPE 16K",
                    "33.1",
                    "[BOLD] 14.7",
                    "[BOLD] 27.8",
                    "[BOLD] 34.8",
                    "[BOLD] 23.0"
                ]
            ],
            "title": "Table 2: BLEU scores for training NMT models with full word and byte pair encoded vocabularies. Full word models limit vocabulary size to 50K. All models are trained with annealing Adam and scores are averaged over 3 optimizer runs."
        },
        "insight": "As shown in Table 2, sub-word systems outperform full-word systems across the board, despite having fewer total parameters. Systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies."
    },
    {
        "id": "31",
        "table": {
            "header": [
                "Model LSTM-Word",
                "PPL 88.0",
                "Model Char-CNN",
                "PPL 92.3"
            ],
            "rows": [
                [
                    "Syl-LSTM",
                    "88.7",
                    "Syl-Avg",
                    "88.5"
                ],
                [
                    "Syl-CNN-2",
                    "86.6",
                    "Syl-Avg-A",
                    "91.4"
                ],
                [
                    "Syl-CNN-3",
                    "[BOLD] 84.6",
                    "Syl-Avg-B",
                    "88.5"
                ],
                [
                    "Syl-CNN-4",
                    "86.8",
                    "Syl-Concat",
                    "[BOLD] 83.7"
                ],
                [
                    "Syl-Sum",
                    "[BOLD] 84.6",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 1: Pre-selection results. PPL stands for test set perplexity, all models have \u22485M parameters."
        },
        "insight": "The results of the pre-selection are reported in Table 1. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to 5M parameters. Surprisingly, a pure word-level model,6 LSTM-Word, also beats the character-aware one under such budget. The three best configurations are Syl-Concat, Syl-Sum, and Syl-CNN-"
    },
    {
        "id": "32",
        "table": {
            "header": [
                "Model",
                "EN",
                "FR",
                "ES",
                "DE",
                "CS",
                "RU",
                "[EMPTY]"
            ],
            "rows": [
                [
                    "Char-CNN",
                    "[BOLD] 78.9",
                    "[BOLD] 184",
                    "[BOLD] 165",
                    "[BOLD] 239",
                    "[BOLD] 371",
                    "[BOLD] 261",
                    "DATA-S"
                ],
                [
                    "Syl-CNN",
                    "80.5",
                    "191",
                    "172",
                    "239",
                    "374",
                    "269",
                    "DATA-S"
                ],
                [
                    "Syl-Sum",
                    "80.3",
                    "193",
                    "170",
                    "243",
                    "389",
                    "273",
                    "DATA-S"
                ],
                [
                    "Syl-Concat",
                    "79.4",
                    "188",
                    "168",
                    "244",
                    "383",
                    "265",
                    "DATA-S"
                ],
                [
                    "Char-CNN",
                    "[BOLD] 160",
                    "[BOLD] 124",
                    "[BOLD] 118",
                    "[BOLD] 198",
                    "[BOLD] 392",
                    "[BOLD] 190",
                    "DATA-L"
                ],
                [
                    "Syl-CNN",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "DATA-L"
                ],
                [
                    "Syl-Sum",
                    "170",
                    "141",
                    "129",
                    "212",
                    "451",
                    "233",
                    "DATA-L"
                ],
                [
                    "Syl-Concat",
                    "176",
                    "139",
                    "129",
                    "225",
                    "449",
                    "225",
                    "DATA-L"
                ]
            ],
            "title": "Table 3: Evaluation of the syllable-aware models against Char-CNN. In each case the smallest model, Syl-Concat, has 18%\u201333% less parameters than Char-CNN and is trained 1.2\u20132.2 times faster (Appendix C)."
        },
        "insight": "The results of evaluating these three models on small (1M tokens) and medium-sized (17M\u2013 57M tokens) data sets against Char-CNN for different languages are provided in Table 3. The models demonstrate similar performance on small data, but Char-CNN scales significantly better on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least [CONTINUE] number of parameters. Therefore in what follows we will make a more detailed comparison of SylConcat with Char-CNN."
    },
    {
        "id": "33",
        "table": {
            "header": [
                "Model",
                "depth",
                "[ITALIC] dLM",
                "Size",
                "PPL"
            ],
            "rows": [
                [
                    "RHN-Char-CNN",
                    "8",
                    "650",
                    "20M",
                    "67.6"
                ],
                [
                    "RHN-Syl-Concat",
                    "8",
                    "439",
                    "13M",
                    "72.0"
                ],
                [
                    "RHN-Syl-Concat",
                    "8",
                    "650",
                    "20M",
                    "69.4"
                ]
            ],
            "title": "Table 5: Replacing LSTM with Variational RHN."
        },
        "insight": "To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5). Moreover, increasing dLM from 439 to 650 did result in better performance for Syl-Concat."
    },
    {
        "id": "34",
        "table": {
            "header": [
                "Problem",
                "Support Accuracy",
                "Support Kappa",
                "Claim Accuracy",
                "Claim Kappa"
            ],
            "rows": [
                [
                    "ESIM on FEVER One",
                    ".760",
                    ".260",
                    ".517",
                    ".297"
                ],
                [
                    "ESIM on FEVER Title One",
                    ".846",
                    ".394",
                    ".639",
                    ".433"
                ],
                [
                    "Transformer on FEVER Title One",
                    ".958",
                    ".660",
                    ".823",
                    ".622"
                ]
            ],
            "title": "Table 1: Effect of adding titles to premises."
        },
        "insight": "On a support basis, we find a 52% increase in Kappa by adding the titles. [CONTINUE] Cohen's Kappa is 68% higher than that for ESIM."
    },
    {
        "id": "35",
        "table": {
            "header": [
                "Problem",
                "Support Accuracy",
                "Support Kappa",
                "Claim Accuracy",
                "Claim Kappa"
            ],
            "rows": [
                [
                    "ESIM on FEVER Title Five Oracle",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".591",
                    ".388"
                ],
                [
                    "ESIM on FEVER Title Five",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".573",
                    ".110"
                ],
                [
                    "ESIM on FEVER Title One",
                    ".846",
                    ".394",
                    ".639",
                    ".433"
                ],
                [
                    "Transformer on FEVER Title Five Oracle",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".673",
                    ".511"
                ],
                [
                    "Transformer on FEVER Title Five",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".801",
                    ".609"
                ],
                [
                    "Transformer on FEVER Title One",
                    ".958",
                    ".660",
                    ".823",
                    ".622"
                ]
            ],
            "title": "Table 2: Concatenating evidence or not."
        },
        "insight": "makes the FEVER Title Five Oracle performance better than FEVER Title Five. [CONTINUE] The Transformer model is accurate enough that oracle guessing does not help."
    },
    {
        "id": "36",
        "table": {
            "header": [
                "System",
                "Retrieval"
            ],
            "rows": [
                [
                    "FEVER Baseline (TFIDF)",
                    "66.1%"
                ],
                [
                    "+ Titles in TFIDF",
                    "68.3%"
                ],
                [
                    "+ Titles + NE",
                    "80.8%"
                ],
                [
                    "+ Titles + NE + Film",
                    "81.2%"
                ],
                [
                    "Entire Articles + NE + Film",
                    "90.1%"
                ]
            ],
            "title": "Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only."
        },
        "insight": "The named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, [CONTINUE] The film retrievals raise evidence retrieval to 81.2%."
    },
    {
        "id": "37",
        "table": {
            "header": [
                "System",
                "Development",
                "Test"
            ],
            "rows": [
                [
                    "FEVER Title Five Oracle",
                    ".5289",
                    "\u2014"
                ],
                [
                    "FEVER Title Five",
                    ".5553",
                    "\u2014"
                ],
                [
                    "FEVER Title One",
                    ".5617",
                    ".5539"
                ],
                [
                    "FEVER Title One (Narrow Evidence)",
                    ".5550",
                    "\u2014"
                ],
                [
                    "FEVER Title One (Entire Articles)",
                    ".5844",
                    ".5736"
                ]
            ],
            "title": "Table 4: FEVER Score of various systems. All use NE+Film retrieval."
        },
        "insight": "Limiting evidence in this way when only five statements are retrieved (\"narrow evidence\" in Table 4) pushes FEVER score down very little, to .5550 from .5617 on the development set, [CONTINUE] Indeed, when the system reviews the extra evidence, FEVER score goes up to .5844 on the development set."
    },
    {
        "id": "38",
        "table": {
            "header": [
                "Pairs (size)",
                "@1",
                "@5",
                "@10"
            ],
            "rows": [
                [
                    "All (38)",
                    "44.7",
                    "73.7",
                    "84.2"
                ],
                [
                    "New (7)",
                    "14.3",
                    "28.6",
                    "42.9"
                ]
            ],
            "title": "Table 2: Projection accuracy for the isolated example experiment mapping from 2000 \u2192 2001."
        },
        "insight": "Then, this projection is applied to the second model embeddings of the 47 locations, which are subject to armed conflicts in the year 2001 (38 after skipping pairs with outof-vocabulary elements). Table 2 demonstrates the resulting performance (reflecting how close the predicted vectors are to the actual armed groups active in this or that location). Note that out of 38 pairs from 2001, 31 were already present in the previous data set (ongoing conflicts). This explains why the evaluation on all the pairs gives high results. However, even for the new conflicts, the projection performance is encouraging."
    },
    {
        "id": "39",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] previous"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10"
                ],
                [
                    "[BOLD] Separate",
                    "0.0",
                    "0.7",
                    "2.1",
                    "0.5",
                    "1.1",
                    "2.4",
                    "0.0",
                    "0.5",
                    "1.6",
                    "0.4",
                    "0.8",
                    "1.8"
                ],
                [
                    "[BOLD] Cumulative",
                    "1.7",
                    "8.3",
                    "13.8",
                    "2.9",
                    "9.6",
                    "15.2",
                    "1.5",
                    "7.4",
                    "12.2",
                    "2.5",
                    "8.5",
                    "13.4"
                ],
                [
                    "[BOLD] Incr. static",
                    "54.9",
                    "82.8",
                    "90.1",
                    "60.4",
                    "79.6",
                    "84.8",
                    "20.8",
                    "31.5",
                    "34.2",
                    "23.0",
                    "30.3",
                    "32.2"
                ],
                [
                    "[BOLD] Incr. dynamic",
                    "32.5",
                    "64.5",
                    "72.2",
                    "42.6",
                    "64.8",
                    "71.5",
                    "28.1",
                    "56.1",
                    "[BOLD] 62.9",
                    "[BOLD] 37.3",
                    "[BOLD] 56.7",
                    "62.6"
                ]
            ],
            "title": "Table 3: Average accuracies of predicting next-year insurgents on the basis of locations, using projections trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous). Results for 3 baselines are shown along with the proposed incremental dynamic approach."
        },
        "insight": "Table 3 presents the results for these experiments, as well as baselines (averaged across 15 years). For the proposed incr. dynamic approach, the performance of the previous projections is [CONTINUE] comparable to that of the up-to-now projections on the accuracies @5 and @10, and is even higher on the accuracy @1 (statistically significant with t-test, p < 0.01). Thus, the single-year projections are somewhat more 'focused', while taking much less time to learn, because of less training pairs. The fact that our models were incrementally updated, not trained from scratch, is crucial. The results of the separate baseline look more like random jitter. The cumulative baseline results are slightly better, probably simply because they are trained on more data. However, they still perform much worse than the models trained using incremental updates. This is because the former models are not connected to each other, and thus are initialized with a different layout of words in the vector space. This gives rise to formally different directions of semantic relations in each yearly model (the relations themselves are still there, but they are rotated and scaled differently). The results for the incr. static baseline, when tested only on the words present in the test model vocabulary (the left part of the table), seem better than those of the proposed incr. dynamic approach. This stems from the fact that incremental updating with static vocabulary means that we never add new words to the models; thus, they contain only the vocabulary learned from the 1994 texts. The result is that at test time we skip many more pairs than with the other approaches (about 62% in average). Subsequently, the projections are tested only on a minor part of the test sets. Of course, skipping large parts of the data would be a major drawback for any realistic application, so the incr. static baseline is not really plausible. For comparison, the right part of Table 3 provides the accuracies for the setup in which all the pairs are evaluated (for pairs with OOV words the accuracy is always 0). Other tested approaches are not much affected by this change, but for incr. static the performance drops drastically. As a result, for the all pairs scenario, incremental updating with vocabulary expansion outperforms all the baselines (the differences are statistically significant with t-test, p < 0.01)."
    },
    {
        "id": "40",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] TE3\u2021  [BOLD] P",
                "[BOLD] TE3\u2021  [BOLD] R",
                "[BOLD] TE3\u2021  [BOLD] F",
                "[BOLD] TD\u2021  [BOLD] P",
                "[BOLD] TD\u2021  [BOLD] R",
                "[BOLD] TD\u2021  [BOLD] F"
            ],
            "rows": [
                [
                    "[ITALIC] Indirect:  [ITALIC] O( [ITALIC] n2)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "TL2RTL ( [ITALIC] L\u03c4)",
                    "53.5",
                    "51.1",
                    "52.3",
                    "59.1",
                    "61.2",
                    "60.1"
                ],
                [
                    "TL2RTL ( [ITALIC] L\u03c4ce)",
                    "53.9",
                    "51.7",
                    "[BOLD] 52.8",
                    "61.2",
                    "60.7",
                    "60.9"
                ],
                [
                    "TL2RTL ( [ITALIC] L\u03c4h)",
                    "52.8",
                    "51.1",
                    "51.9",
                    "57.9",
                    "60.6",
                    "59.2"
                ],
                [
                    "TL2RTL ( [ITALIC] L\u2217)",
                    "52.6",
                    "52.0",
                    "52.3",
                    "62.3",
                    "62.3",
                    "[BOLD] 62.3"
                ],
                [
                    "[ITALIC] Direct:  [ITALIC] O( [ITALIC] n)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "S-TLM ( [ITALIC] L\u03c4)",
                    "50.1",
                    "50.4",
                    "50.2",
                    "57.8",
                    "59.5",
                    "[BOLD] 58.6"
                ],
                [
                    "S-TLM ( [ITALIC] L\u03c4ce)",
                    "50.1",
                    "50.0",
                    "50.1",
                    "53.4",
                    "53.5",
                    "53.5"
                ],
                [
                    "S-TLM ( [ITALIC] L\u03c4h)",
                    "51.5",
                    "51.7",
                    "[BOLD] 51.6",
                    "55.1",
                    "56.4",
                    "55.7"
                ],
                [
                    "S-TLM ( [ITALIC] L\u2217)",
                    "50.9",
                    "51.0",
                    "51.0",
                    "56.5",
                    "55.3",
                    "55.9"
                ],
                [
                    "C-TLM ( [ITALIC] L\u03c4)",
                    "56.2",
                    "56.1",
                    "[BOLD] 56.1",
                    "57.1",
                    "59.7",
                    "[BOLD] 58.4"
                ],
                [
                    "C-TLM ( [ITALIC] L\u03c4ce)",
                    "54.4",
                    "55.4",
                    "54.9",
                    "52.4",
                    "57.3",
                    "54.7"
                ],
                [
                    "C-TLM ( [ITALIC] L\u03c4h)",
                    "55.7",
                    "55.5",
                    "55.6",
                    "55.3",
                    "54.9",
                    "55.1"
                ],
                [
                    "C-TLM ( [ITALIC] L\u2217)",
                    "54.0",
                    "54.3",
                    "54.1",
                    "54.6",
                    "53.5",
                    "54.1"
                ]
            ],
            "title": "Table 4: Evaluation of relative time-lines for each model and loss function, where L\u2217 indicates the (unweighted) sum of L\u03c4, L\u03c4ce, and L\u03c4h."
        },
        "insight": "We compared our three proposed models for the three loss functions L\u03c4 , L\u03c4 ce, and L\u03c4 h, and their linear (unweighted) combination L\u2217, on TE3\u2021 and TD\u2021, for which the results are shown in Table 4. [CONTINUE] A trend that can be observed is that overall performance on TD\u2021 is higher than that of TE3\u2021 [CONTINUE] If we compare loss functions L\u03c4 , L\u03c4 ce, and L\u03c4h, and combination L\u2217, it can be noticed that, although all loss functions seem to give fairly similar performance, L\u03c4 gives the most robust results (never lowest), especially noticeable for the smaller dataset TD\u2021. [CONTINUE] The combination of losses L\u2217 shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL. [CONTINUE] Moreover, we can clearly see that on TE3\u2021, CTLM performs better than the indirect models, across all loss functions. [CONTINUE] On TD\u2021, the indirect models seem to perform slightly better. [CONTINUE] the difference between C-TLM and S-TLM is small on the smaller TD\u2021 [CONTINUE] on TE3\u2021, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions,"
    },
    {
        "id": "41",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "All Onion all",
                "All Onion half 1",
                "All Onion half 2",
                "eBay all",
                "eBay half 1",
                "eBay half 2",
                "Illegal Onion all",
                "Illegal Onion half 1",
                "Illegal Onion half 2",
                "Legal Onion all",
                "Legal Onion half 1",
                "Legal Onion half 2"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "all",
                    "[EMPTY]",
                    "0.23",
                    "0.25",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.33",
                    "0.39",
                    "0.41",
                    "0.35",
                    "0.41",
                    "0.42"
                ],
                [
                    "All Onion",
                    "half 1",
                    "0.23",
                    "[EMPTY]",
                    "0.43",
                    "0.60",
                    "0.62",
                    "0.62",
                    "0.37",
                    "0.33",
                    "0.50",
                    "0.40",
                    "0.36",
                    "0.52"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.25",
                    "0.43",
                    "[EMPTY]",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.39",
                    "0.50",
                    "0.35",
                    "0.39",
                    "0.51",
                    "0.35"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.60",
                    "0.60",
                    "0.61",
                    "[EMPTY]",
                    "0.23",
                    "0.25",
                    "0.59",
                    "0.60",
                    "0.60",
                    "0.66",
                    "0.67",
                    "0.67"
                ],
                [
                    "eBay",
                    "half 1",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.23",
                    "[EMPTY]",
                    "0.43",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.67",
                    "0.67",
                    "0.68"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.25",
                    "0.43",
                    "[EMPTY]",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.67",
                    "0.68",
                    "0.68"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.33",
                    "0.37",
                    "0.39",
                    "0.59",
                    "0.60",
                    "0.60",
                    "[EMPTY]",
                    "0.23",
                    "0.27",
                    "0.61",
                    "0.62",
                    "0.62"
                ],
                [
                    "Illegal Onion",
                    "half 1",
                    "0.39",
                    "0.33",
                    "0.50",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.23",
                    "[EMPTY]",
                    "0.45",
                    "0.62",
                    "0.63",
                    "0.62"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.41",
                    "0.50",
                    "0.35",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.27",
                    "0.45",
                    "[EMPTY]",
                    "0.62",
                    "0.63",
                    "0.63"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.35",
                    "0.40",
                    "0.39",
                    "0.66",
                    "0.67",
                    "0.67",
                    "0.61",
                    "0.62",
                    "0.62",
                    "[EMPTY]",
                    "0.26",
                    "0.26"
                ],
                [
                    "Legal onion",
                    "half 1",
                    "0.41",
                    "0.36",
                    "0.51",
                    "0.67",
                    "0.67",
                    "0.68",
                    "0.62",
                    "0.63",
                    "0.63",
                    "0.26",
                    "[EMPTY]",
                    "0.47"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.42",
                    "0.52",
                    "0.35",
                    "0.67",
                    "0.68",
                    "0.68",
                    "0.62",
                    "0.62",
                    "0.63",
                    "0.26",
                    "0.47",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 1: Jensen-Shannon divergence between word distribution in all Onion drug sites, Legal and Illegal Onion drug sites, and eBay sites. Each domain was also split in half for within-domain comparison."
        },
        "insight": "Table 1 presents our results. The self-distance f the eBay, Legal Onion and Illegal Onion corpora lies between 0.40 to 0.45 by the JensenShannon divergence, but the distance between each pair is 0.60 to 0.65, with the three approximately forming an equilateral triangle in the space of word distributions. Similar results are obtained using Variational distance, and are omitted for brevity."
    },
    {
        "id": "42",
        "table": {
            "header": [
                "[EMPTY]",
                "% Wikifiable"
            ],
            "rows": [
                [
                    "eBay",
                    "38.6\u00b12.00"
                ],
                [
                    "Illegal Onion",
                    "32.5\u00b11.35"
                ],
                [
                    "Legal Onion",
                    "50.8\u00b12.31"
                ]
            ],
            "title": "Table 2: Average percentage of wikifiable named entities in a website per domain, with standard error."
        },
        "insight": "each average. According to our results (Table 2), the Wikification success ratios of eBay and Illegal [CONTINUE] Onion named entities is comparable and relatively low. However, sites selling legal drugs on Onion have a much higher Wikification percentage."
    },
    {
        "id": "43",
        "table": {
            "header": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "rows": [
                [
                    "UKB (this work)",
                    "[BOLD] 67.3",
                    "68.8",
                    "66.1",
                    "53.0",
                    "[BOLD] 68.8",
                    "[BOLD] 70.3"
                ],
                [
                    "UKB (elsewhere)\u2020\u2021",
                    "57.5",
                    "60.6",
                    "54.1",
                    "42.0",
                    "59.0",
                    "61.2"
                ],
                [
                    "Chaplot and Sakajhutdinov ( 2018 ) \u2021",
                    "66.9",
                    "[BOLD] 69.0",
                    "[BOLD] 66.9",
                    "55.6",
                    "65.3",
                    "69.6"
                ],
                [
                    "Babelfy Moro et\u00a0al. ( 2014 )\u2020",
                    "65.5",
                    "67.0",
                    "63.5",
                    "51.6",
                    "66.4",
                    "70.3"
                ],
                [
                    "MFS",
                    "65.2",
                    "66.8",
                    "66.2",
                    "55.2",
                    "63.0",
                    "67.8"
                ],
                [
                    "Basile et\u00a0al. ( 2014 )\u2020",
                    "63.7",
                    "63.0",
                    "63.7",
                    "[BOLD] 56.7",
                    "66.2",
                    "64.6"
                ],
                [
                    "Banerjee and Pedersen ( 2003 )\u2020",
                    "48.7",
                    "50.6",
                    "44.5",
                    "32.0",
                    "53.6",
                    "51.0"
                ]
            ],
            "title": "Table 1: F1 results for knowledge-based systems on the\u00a0Raganato et\u00a0al. (2017a) dataset. Top rows show conflicting results for UKB. \u2020 for results reported in Raganato et\u00a0al. (2017a), \u2021 for results reported in Chaplot and Sakajhutdinov (2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015."
        },
        "insight": "The two top rows in Table 1 show conflicting results for UKB. [CONTINUE] As the results show, that paper reports a suboptimal use of UKB. [CONTINUE] the table also reports the [CONTINUE] best performing knowledge-based systems on this dataset. [CONTINUE] also report We (Chaplot and Sakajhutdinov, 2018), the latest work on this area, as well as the most frequent sense as given by WordNet counts [CONTINUE] The table shows that UKB yields the best overall result."
    },
    {
        "id": "44",
        "table": {
            "header": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "rows": [
                [
                    "Yuan et\u00a0al. ( 2016 )",
                    "[BOLD] 71.5",
                    "[BOLD] 73.8",
                    "[BOLD] 71.8",
                    "63.5",
                    "[BOLD] 69.5",
                    "[BOLD] 72.6"
                ],
                [
                    "Raganato et\u00a0al. ( 2017b )",
                    "69.9",
                    "72.0",
                    "69.1",
                    "[BOLD] 64.8",
                    "66.9",
                    "71.5"
                ],
                [
                    "Iacobacci et\u00a0al. ( 2016 )\u2020",
                    "69.7",
                    "73.3",
                    "69.6",
                    "61.1",
                    "66.7",
                    "70.4"
                ],
                [
                    "Melamud et\u00a0al. ( 2016 )\u2020",
                    "69.4",
                    "72.3",
                    "68.2",
                    "61.5",
                    "67.2",
                    "71.7"
                ],
                [
                    "IMS Zhong and Ng ( 2010 )\u2020",
                    "68.8",
                    "72.8",
                    "69.2",
                    "60.0",
                    "65.0",
                    "69.3"
                ]
            ],
            "title": "Table 2: F1 results for supervised systems on the\u00a0Raganato et\u00a0al. (2017a) dataset. \u2020 for results reported in Raganato et\u00a0al. (2017a). Best results in bold. Note that Raganato et\u00a0al. (2017b) used S07 for development."
        },
        "insight": "Table 2 reports the results of supervised systems on the same dataset, taken from the two works that use the dataset [CONTINUE] As expected, supervised systems outperform knowledge-based systems, by a small margin in some of the cases."
    },
    {
        "id": "45",
        "table": {
            "header": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "rows": [
                [
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence"
                ],
                [
                    "ppr_w2w",
                    "66.9",
                    "[BOLD] 69.0",
                    "65.7",
                    "53.9",
                    "67.1",
                    "69.9"
                ],
                [
                    "dfs_ppr",
                    "65.2",
                    "67.5",
                    "65.6",
                    "53.6",
                    "62.7",
                    "68.2"
                ],
                [
                    "ppr",
                    "65.5",
                    "67.5",
                    "[BOLD] 66.5",
                    "[BOLD] 54.7",
                    "63.3",
                    "67.4"
                ],
                [
                    "ppr_w2wnf",
                    "60.2",
                    "63.7",
                    "55.1",
                    "42.2",
                    "63.5",
                    "63.8"
                ],
                [
                    "pprnf",
                    "57.1",
                    "60.5",
                    "53.8",
                    "41.3",
                    "58.0",
                    "61.4"
                ],
                [
                    "dfsnf",
                    "58.7",
                    "63.3",
                    "52.8",
                    "40.4",
                    "61.6",
                    "62.5"
                ],
                [
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)",
                    "One or more context sentences (# [ITALIC] words\u226520)"
                ],
                [
                    "ppr_w2w",
                    "[BOLD] 67.3",
                    "68.8",
                    "66.1",
                    "53.0",
                    "[BOLD] 68.8",
                    "[BOLD] 70.3"
                ],
                [
                    "ppr",
                    "65.6",
                    "67.5",
                    "66.4",
                    "54.1",
                    "64.0",
                    "67.8"
                ],
                [
                    "dfs",
                    "65.7",
                    "67.9",
                    "65.9",
                    "54.5",
                    "64.2",
                    "68.1"
                ],
                [
                    "ppr_w2wnf",
                    "60.4",
                    "64.2",
                    "54.8",
                    "40.0",
                    "64.5",
                    "64.5"
                ],
                [
                    "pprnf",
                    "58.6",
                    "61.3",
                    "54.9",
                    "42.2",
                    "60.9",
                    "62.9"
                ],
                [
                    "dfsnf",
                    "59.1",
                    "62.7",
                    "54.4",
                    "39.3",
                    "62.8",
                    "62.2"
                ]
            ],
            "title": "Table 3: Additional results on other settings of UKB. nf subscript stands for \u201cno sense frequency\u201d. Top rows use a single sentence as context, while the bottom rows correspond to extended context (cf. Sect. 3). Best results in bold."
        },
        "insight": "Ta [CONTINUE] The table shows that the key factor is the use of sense frequencies, and systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in F1. [CONTINUE] The table also shows that extending the context is mildly effective. [CONTINUE] Regarding the algorithm, the table confirms that the best method is ppr w2w, followed by the subgraph approach (dfs) and ppr."
    },
    {
        "id": "46",
        "table": {
            "header": [
                "[BOLD] Models",
                "[BOLD] SWBD",
                "[BOLD] SWBD2"
            ],
            "rows": [
                [
                    "BoW + Logsitic",
                    "78.95",
                    "87.76"
                ],
                [
                    "BoW + SVM",
                    "73.68",
                    "[BOLD] 90.82"
                ],
                [
                    "Bigram + SVM",
                    "52.63",
                    "79.59"
                ],
                [
                    "BoW + TF-IDF + Logistic",
                    "52.63",
                    "81.63"
                ],
                [
                    "nGram + Logistic",
                    "52.63",
                    "78.57"
                ],
                [
                    "nGram + TF-IDF + Logistic",
                    "57.89",
                    "87.76"
                ],
                [
                    "Bag of Means + Logistic",
                    "78.95",
                    "87.76"
                ],
                [
                    "Avg. Skipgram + Logistic",
                    "26.32",
                    "59.18"
                ],
                [
                    "Doc2Vec + SVM",
                    "73.68",
                    "86.73"
                ],
                [
                    "HN",
                    "31.58",
                    "54.08"
                ],
                [
                    "HN-ATT Yang et\u00a0al. ( 2016 )",
                    "73.68",
                    "85.71"
                ],
                [
                    "CNN Kim ( 2014 )",
                    "84.21",
                    "93.87"
                ],
                [
                    "HN-SA (our model)",
                    "[BOLD] 89.47",
                    "[BOLD] 95.92"
                ]
            ],
            "title": "Table 2: Accuracy (in %) of our model and other text classification models on both versions of SWBD."
        },
        "insight": "We compare the performance of our model (Table 2) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram (Mikolov et al., 2013), Doc2Vec (Le and Mikolov, 2014), CNN (Kim, 2014), Hierarchical Attention (HN-ATT) (Yang et al., 2016) and hierarchical network (HN) models."
    },
    {
        "id": "47",
        "table": {
            "header": [
                "[BOLD] Approach",
                "[BOLD] Dev",
                "[BOLD] Test"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "1.9%",
                    "3.7%"
                ],
                [
                    "Seq2Seq + Attention",
                    "1.8%",
                    "4.8%"
                ],
                [
                    "Seq2Seq + Copying",
                    "4.1%",
                    "5.3%"
                ],
                [
                    "TypeSQL",
                    "8.0%",
                    "8.2%"
                ],
                [
                    "SQLNet",
                    "10.9%",
                    "12.4%"
                ],
                [
                    "SyntaxSQLNet",
                    "18.9%",
                    "19.7%"
                ],
                [
                    "SyntaxSQLNet(augment)",
                    "24.8%",
                    "27.2%"
                ],
                [
                    "[BOLD] IRNet",
                    "[BOLD] 53.2%",
                    "[BOLD] 46.7%"
                ],
                [
                    "[BOLD] BERT",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SyntaxSQLNet(BERT)",
                    "25.0%",
                    "25.4%"
                ],
                [
                    "[BOLD] IRNet(BERT)",
                    "[BOLD] 61.9%",
                    "[BOLD] 54.7%"
                ]
            ],
            "title": "Table 1: Exact matching accuracy on SQL queries."
        },
        "insight": "Table 1 presents the exact matching accuracy of IRNet and various baselines on the development set and the test set. IRNet clearly outperforms all the baselines by a substantial margin. It obtains 27.0% absolute improvement over SyntaxSQLIt also obtains 19.5% absolute Net on test set. improvement over SyntaxSQLNet(augment) that performs large-scale data augmentation. When incorporating BERT, the performance of both SyntaxSQLNet and IRNet is substantially improved and the accuracy gap between them on both the development set and the test set is widened."
    },
    {
        "id": "48",
        "table": {
            "header": [
                "[BOLD] Approach",
                "[BOLD] Easy",
                "[BOLD] Medium",
                "[BOLD] Hard",
                "[BOLD] Extra"
            ],
            "rows": [
                [
                    "[BOLD] Approach",
                    "[BOLD] Easy",
                    "[BOLD] Medium",
                    "[BOLD] Hard",
                    "[BOLD] Hard"
                ],
                [
                    "SyntaxSQLNet",
                    "38.6%",
                    "17.6%",
                    "16.3%",
                    "4.9%"
                ],
                [
                    "SyntaxSQLNet",
                    "42.9%",
                    "24.9%",
                    "21.9%",
                    "8.6%"
                ],
                [
                    "(BERT)",
                    "42.9%",
                    "24.9%",
                    "21.9%",
                    "8.6%"
                ],
                [
                    "[BOLD] IRNet",
                    "[BOLD] 70.1%",
                    "[BOLD] 49.2%",
                    "[BOLD] 39.5%",
                    "[BOLD] 19.1%"
                ],
                [
                    "[BOLD] IRNet(BERT)",
                    "[BOLD] 77.2%",
                    "[BOLD] 58.7%",
                    "[BOLD] 48.1%",
                    "[BOLD] 25.3%"
                ]
            ],
            "title": "Table 2: Exact matching accuracy of SyntaxSQLNet, SyntaxSQLNet(BERT), IRNet and IRNet(BERT) on test set by hardness level."
        },
        "insight": "As shown in Table 2, IRNet significantly outperforms SyntaxSQLNet in all four hardness levels with or without BERT. For example, compared with SyntaxSQLNet, IRNet obtains 23.3% absolute improvement in Hard level."
    },
    {
        "id": "49",
        "table": {
            "header": [
                "[BOLD] Approach",
                "[BOLD] SQL",
                "[BOLD] SemQL"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "1.9%",
                    "11.4%( [BOLD] +9.5)"
                ],
                [
                    "Seq2Seq + Attention",
                    "1.8%",
                    "14.7%( [BOLD] +12.9)"
                ],
                [
                    "Seq2Seq + Copying",
                    "4.1%",
                    "18.5%( [BOLD] +14.1)"
                ],
                [
                    "TypeSQL",
                    "8.0%",
                    "14.4%( [BOLD] +6.4)"
                ],
                [
                    "SQLNet",
                    "10.9%",
                    "17.5%( [BOLD] +6.6)"
                ],
                [
                    "SyntaxSQLNet",
                    "18.9%",
                    "27.5%( [BOLD] +8.6)"
                ],
                [
                    "[BOLD] BERT",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SyntaxSQLNet(BERT)",
                    "25.0%",
                    "35.8%( [BOLD] +10.8)"
                ]
            ],
            "title": "Table 3: Exact matching accuracy on development set. The header \u2018SQL\u2019 means that the approaches are learned to generate SQL, while the header \u2018SemQL\u2019 indicates that they are learned to generate SemQL queries."
        },
        "insight": "As shown in Table 3, there are at least 6.6% and up to 14.4% absolute improvements on accuracy of exact matching on the development set. For example, when SyntaxSQLNet is learned to generate SemQL queries instead of SQL queries, it registers 8.6% absolute improvement and even outperforms SyntaxSQLNet(augment) which performs largescale data augmentation. [CONTINUE] Table 4 presents the ablation study results. It is clear that our base model significantly outperforms SyntaxSQLNet, SyntaxSQLNet( augment) and SyntaxSQLNet(BERT). Performing schema linking ('+SL') brings about 8.5% and 6.4% absolute improvement on IRNet and IRNet(BERT). [CONTINUE] The F1 score on the WHERE clause increases by 12.5% when IRNet performs schema linking. [CONTINUE] The number of examples suffering from this problem decreases by 70%, when using the memory augmented pointer network."
    },
    {
        "id": "50",
        "table": {
            "header": [
                "Model",
                "FEVER Dev base",
                "FEVER Dev r.w",
                "Generated base",
                "Generated r.w"
            ],
            "rows": [
                [
                    "NSMN",
                    "81.8",
                    "-",
                    "58.7",
                    "-"
                ],
                [
                    "ESIM",
                    "80.8",
                    "76.0",
                    "55.9",
                    "59.3"
                ],
                [
                    "BERT",
                    "[BOLD] 86.2",
                    "84.6",
                    "58.3",
                    "[BOLD] 61.6"
                ]
            ],
            "title": "Table 3: Classifiers\u2019 accuracy on the Supports and Refutes cases from the FEVER Dev set and on the Generated pairs for the Symmetric Test Set in the setting of without (base) and with (r.w) re-weight."
        },
        "insight": "Table 3 summarizes the performance of the three models on the SUPPORTS and REFUTES pairs from the FEVER DEV set and on the created SYMMETRIC TEST SET pairs. [CONTINUE] All models perform relatively well on FEVER DEV but achieve less than 60% accuracy on the synthetic ones. [CONTINUE] The re-weighting method increases the accuracy of the ESIM and BERT models by an absolute 3.4% and 3.3% respectively. One can notice that this improvement comes at a cost in the accuracy over the FEVER DEV pairs. [CONTINUE] Applying the regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement."
    },
    {
        "id": "51",
        "table": {
            "header": [
                "[BOLD] Bigram",
                "[BOLD] Train  [BOLD] LMI \u22c510\u22126",
                "[BOLD] Train  [ITALIC] p( [ITALIC] l| [ITALIC] w)",
                "[BOLD] Development  [BOLD] LMI \u22c510\u22126",
                "[BOLD] Development  [ITALIC] p( [ITALIC] l| [ITALIC] w)"
            ],
            "rows": [
                [
                    "united states",
                    "271",
                    "0.64",
                    "268",
                    "0.44"
                ],
                [
                    "least one",
                    "269",
                    "0.90",
                    "267",
                    "0.77"
                ],
                [
                    "at least",
                    "256",
                    "0.72",
                    "163",
                    "0.48"
                ],
                [
                    "person who",
                    "162",
                    "0.90",
                    "135",
                    "0.61"
                ],
                [
                    "stars actor",
                    "143",
                    "0.86",
                    "111",
                    "0.71"
                ],
                [
                    "won award",
                    "133",
                    "0.80",
                    "50",
                    "0.56"
                ],
                [
                    "american actor",
                    "126",
                    "0.79",
                    "55",
                    "0.45"
                ],
                [
                    "starred movie",
                    "100",
                    "0.88",
                    "34",
                    "0.80"
                ],
                [
                    "from united",
                    "100",
                    "0.82",
                    "108",
                    "0.67"
                ],
                [
                    "from america",
                    "96",
                    "0.89",
                    "108",
                    "0.74"
                ]
            ],
            "title": "Table 7: Top 10 LMI-ranked bigrams in the train set of FEVER for Support."
        },
        "insight": "Table 7 and Table 8 summarize the top 10 bigrams for SUPPORT and NOT ENOUGH INFO. The correlation between the biased phrases in the two dataset splits is not as strong as in the REFUTE label, presented in the paper. [CONTINUE] However, one can notice that some of the biased bigrams in the training set, such as \"least one\" and \"starred movie\", translate to cues that can help in predictions over the development set."
    },
    {
        "id": "52",
        "table": {
            "header": [
                "System",
                "EM",
                "F1"
            ],
            "rows": [
                [
                    "[ITALIC] Mnewsqa",
                    "46.3",
                    "60.8"
                ],
                [
                    "[ITALIC] Mnewsqa +  [ITALIC] Snet",
                    "47.9",
                    "61.5"
                ]
            ],
            "title": "Table 3: NewsQA to SQuAD. Exact match (EM) and span F1 results on SQuAD development set of a NewsQA BIDAF model baseline vs. one finetuned on SQuAD using the data generated by a 2-stage SynNet (Snet)."
        },
        "insight": "report brief results on SQuAD (Table 3), [CONTINUE] We also evaluate the SynNet on the NewsQAto-SQuAD direction. We directly apply the best setting from the other direction and report the result in Table 3. [CONTINUE] The SynNet improves over the baseline by 1.6% in EM and 0.7% in F1. Limited by space, we leave out ablation studies in this direction."
    },
    {
        "id": "53",
        "table": {
            "header": [
                "Model",
                "Dataset 1  [ITALIC] vs. Simulator",
                "Dataset 1  [ITALIC] vs. Simulator",
                "Dataset 1  [ITALIC] vs. Human",
                "Dataset 1  [ITALIC] vs. Human",
                "Dataset 2  [ITALIC] vs. Simulator",
                "Dataset 2  [ITALIC] vs. Simulator",
                "Dataset 2  [ITALIC] vs. Human",
                "Dataset 2  [ITALIC] vs. Human"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns"
                ],
                [
                    "Seq2Seq(goal)",
                    "76.00",
                    "4.74",
                    "67.74",
                    "7.87",
                    "67.10",
                    "7.38",
                    "54.1",
                    "7.56"
                ],
                [
                    "Seq2Seq(goal+state)",
                    "79.41",
                    "4.74",
                    "70.97",
                    "6.35",
                    "67.37",
                    "7.42",
                    "58.1",
                    "8.04"
                ],
                [
                    "Seq2Seq(goal+look)",
                    "80.64",
                    "6.54",
                    "74.19",
                    "5.41",
                    "83.54",
                    "[BOLD] 5.82",
                    "60.3",
                    "[BOLD] 6.94"
                ],
                [
                    "Seq2Seq(goal+look+state)",
                    "[BOLD] 85.07",
                    "[BOLD] 4.10",
                    "[BOLD] 77.42",
                    "[BOLD] 5.02",
                    "[BOLD] 83.58",
                    "6.36",
                    "[BOLD] 61.2",
                    "7.30"
                ]
            ],
            "title": "Table 3: Performance on two datasets against the user simulator and human."
        },
        "insight": "Table 3 shows the performance of baselines against user simulator and human on the two datasets. [CONTINUE] Equation (11)) is also proven effective in deliver [CONTINUE] ing more achievement, which can be seen from the second and last rows of Table 3. [CONTINUE] The results are consistent with those in Table 3."
    },
    {
        "id": "54",
        "table": {
            "header": [
                "[BOLD] Type  [BOLD] POS Tags",
                "[BOLD] Type Noun",
                "[BOLD] Zh\u21d2En  [BOLD] 21.0%",
                "[BOLD] En\u21d2Fr 1.9%",
                "[BOLD] En\u21d2Ja 0.7%"
            ],
            "rows": [
                [
                    "[BOLD] POS Tags",
                    "Verb",
                    "0.3%",
                    "[BOLD] 25.0%",
                    "0.3%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Adj.",
                    "0.4%",
                    "9.3%",
                    "0.7%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Prep.",
                    "1.3%",
                    "4.5%",
                    "[BOLD] 26.7%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Dete.",
                    "3.0%",
                    "5.7%",
                    "2.1%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Punc.",
                    "3.5%",
                    "[BOLD] 18.3%",
                    "[BOLD] 30.5%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Others",
                    "0.5%",
                    "1.2%",
                    "4.7%"
                ],
                [
                    "[BOLD] Fertility",
                    "\u22652",
                    "[BOLD] 50.2%",
                    "[BOLD] 21.4%",
                    "[BOLD] 21.7%"
                ],
                [
                    "[BOLD] Fertility",
                    "1",
                    "[BOLD] 15.4%",
                    "7.0%",
                    "3.1%"
                ],
                [
                    "[BOLD] Fertility",
                    "(0,1)",
                    "2.5%",
                    "0.4%",
                    "3.0%"
                ],
                [
                    "[BOLD] Fertility",
                    "0",
                    "0.0%",
                    "1.9%",
                    "3.8%"
                ],
                [
                    "[BOLD] Syntactic",
                    "Low",
                    "1.6%",
                    "2.5%",
                    "1.2%"
                ],
                [
                    "[BOLD] Syntactic",
                    "Middle",
                    "0.3%",
                    "0.8%",
                    "1.4%"
                ],
                [
                    "[BOLD] Syntactic",
                    "High",
                    "0.0%",
                    "0.1%",
                    "0.1%"
                ]
            ],
            "title": "Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many (\u201c\u22652\u201d), one-to-one (\u201c1\u201d), many-to-one (\u201c(0,1)\u201d), and null-aligned (\u201c0\u201d). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word."
        },
        "insight": "Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many (\"\u2265 2\"), one-to-one (\"1\"), many-to-one (\"(0, 1)\"), and null-aligned (\"0\"). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word."
    },
    {
        "id": "55",
        "table": {
            "header": [
                "Data",
                "Method",
                "SemEval-15 Acc.",
                "SemEval-15 Macro-F1",
                "SemEval-16 Acc.",
                "SemEval-16 Macro-F1"
            ],
            "rows": [
                [
                    "A",
                    "TDLSTM+ATT Tang et al. ( 2016a )",
                    "77.10",
                    "59.46",
                    "83.11",
                    "57.53"
                ],
                [
                    "A",
                    "ATAE-LSTM Wang et al. ( 2016 )",
                    "78.48",
                    "62.84",
                    "83.77",
                    "61.71"
                ],
                [
                    "A",
                    "MM Tang et al. ( 2016b )",
                    "77.89",
                    "59.52",
                    "83.04",
                    "57.91"
                ],
                [
                    "A",
                    "RAM Chen et al. ( 2017 )",
                    "79.98",
                    "60.57",
                    "83.88",
                    "62.14"
                ],
                [
                    "A",
                    "LSTM+SynATT+TarRep He et al. ( 2018a )",
                    "81.67",
                    "66.05",
                    "84.61",
                    "67.45"
                ],
                [
                    "S+A",
                    "Semisupervised He et al. ( 2018b )",
                    "81.30",
                    "[BOLD] 68.74",
                    "85.58",
                    "69.76"
                ],
                [
                    "S",
                    "BiLSTM-104 Sentence Training",
                    "80.24 \u00b1 1.64",
                    "61.89 \u00b1 0.94",
                    "80.89 \u00b1 2.79",
                    "61.40 \u00b1 2.49"
                ],
                [
                    "S+A",
                    "BiLSTM-104 Sentence Training \u2192Aspect Based Finetuning",
                    "77.75 \u00b1 2.09",
                    "60.83 \u00b1 4.53",
                    "84.87\u00b1 0.31",
                    "61.87 \u00b1 5.44"
                ],
                [
                    "N",
                    "BiLSTM-XR-Dev Estimation",
                    "[BOLD] 83.31\u2217\u00b1 0.62",
                    "62.24 \u00b1 0.66",
                    "[BOLD] 87.68\u2217\u00b1 0.47",
                    "63.23 \u00b1 1.81"
                ],
                [
                    "N",
                    "BiLSTM-XR",
                    "[BOLD] 83.31\u2217\u00b1 0.77",
                    "64.42 \u00b1 2.78",
                    "[BOLD] 88.12\u2217\u00b1 0.24",
                    "68.60 \u00b1 1.79"
                ],
                [
                    "N+A",
                    "BiLSTM-XR \u2192Aspect Based Finetuning",
                    "[BOLD] 83.44\u2217\u00b1 0.74",
                    "[BOLD] 67.23 \u00b1 1.42",
                    "[BOLD] 87.66\u2217\u00b1 0.28",
                    "[BOLD] 71.19\u2020\u00b1 1.40"
                ]
            ],
            "title": "Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. \u2217 indicates that the method\u2019s result is significantly better than all baseline methods, \u2020 indicates that the method\u2019s result is significantly better than all baselines methods that use the aspect-based data only, with p<0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from He et al. (2018a). Numbers for Semisupervised are from He et al. (2018b)."
        },
        "insight": "Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. \u2217 indicates that the method's result is significantly better than all baseline methods, \u2020 indicates that the method's result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, [CONTINUE] and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b)."
    },
    {
        "id": "56",
        "table": {
            "header": [
                "[BOLD] A)",
                "EM",
                "F1",
                "[BOLD] B)",
                "EM",
                "F1"
            ],
            "rows": [
                [
                    "k=0",
                    "27.2",
                    "40.5",
                    "2s +  [ITALIC] Aner",
                    "22.8",
                    "36.1"
                ],
                [
                    "k=2",
                    "29.8",
                    "43.9",
                    "all +  [ITALIC] Aner",
                    "27.2",
                    "40.5"
                ],
                [
                    "k=4",
                    "30.4",
                    "44.3",
                    "2s +  [ITALIC] Aoracle",
                    "31.3",
                    "45.2"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "all +  [ITALIC] Aoracle",
                    "32.5",
                    "46.8"
                ]
            ],
            "title": "Table 4: Ablation Studies. Exact match (EM) and span F1 results on NewsQA test set of a BIDAF model finetuned with a 2-stage SynNet. In study A, we vary k, the number of mini-batches from SQuAD for every batch in NewsQA. In study B, we set k=0, and vary the answer type and how much of the paragraph we use for question synthesis. 2\u2212sent refers to using two sentences before answer span, while all refers to using the entire paragraph. Aner refers to using an NER system and Aor refers to using the human-annotated answers to generate questions."
        },
        "insight": "conduct ablation studies (Table 4), [CONTINUE] To better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies, as summarized in Table 4. [CONTINUE] Results in Table 4(A) show that using human-annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module. [CONTINUE] This supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model. [CONTINUE] To see how copying impacts performance, we explore using the entire paragraph to generate the question vs. only the two sentences before and one sentence after the answer span and report results in Table 4(B)."
    },
    {
        "id": "57",
        "table": {
            "header": [
                "[BOLD] Transfer learning schedule",
                "[BOLD] es2en  [BOLD] Khresmoi",
                "[BOLD] es2en  [BOLD] Health",
                "[BOLD] es2en  [BOLD] Bio",
                "[BOLD] en2es  [BOLD] Khresmoi",
                "[BOLD] en2es  [BOLD] Health",
                "[BOLD] en2es  [BOLD] Bio"
            ],
            "rows": [
                [
                    "Health",
                    "45.1",
                    "35.7",
                    "34.0",
                    "41.2",
                    "34.7",
                    "36.1"
                ],
                [
                    "All-biomed",
                    "49.8",
                    "35.4",
                    "35.7",
                    "43.4",
                    "33.9",
                    "37.5"
                ],
                [
                    "All-biomed \u2192 Health",
                    "48.9",
                    "36.4",
                    "35.9",
                    "43.0",
                    "35.2",
                    "38.0"
                ],
                [
                    "All-biomed \u2192 Bio",
                    "48.0",
                    "34.6",
                    "37.2",
                    "43.2",
                    "34.1",
                    "40.5"
                ],
                [
                    "Health \u2192 All-biomed",
                    "[BOLD] 52.1",
                    "36.7",
                    "37.0",
                    "44.2",
                    "35.0",
                    "39.0"
                ],
                [
                    "Health \u2192 All-biomed \u2192 Health",
                    "51.1",
                    "[BOLD] 37.0",
                    "37.2",
                    "44.0",
                    "[BOLD] 36.3",
                    "39.5"
                ],
                [
                    "Health \u2192 All-biomed \u2192 Bio",
                    "50.6",
                    "36.0",
                    "[BOLD] 38.0",
                    "[BOLD] 45.2",
                    "35.3",
                    "[BOLD] 41.3"
                ]
            ],
            "title": "Table 2: Validation BLEU for English-Spanish models with transfer learning. We use the final three models in our submission."
        },
        "insight": "Table 2 gives single model validation scores for es2en and en2es models with standard and iterative transfer learning. We find that the all-biomed domain gains 1-2 BLEU points from transfer learning."
    },
    {
        "id": "58",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] es2en  [BOLD] Khresmoi",
                "[BOLD] es2en  [BOLD] Health",
                "[BOLD] es2en  [BOLD] Bio",
                "[BOLD] es2en  [BOLD] Test",
                "[BOLD] en2es  [BOLD] Khresmoi",
                "[BOLD] en2es  [BOLD] Health",
                "[BOLD] en2es  [BOLD] Bio",
                "[BOLD] en2es  [BOLD] Test"
            ],
            "rows": [
                [
                    "Health \u2192 All-biomed",
                    "52.1",
                    "36.7",
                    "37.0",
                    "42.4",
                    "44.2",
                    "35.0",
                    "39.0",
                    "44.9"
                ],
                [
                    "Health \u2192 All-biomed \u2192 Health",
                    "51.1",
                    "37.0",
                    "37.2",
                    "-",
                    "44.0",
                    "36.3",
                    "39.5",
                    "-"
                ],
                [
                    "Health \u2192 All-biomed \u2192 Bio",
                    "50.6",
                    "36.0",
                    "38.0",
                    "-",
                    "45.2",
                    "35.3",
                    "41.3",
                    "-"
                ],
                [
                    "Uniform ensemble",
                    "[BOLD] 52.2",
                    "36.9",
                    "37.9",
                    "[BOLD] 43.0",
                    "[BOLD] 45.1",
                    "35.6",
                    "40.2",
                    "45.4"
                ],
                [
                    "BI ensemble ( [ITALIC] \u03b1=0.5)",
                    "52.1",
                    "[BOLD] 37.0",
                    "[BOLD] 38.1",
                    "42.9",
                    "44.5",
                    "[BOLD] 35.7",
                    "[BOLD] 41.2",
                    "[BOLD] 45.6"
                ]
            ],
            "title": "Table 3: Validation and test BLEU for models used in English-Spanish language pair submissions."
        },
        "insight": "For de2en and es2en, uniform ensembling performs similarly to the oracles, and performs similarly to BI. [CONTINUE] We submitted three runs to the WMT19 biomedical task for each language pair: the best single all-biomed model, a uniform ensemble of models on two en-de and three es-en domains, and an ensemble with Bayesian Interpolation. Tables 3 and 4 give validation and test scores. [CONTINUE] that a uniform multi-domain ensemble performs well, giving 0.5-1.2 BLEU improvement on the test set over strong single models. [CONTINUE] We see small gains from using BI with ensembles on most validation sets, but only on en2es test. [CONTINUE] we noted that, in general, we could predict BI (\u03b1 = 0.5) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain. [CONTINUE] For en2es uniform ensembling underperforms the health and bio oracle models on their validation sets, and the uniform ensemble slightly underperforms BI on the test data."
    },
    {
        "id": "59",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] de2en  [BOLD] Khresmoi",
                "[BOLD] de2en  [BOLD] Cochrane",
                "[BOLD] de2en  [BOLD] Test",
                "[BOLD] en2de  [BOLD] Khresmoi",
                "[BOLD] en2de  [BOLD] Cochrane",
                "[BOLD] en2de  [BOLD] Test"
            ],
            "rows": [
                [
                    "News",
                    "43.8",
                    "46.8",
                    "-",
                    "30.4",
                    "40.7",
                    "-"
                ],
                [
                    "News \u2192 All-biomed",
                    "44.5",
                    "47.6",
                    "27.4",
                    "31.1",
                    "39.5",
                    "26.5"
                ],
                [
                    "Uniform ensemble",
                    "45.3",
                    "48.4",
                    "[BOLD] 28.6",
                    "[BOLD] 32.6",
                    "42.9",
                    "[BOLD] 27.2"
                ],
                [
                    "BI ensemble ( [ITALIC] \u03b1=0.5)",
                    "[BOLD] 45.4",
                    "[BOLD] 48.8",
                    "28.5",
                    "32.4",
                    "[BOLD] 43.1",
                    "26.4"
                ]
            ],
            "title": "Table 4: Validation and test BLEU for models used in English-German language pair submissions."
        },
        "insight": "We see small gains from using BI with ensembles on most validation sets, but only on en2es test. [CONTINUE] For en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets, and outperforms BI on the test data."
    },
    {
        "id": "60",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] es2en",
                "[BOLD] en2es",
                "[BOLD] de2en",
                "[BOLD] en2de"
            ],
            "rows": [
                [
                    "Uniform",
                    "[BOLD] 43.2",
                    "45.3",
                    "28.3",
                    "25.9"
                ],
                [
                    "BI ( [ITALIC] \u03b1=0.5)",
                    "43.0",
                    "[BOLD] 45.5",
                    "28.2",
                    "25.2"
                ],
                [
                    "BI ( [ITALIC] \u03b1=0.1)",
                    "[BOLD] 43.2",
                    "[BOLD] 45.5",
                    "[BOLD] 28.5",
                    "[BOLD] 26.0"
                ]
            ],
            "title": "Table 5: Comparing uniform ensembles and BI with varying smoothing factor on the WMT19 test data. Small deviations from official test scores on submitted runs are due to tokenization differences. \u03b1=0.5 was chosen for submission based on results on available development data."
        },
        "insight": "Consequently in Table 5 we experiment with BI (\u03b1 = 0.1). In this case BI matches or out-performs the uniform ensemble. Notably, for en2es, where BI (\u03b1 = 0.5) performed well, taking \u03b1 = 0.1 does not harm performance."
    },
    {
        "id": "61",
        "table": {
            "header": [
                "dev",
                "train",
                "eScape"
            ],
            "rows": [
                [
                    "77.15",
                    "77.42",
                    "37.68"
                ]
            ],
            "title": "Table 1: BLEU Scores of Data Sets"
        },
        "insight": "We then tested the BLEU scores between machine translation results and corresponding gold standard post-editing results on the original development set, the training set and the synthetic data as shown in Table 1. [CONTINUE] Table 1 shows that there is a significant gap between the synthetic eScape data set (Negri et al., 2018) and the real-life data sets (the development set and the original training set from posteditors),"
    },
    {
        "id": "62",
        "table": {
            "header": [
                "Models",
                "TER",
                "BLEU"
            ],
            "rows": [
                [
                    "MT as PE",
                    "16.84",
                    "74.73"
                ],
                [
                    "Gaussian",
                    "16.79",
                    "75.03"
                ],
                [
                    "Uniform",
                    "16.80",
                    "75.03"
                ],
                [
                    "Ensemble x5",
                    "16.77",
                    "75.03"
                ]
            ],
            "title": "Table 3: Results on the Test Set"
        },
        "insight": " Even the ensemble of 5 models did not result in significant differences especially in BLEU scores."
    },
    {
        "id": "63",
        "table": {
            "header": [
                "Models",
                "BLEU"
            ],
            "rows": [
                [
                    "MT as PE",
                    "76.76"
                ],
                [
                    "Processed MT",
                    "76.61"
                ],
                [
                    "Base",
                    "76.91 \u223c 77.13"
                ],
                [
                    "Gaussian",
                    "76.94 \u223c 77.08"
                ],
                [
                    "Uniform",
                    "77.01 \u223c 77.10"
                ],
                [
                    "Ensemble x5",
                    "[BOLD] 77.22"
                ]
            ],
            "title": "Table 2: BLEU Scores on the Development Set"
        },
        "insight": "Table 2 shows that the performance got slightly hurt (comparing \"Processed MT\" with \"MT as PE\") with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size. The multi-source transformer (Base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task. [CONTINUE] Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46)."
    },
    {
        "id": "64",
        "table": {
            "header": [
                "Condition",
                "Accuracy (%) Corr",
                "Accuracy (%) Gram",
                "Time (s)",
                "Diversity Distinct",
                "Diversity PINC"
            ],
            "rows": [
                [
                    "Baseline",
                    "74",
                    "97",
                    "36",
                    "99",
                    "68"
                ],
                [
                    "Lexical Examples",
                    "[BOLD] 90\u2020",
                    "98",
                    "[BOLD] 27",
                    "[BOLD] 93",
                    "[BOLD] 55\u2020"
                ],
                [
                    "Mixed Examples",
                    "[BOLD] 89\u2020",
                    "96",
                    "36",
                    "[BOLD] 87\u2020",
                    "[BOLD] 58\u2020"
                ],
                [
                    "No Examples",
                    "84",
                    "96",
                    "30",
                    "95",
                    "63"
                ],
                [
                    "Novelty Bonus",
                    "72",
                    "96",
                    "30",
                    "99",
                    "69"
                ],
                [
                    "No Bonus",
                    "78",
                    "94",
                    "28",
                    "99",
                    "66"
                ],
                [
                    "One Paraphrase",
                    "82",
                    "[BOLD] 89",
                    "38",
                    "96",
                    "65"
                ],
                [
                    "Chain",
                    "68",
                    "94",
                    "[BOLD] 25",
                    "98",
                    "[BOLD] 74"
                ],
                [
                    "Answers",
                    "80",
                    "94",
                    "[BOLD] 29",
                    "96",
                    "65"
                ],
                [
                    "advising",
                    "78",
                    "94",
                    "31",
                    "97",
                    "70"
                ],
                [
                    "geoquery",
                    "77",
                    "[BOLD] 85\u2020",
                    "[BOLD] 25\u2020",
                    "[BOLD] 94",
                    "[BOLD] 63"
                ],
                [
                    "wsj",
                    "68",
                    "[BOLD] 90",
                    "[BOLD] 61\u2020",
                    "[BOLD] 94\u2020",
                    "[BOLD] 38\u2020"
                ],
                [
                    "ubuntu",
                    "[BOLD] 56\u2020",
                    "92",
                    "44",
                    "97",
                    "67"
                ]
            ],
            "title": "Table 1: Variation across conditions for a range of metrics (defined in \u00a7\u00a03.4). Bold indicates a statistically significant difference compared to the baseline at the 0.05 level, and a \u2020 indicates significance at the 0.01 level, both after applying the Holm\u2013-Bonferroni method across each row\u00a0Holm (1979)."
        },
        "insight": "We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, [CONTINUE] Our analysis shows that the most important factor is how workers are primed for a task, with the choice of examples and the prompt sentence affecting diversity and correctness significantly. [CONTINUE] There was relatively little variation in grammaticality or time across the conditions. [CONTINUE] Priming had a major impact, with the shift to lexical examples leading to a significant improvement in correctness, but much lower diversity. The surprising increase in correctness when providing no examples [CONTINUE] changing the incentives by providing either a bonus for novelty, or no bonus at all, did not substantially impact any of the metrics. [CONTINUE] Changing the number of paraphrases written by each worker did not significantly impact diversity (we worried that collecting more than one may lead to a decrease). [CONTINUE] the One Paraphrase condition did have lower grammaticality, [CONTINUE] Changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity. [CONTINUE] showing the answer to the question being para [CONTINUE] phrased did not significantly affect correctness or diversity,"
    },
    {
        "id": "65",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Verb  [BOLD] nmPU",
                "[BOLD] Verb  [BOLD] niPU",
                "[BOLD] Verb  [BOLD] F1",
                "[BOLD] Subject  [BOLD] nmPU",
                "[BOLD] Subject  [BOLD] niPU",
                "[BOLD] Subject  [BOLD] F1",
                "[BOLD] Object  [BOLD] nmPU",
                "[BOLD] Object  [BOLD] niPU",
                "[BOLD] Object  [BOLD] F1",
                "[BOLD] Frame  [BOLD] nmPU",
                "[BOLD] Frame  [BOLD] niPU",
                "[BOLD] Frame  [BOLD] F1"
            ],
            "rows": [
                [
                    "Triframes Watset",
                    "42.84",
                    "88.35",
                    "[BOLD] 57.70",
                    "54.22",
                    "81.40",
                    "65.09",
                    "53.04",
                    "83.25",
                    "64.80",
                    "55.19",
                    "60.81",
                    "[BOLD] 57.87"
                ],
                [
                    "HOSG Cotterell et\u00a0al. ( 2017 )",
                    "44.41",
                    "68.43",
                    "53.86",
                    "52.84",
                    "74.53",
                    "61.83",
                    "54.73",
                    "74.05",
                    "62.94",
                    "55.74",
                    "50.45",
                    "52.96"
                ],
                [
                    "NOAC Egurnov et\u00a0al. ( 2017 )",
                    "20.73",
                    "88.38",
                    "33.58",
                    "57.00",
                    "80.11",
                    "[BOLD] 66.61",
                    "57.32",
                    "81.13",
                    "[BOLD] 67.18",
                    "44.01",
                    "63.21",
                    "51.89"
                ],
                [
                    "Triadic Spectral",
                    "49.62",
                    "24.90",
                    "33.15",
                    "50.07",
                    "41.07",
                    "45.13",
                    "50.50",
                    "41.82",
                    "45.75",
                    "52.05",
                    "28.60",
                    "36.91"
                ],
                [
                    "Triadic  [ITALIC] k-Means",
                    "[BOLD] 63.87",
                    "23.16",
                    "33.99",
                    "[BOLD] 63.15",
                    "38.20",
                    "47.60",
                    "[BOLD] 63.98",
                    "37.43",
                    "47.23",
                    "[BOLD] 63.64",
                    "24.11",
                    "34.97"
                ],
                [
                    "LDA-Frames Materna ( 2013 )",
                    "26.11",
                    "66.92",
                    "37.56",
                    "17.28",
                    "83.26",
                    "28.62",
                    "20.80",
                    "90.33",
                    "33.81",
                    "18.80",
                    "71.17",
                    "29.75"
                ],
                [
                    "Triframes CW",
                    "7.75",
                    "6.48",
                    "7.06",
                    "3.70",
                    "14.07",
                    "5.86",
                    "51.91",
                    "76.92",
                    "61.99",
                    "21.67",
                    "26.50",
                    "23.84"
                ],
                [
                    "Singletons",
                    "0.00",
                    "25.23",
                    "0.00",
                    "0.00",
                    "25.68",
                    "0.00",
                    "0.00",
                    "20.80",
                    "0.00",
                    "32.34",
                    "22.15",
                    "26.29"
                ],
                [
                    "Whole",
                    "3.62",
                    "[BOLD] 100.0",
                    "6.98",
                    "2.41",
                    "[BOLD] 98.41",
                    "4.70",
                    "2.38",
                    "[BOLD] 100.0",
                    "4.64",
                    "2.63",
                    "[BOLD] 99.55",
                    "5.12"
                ]
            ],
            "title": "Table 3: Frame evaluation results on the triples from the FrameNet 1.7 corpus\u00a0Baker et\u00a0al. (1998). The results are sorted by the descending order of the Frame F1-score. Best results are boldfaced."
        },
        "insight": "In Table 3 and Figure 1, the results of the experiment are presented. Triframes based on WATSET clustering outperformed the other methods on both Verb F1 and overall Frame F1. [CONTINUE] the use of the WATSET fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results (see Table 3)."
    },
    {
        "id": "66",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] nmPU",
                "[BOLD] niPU",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "LDA-Frames",
                    "[BOLD] 52.60",
                    "45.84",
                    "[BOLD] 48.98"
                ],
                [
                    "Triframes Watset",
                    "40.05",
                    "62.09",
                    "48.69"
                ],
                [
                    "NOAC",
                    "37.19",
                    "64.09",
                    "47.07"
                ],
                [
                    "HOSG",
                    "38.22",
                    "43.76",
                    "40.80"
                ],
                [
                    "Triadic Spectral",
                    "35.76",
                    "38.96",
                    "36.86"
                ],
                [
                    "Triadic  [ITALIC] k-Means",
                    "52.22",
                    "27.43",
                    "35.96"
                ],
                [
                    "Triframes CW",
                    "18.05",
                    "12.72",
                    "14.92"
                ],
                [
                    "Whole",
                    "24.14",
                    "[BOLD] 79.09",
                    "36.99"
                ],
                [
                    "Singletons",
                    "0.00",
                    "27.21",
                    "0.00"
                ]
            ],
            "title": "Table 4: Evaluation results on the dataset of polysemous verb classes by Korhonen et\u00a0al. (2003)."
        },
        "insight": "Table 4 presents results on the second dataset for the best models identified on the first dataset. The LDA-Frames yielded the best results with our approach performing comparably in terms of the F1-score."
    },
    {
        "id": "67",
        "table": {
            "header": [
                "Method",
                "Max F1",
                "11-point IAP"
            ],
            "rows": [
                [
                    "Baseline",
                    "54.92",
                    "50.99"
                ],
                [
                    "RR",
                    "62.94",
                    "59.62"
                ],
                [
                    "RR_FR_1step",
                    "68.35",
                    "64.42"
                ],
                [
                    "RR_FR_2step",
                    "69.72",
                    "67.29"
                ]
            ],
            "title": "Table 1: French-English Performance. Baseline indicates current state of the art performance."
        },
        "insight": "Table 1 shows these performance measures for French-English, [CONTINUE] using global structure greatly improves upon the state of the art baseline performance."
    },
    {
        "id": "68",
        "table": {
            "header": [
                "Method",
                "Max F1",
                "11-point IAP"
            ],
            "rows": [
                [
                    "Baseline",
                    "55.08",
                    "51.35"
                ],
                [
                    "RR",
                    "60.88",
                    "58.79"
                ],
                [
                    "RR_FR_1step",
                    "65.87",
                    "63.55"
                ],
                [
                    "RR_FR_2step",
                    "65.76",
                    "65.26"
                ]
            ],
            "title": "Table 4: French-English Performance (large data). Baseline indicates state of the art performance."
        },
        "insight": "Tables 4 through 6 show the summary metrics for the three language pairs for the large data experiments. [CONTINUE] We can see that the reverse rank and forward rank methods of taking into account the global structure of interactions among predictions is still helpful, providing large improvements in performance even in this challenging large data condition over strong state of the art baselines"
    },
    {
        "id": "69",
        "table": {
            "header": [
                "[BOLD] Data set",
                "[BOLD] Size",
                "[BOLD] Entity Types",
                "[BOLD] Description"
            ],
            "rows": [
                [
                    "CADEC",
                    "120,341",
                    "Adverse Drug Event, Disease, Drug, Finding, Symptom",
                    "Posts taken from AskaPatient, which is a forum where consumers can discuss their experiences with medications."
                ],
                [
                    "CoNLL2003",
                    "301,418",
                    "Person, Organization, Location, Miscellany",
                    "Newswire from the Reuters RCV1 corpus."
                ],
                [
                    "CRAFT",
                    "561,015",
                    "Cell, Chemical entity, Biological taxa, Protein, Biomacromolecular sequence, Entrez gene, Biological process and molecular function, Cellular component",
                    "Full-length, open-access journal articles about biology."
                ],
                [
                    "JNLPBA",
                    "593,590",
                    "Protein, DNA, RNA, Cell line and Cell type",
                    "Abstract of journal articles about biology."
                ],
                [
                    "ScienceIE",
                    "99,555",
                    "Process (including methods, equipment), Task and Material (including corpora, physical materials)",
                    "Journal articles about Computer Science, Material Sciences and Physics."
                ],
                [
                    "Wetlab",
                    "220,618",
                    "Action, 9 object-based (Amount, Concentration, Device, Location, Method, Reagent, Speed, Temperature, Time) entity types, 5 measure-based (Numerical, Generic-Measure, Size, pH, Measure-Type) and 3 other (Mention, Modifier, Seal) types",
                    "Protocols written by researchers about conducting biology and chemistry experiments."
                ]
            ],
            "title": "Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens."
        },
        "insight": "five source and six target NER data sets, each selected to provide a range of fields (i.e., biology, computer science, medications, local business) and tenors (i.e., encyclopedia articles, journal articles, experimental protocols, online reviews). [CONTINUE] We use five data sets as source data, covering a range of fields (i.e., clinical, biomedical, local business and Wiki with diverse fields) and tenors (i.e., popular reporting, notes, scholarly publications, online reviews and encyclopedia). [CONTINUE] Details of these target data are listed in Table 2."
    },
    {
        "id": "70",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Word vectors",
                "[BOLD] LMs"
            ],
            "rows": [
                [
                    "TVC",
                    "00.454",
                    "00.666"
                ],
                [
                    "TVcC",
                    "00.469",
                    "00.739"
                ],
                [
                    "PPL",
                    "-0.398",
                    "-0.618"
                ],
                [
                    "WVV",
                    "-0.406",
                    "-0.747"
                ]
            ],
            "title": "Table 4: Correlation coefficients between similarity measures and the effectiveness of pretrained models. The coefficients vary between -1 (negative correlation) and 1 (positive correlation). Zero means no correlation."
        },
        "insight": "The results in Table 4 show that our proposed similarity measures are predictive of the effectiveness of the pretraining data. [CONTINUE] VCcR is the most informative factor in predicting the effectiveness of pretrained word vectors given a target data set."
    },
    {
        "id": "71",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Word vectors GloVe",
                "[BOLD] Word vectors Ours",
                "[BOLD] LMs ELMo",
                "[BOLD] LMs Ours"
            ],
            "rows": [
                [
                    "CADEC",
                    "[BOLD] 70.30",
                    "70.27",
                    "[BOLD] 71.91",
                    "70.46"
                ],
                [
                    "CoNLL2003",
                    "[BOLD] 90.25",
                    "86.36",
                    "[BOLD] 91.34",
                    "89.78"
                ],
                [
                    "CRAFT",
                    "74.22",
                    "[BOLD] 75.45",
                    "[BOLD] 75.77",
                    "75.45"
                ],
                [
                    "JNLPBA",
                    "73.19",
                    "[BOLD] 73.24",
                    "73.65",
                    "[BOLD] 74.29"
                ],
                [
                    "ScienceIE",
                    "37.10",
                    "[BOLD] 37.91",
                    "41.15",
                    "[BOLD] 42.07"
                ],
                [
                    "WetLab",
                    "[BOLD] 79.15",
                    "78.93",
                    "79.57",
                    "[BOLD] 79.62"
                ]
            ],
            "title": "Table 5: Comparison between our best performance pretrained models and the publicly available ones, which are pretrained on much larger corpora."
        },
        "insight": "We find that word vectors and LMs pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources (Table 5). [CONTINUE] On JNLPBA, ScienceIE and Wetlab, LMs pretrained on the small similar source perform better, while word vectors pretrained on the small similar source perform better on CRAFT, JNLPBA, and ScienceIE."
    },
    {
        "id": "72",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] ScienceIE  [BOLD] Def",
                "[BOLD] ScienceIE  [BOLD] Opt",
                "[BOLD] WetLab  [BOLD] Def",
                "[BOLD] WetLab  [BOLD] Opt"
            ],
            "rows": [
                [
                    "1BWB",
                    "34.40",
                    "34.57",
                    "78.66",
                    "79.12"
                ],
                [
                    "MIMIC",
                    "31.23",
                    "34.14",
                    "78.68",
                    "78.65"
                ],
                [
                    "PubMed",
                    "[BOLD] 37.91",
                    "[BOLD] 38.86",
                    "[BOLD] 78.93",
                    "[BOLD] 79.28"
                ],
                [
                    "Wiki",
                    "36.15",
                    "35.63",
                    "78.45",
                    "78.99"
                ],
                [
                    "Yelp",
                    "33.92",
                    "34.25",
                    "78.48",
                    "78.78"
                ]
            ],
            "title": "Table 6: Impact of hyper-parameter setting on the effectiveness of pretrained word vectors. \u2018Opt\u2019 is hyper-parameter setting proposed in\u00a0(Chiu et\u00a0al., 2016), whereas \u2018Def\u2019 is the default setting in word2vec."
        },
        "insight": "Our results suggest that this hyper-parameter setting can overall (except Wiki-ScienceIE and MIMIC-WetLab pairs) produce better performance compare to the default setting (Table 6)."
    },
    {
        "id": "73",
        "table": {
            "header": [
                "[BOLD] type of discrepancy",
                "[BOLD] frequency"
            ],
            "rows": [
                [
                    "T-V distinction",
                    "67%"
                ],
                [
                    "speaker/addressee gender:",
                    "[EMPTY]"
                ],
                [
                    "same speaker",
                    "22%"
                ],
                [
                    "different speaker",
                    "09%"
                ],
                [
                    "other",
                    "02%"
                ]
            ],
            "title": "Table 3: Types of discrepancy in context-agnostic translation caused by deixis (excluding anaphora)"
        },
        "insight": "Most errors in our annotated corpus are related to person deixis, specifically gender marking in the Russian translation, and the T-V distinction between informal and formal you (Latin \"tu\" and \"vos\"). [CONTINUE] From Table 3, we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of T-V forms when translating second person pronouns."
    },
    {
        "id": "74",
        "table": {
            "header": [
                "Model",
                "EE",
                "TE",
                "0-100",
                "100-500",
                "500+",
                "All"
            ],
            "rows": [
                [
                    "RC (random initializations)",
                    "44.5",
                    "64.4",
                    "40.5",
                    "57.8",
                    "63.4",
                    "53.4"
                ],
                [
                    "RC (SG initializations)",
                    "49.5",
                    "68.6",
                    "44.1",
                    "62.5",
                    "67.0",
                    "57.3"
                ],
                [
                    "RC (SG fixed)",
                    "48.9",
                    "[BOLD] 68.7",
                    "44.1",
                    "62.7",
                    "67.3",
                    "57.6"
                ],
                [
                    "RC + SG",
                    "51.6",
                    "67.4",
                    "[BOLD] 46.4",
                    "62.5",
                    "66.8",
                    "58.2"
                ],
                [
                    "RC + SGLR",
                    "[BOLD] 51.7",
                    "68.5",
                    "45.3",
                    "[BOLD] 63.0",
                    "[BOLD] 68.1",
                    "[BOLD] 58.4"
                ]
            ],
            "title": "Table 1: Evaluation on subsets of THYME Dev (in F-measure). The subsets of Event\u00d7Event (EE) and Timex3\u00d7Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively. The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively)."
        },
        "insight": "In Table 1 we can see that also for our model, EE relations are harder to recognize than the TE relations, as all models achieve higher scores for TE compared to EE relations. [CONTINUE] What is interesting to see is that when training with the combined loss (SG or SGLR) we obtain a clear improvement on the more difficult EE relations, and perform slightly worse on TE relations compared to using pre-trained embeddings (the three upper settings). [CONTINUE] What can be observed is that the RC+SG model performs best for low-frequency words, and RC+SGLR performs [CONTINUE] best for the higher frequency ranges. [CONTINUE] When evaluating on the full Dev set, both combined loss settings outperform the baselines consistently."
    },
    {
        "id": "75",
        "table": {
            "header": [
                "Model",
                "P",
                "R",
                "F"
            ],
            "rows": [
                [
                    "[ITALIC] With specialized resources:",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Best Clinical TempEval (2016)",
                    "58.8",
                    "55.9",
                    "57.3"
                ],
                [
                    "Lin et al. (2016)",
                    "66.9",
                    "53.4",
                    "59.4"
                ],
                [
                    "Leeuwenberg et al. (2017)",
                    "-",
                    "-",
                    "60.8"
                ],
                [
                    "Tourille et al. (2017)",
                    "65.7",
                    "57.5",
                    "61.3"
                ],
                [
                    "Lin et al. (2017)",
                    "66.2",
                    "58.5",
                    "62.1"
                ],
                [
                    "[ITALIC] No specialized resources:",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "RC (random initialization)",
                    "67.9",
                    "52.1",
                    "58.9\u00b10.2"
                ],
                [
                    "RC (SG initialization)",
                    "[BOLD] 71.2",
                    "52.0",
                    "60.0\u00b11.2"
                ],
                [
                    "RC (SG fixed)",
                    "68.9",
                    "54.6",
                    "60.9\u00b10.8"
                ],
                [
                    "RC + SG",
                    "66.2",
                    "[BOLD] 59.7",
                    "[BOLD] 62.8\u00b10.2"
                ],
                [
                    "RC + SGLR",
                    "68.7",
                    "57.5",
                    "62.5\u00b10.3"
                ]
            ],
            "title": "Table 2: THYME test set results, reporting precision (P), recall (R) and F-measure (F), macro-averaged over three runs. The standard deviation for F is also given."
        },
        "insight": "Table 2 shows that initializing the model with the pre-trained embeddings gives a significant 4 1.1 point increase in F-measure compared to random initialization, due to an increase in precision. [CONTINUE] Fixing the embeddings gives slightly better performance than using them as initialization, an increase of 0.9 point in F-measure, mostly due to higher recall. [CONTINUE] When extending the loss with the SGLR loss, we gain6 1.6 in F-measure compared to fixing the word embeddings, and also surpass the state of the art by 0.4 even without specialized resources. [CONTINUE] If we train our model using the SG loss extension we obtain the best results, and gain6 1.9 points in F-measure compared to using pre-trained fixed word embeddings. [CONTINUE] This setting also exceeds the state of the art (Lin et al., 2017) by 0.7 points in F-measure, due to a gain of 1.2 points in recall, again without using any specialized clinical NLP tools for feature engineering, in contrast to all state-of-the-art baselines."
    },
    {
        "id": "76",
        "table": {
            "header": [
                "Error Type",
                "RC + SG",
                "RC (SG fixed)",
                "RC (SG init.)"
            ],
            "rows": [
                [
                    "Cross-Clause Relations (CCR)",
                    "42",
                    "39",
                    "36"
                ],
                [
                    "Infrequent Arguments (<10)",
                    "11",
                    "15",
                    "26"
                ],
                [
                    "Frequent Arguments (>250)",
                    "37",
                    "50",
                    "40"
                ],
                [
                    "Mistake in Ground-Truth",
                    "10",
                    "8",
                    "5"
                ],
                [
                    "Other",
                    "21",
                    "15",
                    "28"
                ]
            ],
            "title": "Table 3: Error analysis on 50 FP and 50 FN (random from test) for different settings. Clause boundaries are: newlines and sub-clause or sentence boundaries. Error categories are not mutually exclusive."
        },
        "insight": "From Table 3 we can see that all models have difficulties with distant relations that cross sentence or clause boundaries (CCR). [CONTINUE] Furthermore, arguments that are frequent in the supervised data (> 250) are a dominant error category. [CONTINUE] Furthermore it can be noticed that RC+SG has less errors for infrequent arguments (< 10) in the supervised data."
    },
    {
        "id": "77",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Sentiment",
                "[BOLD] Stance",
                "[BOLD] Hate"
            ],
            "rows": [
                [
                    "Frequency",
                    "0.332",
                    "0.397",
                    "0.057"
                ],
                [
                    "LING",
                    "0.676",
                    "0.569",
                    "0.624"
                ],
                [
                    "LING+random",
                    "0.657",
                    "0.571",
                    "0.600"
                ],
                [
                    "LING+PV",
                    "0.671",
                    "0.601\u2217",
                    "0.667\u2217"
                ],
                [
                    "LING+N2V",
                    "0.672",
                    "0.629\u2217\u22c4",
                    "0.656\u2217"
                ],
                [
                    "LING+GAT",
                    "0.666",
                    "0.640\u2217\u22c4\u2020",
                    "0.674\u2217\u22c4\u2020"
                ]
            ],
            "title": "Table 2: Results for all the models on the three datasets in our experiment. Marked with \u2217 are the results which significantly improve over LING and LING+random (p<0.05, also for the following results); \u22c4 indicates a significant improvement over LING+PV; \u2020 a significant improvement over LING+N2V."
        },
        "insight": "In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.9 We use the unpaired Welch's t test to check for statistically significant difference between models. [CONTINUE] The results show that social information helps improve the performance on Stance and Hate Speech detection, while it has no effect for Sentiment Analysis. [CONTINUE] LING+random never improves over LING: [CONTINUE] We find that both PV and N2V user representations lead to an improvement over LING. [CONTINUE] where LING+N2V outperforms LING+PV, [CONTINUE] while for Hate Speech the performance of the two models is comparable (the difference between LING+PV and LING+N2V is not statistically significative due to the high variance of the LING+PV results - see extended results table in the supplementary material). [CONTINUE] our model outperforms any other model on both Stance and Hate Speech detection."
    },
    {
        "id": "78",
        "table": {
            "header": [
                "[BOLD] Languages  [BOLD] EN\u2013BG",
                "[BOLD] # sents 306,380",
                "[BOLD] Languages  [BOLD] EN\u2013IT",
                "[BOLD] # sents 1,297,635"
            ],
            "rows": [
                [
                    "[BOLD] EN\u2013CS",
                    "491,848",
                    "[BOLD] EN\u2013LT",
                    "481,570"
                ],
                [
                    "[BOLD] EN\u2013DA",
                    "1,421,197",
                    "[BOLD] EN\u2013LV",
                    "487,287"
                ],
                [
                    "[BOLD] EN\u2013DE",
                    "1,296,843",
                    "[BOLD] EN\u2013NL",
                    "1,419,359"
                ],
                [
                    "[BOLD] EN\u2013EL",
                    "921,540",
                    "[BOLD] EN\u2013PL",
                    "478,008"
                ],
                [
                    "[BOLD] EN\u2013ES",
                    "1,419,507",
                    "[BOLD] EN\u2013PT",
                    "1,426,043"
                ],
                [
                    "[BOLD] EN\u2013ET",
                    "494,645",
                    "[BOLD] EN\u2013RO",
                    "303,396"
                ],
                [
                    "[BOLD] EN\u2013FI",
                    "1,393,572",
                    "[BOLD] EN\u2013SK",
                    "488,351"
                ],
                [
                    "[BOLD] EN\u2013FR",
                    "1,440,620",
                    "[BOLD] EN\u2013SL",
                    "479,313"
                ],
                [
                    "[BOLD] EN\u2013HU",
                    "251,833",
                    "[BOLD] EN\u2013SV",
                    "1,349,472"
                ]
            ],
            "title": "Table 1: Overview of annotated parallel sentences per language pair"
        },
        "insight": "An overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in Table 1. [CONTINUE] Overview of annotated parallel sentences per language pair"
    },
    {
        "id": "79",
        "table": {
            "header": [
                "[BOLD] type of discrepancy",
                "[BOLD] frequency"
            ],
            "rows": [
                [
                    "wrong morphological form",
                    "66%"
                ],
                [
                    "wrong verb (VP-ellipsis)",
                    "20%"
                ],
                [
                    "other error",
                    "14%"
                ]
            ],
            "title": "Table 4: Types of discrepancy in context-agnostic translation caused by ellipsis"
        },
        "insight": "We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. [CONTINUE] From Table 4, we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature, hence we construct individual test sets for each of them."
    },
    {
        "id": "80",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Dev Acc.",
                "[BOLD] F - M"
            ],
            "rows": [
                [
                    "[BOLD] M.1 (BoW+LogReg)",
                    "0.827",
                    "0.035**"
                ],
                [
                    "[BOLD] M.2 (BiLSTM)",
                    "0.841",
                    "0.077**"
                ],
                [
                    "[BOLD] M.3 (BERT)",
                    "0.930",
                    "-0.040**"
                ]
            ],
            "title": "Table 1: Results. Dev Acc. represents accuracy on SST-2 dev set. F - M represents difference between means of predicted positive class probabilities for sentences with female nouns and sentences with male nouns. ** denotes statistical significance with p<0.01 (after applying Bonferroni correction)."
        },
        "insight": "The main results of our experiments are shown in Table 1. [CONTINUE] We notice that M.1 (Bag-of-words + Logistic Regression) and M.2 (BiLSTM) show a statistically significant difference between the two genders, with higher predicted positive class probabilities for sentences with female nouns. [CONTINUE] On the contrary, M.3 (BERT) shows that sentences with male nouns have a statistically significant higher predicted positive class probability than sentences with female nouns."
    },
    {
        "id": "81",
        "table": {
            "header": [
                "Test Condition",
                "[BOLD] Cosine Similarity \u00b1 SD  (\u2191)  [BOLD] RGB",
                "[BOLD] Cosine Similarity \u00b1 SD  (\u2191)  [BOLD] WM18",
                "[BOLD] Cosine Similarity \u00b1 SD  (\u2191)  [BOLD] HSV",
                "[BOLD] Cosine Similarity \u00b1 SD  (\u2191)  [BOLD] Ensemble",
                "[BOLD] Cosine Similarity \u00b1 SD  (\u2191)  [BOLD] WM18\u2217"
            ],
            "rows": [
                [
                    "Seen Pairings",
                    "0.954\u00b10.001",
                    "0.953\u00b10.000",
                    "0.934\u00b10.089",
                    "[BOLD] 0.954\u00b10.0",
                    "0.68"
                ],
                [
                    "Unseen Pairings",
                    "0.799\u00b10.044",
                    "0.771\u00b10.032",
                    "[BOLD] 0.843\u00b10.144",
                    "0.797\u00b10.0",
                    "0.68"
                ],
                [
                    "Unseen Ref. Color",
                    "0.781\u00b10.015",
                    "0.767\u00b10.010",
                    "[BOLD] 0.945\u00b10.019",
                    "0.804\u00b10.0",
                    "0.40"
                ],
                [
                    "Unseen Modifiers",
                    "0.633\u00b10.042",
                    "0.637\u00b10.032",
                    "[BOLD] 0.724\u00b10.131",
                    "0.629\u00b10.0",
                    "0.41"
                ],
                [
                    "Fully Unseen",
                    "0.370\u00b10.029",
                    "0.358\u00b10.038",
                    "[BOLD] 0.919\u00b10.026",
                    "0.445\u00b10.0",
                    "-0.21"
                ],
                [
                    "Overall",
                    "0.858\u00b10.006",
                    "0.856\u00b10.003",
                    "[BOLD] 0.911\u00b10.057",
                    "0.868\u00b10.0",
                    "0.65"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Delta-E \u00b1 SD  (\u2193)",
                    "[BOLD] Delta-E \u00b1 SD  (\u2193)",
                    "[BOLD] Delta-E \u00b1 SD  (\u2193)",
                    "[BOLD] Delta-E \u00b1 SD  (\u2193)",
                    "[BOLD] Delta-E \u00b1 SD  (\u2193)"
                ],
                [
                    "Test Condition",
                    "RGB",
                    "WM18",
                    "HSV",
                    "Ensemble",
                    "WM18\u2217"
                ],
                [
                    "Seen Pairings",
                    "[BOLD] 3.121\u00b10.027",
                    "3.188\u00b10.062",
                    "5.380\u00b14.846",
                    "4.093\u00b10.1",
                    "6.1"
                ],
                [
                    "Unseen Pairings",
                    "6.454\u00b10.233",
                    "6.825\u00b10.093",
                    "11.701\u00b13.358",
                    "[BOLD] 5.873\u00b10.0",
                    "7.9"
                ],
                [
                    "Unseen Ref. Color",
                    "7.456\u00b10.184",
                    "7.658\u00b10.363",
                    "10.429\u00b12.523",
                    "[BOLD] 7.171\u00b10.0",
                    "11.4"
                ],
                [
                    "Unseen Modifiers",
                    "13.288\u00b11.082",
                    "13.891\u00b11.077",
                    "14.183\u00b15.175",
                    "10.927\u00b10.0",
                    "[BOLD] 10.5"
                ],
                [
                    "Fully Unseen",
                    "13.859\u00b10.874",
                    "14.516\u00b10.587",
                    "[BOLD] 12.432\u00b12.170",
                    "13.448\u00b10.0",
                    "15.9"
                ],
                [
                    "Overall",
                    "[BOLD] 5.412\u00b10.169",
                    "5.595\u00b10.128",
                    "7.487\u00b13.940",
                    "5.777\u00b10.0",
                    "6.8"
                ]
            ],
            "title": "Table 1: Average cosine similarity score and Delta-E distance over 5 runs. A smaller Delta-E distance means a less significant difference between two colors. Bold: best performance. Hard: the hard ensemble model. WM18\u2217: the performance from WM18 paper. See Supplementary Material for example outputs and ensemble analysis."
        },
        "insight": "Because of the small size of the dataset, we report the average performance over 5 runs with different random seeds. [CONTINUE] Table 1 shows the results. Compared with WM18, our RGB model outperforms under all conditions. [CONTINUE] According to the cosine similarity, the HSV model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space). However for Delta-E, the RGB model and ensemble perform better. Unlike cosine, Delta-E is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors. Accordingly the HSV model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions. Over- all the ensemble does well according to both met rics, and has the best performance for several test conditions with Delta-E."
    },
    {
        "id": "82",
        "table": {
            "header": [
                "[BOLD] model",
                "[BOLD] BLEU"
            ],
            "rows": [
                [
                    "baseline (1.5m)",
                    "29.10"
                ],
                [
                    "baseline (6m)",
                    "[BOLD] 32.40"
                ],
                [
                    "concat",
                    "31.56"
                ],
                [
                    "s-hier-to-2.tied",
                    "26.68"
                ],
                [
                    "CADec",
                    "[BOLD] 32.38"
                ]
            ],
            "title": "Table 6: BLEU scores. CADec trained with p=0.5. Scores for CADec are not statistically different from the baseline (6m)."
        },
        "insight": "BLEU scores for our model and the baselines are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. We observe that our model is no worse in BLEU than the baseline despite the second-pass model [CONTINUE] being trained only on a fraction of the data. In contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 BLEU below the context-agnostic baseline and our model when using all 3 context sentences. CADec's performance remains the same independently from the number of context sentences (1, 2 or 3) as measured with BLEU. s-hier-to-2.tied performs worst in terms of BLEU, but note that this is a shallow recurrent model, while others are Transformer-based. It also suffers from the asymmetric data setting, like the concatenation baseline."
    },
    {
        "id": "83",
        "table": {
            "header": [
                "All Labels",
                "Orig F&C",
                "Train 587",
                "Dev 5,418",
                "Test 6,007",
                "All 12,012"
            ],
            "rows": [
                [
                    "All Labels",
                    "No-Leak F&C",
                    "712",
                    "3,000",
                    "4,497",
                    "8,209"
                ],
                [
                    "Subset Labels",
                    "Orig F&C",
                    "361",
                    "3,311",
                    "3,650",
                    "7,322"
                ],
                [
                    "Subset Labels",
                    "Clean F&C",
                    "173",
                    "1,268",
                    "1,523",
                    "2,964"
                ]
            ],
            "title": "Table 2: F&C dataset size. All Labels represent the original dataset with all the labels. Subset Labels are the subset labels which are inferable by the resource."
        },
        "insight": "we reorganized the train/dev/test sets, forming new splits, which we refer to as NO-LEAK F&C. The new split sizes can be found in Table 2. We re-ran the current models on NO-LEAK F&C and, as expected, we observe a drop of 5-6% in accuracy: from the original 76% accuracy on the dev/test sets, to 70% and 71% accuracy, respectively."
    },
    {
        "id": "84",
        "table": {
            "header": [
                "Model/Dataset",
                "F&C Clean Dev",
                "F&C Clean Test",
                "New Data Dev",
                "New Data Test"
            ],
            "rows": [
                [
                    "Majority",
                    "0.54",
                    "0.57",
                    "0.51",
                    "0.50"
                ],
                [
                    "Yang et al. (PCE LSTM)",
                    "[BOLD] 0.86",
                    "[BOLD] 0.87",
                    "0.60",
                    "0.57"
                ],
                [
                    "DoQ",
                    "0.78",
                    "0.77",
                    "[BOLD] 0.62",
                    "[BOLD] 0.62"
                ],
                [
                    "DoQ\u00a0+ 10-distance",
                    "0.78",
                    "0.77",
                    "[BOLD] 0.62",
                    "[BOLD] 0.62"
                ],
                [
                    "DoQ\u00a0+ 3-distance",
                    "0.81",
                    "0.80",
                    "[BOLD] 0.62",
                    "0.61"
                ]
            ],
            "title": "Table 4: Results on the noun comparison datasets."
        },
        "insight": "The left column of Table 4 presents results for the cleaned version of the Forbes and Choi (2017) dataset. [CONTINUE] Results on the new objects comparison dataset we created are shown in the rightmost column of Table 4. [CONTINUE] We get better results than previous methods on this dataset: 63% and 61% accuracy on the dev/test sets compared to 60% and 57%. These relatively low results on this new dataset indicate that it is more challenging."
    },
    {
        "id": "85",
        "table": {
            "header": [
                "Model",
                "Accuracy"
            ],
            "rows": [
                [
                    "Chance",
                    "0.5"
                ],
                [
                    "Bagherinezhad et al.",
                    "0.835"
                ],
                [
                    "Yang et al. (Transfer)",
                    "0.858"
                ],
                [
                    "DoQ",
                    "0.872"
                ],
                [
                    "DoQ\u00a0+ 10-distance",
                    "[BOLD] 0.877"
                ],
                [
                    "DoQ\u00a0+ 3-distance",
                    "0.858"
                ]
            ],
            "title": "Table 5: Results on the Relative dataset. Yang et\u00a0al. (2018) result was achieved by running their model on their training set, and using it as a transfer method on Relative. Finally, we present our own predictions, with different thresholds, which surpass previous work."
        },
        "insight": "noun comparatives is on RELATIVE (Bagherinezhad et al., 2016), presented in Table 5. [CONTINUE] We report the results of the original work, where the best score used a combination of visual and textual signals, achieving 83.5% accuracy. We also tested the method by Yang et al. (2018) on this dataset. [CONTINUE] The accuracy achieved by this method is 85.8%, surpassing the previous method by more than 2 points. We evaluated our method on this dataset, achieving a new state-of-the-art result of 87.7% accuracy with k = 10 as a filter method."
    },
    {
        "id": "86",
        "table": {
            "header": [
                "Method/Data",
                "Mass",
                "Length",
                "Speed",
                "Currency",
                "All"
            ],
            "rows": [
                [
                    "Indian Annotators",
                    "0.61",
                    "0.79",
                    "0.77",
                    "0.58",
                    "0.69"
                ],
                [
                    "US Annotators",
                    "-",
                    "-",
                    "-",
                    "0.76",
                    "-"
                ]
            ],
            "title": "Table 7: Intrinsic Evaluation. Accuracy of the number of objects which our proposed median fall into range of the object, given the dimension."
        },
        "insight": "The results of the intrinsic evaluation on a sample of DOQ are shown in Table 7. The total agreement is 69%, while the specific agreements for MASS, LENGTH, SPEED and CURRENCY are 61%, 79%, 77% and 58% respectively. [CONTINUE] We re-annotated the samples in the currency category with annotators from the U.S. and found a much higher agreement score: 76%."
    },
    {
        "id": "87",
        "table": {
            "header": [
                "Satire associated",
                "PCA Component  [BOLD] RC19",
                "Description First person singular pronoun incidence",
                "estimate 1.80",
                "std.error 0.41",
                "statistic 4.38",
                "***"
            ],
            "rows": [
                [
                    "Satire associated",
                    "[BOLD] RC5",
                    "Sentence length, number of words",
                    "0.66",
                    "0.18",
                    "3.68",
                    "***"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC15",
                    "Estimates of hypernymy for nouns",
                    "0.61",
                    "0.19",
                    "3.18",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC49",
                    "Word Concreteness",
                    "0.54",
                    "0.17",
                    "3.18",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC35",
                    "Ratio of casual particles to causal verbs",
                    "0.56",
                    "0.18",
                    "3.10",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC91",
                    "Text Easability PC Referential cohesion",
                    "0.45",
                    "0.16",
                    "2.89",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC20",
                    "Incidence score of gerunds",
                    "0.43",
                    "0.16",
                    "2.77",
                    "**"
                ],
                [
                    "Satire associated",
                    "RC32",
                    "Expanded temporal connectives incidence",
                    "0.44",
                    "0.16",
                    "2.75",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC9",
                    "Third person singular pronoun incidence",
                    "0.44",
                    "0.16",
                    "2.67",
                    "**"
                ],
                [
                    "Satire associated",
                    "RC43",
                    "Word length, number of letters",
                    "0.45",
                    "0.20",
                    "2.27",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC46",
                    "Verb phrase density",
                    "0.37",
                    "0.16",
                    "2.25",
                    "*"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC97",
                    "Coh-Metrix L2 Readability",
                    "0.34",
                    "0.16",
                    "2.16",
                    "*"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC61",
                    "Average word frequency for all words",
                    "0.50",
                    "0.24",
                    "2.13",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC84",
                    "The average givenness of each sentence",
                    "0.37",
                    "0.18",
                    "2.11",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC65",
                    "Text Easability PC Syntactic simplicity",
                    "0.38",
                    "0.18",
                    "2.08",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC50",
                    "Lexical diversity",
                    "0.37",
                    "0.18",
                    "2.05",
                    "*"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC30",
                    "Agentless passive voice density",
                    "-1.05",
                    "0.21",
                    "-4.96",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC73",
                    "Average word frequency for content words",
                    "-0.72",
                    "0.20",
                    "-3.68",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC59",
                    "Adverb incidence",
                    "-0.62",
                    "0.18",
                    "-3.43",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC55",
                    "Number of sentences",
                    "-0.79",
                    "0.26",
                    "-3.09",
                    "**"
                ],
                [
                    "Fake news associated",
                    "RC62",
                    "Causal and intentional connectives",
                    "-0.42",
                    "0.15",
                    "-2.72",
                    "**"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC34",
                    "LSA overlap between verbs",
                    "-0.35",
                    "0.16",
                    "-2.22",
                    "*"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC44",
                    "LSA overlap, adjacent sentences",
                    "-0.36",
                    "0.16",
                    "-2.16",
                    "*"
                ],
                [
                    "Fake news associated",
                    "RC47",
                    "Sentence length, number of words",
                    "-0.36",
                    "0.18",
                    "-2.03",
                    "*"
                ],
                [
                    "Fake news associated",
                    "RC89",
                    "LSA overlap, all sentences in paragraph",
                    "-0.34",
                    "0.17",
                    "-1.97",
                    "*"
                ],
                [
                    "[EMPTY]",
                    "(Intercept)",
                    "[EMPTY]",
                    "-0.54",
                    "0.19",
                    "-2.91",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05."
        },
        "insight": "as our dependent variable. Significant features of the logistic regression model are shown in Table 1 with the respective significance levels. We also run a step-wise backward elimination regression. [CONTINUE] Those components that are also significant in the step-wise model appear in bold. [CONTINUE] Observing the significant features, in bold in Table 1, we see a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences. Semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on Fuzzytrace theory (Reyna, 2012), a well-known theory of decision making under risk, gist representation of content drives individual's decision to spread misinformation online. Also among the significant features, we observe the causal connectives, that are proven to be important in text comprehension, and two indices related to the text easability and readability, both suggesting that satire articles are more sophisticated, or less easy to read, than fake news articles."
    },
    {
        "id": "88",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Headline only",
                    "0.46",
                    "0.89",
                    "0.61"
                ],
                [
                    "Text body only",
                    "0.78",
                    "0.52",
                    "0.62"
                ],
                [
                    "Headline + text body",
                    "[BOLD] 0.81",
                    "[BOLD] 0.75",
                    "[BOLD] 0.78"
                ]
            ],
            "title": "Table 2: Results of classification between fake news and satire articles using BERT pre-trained models, based on the headline, body and full text. Bold: best performing model. P: Precision, and R: Recall"
        },
        "insight": "text of a story, and in combination. Results are shown in Table 2. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance."
    },
    {
        "id": "89",
        "table": {
            "header": [
                "[EMPTY]",
                "total",
                "latest relevant context 1st",
                "latest relevant context 2nd",
                "latest relevant context 3rd"
            ],
            "rows": [
                [
                    "[BOLD] deixis",
                    "[BOLD] deixis",
                    "[BOLD] deixis",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "baseline",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0"
                ],
                [
                    "concat",
                    "[BOLD] 83.5",
                    "[BOLD] 88.8",
                    "[BOLD] 85.6",
                    "[BOLD] 76.4"
                ],
                [
                    "s-hier-to-2.tied",
                    "60.9",
                    "83.0",
                    "50.1",
                    "50.0"
                ],
                [
                    "CADec",
                    "81.6",
                    "84.6",
                    "84.4",
                    "75.9"
                ],
                [
                    "[BOLD] lexical cohesion",
                    "[BOLD] lexical cohesion",
                    "[BOLD] lexical cohesion",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "baseline",
                    "45.9",
                    "46.1",
                    "45.9",
                    "45.4"
                ],
                [
                    "concat",
                    "47.5",
                    "48.6",
                    "46.7",
                    "46.7"
                ],
                [
                    "s-hier-to-2.tied",
                    "48.9",
                    "53.0",
                    "46.1",
                    "45.4"
                ],
                [
                    "CADec",
                    "[BOLD] 58.1",
                    "[BOLD] 63.2",
                    "[BOLD] 52.0",
                    "[BOLD] 56.7"
                ]
            ],
            "title": "Table 7: Accuracy for deixis and lexical cohesion."
        },
        "insight": "For all tasks, we observe a large improvement from using context. For deixis, the concatenation model (concat) and CADec improve over the baseline by 33.5 and 31.6 percentage points, respectively. On the lexical cohesion test set, CADec shows a large improvement over the context-agnostic baseline (12.2 percentage points), while concat performs similarly to the baseline. [CONTINUE] When looking only at the scores where the latest relevant context is in the model's context window (column 2 in Table 7), s-hier-to-2.tied outperforms the concatenation baseline for lexical cohesion, but remains behind the performance of CADec."
    },
    {
        "id": "90",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Baseline",
                    "0.70",
                    "0.64",
                    "0.67"
                ],
                [
                    "Coh-Metrix",
                    "0.72",
                    "0.66",
                    "0.74*"
                ],
                [
                    "Pre-trained BERT",
                    "[BOLD] 0.81",
                    "[BOLD] 0.75",
                    "[BOLD] 0.78*"
                ]
            ],
            "title": "Table 3: Summary of results of classification between fake news and satire articles using the baseline Multinomial Naive Bayes method, the linguistic cues of text coherence and semantic representation with a pre-trained BERT model. Statistically significant differences with the baseline are marked with \u2019*\u2019. Bold: best performing model. P: Precision, and R: Recall. For Coh-Metrix, we report the mean Precision, Recall, and F1 on the test set."
        },
        "insight": "Table 3 provides a summary of the results. We compare the results of our methods of the pre-trained BERT, using both the headline and text body, and the Coh-Mertix approach, to the language-based baseline with Multinomial Naive Bayes from (Golbeck et al., 2018)2. Both the semantic cues with BERT and the linguistic cues with Coh-Metrix significantly outperform the baseline on the F1 score. The two-tailed paired t-test with a 0.05 significance level was used for testing statistical significance of performance differences. The best result is given by the BERT model. Overall, these results provide an answer to research question RQ1 regarding the existence of semantic and linguistic difference between fake news and satire."
    },
    {
        "id": "91",
        "table": {
            "header": [
                "Methods",
                "Aida-B"
            ],
            "rows": [
                [
                    "Q15-1011",
                    "88.7"
                ],
                [
                    "guorobust",
                    "89.0"
                ],
                [
                    "P16-1059",
                    "91.0"
                ],
                [
                    "K16-1025",
                    "91.5"
                ],
                [
                    "D17-1276",
                    "92.22\u00b10.14"
                ],
                [
                    "rel-norm",
                    "92.41\u00b10.19"
                ],
                [
                    "ment-norm",
                    "[BOLD] 93.07\u00b10.27"
                ],
                [
                    "ment-norm ( [ITALIC] K=1)",
                    "92.89\u00b10.21"
                ],
                [
                    "ment-norm (no pad)",
                    "92.37\u00b10.26"
                ]
            ],
            "title": "Table 1: F1 scores on AIDA-B (test set)."
        },
        "insight": "Table 1 shows micro F1 scores on AIDA-B of the SOTA methods and ours, which all use Wikipedia and YAGO mention-entity index. To our knowledge, ours are the only (unsupervisedly) inducing and employing more than one relations on this dataset. The others use only one relation, coreference, which is given by simple heuristics or supervised third-party resolvers. All four our models outperform any previous method, with ment-norm achieving the best results, 0.85% higher than that of Ganea and Hofmann (2017). [CONTINUE] The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role."
    },
    {
        "id": "92",
        "table": {
            "header": [
                "Methods",
                "MSNBC",
                "AQUAINT",
                "ACE2004",
                "CWEB",
                "WIKI",
                "Avg"
            ],
            "rows": [
                [
                    "milne2008learning",
                    "78",
                    "85",
                    "81",
                    "64.1",
                    "81.7",
                    "77.96"
                ],
                [
                    "D11-1072",
                    "79",
                    "56",
                    "80",
                    "58.6",
                    "63",
                    "67.32"
                ],
                [
                    "P11-1138",
                    "75",
                    "83",
                    "82",
                    "56.2",
                    "67.2",
                    "72.68"
                ],
                [
                    "cheng-roth:2013:EMNLP",
                    "90",
                    "[BOLD] 90",
                    "86",
                    "67.5",
                    "73.4",
                    "81.38"
                ],
                [
                    "guorobust",
                    "92",
                    "87",
                    "88",
                    "77",
                    "[BOLD] 84.5",
                    "[BOLD] 85.7"
                ],
                [
                    "D17-1276",
                    "93.7 \u00b1 0.1",
                    "88.5 \u00b1 0.4",
                    "88.5 \u00b1 0.3",
                    "[BOLD] 77.9 \u00b1 0.1",
                    "77.5 \u00b1 0.1",
                    "85.22"
                ],
                [
                    "rel-norm",
                    "92.2 \u00b1 0.3",
                    "86.7 \u00b1 0.7",
                    "87.9 \u00b1 0.3",
                    "75.2 \u00b1 0.5",
                    "76.4 \u00b1 0.3",
                    "83.67"
                ],
                [
                    "ment-norm",
                    "[BOLD] 93.9 \u00b1 0.2",
                    "88.3 \u00b1 0.6",
                    "[BOLD] 89.9 \u00b1 0.8",
                    "77.5 \u00b1 0.1",
                    "78.0 \u00b1 0.1",
                    "85.51"
                ],
                [
                    "ment-norm ( [ITALIC] K=1)",
                    "93.2 \u00b1 0.3",
                    "88.4 \u00b1 0.4",
                    "88.9 \u00b1 1.0",
                    "77.0 \u00b1 0.2",
                    "77.2 \u00b1 0.1",
                    "84.94"
                ],
                [
                    "ment-norm (no pad)",
                    "93.6 \u00b1 0.3",
                    "87.8 \u00b1 0.5",
                    "[BOLD] 90.0 \u00b1 0.3",
                    "77.0 \u00b1 0.2",
                    "77.3 \u00b1 0.3",
                    "85.13"
                ]
            ],
            "title": "Table 2: F1 scores on five out-domain test sets. Underlined scores show cases where the corresponding model outperforms the baseline."
        },
        "insight": "Table 2 shows micro F1 scores on 5 out-domain test sets. Besides ours, only Cheng and Roth (2013) employs several mention relations. Mentnorm achieves the highest F1 scores on MSNBC and ACE2004. On average, ment-norm's F1 score is 0.3% higher than that of Ganea and Hofmann (2017), but 0.2% lower than Guo and Barbosa (2016)'s. It is worth noting that Guo and Barbosa (2016) performs exceptionally well on WIKI, but substantially worse than ment-norm on all other datasets. Our other three models, however, have lower average F1 scores compared to the best previous model. [CONTINUE] The experimental results show that ment-norm outperforms rel-norm, and that mention padding plays an important role."
    },
    {
        "id": "93",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] UAR",
                "[ITALIC] \u03ba",
                "[ITALIC] \u03c1",
                "[ITALIC] eA",
                "[ITALIC] Ep."
            ],
            "rows": [
                [
                    "LSTM",
                    "0.78",
                    "0.85",
                    "0.91",
                    "[BOLD] 0.99",
                    "101"
                ],
                [
                    "BiLSTM",
                    "[BOLD] 0.78",
                    "[BOLD] 0.85",
                    "[BOLD] 0.92",
                    "[BOLD] 0.99",
                    "100"
                ],
                [
                    "LSTM+att",
                    "0.74",
                    "0.82",
                    "0.91",
                    "[BOLD] 0.99",
                    "101"
                ],
                [
                    "BiLSTM+att",
                    "0.75",
                    "0.83",
                    "0.91",
                    "[BOLD] 0.99",
                    "93"
                ],
                [
                    "Rach et al. ( 2017 )",
                    "0.55",
                    "0.68",
                    "0.83",
                    "0.94",
                    "-"
                ],
                [
                    "Ultes et al. ( 2015 )",
                    "0.55",
                    "-",
                    "-",
                    "0.89",
                    "-"
                ]
            ],
            "title": "Table 2: Performance of the proposed LSTM-based variants with the traditional cross-validation setup. Due to overlapping sub-dialogues in the train and test sets, the performance of the LSTM-based models achieve unrealistically high performance."
        },
        "insight": "The results are presented in Table 2. [CONTINUE] the results in Table 2 show very high performance, which is likely to further increase with ongoing training."
    },
    {
        "id": "94",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] ellipsis (infl.)",
                "[BOLD] ellipsis (VP)"
            ],
            "rows": [
                [
                    "baseline",
                    "53.0",
                    "28.4"
                ],
                [
                    "concat",
                    "[BOLD] 76.2",
                    "76.6"
                ],
                [
                    "s-hier-to-2.tied",
                    "66.4",
                    "65.6"
                ],
                [
                    "CADec",
                    "72.2",
                    "[BOLD] 80.0"
                ]
            ],
            "title": "Table 8: Accuracy on ellipsis test set."
        },
        "insight": "For ellipsis, both models improve substantially over the baseline (by 19-51 percentage points), with concat stronger for inflection tasks and CADec stronger for VPellipsis."
    },
    {
        "id": "95",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] UAR",
                "[ITALIC] \u03ba",
                "[ITALIC] \u03c1",
                "[ITALIC] eA",
                "[ITALIC] Ep."
            ],
            "rows": [
                [
                    "LSTM",
                    "0.51",
                    "0.63",
                    "0.78",
                    "0.93",
                    "8"
                ],
                [
                    "BiLSTM",
                    "0.53",
                    "0.63",
                    "0.78",
                    "0.93",
                    "8"
                ],
                [
                    "LSTM+att",
                    "0.52",
                    "0.63",
                    "0.79",
                    "0.92",
                    "40"
                ],
                [
                    "BiLSTM+att",
                    "[BOLD] 0.54",
                    "[BOLD] 0.65",
                    "[BOLD] 0.81",
                    "[BOLD] 0.94",
                    "40"
                ],
                [
                    "Rach et al. ( 2017 )",
                    "0.45",
                    "0.58",
                    "0.79",
                    "0.88",
                    "82"
                ],
                [
                    "Ultes et al. ( 2015 )",
                    "0.44",
                    "0.53",
                    "0.69",
                    "0.86",
                    "-"
                ]
            ],
            "title": "Table 3: Performance of the proposed LSTM-based variants with the dialogue-wise cross-validation setup. The models by Rach et al. (2017) and Ultes et al. (2015) have been re-implemented. The BiLSTM with attention mechanism performs best in all evaluation metrics."
        },
        "insight": "All results of these experiments are presented in Table 3 with the absolute improvement of the two main measures UAR and eA over the SVM-based approach"
    },
    {
        "id": "96",
        "table": {
            "header": [
                "This work Model",
                "This work Mean F1-measure"
            ],
            "rows": [
                [
                    "ner",
                    "81.07"
                ],
                [
                    "joint1",
                    "81.28"
                ],
                [
                    "joint2",
                    "81.84"
                ],
                [
                    "j_multi",
                    "[BOLD] 83.21"
                ],
                [
                    "Previous work",
                    "Previous work"
                ],
                [
                    "ext_m_feat",
                    "[BOLD] 83.47"
                ]
            ],
            "title": "Table 1: Evaluation of our models for NER performance with our dataset. We report F1-measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters."
        },
        "insight": "The results are shown in Table 1. We see that the mean NER performance increases in joint models. [CONTINUE] As one can see from the table, it achieved the best results compared to our joint models. However, we cannot confirm the difference between EXT M FEAT and J MULTI models as the calculated p is well above .05."
    },
    {
        "id": "97",
        "table": {
            "header": [
                "This work Model",
                "This work Mean Accuracy"
            ],
            "rows": [
                [
                    "md",
                    "88.61"
                ],
                [
                    "joint1",
                    "88.17"
                ],
                [
                    "joint2",
                    "86.86"
                ],
                [
                    "j_multi",
                    "88.05"
                ],
                [
                    "Previous work",
                    "Previous work"
                ],
                [
                    "yuret2006decision_lists",
                    "89.55"
                ],
                [
                    "shen2016role",
                    "[BOLD] 91.03"
                ]
            ],
            "title": "Table 2: Evaluation of our models for MD performance. As in the NER evaluation, we report accuracies over the test dataset averaged over 10 replications of the training."
        },
        "insight": "As can be seen from Table 2, we are very close to the state of the [CONTINUE] art MD performance even if we only trained with a low number of parameters as stated in the beginning of this section. We have to also note that in contrast with the NER task, the MD task did not enjoy a performance increase from joint learning."
    },
    {
        "id": "98",
        "table": {
            "header": [
                "Model",
                "Accuracy"
            ],
            "rows": [
                [
                    "fastText",
                    "68.7%"
                ],
                [
                    "GloVe",
                    "73.1%"
                ],
                [
                    "[ITALIC] BioELMo\u00a0Jin et al. ( 2019 )",
                    "78.2%"
                ],
                [
                    "[ITALIC] ESIMw/ [ITALIC] K\u00a0Lu et al. ( 2019 )",
                    "77.8%"
                ],
                [
                    "fastText w/ KG+Sentiment",
                    "73.67%"
                ],
                [
                    "GloVe w/ KG+Sentiment",
                    "74.46%"
                ],
                [
                    "BioELMo w/ KG",
                    "78.76%"
                ],
                [
                    "[BOLD] BioELMo w/ KG+Sentiment",
                    "[BOLD] 79.04%"
                ]
            ],
            "title": "Table 1: Performance of our models (bottom four) along with the state-of-the-art baseline models (top four). Baseline results for fastText, GloVe are obtained from Romanov and Shivade (2018). Adding knowledge graph information to the base models showed an absolute improvement of 4.97% in case of fastText and 1.36% in case of GloVe. The baseline model utilizing BioELMo as base embeddings\u00a0Jin et al. (2019) showed an accuracy of 78.2%. On adding knowledge graph information, we were able to improve these results to 78.76% and on further addition of sentiment information, the accuracy rose to 79.04%"
        },
        "insight": "We report accuracy as the performance metric. Table 1 represents the performance comparison of our proposed models and the baselines, which shows that incorporation of knowledge graph embeddings helps to improve the model performance. [CONTINUE] All results are summarized in Table 1."
    },
    {
        "id": "99",
        "table": {
            "header": [
                "[ITALIC] p",
                "[BOLD] BLEU",
                "[BOLD] deixis",
                "[BOLD] lex. c.",
                "[BOLD] ellipsis"
            ],
            "rows": [
                [
                    "[ITALIC] p=0",
                    "32.34",
                    "84.1",
                    "48.7",
                    "65 / 75"
                ],
                [
                    "[ITALIC] p=0.25",
                    "32.31",
                    "83.3",
                    "52.4",
                    "67 / 78"
                ],
                [
                    "[ITALIC] p=0.5",
                    "32.38",
                    "81.6",
                    "58.1",
                    "72 / 80"
                ],
                [
                    "[ITALIC] p=0.75",
                    "32.45",
                    "80.0",
                    "65.0",
                    "70 / 80"
                ]
            ],
            "title": "Table 9: Results for different probabilities of using corrupted reference at training time. BLEU for 3 context sentences. For ellipsis, we show inflection/VP scores."
        },
        "insight": "Results for different values of p are given in Table 9. All models have about the same BLEU, not statistically significantly different from the baseline, but they are quite different in terms of incorporating context. The denoising positively influences almost all tasks except for deixis, yielding the largest improvement on lexical cohesion."
    },
    {
        "id": "100",
        "table": {
            "header": [
                "Dataset Input \u2216 Unseen rate",
                "DBpedia 50%",
                "DBpedia 25%",
                "20news 50%",
                "20news 25%"
            ],
            "rows": [
                [
                    "[ITALIC] vw",
                    "0.993",
                    "0.992",
                    "0.878",
                    "0.861"
                ]
            ],
            "title": "Table 5: The accuracy of the traditional classifier in Phase 2 given documents from seen classes only."
        },
        "insight": "The purpose of Table 5 is to show that the traditional CNN classifier in Phase 2 was highly accurate."
    },
    {
        "id": "101",
        "table": {
            "header": [
                "Model",
                "Total P",
                "Total R",
                "Total F1",
                "General P",
                "General R",
                "General F1",
                "Fine P",
                "Fine R",
                "Fine F1",
                "Ultra-Fine P",
                "Ultra-Fine R",
                "Ultra-Fine F1"
            ],
            "rows": [
                [
                    "Ours + GloVe w/o augmentation",
                    "46.4",
                    "23.3",
                    "31.0",
                    "57.7",
                    "65.5",
                    "61.4",
                    "41.3",
                    "31.3",
                    "35.6",
                    "42.4",
                    "9.2",
                    "15.1"
                ],
                [
                    "Ours + ELMo w/o augmentation",
                    "[BOLD] 55.6",
                    "28.1",
                    "37.3",
                    "[BOLD] 69.3",
                    "77.3",
                    "73.0",
                    "[BOLD] 47.9",
                    "35.4",
                    "40.7",
                    "[BOLD] 48.9",
                    "12.6",
                    "20.0"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "55.2",
                    "26.4",
                    "35.7",
                    "69.4",
                    "72.0",
                    "70.7",
                    "46.6",
                    "38.5",
                    "42.2",
                    "48.7",
                    "10.3",
                    "17.1"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "50.7",
                    "[BOLD] 33.1",
                    "[BOLD] 40.1",
                    "66.9",
                    "[BOLD] 80.7",
                    "73.2",
                    "41.7",
                    "46.2",
                    "43.8",
                    "45.6",
                    "[BOLD] 17.4",
                    "[BOLD] 25.2"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.6",
                    "32.8",
                    "[BOLD] 40.1",
                    "67.4",
                    "80.6",
                    "[BOLD] 73.4",
                    "41.6",
                    "[BOLD] 54.7",
                    "[BOLD] 47.3",
                    "46.3",
                    "15.6",
                    "23.4"
                ],
                [
                    "Choi et\u00a0al. ( 2018 ) w augmentation",
                    "48.1",
                    "23.2",
                    "31.3",
                    "60.3",
                    "61.6",
                    "61.0",
                    "40.4",
                    "38.4",
                    "39.4",
                    "42.8",
                    "8.8",
                    "14.6"
                ]
            ],
            "title": "Table 1: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et\u00a0al. (2018) comparing various systems. ELMo gives a substantial improvement over baselines. Over an ELMo-equipped model, data augmentation using the method of Choi et\u00a0al. (2018) gives no benefit. However, our denoising technique allow us to effectively incorporate distant data, matching the results of a BERT model on this task Devlin et\u00a0al. (2018)."
        },
        "insight": "Table 1 compares the performance of these systems on the development set. Our model with no augmentation already matches the system of Choi et al. (2018) with augmentation, and incorporating ELMo gives further gains on both precision and recall. On top of this model, adding the distantly-annotated data lowers the performance; the loss function-based approach of (Choi et al., 2018) does not sufficiently mitigate the noise in this data. However, denoising makes the distantlyannotated data useful, improving recall by a substantial margin especially in the general class. [CONTINUE] BERT performs similarly to ELMo with denoised distant data. As can be seen in the performance breakdown, BERT gains from improvements in recall in the fine class."
    },
    {
        "id": "102",
        "table": {
            "header": [
                "Model",
                "P",
                "R",
                "F1"
            ],
            "rows": [
                [
                    "Ours + GloVe w/o augmentation",
                    "47.6",
                    "23.3",
                    "31.3"
                ],
                [
                    "Ours + ELMo w/o augmentation",
                    "[BOLD] 55.8",
                    "27.7",
                    "37.0"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "55.5",
                    "26.3",
                    "35.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "51.5",
                    "[BOLD] 33.0",
                    "[BOLD] 40.2"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.6",
                    "[BOLD] 33.0",
                    "[BOLD] 40.2"
                ],
                [
                    "Choi et\u00a0al. ( 2018 ) w augmentation",
                    "47.1",
                    "24.2",
                    "32.0"
                ],
                [
                    "LabelGCN Xiong et\u00a0al. ( 2019 )",
                    "50.3",
                    "29.2",
                    "36.9"
                ]
            ],
            "title": "Table 2: Macro-averaged P/R/F1 on the test set for the entity typing task of Choi et\u00a0al. (2018). Our denoising approach gives substantial gains over naive augmentation and matches the performance of a BERT model."
        },
        "insight": "Table 2 shows the performance of all settings on the test set, with the same trend as the performance on the development set. Our approach outperforms the concurrently-published Xiong et al. (2019);"
    },
    {
        "id": "103",
        "table": {
            "header": [
                "Type",
                "Denoising Method",
                "EL&HEAD P",
                "EL&HEAD R",
                "EL&HEAD F1",
                "EL P",
                "EL R",
                "EL F1",
                "HEAD P",
                "HEAD R",
                "HEAD F1"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Raw Data",
                    "[BOLD] 55.2",
                    "26.4",
                    "35.7",
                    "52.3",
                    "26.1",
                    "34.8",
                    "[BOLD] 52.8",
                    "28.4",
                    "36.9"
                ],
                [
                    "Heuristic Baselines",
                    "Synonyms&Hypernyms",
                    "43.0",
                    "30.0",
                    "35.3",
                    "47.5",
                    "26.3",
                    "33.9",
                    "44.8",
                    "31.7",
                    "37.1"
                ],
                [
                    "[EMPTY]",
                    "Pair",
                    "50.2",
                    "29.0",
                    "36.8",
                    "49.6",
                    "27.0",
                    "35.0",
                    "50.6",
                    "31.2",
                    "38.6"
                ],
                [
                    "[EMPTY]",
                    "Overlap",
                    "50.0",
                    "32.3",
                    "39.2",
                    "49.5",
                    "[BOLD] 30.8",
                    "38.0",
                    "50.6",
                    "31.4",
                    "38.7"
                ],
                [
                    "Proposed Approach",
                    "Filter",
                    "53.1",
                    "28.2",
                    "36.8",
                    "51.9",
                    "26.5",
                    "35.1",
                    "51.2",
                    "31.2",
                    "38.7"
                ],
                [
                    "[EMPTY]",
                    "Relabel",
                    "52.1",
                    "32.2",
                    "39.8",
                    "50.2",
                    "31.4",
                    "38.6",
                    "50.2",
                    "31.8",
                    "38.9"
                ],
                [
                    "[EMPTY]",
                    "Filter&Relabel",
                    "50.7",
                    "[BOLD] 33.1",
                    "[BOLD] 40.1",
                    "[BOLD] 52.7",
                    "30.5",
                    "[BOLD] 38.7",
                    "50.7",
                    "[BOLD] 32.1",
                    "[BOLD] 39.3"
                ],
                [
                    "[EMPTY]",
                    "Choi et\u00a0al. ( 2018 )",
                    "48.1",
                    "23.2",
                    "31.3",
                    "50.3",
                    "19.6",
                    "28.2",
                    "48.4",
                    "22.3",
                    "30.6"
                ]
            ],
            "title": "Table 3: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et\u00a0al. (2018) with various types of augmentation added. The customized loss from Choi et\u00a0al. (2018) actually causes a decrease in performance from adding any of the datasets. Heuristics can improve incorporation of this data: a relabeling heuristic (Pair) helps on HEAD and a filtering heuristic (Overlap) is helpful in both settings. However, our trainable filtering and relabeling models outperform both of these techniques."
        },
        "insight": "Table 3 compares the results on the development set. [CONTINUE] On top of the baseline ORIGINAL, adding synonyms and hypernyms by consulting external knowledge does not improve the performance. [CONTINUE] the PAIR technique results in small gains over ORIGINAL. OVERLAP is the most ef [CONTINUE] fective heuristic technique. This simple [CONTINUE] heuristic improves recall on EL. [CONTINUE] FILTER, [CONTINUE] gives similar improvements to PAIR and OVERLAP on the HEAD setting, [CONTINUE] RELABEL and OVERLAP both improve performance on both EL and HEAD while other methods do poorly on EL. [CONTINUE] FILTER & RELABEL outperforms all the baselines."
    },
    {
        "id": "104",
        "table": {
            "header": [
                "Model",
                "Acc.",
                "Ma-F1",
                "Mi-F1"
            ],
            "rows": [
                [
                    "Ours + ELMo w/o augmentation",
                    "42.7",
                    "72.7",
                    "66.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "59.3",
                    "76.5",
                    "70.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "63.9",
                    "[BOLD] 84.5",
                    "78.9"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "[BOLD] 64.9",
                    "[BOLD] 84.5",
                    "[BOLD] 79.2"
                ],
                [
                    "by Choi et\u00a0al. ( 2018 )",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.8",
                    "76.6",
                    "69.1"
                ],
                [
                    "Shimaoka et\u00a0al. ( 2017 )",
                    "51.7",
                    "70.9",
                    "64.9"
                ],
                [
                    "AFET Ren et\u00a0al. ( 2016a )",
                    "55.1",
                    "71.1",
                    "64.7"
                ],
                [
                    "PLE Ren et\u00a0al. ( 2016b )",
                    "57.2",
                    "71.5",
                    "66.1"
                ],
                [
                    "Choi et\u00a0al. ( 2018 )",
                    "59.5",
                    "76.8",
                    "71.8"
                ],
                [
                    "LabelGCN Xiong et\u00a0al. ( 2019 )",
                    "59.6",
                    "77.8",
                    "72.2"
                ]
            ],
            "title": "Table 4: Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et\u00a0al. (2018) (see text)."
        },
        "insight": "Table 4 lists the results on the OntoNotes test set following the adaptation setting of Choi et al. (2018). [CONTINUE] denoising significantly improves over naive incorporation of distant data, [CONTINUE] BERT still performs well but not as well as our model with augmented training data."
    },
    {
        "id": "105",
        "table": {
            "header": [
                "Data",
                "General Add",
                "General Del",
                "Fine Add",
                "Fine Del",
                "Ultra-Fine Add",
                "Ultra-Fine Del",
                "Filter (%)"
            ],
            "rows": [
                [
                    "EL",
                    "0.87",
                    "0.01",
                    "0.36",
                    "0.17",
                    "2.03",
                    "0.12",
                    "9.4"
                ],
                [
                    "HEAD",
                    "1.18",
                    "0.00",
                    "0.51",
                    "0.01",
                    "1.15",
                    "0.16",
                    "10.0"
                ]
            ],
            "title": "Table 5: The average number of types added or deleted by the relabeling function per example. The right-most column shows that the rate of examples discarded by the filtering function."
        },
        "insight": "Table 5 reports the average numbers of types added/deleted by the relabeling function and the ratio of examples discarded by the filtering function. [CONTINUE] The HEAD examples have more general types added than the EL examples since the noisy HEAD labels are typically finer. Fine-grained types are added to both EL and HEAD examples less frequently. Ultra-fine examples are frequently added to both datasets, with more added to EL; [CONTINUE] The filtering function discards similar numbers of examples for the EL and HEAD data: 9.4% and 10% respectively."
    },
    {
        "id": "106",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] # Samples Train",
                "[BOLD] # Samples Val.",
                "[BOLD] # Samples Test",
                "[BOLD] Message (Avg.) # tokens",
                "[BOLD] Message (Avg.) # groups",
                "[BOLD] Message (Avg.) # tokens /group",
                "[BOLD] Response (Avg.) # tokens",
                "[BOLD] Response (Avg.) # groups",
                "[BOLD] Response (Avg.) # tokens /group"
            ],
            "rows": [
                [
                    "Ubuntu-v1",
                    "1M",
                    "35,609",
                    "35,517",
                    "162.47 \u00b1132.47",
                    "8.43 \u00b16.32",
                    "20.14 \u00b118.41",
                    "14.44 \u00b113.93",
                    "1",
                    "-"
                ],
                [
                    "Ubuntu-v2",
                    "1M",
                    "19,560",
                    "18,920",
                    "85.92 \u00b174.71",
                    "4.95 \u00b12.98",
                    "20.73 \u00b120.19",
                    "17.01 \u00b116.41",
                    "1",
                    "-"
                ],
                [
                    "Samsung QA",
                    "163,616",
                    "10,000",
                    "10,000",
                    "12.84 \u00b16.42",
                    "1",
                    "-",
                    "173.48 \u00b1192.12",
                    "6.09 \u00b15.58",
                    "29.28 \u00b131.91"
                ]
            ],
            "title": "Table 1: Properties of the Ubuntu and Samsung QA dataset. The message and response are {context}, {response} in Ubuntu and {question}, {answer} in the Samsung QA dataset."
        },
        "insight": "we found that it still showed a limitation when we consider very large sequential length data such as 162 steps average in the Ubuntu Dialogue Corpus dataset (see Table 1). [CONTINUE] Table 1 shows properties of the Ubuntu dataset. [CONTINUE] we generated ({question}, {answer}, flag) triples (see Table 1). [CONTINUE] The maximum time step for calculating gradient of the RNN is determined according to the input data statistics in Table 1."
    },
    {
        "id": "107",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Ubuntu-v1 1 in 2R@1",
                "[BOLD] Ubuntu-v1 1 in 10R@1",
                "[BOLD] Ubuntu-v1 1 in 10R@2",
                "[BOLD] Ubuntu-v1 1 in 10R@5"
            ],
            "rows": [
                [
                    "TF-IDF\u00a0[1]",
                    "0.659",
                    "0.410",
                    "0.545",
                    "0.708"
                ],
                [
                    "CNN [2]",
                    "0.848",
                    "0.549",
                    "0.684",
                    "0.896"
                ],
                [
                    "LSTM [2]",
                    "0.901",
                    "0.638",
                    "0.784",
                    "0.949"
                ],
                [
                    "CompAgg [3]",
                    "0.884",
                    "0.631",
                    "0.753",
                    "0.927"
                ],
                [
                    "BiMPM [4]",
                    "0.897",
                    "0.665",
                    "0.786",
                    "0.938"
                ],
                [
                    "RDE",
                    "0.898 \u00b10.002",
                    "0.643 \u00b10.009",
                    "0.784 \u00b10.007",
                    "0.945 \u00b10.002"
                ],
                [
                    "RDE-LTC",
                    "0.903 \u00b10.001",
                    "0.656 \u00b10.003",
                    "0.794 \u00b10.003",
                    "0.948 \u00b10.001"
                ],
                [
                    "HRDE",
                    "0.915 \u00b10.001",
                    "0.681 \u00b10.001",
                    "0.820 \u00b10.001",
                    "0.959 \u00b10.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.916 \u00b10.001",
                    "[BOLD] 0.684 \u00b10.001",
                    "[BOLD] 0.822 \u00b10.001",
                    "[BOLD] 0.960 \u00b10.001"
                ]
            ],
            "title": "Table 3: Model performance results for the Ubuntu-v1 dataset. Models [1-4] are from Lowe et\u00a0al. (2015); Kadlec et\u00a0al. (2015); Wang and Jiang (2016); Wang et\u00a0al. (2017), respectively."
        },
        "insight": "As Table 3 shows, our proposed HRDE and HRDE-LTC models achieve the best performance for the Ubuntu-v1 dataset. We also find that the RDE-LTC model shows improvements from the baseline model, RDE."
    },
    {
        "id": "108",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Ubuntu-v2 1 in 2R@1",
                "[BOLD] Ubuntu-v2 1 in 10R@1",
                "[BOLD] Ubuntu-v2 1 in 10R@2",
                "[BOLD] Ubuntu-v2 1 in 10R@5"
            ],
            "rows": [
                [
                    "LSTM\u00a0[1]",
                    "0.869",
                    "0.552",
                    "0.721",
                    "0.924"
                ],
                [
                    "RNN [5]",
                    "0.907 \u00b10.002",
                    "0.664 \u00b10.004",
                    "0.799 \u00b10.004",
                    "0.951 \u00b10.001"
                ],
                [
                    "CNN [5]",
                    "0.863 \u00b10.003",
                    "0.587 \u00b10.004",
                    "0.721 \u00b10.005",
                    "0.907 \u00b10.003"
                ],
                [
                    "RNN-CNN [5]",
                    "0.911 \u00b10.001",
                    "[BOLD] 0.672 \u00b10.002",
                    "0.809 \u00b10.002",
                    "0.956 \u00b10.001"
                ],
                [
                    "Attention \\scriptsize{[6]}(RNN-CNN)",
                    "0.903 \u00b10.002",
                    "0.653 \u00b10.005",
                    "0.788 \u00b10.005",
                    "0.945 \u00b10.002"
                ],
                [
                    "CompAgg [3]",
                    "0.895",
                    "0.641",
                    "0.776",
                    "0.937"
                ],
                [
                    "BiMPM [4]",
                    "0.877",
                    "0.611",
                    "0.747",
                    "0.921"
                ],
                [
                    "RDE",
                    "0.894 \u00b10.002",
                    "0.610 \u00b10.008",
                    "0.776 \u00b10.006",
                    "0.947 \u00b10.002"
                ],
                [
                    "RDE-LTC",
                    "0.899 \u00b10.002",
                    "0.625 \u00b10.004",
                    "0.788 \u00b10.004",
                    "0.951 \u00b10.001"
                ],
                [
                    "HRDE",
                    "0.914 \u00b10.001",
                    "0.649 \u00b10.001",
                    "0.813 \u00b10.001",
                    "0.964 \u00b10.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.915 \u00b10.002",
                    "0.652 \u00b10.003",
                    "[BOLD] 0.815 \u00b10.001",
                    "[BOLD] 0.966 \u00b10.001"
                ]
            ],
            "title": "Table 4: Model performance results for the Ubuntu-v2 dataset. Models [1,3-6] are from Lowe et\u00a0al. (2015); Wang and Jiang (2016); Wang et\u00a0al. (2017); Baudi\u0161 et\u00a0al. (2016); Tan et\u00a0al. (2015), respectively."
        },
        "insight": "Table 4 reveals that the HRDE-LTC model is best for three cases (1 in 2 R@1, 1 in 10 R@2 and 1 in 10 R@5). [CONTINUE] we see improvements from the RDE model to the HRDE model and additional improvements with the LTC module in all test cases (the Ubuntuv1/v2 and the Samsung QA)."
    },
    {
        "id": "109",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Samsung QA 1 in 2R@1",
                "[BOLD] Samsung QA 1 in 10R@1",
                "[BOLD] Samsung QA 1 in 10R@2",
                "[BOLD] Samsung QA 1 in 10R@5"
            ],
            "rows": [
                [
                    "TF-IDF",
                    "0.939",
                    "0.834",
                    "0.897",
                    "0.953"
                ],
                [
                    "RDE",
                    "0.978 \u00b10.002",
                    "0.869 \u00b10.009",
                    "0.966 \u00b10.003",
                    "0.997 \u00b10.001"
                ],
                [
                    "RDE-LTC",
                    "0.981 \u00b10.002",
                    "0.880 \u00b10.009",
                    "0.970 \u00b10.003",
                    "0.997 \u00b10.001"
                ],
                [
                    "HRDE",
                    "0.981 \u00b10.002",
                    "0.885 \u00b10.011",
                    "0.971 \u00b10.004",
                    "0.997 \u00b10.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.983 \u00b10.002",
                    "[BOLD] 0.890 \u00b10.010",
                    "[BOLD] 0.972 \u00b10.003",
                    "[BOLD] 0.998 \u00b10.001"
                ]
            ],
            "title": "Table 5: Model performance results for the Samsung QA dataset."
        },
        "insight": "Table 5 indicates the proposed RDE-LTC, HRDE, and the that HRDE-LTC model show performance improvements when compared to the baseline model, TFIDF and RDE."
    },
    {
        "id": "110",
        "table": {
            "header": [
                "Model",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4",
                "METEOR",
                "ROUGE-L"
            ],
            "rows": [
                [
                    "Zhou et al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "13.29",
                    "-",
                    "-"
                ],
                [
                    "Zhao et al. ( 2018 )*",
                    "45.69",
                    "29.58",
                    "22.16",
                    "16.85",
                    "20.62",
                    "44.99"
                ],
                [
                    "Kim et al. ( 2019 )",
                    "-",
                    "-",
                    "-",
                    "16.17",
                    "-",
                    "-"
                ],
                [
                    "Liu et al. ( 2019 )",
                    "46.58",
                    "30.90",
                    "22.82",
                    "17.55",
                    "21.24",
                    "44.53"
                ],
                [
                    "[BOLD] IWAQG",
                    "[BOLD] 47.69",
                    "[BOLD] 32.24",
                    "[BOLD] 24.01",
                    "[BOLD] 18.53",
                    "[BOLD] 22.33",
                    "[BOLD] 46.94"
                ]
            ],
            "title": "Table 2: Comparison of our model with the baselines. \u201c*\u201d is our QG module."
        },
        "insight": "Our model outperforms all other models in all the metrics. This improvement is consistent, around 2%."
    },
    {
        "id": "111",
        "table": {
            "header": [
                "Accuracy",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4",
                "METEOR",
                "ROUGE-L"
            ],
            "rows": [
                [
                    "[BOLD] Only QG*",
                    "[BOLD] 45.63",
                    "[BOLD] 30.43",
                    "[BOLD] 22.51",
                    "[BOLD] 17.30",
                    "[BOLD] 21.06",
                    "[BOLD] 45.42"
                ],
                [
                    "60%",
                    "45.80",
                    "30.61",
                    "22.57",
                    "17.30",
                    "21.47",
                    "44.70"
                ],
                [
                    "70%",
                    "47.05",
                    "31.62",
                    "23.46",
                    "18.05",
                    "22.00",
                    "45.88"
                ],
                [
                    "[BOLD] IWAQG (73.8%)",
                    "[BOLD] 47.69",
                    "[BOLD] 32.24",
                    "[BOLD] 24.01",
                    "[BOLD] 18.53",
                    "[BOLD] 22.33",
                    "[BOLD] 46.94"
                ],
                [
                    "80%",
                    "48.11",
                    "32.36",
                    "24.00",
                    "18.42",
                    "22.43",
                    "47.22"
                ],
                [
                    "90%",
                    "49.33",
                    "33.43",
                    "24.91",
                    "19.20",
                    "22.98",
                    "48.41"
                ],
                [
                    "[BOLD] Upper Bound (100%)",
                    "[BOLD] 50.51",
                    "[BOLD] 34.28",
                    "[BOLD] 25.60",
                    "[BOLD] 19.75",
                    "[BOLD] 23.45",
                    "[BOLD] 49.65"
                ]
            ],
            "title": "Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classifier. \u201c*\u201d is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018)."
        },
        "insight": "Table 3 and Figure 3 show a linear relationship between the accuracy of the classifier and the IWAQG. This demonstrates the effectiveness of our pipelined approach regardless of the interrogative-word classifier model."
    },
    {
        "id": "112",
        "table": {
            "header": [
                "Model",
                "What",
                "Which",
                "Where",
                "When",
                "Who",
                "Why",
                "How",
                "Others",
                "Total"
            ],
            "rows": [
                [
                    "Only QG*",
                    "82.24%",
                    "0.29%",
                    "51.90%",
                    "60.82%",
                    "68.34%",
                    "12.66%",
                    "60.62%",
                    "2.13%",
                    "68.29%"
                ],
                [
                    "IWAQG",
                    "87.66%",
                    "1.46%",
                    "66.24%",
                    "49.41%",
                    "76.41%",
                    "50.63%",
                    "70.26%",
                    "14.89%",
                    "74.10%"
                ],
                [
                    "Upper Bound",
                    "99.87%",
                    "99.71%",
                    "100.00%",
                    "99.71%",
                    "99.84%",
                    "98.73%",
                    "99.67%",
                    "89.36%",
                    "99.72%"
                ]
            ],
            "title": "Table 4: Recall of interrogative words of the QG model. \u201c*\u201d is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018)."
        },
        "insight": "we analyze the recall of the interrogative words generated by our pipelined system. As shown in the Table 4, the total recall of using only the QG module is 68.29%, while the recall of our proposed system, IWAQG, is 74.10%, an improvement of almost 6%. Furthermore, if we assume a perfect interrogative-word classifier, the recall would be 99.72%, a dramatic improvement which proves the validity of our hypothesis."
    },
    {
        "id": "113",
        "table": {
            "header": [
                "Classifier",
                "Accuracy"
            ],
            "rows": [
                [
                    "CLS",
                    "56.0%"
                ],
                [
                    "CLS + NER",
                    "56.6%"
                ],
                [
                    "CLS + AE",
                    "70.3%"
                ],
                [
                    "CLS + AT",
                    "73.3%"
                ],
                [
                    "[BOLD] CLS + AT + NER",
                    "[BOLD] 73.8%"
                ]
            ],
            "title": "Table 6: Ablation Study of our interrogative-word classifier."
        },
        "insight": "We tried to combine different features shown in Table 6 for the interrogative-word classifier. [CONTINUE] The first model is only using the [CLS] BERT token embedding [CONTINUE] The second model is the previous one with the entity type of the answer as an additional feature. The performance of this model is a bit better than the first one but it is not enough to be utilized effectively for our pipeline. [CONTINUE] As we can see, the performance noticeably increased, which indicates that answer information is the key to predict the interrogative word needed. [CONTINUE] the fourth model, [CONTINUE] clearly outperforms the previous one, [CONTINUE] The fifth model is the same as the previous one but with the addition of the entitytype embedding of the answer. The combination of the three features (answer, answer entity type, and passage) yields to the best performance."
    },
    {
        "id": "114",
        "table": {
            "header": [
                "Class",
                "Recall",
                "Precision"
            ],
            "rows": [
                [
                    "What",
                    "87.7%",
                    "76.0%"
                ],
                [
                    "Which",
                    "1.4%",
                    "38.0%"
                ],
                [
                    "Where",
                    "65.9%",
                    "55.8%"
                ],
                [
                    "When",
                    "49.2%",
                    "69.8%"
                ],
                [
                    "Who",
                    "76.9%",
                    "66.7%"
                ],
                [
                    "Why",
                    "50.1%",
                    "74.1%"
                ],
                [
                    "How",
                    "70.5%",
                    "79.0%"
                ],
                [
                    "Others",
                    "10.5%",
                    "57.0%"
                ]
            ],
            "title": "Table 7: Recall and precision of interrogative words of our interrogative-word classifier."
        },
        "insight": "In addition, we provide the recall and precision per class for our final interrogative-word classifier (CLS + AT + NER in Table 7). [CONTINUE] However, the recall of which is very low. [CONTINUE] Our model has also problem with why [CONTINUE] Lastly, the recall of 'when is also low"
    },
    {
        "id": "115",
        "table": {
            "header": [
                "[ITALIC] \u03b3",
                "#Edge/#Node",
                "LAS",
                "Conn. Ratio(%)"
            ],
            "rows": [
                [
                    "0.05",
                    "2.09",
                    "92.5",
                    "100.0"
                ],
                [
                    "0.1",
                    "1.57",
                    "91.2",
                    "99.5"
                ],
                [
                    "0.2",
                    "1.34",
                    "90.5",
                    "94.2"
                ],
                [
                    "0.3",
                    "1.04",
                    "88.0",
                    "77.6"
                ],
                [
                    "[ITALIC] K",
                    "#Edge/#Node",
                    "LAS",
                    "Conn. Ratio(%)"
                ],
                [
                    "1",
                    "1.00",
                    "86.4",
                    "100.0"
                ],
                [
                    "2",
                    "1.03",
                    "87.3",
                    "100.0"
                ],
                [
                    "5",
                    "1.09",
                    "89.1",
                    "100.0"
                ],
                [
                    "10",
                    "1.14",
                    "89.8",
                    "100.0"
                ]
            ],
            "title": "Table 1: Statistics on forests generated with various \u03b3 (upper half) and K (lower half) on the development set."
        },
        "insight": "Table 1 demonstrates several characteristics of the generated forests of both the EDGEWISE and KBESTEISNER algorithms in Section 5.1, where \"#Edge/#Sent\" measures the forest density with the number of edges divided by the sentence length, \"LAS\" represents the oracle LAS score on 100 biomedical sentences with manually annotated dependency trees, and \"Conn. Ratio (%)\" shows the percentage of forests where both related entity mentions are connected. [CONTINUE] Regarding the forest density, forests produced by EDGEWISE generally contain more edges than those from KBESTEISNER. [CONTINUE] For connectivity, KBESTEISNER guarantees to generate spanning forests. On the other hand, the connectivity ratio for the forests produced by EDGEWISE drops when increasing the threshold \u03b3. We can have more than 94% being connected with \u03b3 \u2264 0.2."
    },
    {
        "id": "116",
        "table": {
            "header": [
                "Model",
                "F1 score"
            ],
            "rows": [
                [
                    "GRU+Attn (Liu et al.,  2017 )\u2020",
                    "49.5"
                ],
                [
                    "Bran (Verga et al.,  2018 )\u2020",
                    "50.8"
                ],
                [
                    "TextOnly",
                    "50.6"
                ],
                [
                    "DepTree",
                    "51.4"
                ],
                [
                    "KBestEisnerPS",
                    "**52.4**"
                ],
                [
                    "EdgewisePS",
                    "** [BOLD] 53.4**"
                ]
            ],
            "title": "Table 2: Test results of Biocreative VI CPR. \u2020 indicates previously reported numbers. ** means significant over DepTree at p<0.01 with 1000 bootstrap tests (Efron and Tibshirani, 1994)."
        },
        "insight": "Table 2 shows the main comparison results on the BioCreative CPR testset, with comparisons to the previous state-of-the-art and our baselines. [CONTINUE] TEXTONLY gives a performance comparable with Bran. With 1-best dependency trees, our DEPTREE baseline gives better performances than the previous state of the art. [CONTINUE] both KBESTEISNERPS and EDGEWISEPS obtain significantly higher numbers than DEPTREE."
    },
    {
        "id": "117",
        "table": {
            "header": [
                "Model",
                "F1 score"
            ],
            "rows": [
                [
                    "BO-LSTM (Lamurias et al.,  2019 )\u2020",
                    "52.3"
                ],
                [
                    "BioBERT (Lee et al.,  2019 )\u2020",
                    "67.2"
                ],
                [
                    "TextOnly",
                    "76.0"
                ],
                [
                    "DepTree",
                    "78.9"
                ],
                [
                    "KBestEisnerPS",
                    "*83.6*"
                ],
                [
                    "EdgewisePS",
                    "** [BOLD] 85.7**"
                ]
            ],
            "title": "Table 3: Main results on PGR testest. \u2020 denotes previous numbers rounded into 3 significant digits. * and ** indicate significance over DepTree at p<0.05 and p<0.01 with 1000 bootstrap tests."
        },
        "insight": "Table 3 shows the comparison with previous work on the PGR testset, where our models are significantly better than the existing models. [CONTINUE] With 1-best trees, DEPTREE is 2.9 points better than TEXTONLY, [CONTINUE] both KBESTEISNERPS and EDGEWISEPS significantly outperform DEPTR"
    },
    {
        "id": "118",
        "table": {
            "header": [
                "Model",
                "F1 score"
            ],
            "rows": [
                [
                    "C-GCN (Zhang et al.,  2018b )\u2020",
                    "84.8"
                ],
                [
                    "C-AGGCN (Guo et al.,  2019 )\u2020",
                    "85.7"
                ],
                [
                    "DepTree",
                    "84.6"
                ],
                [
                    "KBestEisnerPS",
                    "85.8"
                ],
                [
                    "EdgewisePS",
                    "86.3"
                ]
            ],
            "title": "Table 4: Main results on SemEval-2010 task 8 testest. \u2020 denotes previous numbers."
        },
        "insight": "As shown in Table 4, we conduct a preliminary study on SemEval-2010 task 8 [CONTINUE] DEPTREE achieves similar performance as CGCN and is slightly worse than C-AGGCN, [CONTINUE] both KBESTEISNERPS and EDGEWISEPS outperform DEPTREE [CONTINUE] they show comparable and slightly better performances than C-AGGCN. [CONTINUE] EDGEWISEPS is better than KBESTEISNERPS,"
    },
    {
        "id": "119",
        "table": {
            "header": [
                "Training Instances Hits@K (Macro)",
                "Training Instances Hits@K (Macro)",
                "<100 10",
                "<100 15",
                "<100 20",
                "<200 10",
                "<200 15",
                "<200 20"
            ],
            "rows": [
                [
                    "CNN",
                    "+ATT",
                    "<5.0",
                    "<5.0",
                    "18.5",
                    "<5.0",
                    "16.2",
                    "33.3"
                ],
                [
                    "[EMPTY]",
                    "+HATT",
                    "5.6",
                    "31.5",
                    "57.4",
                    "22.7",
                    "43.9",
                    "65.1"
                ],
                [
                    "[EMPTY]",
                    "+KATT",
                    "[BOLD] 9.1",
                    "[BOLD] 41.3",
                    "[BOLD] 58.5",
                    "[BOLD] 23.3",
                    "[BOLD] 44.1",
                    "[BOLD] 65.4"
                ],
                [
                    "PCNN",
                    "+ATT",
                    "<5.0",
                    "7.4",
                    "40.7",
                    "17.2",
                    "24.2",
                    "51.5"
                ],
                [
                    "[EMPTY]",
                    "+HATT",
                    "29.6",
                    "51.9",
                    "61.1",
                    "41.4",
                    "60.6",
                    "68.2"
                ],
                [
                    "[EMPTY]",
                    "+KATT",
                    "[BOLD] 35.3",
                    "[BOLD] 62.4",
                    "[BOLD] 65.1",
                    "[BOLD] 43.2",
                    "[BOLD] 61.3",
                    "[BOLD] 69.2"
                ]
            ],
            "title": "Table 1: Accuracy (%) of Hits@K on relations with training instances fewer than 100/200."
        },
        "insight": "From the results shown in Table 1, we observe that for both CNN and PCNN models, our model outperforms the plain attention model and the HATT model."
    },
    {
        "id": "120",
        "table": {
            "header": [
                "Training Instances Hits@K (Macro)",
                "<100 10",
                "<100 15",
                "<100 20",
                "<200 10",
                "<200 15",
                "<200 20"
            ],
            "rows": [
                [
                    "+KATT",
                    "[BOLD] 35.3",
                    "[BOLD] 62.4",
                    "[BOLD] 65.1",
                    "[BOLD] 43.2",
                    "[BOLD] 61.3",
                    "[BOLD] 69.2"
                ],
                [
                    "w/o hier",
                    "34.2",
                    "62.1",
                    "65.1",
                    "42.5",
                    "60.2",
                    "68.1"
                ],
                [
                    "w/o GCNs",
                    "30.5",
                    "61.9",
                    "63.1",
                    "39.5",
                    "58.4",
                    "66.1"
                ],
                [
                    "Word2vec",
                    "30.2",
                    "62.0",
                    "62.5",
                    "39.6",
                    "57.5",
                    "65.8"
                ],
                [
                    "w/o KG",
                    "30.0",
                    "61.0",
                    "61.3",
                    "39.5",
                    "56.5",
                    "62.5"
                ]
            ],
            "title": "Table 2: Results of ablation study with PCNN."
        },
        "insight": "From the evaluation results in Table 2, we observe that the performance slightly degraded without coarse-tofine attention, [CONTINUE] We also noticed that the performance slightly degraded without KG or using word embeddings, and the performance significantly degraded when we removed GCNs."
    },
    {
        "id": "121",
        "table": {
            "header": [
                "WordNet",
                "LCH  [ITALIC] 100",
                "ShP  [ITALIC] 100",
                "WuP  [ITALIC] 100",
                "LCH 51.3",
                "ShP 51.3",
                "WuP 47.4"
            ],
            "rows": [
                [
                    "path2vec",
                    "[BOLD] 93.5",
                    "[BOLD] 95.2",
                    "[BOLD] 93.1",
                    "[BOLD] 53.2",
                    "[BOLD] 55.5",
                    "[BOLD] 55.5"
                ],
                [
                    "TransR",
                    "77.6",
                    "77.6",
                    "72.5",
                    "38.6",
                    "38.6",
                    "38.6"
                ],
                [
                    "node2vec",
                    "75.9",
                    "75.9",
                    "78.7",
                    "46.2",
                    "46.2",
                    "46.2"
                ],
                [
                    "DeepWalk",
                    "86.8",
                    "86.8",
                    "85.0",
                    "53.3",
                    "53.3",
                    "53.3"
                ],
                [
                    "FSE",
                    "90.0",
                    "90.0",
                    "89.0",
                    "55.6",
                    "55.6",
                    "55.6"
                ]
            ],
            "title": "Table 1: Spearman correlations with WordNet similarities (left) and human judgments (right) \u00d7100."
        },
        "insight": "Discussion of Results The left part of Table 1 shows results with the WordNet similarity scores used as gold standard. Path2vec outperforms other graph embeddings, achieving high correlations with WordNet similarities. This shows that our model efficiently approximates different graph measures. The right part of Table 1 shows results [CONTINUE] for the correlations with human judgments (SimLex999). We report the results for the best models for each method, all of them (except FSE) using vector size 300 for comparability."
    },
    {
        "id": "122",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Senseval2",
                "[BOLD] Senseval3",
                "[BOLD] SemEval-15"
            ],
            "rows": [
                [
                    "Random sense",
                    "0.381",
                    "0.312",
                    "0.393"
                ],
                [
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures"
                ],
                [
                    "LCH (WordNet)",
                    "0.547\u21930.000",
                    "0.494\u21930.000",
                    "0.550\u21930.000"
                ],
                [
                    "LCH (path2vec)",
                    "0.527\u21930.020",
                    "0.472\u21930.022",
                    "0.536\u21930.014"
                ],
                [
                    "ShP (WordNet)",
                    "0.548\u21930.000",
                    "0.495\u21930.000",
                    "0.550\u21930.000"
                ],
                [
                    "ShP (path2vec)",
                    "0.534\u21930.014",
                    "0.489\u21930.006",
                    "0.563\u21910.013"
                ],
                [
                    "WuP (WordNet)",
                    "0.547\u21930.000",
                    "0.487\u21930.000",
                    "0.542\u21930.000"
                ],
                [
                    "WuP (path2vec)",
                    "0.543\u21930.004",
                    "0.489\u21910.002",
                    "0.545\u21910.003"
                ],
                [
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet"
                ],
                [
                    "TransR",
                    "0.540",
                    "0.466",
                    "0.536"
                ],
                [
                    "node2vec",
                    "0.503",
                    "0.467",
                    "0.489"
                ],
                [
                    "DeepWalk",
                    "0.528",
                    "0.476",
                    "0.552"
                ],
                [
                    "FSE",
                    "0.536",
                    "0.476",
                    "0.523"
                ]
            ],
            "title": "Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts."
        },
        "insight": "Discussion of Results Table 2 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, DeepWalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: [CONTINUE] Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number."
    },
    {
        "id": "123",
        "table": {
            "header": [
                "[EMPTY]",
                "Emb.",
                "Vocab",
                "Polyglot",
                "All"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "dim",
                    "size",
                    "in-vocab",
                    "pairs"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] N=862",
                    "[ITALIC] N=2034"
                ],
                [
                    "VarEmbed",
                    "128",
                    "100K",
                    "41.9",
                    "25.5"
                ],
                [
                    "Polyglot",
                    "64",
                    "100K",
                    "40.8",
                    "8.7"
                ],
                [
                    "mimick",
                    "64",
                    "0",
                    "17.9",
                    "17.5"
                ],
                [
                    "Polyglot",
                    "64",
                    "100K",
                    "40.8",
                    "27.0"
                ],
                [
                    "+mimick",
                    "64",
                    "100K",
                    "40.8",
                    "27.0"
                ],
                [
                    "Fasttext",
                    "300",
                    "2.51M",
                    "[EMPTY]",
                    "47.3"
                ]
            ],
            "title": "Table 3: Similarity results on the RareWord set, measured as Spearman\u2019s \u03c1\u00d7100. VarEmbed was trained on a 20-million token dataset, Polyglot on a 1.7B-token dataset."
        },
        "insight": "The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the \"All pairs\" condition. MIMICK also outperforms VarEmbed. FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data."
    },
    {
        "id": "124",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] w/ System Retrieval  [BOLD] BLEU",
                "[ITALIC] w/ System Retrieval  [BOLD] MTR",
                "[ITALIC] w/ System Retrieval  [BOLD] Len",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] BLEU",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] Len"
            ],
            "rows": [
                [
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline"
                ],
                [
                    "Retrieval",
                    "15.32",
                    "[BOLD] 12.19",
                    "151.2",
                    "10.24",
                    "[BOLD] 16.22",
                    "132.7"
                ],
                [
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons"
                ],
                [
                    "Seq2seq",
                    "10.21",
                    "5.74",
                    "34.9",
                    "7.44",
                    "5.25",
                    "31.1"
                ],
                [
                    "+  [ITALIC] encode evd",
                    "18.03",
                    "7.32",
                    "67.0",
                    "13.79",
                    "10.06",
                    "68.1"
                ],
                [
                    "+  [ITALIC] encode KP",
                    "21.94",
                    "8.63",
                    "74.4",
                    "12.96",
                    "10.50",
                    "78.2"
                ],
                [
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models"
                ],
                [
                    "Dec-shared",
                    "21.22",
                    "8.91",
                    "69.1",
                    "15.78",
                    "11.52",
                    "68.2"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "[BOLD] 24.71",
                    "10.05",
                    "74.8",
                    "11.48",
                    "10.08",
                    "40.5"
                ],
                [
                    "Dec-separate",
                    "24.24",
                    "10.63",
                    "88.6",
                    "17.48",
                    "13.15",
                    "86.9"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "24.52",
                    "11.27",
                    "88.3",
                    "[BOLD] 17.80",
                    "13.67",
                    "86.8"
                ]
            ],
            "title": "Table 3: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval. The best performing model is highlighted in bold per metric. Our separate decoder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization testing\u00a0Noreen (1989), p<0.0001."
        },
        "insight": "As can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons. Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do. Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments. [CONTINUE] Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance. Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing."
    },
    {
        "id": "125",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] Ntrain=5000 No-Char",
                "[ITALIC] Ntrain=5000 mimick",
                "[ITALIC] Ntrain=5000 char",
                "[ITALIC] Ntrain=5000 Both",
                "Full data  [ITALIC] Ntrain",
                "Full data No-Char",
                "Full data mimick",
                "Full data char",
                "Both",
                "PSG"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2192tag",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2192tag",
                    "[EMPTY]",
                    "2016*"
                ],
                [
                    "kk",
                    "\u2014",
                    "\u2014",
                    "\u2014",
                    "\u2014",
                    "4,949",
                    "81.94",
                    "83.95",
                    "83.64",
                    "84.88",
                    "[EMPTY]"
                ],
                [
                    "ta",
                    "82.30",
                    "81.55",
                    "84.97",
                    "85.22",
                    "6,329",
                    "80.44",
                    "[BOLD] 82.96",
                    "84.11",
                    "84.46",
                    "[EMPTY]"
                ],
                [
                    "lv",
                    "80.44",
                    "[BOLD] 84.32",
                    "84.49",
                    "[BOLD] 85.91",
                    "13,781",
                    "85.77",
                    "[BOLD] 87.95",
                    "89.55",
                    "89.99",
                    "[EMPTY]"
                ],
                [
                    "vi",
                    "85.67",
                    "[ITALIC] 84.22",
                    "84.85",
                    "85.43",
                    "31,800",
                    "89.94",
                    "90.34",
                    "90.50",
                    "90.19",
                    "[EMPTY]"
                ],
                [
                    "hu",
                    "82.88",
                    "[BOLD] 88.93",
                    "85.83",
                    "[BOLD] 88.34",
                    "33,017",
                    "91.52",
                    "[BOLD] 93.88",
                    "94.07",
                    "93.74",
                    "[EMPTY]"
                ],
                [
                    "tr",
                    "83.69",
                    "[BOLD] 85.60",
                    "84.23",
                    "[BOLD] 86.25",
                    "41,748",
                    "90.19",
                    "[BOLD] 91.82",
                    "93.11",
                    "92.68",
                    "[EMPTY]"
                ],
                [
                    "el",
                    "93.10",
                    "[BOLD] 93.63",
                    "94.05",
                    "[BOLD] 94.64",
                    "47,449",
                    "97.27",
                    "[BOLD] 98.08",
                    "98.09",
                    "98.22",
                    "[EMPTY]"
                ],
                [
                    "bg",
                    "90.97",
                    "[BOLD] 93.16",
                    "93.03",
                    "[BOLD] 93.52",
                    "50,000",
                    "96.63",
                    "[BOLD] 97.29",
                    "97.95",
                    "97.78",
                    "98.23"
                ],
                [
                    "sv",
                    "90.87",
                    "[BOLD] 92.30",
                    "92.27",
                    "[BOLD] 93.02",
                    "66,645",
                    "95.26",
                    "[BOLD] 96.27",
                    "96.69",
                    "96.87",
                    "96.60"
                ],
                [
                    "eu",
                    "82.67",
                    "[BOLD] 84.44",
                    "86.01",
                    "[BOLD] 86.93",
                    "72,974",
                    "91.67",
                    "[BOLD] 93.16",
                    "94.46",
                    "94.29",
                    "95.38"
                ],
                [
                    "ru",
                    "87.40",
                    "[BOLD] 89.72",
                    "88.65",
                    "[BOLD] 90.91",
                    "79,772",
                    "92.59",
                    "[BOLD] 95.21",
                    "95.98",
                    "95.84",
                    "[EMPTY]"
                ],
                [
                    "da",
                    "89.46",
                    "90.13",
                    "89.96",
                    "90.55",
                    "88,980",
                    "94.14",
                    "[BOLD] 95.04",
                    "96.13",
                    "96.02",
                    "96.16"
                ],
                [
                    "id",
                    "89.07",
                    "89.34",
                    "89.81",
                    "90.21",
                    "97,531",
                    "92.92",
                    "93.24",
                    "93.41",
                    "[BOLD] 93.70",
                    "93.32"
                ],
                [
                    "zh",
                    "80.84",
                    "[BOLD] 85.69",
                    "81.84",
                    "[BOLD] 85.53",
                    "98,608",
                    "90.91",
                    "[BOLD] 93.31",
                    "93.36",
                    "93.72",
                    "[EMPTY]"
                ],
                [
                    "fa",
                    "93.50",
                    "93.58",
                    "93.53",
                    "93.71",
                    "121,064",
                    "96.77",
                    "[BOLD] 97.03",
                    "97.20",
                    "97.16",
                    "97.60"
                ],
                [
                    "he",
                    "90.73",
                    "[BOLD] 91.69",
                    "91.93",
                    "91.70",
                    "135,496",
                    "95.65",
                    "[BOLD] 96.15",
                    "96.59",
                    "96.37",
                    "96.62"
                ],
                [
                    "ro",
                    "87.73",
                    "[BOLD] 89.18",
                    "88.96",
                    "[BOLD] 89.38",
                    "163,262",
                    "95.68",
                    "[BOLD] 96.72",
                    "97.07",
                    "97.09",
                    "[EMPTY]"
                ],
                [
                    "en",
                    "87.48",
                    "[BOLD] 88.45",
                    "88.89",
                    "88.89",
                    "204,587",
                    "93.39",
                    "[BOLD] 94.04",
                    "94.90",
                    "94.70",
                    "95.17"
                ],
                [
                    "ar",
                    "89.01",
                    "[BOLD] 90.58",
                    "90.49",
                    "90.62",
                    "225,853",
                    "95.51",
                    "[BOLD] 95.72",
                    "96.37",
                    "96.24",
                    "98.87"
                ],
                [
                    "hi",
                    "87.89",
                    "87.77",
                    "87.92",
                    "88.09",
                    "281,057",
                    "96.31",
                    "96.45",
                    "96.64",
                    "96.61",
                    "96.97"
                ],
                [
                    "it",
                    "91.35",
                    "[BOLD] 92.50",
                    "92.45",
                    "[BOLD] 93.01",
                    "289,440",
                    "97.22",
                    "97.47",
                    "97.76",
                    "97.69",
                    "97.90"
                ],
                [
                    "es",
                    "90.54",
                    "[BOLD] 91.41",
                    "91.71",
                    "91.78",
                    "382,436",
                    "94.68",
                    "94.84",
                    "95.08",
                    "95.05",
                    "95.67"
                ],
                [
                    "cs",
                    "87.97",
                    "[BOLD] 90.81",
                    "90.17",
                    "[BOLD] 91.29",
                    "1,173,282",
                    "96.34",
                    "[BOLD] 97.62",
                    "98.18",
                    "[ITALIC] 97.93",
                    "98.02"
                ]
            ],
            "title": "Table 5: POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degradation) by McNemar\u2019s test, p"
        },
        "insight": "We report the results on the full sets and on [CONTINUE] = 5000 tokens in Table 5 (partof-speech tagging accuracy) [CONTINUE] For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative. Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD corpus."
    },
    {
        "id": "126",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] Ntrain=5000 No-Char",
                "[ITALIC] Ntrain=5000 mimick",
                "[ITALIC] Ntrain=5000 char",
                "[ITALIC] Ntrain=5000 Both",
                "Full data No-Char",
                "Full data mimick",
                "Full data char",
                "Full data Both"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2192tag",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2192tag",
                    "[EMPTY]"
                ],
                [
                    "kk",
                    "\u2014",
                    "\u2014",
                    "\u2014",
                    "\u2014",
                    "21.48",
                    "20.07",
                    "28.47",
                    "20.98"
                ],
                [
                    "ta",
                    "80.68",
                    "[BOLD] 81.96",
                    "84.26",
                    "[BOLD] 85.63",
                    "79.90",
                    "[BOLD] 81.93",
                    "84.55",
                    "85.01"
                ],
                [
                    "lv",
                    "56.98",
                    "[BOLD] 59.86",
                    "64.81",
                    "[BOLD] 65.82",
                    "66.16",
                    "66.61",
                    "76.11",
                    "75.44"
                ],
                [
                    "hu",
                    "73.13",
                    "[BOLD] 76.30",
                    "73.62",
                    "[BOLD] 76.85",
                    "80.04",
                    "80.64",
                    "86.43",
                    "84.12"
                ],
                [
                    "tr",
                    "69.58",
                    "[BOLD] 75.21",
                    "75.81",
                    "[BOLD] 78.93",
                    "78.31",
                    "[BOLD] 83.32",
                    "91.51",
                    "90.86"
                ],
                [
                    "el",
                    "86.87",
                    "[ITALIC] 86.07",
                    "86.40",
                    "[BOLD] 87.50",
                    "94.64",
                    "[BOLD] 94.96",
                    "96.55",
                    "[BOLD] 96.76"
                ],
                [
                    "bg",
                    "78.26",
                    "[BOLD] 81.77",
                    "82.74",
                    "[BOLD] 84.93",
                    "91.98",
                    "[BOLD] 93.48",
                    "96.12",
                    "95.96"
                ],
                [
                    "sv",
                    "82.09",
                    "[BOLD] 84.12",
                    "85.26",
                    "[BOLD] 88.16",
                    "92.45",
                    "[BOLD] 94.20",
                    "96.37",
                    "[BOLD] 96.57"
                ],
                [
                    "eu",
                    "65.29",
                    "[BOLD] 66.00",
                    "70.67",
                    "[ITALIC] 70.27",
                    "82.75",
                    "[BOLD] 84.74",
                    "90.58",
                    "[BOLD] 91.39"
                ],
                [
                    "ru",
                    "77.31",
                    "[BOLD] 81.84",
                    "79.83",
                    "[BOLD] 83.53",
                    "88.80",
                    "[BOLD] 91.24",
                    "93.54",
                    "93.56"
                ],
                [
                    "da",
                    "80.26",
                    "[BOLD] 82.74",
                    "83.59",
                    "82.65",
                    "92.06",
                    "[BOLD] 94.14",
                    "96.05",
                    "95.96"
                ],
                [
                    "zh",
                    "63.29",
                    "[BOLD] 71.44",
                    "63.50",
                    "[BOLD] 74.66",
                    "84.95",
                    "85.70",
                    "84.86",
                    "85.87"
                ],
                [
                    "fa",
                    "84.73",
                    "[BOLD] 86.07",
                    "85.94",
                    "81.75",
                    "95.30",
                    "[BOLD] 95.55",
                    "96.90",
                    "96.80"
                ],
                [
                    "he",
                    "75.35",
                    "68.57",
                    "81.06",
                    "75.24",
                    "90.25",
                    "[BOLD] 90.99",
                    "93.35",
                    "93.63"
                ],
                [
                    "ro",
                    "84.20",
                    "[BOLD] 85.64",
                    "85.61",
                    "[BOLD] 87.31",
                    "94.97",
                    "[BOLD] 96.10",
                    "97.18",
                    "97.14"
                ],
                [
                    "en",
                    "86.71",
                    "[BOLD] 87.99",
                    "88.50",
                    "[BOLD] 89.61",
                    "95.30",
                    "[BOLD] 95.59",
                    "96.40",
                    "96.30"
                ],
                [
                    "ar",
                    "84.14",
                    "84.17",
                    "81.41",
                    "[ITALIC] 81.11",
                    "94.43",
                    "[BOLD] 94.85",
                    "95.50",
                    "95.37"
                ],
                [
                    "hi",
                    "83.45",
                    "[BOLD] 86.89",
                    "85.64",
                    "85.27",
                    "96.15",
                    "96.21",
                    "96.59",
                    "[BOLD] 96.67"
                ],
                [
                    "it",
                    "89.96",
                    "[BOLD] 92.07",
                    "91.27",
                    "[BOLD] 92.62",
                    "97.32",
                    "[BOLD] 97.80",
                    "98.18",
                    "98.31"
                ],
                [
                    "es",
                    "88.11",
                    "[BOLD] 89.81",
                    "88.58",
                    "[BOLD] 89.63",
                    "94.84",
                    "[BOLD] 95.44",
                    "96.21",
                    "[BOLD] 96.84"
                ],
                [
                    "cs",
                    "68.66",
                    "[BOLD] 72.65",
                    "71.02",
                    "[BOLD] 73.61",
                    "91.75",
                    "[BOLD] 93.71",
                    "95.29",
                    "95.31"
                ]
            ],
            "title": "Table 6: Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant improvement (degradation) by a bootstrapped Z-test, p"
        },
        "insight": "Table 6 (morphosyntactic attribute tagging micro-F1). [CONTINUE] In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages \u2014 especially Tamil and Turkish \u2014 where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal."
    },
    {
        "id": "127",
        "table": {
            "header": [
                "Test set embeddings",
                "Missing embeddings",
                "Full vocabulary",
                "Full vocabulary",
                "OOV (UD)",
                "OOV (UD)"
            ],
            "rows": [
                [
                    "char\u2192tag",
                    "[EMPTY]",
                    "w/o",
                    "with",
                    "w/o",
                    "with"
                ],
                [
                    "Persian",
                    "2.2%",
                    "0.03",
                    "[BOLD] 0.41",
                    "[BOLD] 0.83",
                    "[BOLD] 0.81"
                ],
                [
                    "Hindi",
                    "3.8%",
                    "[BOLD] 0.59",
                    "0.21",
                    "[BOLD] 3.61",
                    "0.36"
                ],
                [
                    "English",
                    "4.5%",
                    "[BOLD] 0.83",
                    "0.25",
                    "[BOLD] 3.26",
                    "0.49"
                ],
                [
                    "Spanish",
                    "5.2%",
                    "0.33",
                    "-0.26",
                    "1.03",
                    "-0.66"
                ],
                [
                    "Italian",
                    "6.6%",
                    "[BOLD] 0.84",
                    "0.28",
                    "[BOLD] 1.83",
                    "0.21"
                ],
                [
                    "Danish",
                    "7.8%",
                    "0.65",
                    "[BOLD] 0.99",
                    "[BOLD] 2.41",
                    "[BOLD] 1.72"
                ],
                [
                    "Hebrew",
                    "9.2%",
                    "[BOLD] 1.25",
                    "[BOLD] 0.40",
                    "[BOLD] 3.03",
                    "0.06"
                ],
                [
                    "Swedish",
                    "9.2%",
                    "[BOLD] 1.50",
                    "[BOLD] 0.55",
                    "[BOLD] 4.75",
                    "[BOLD] 1.79"
                ],
                [
                    "Bulgarian",
                    "9.4%",
                    "[BOLD] 0.96",
                    "0.12",
                    "[BOLD] 1.83",
                    "-0.11"
                ],
                [
                    "Czech",
                    "10.6%",
                    "[BOLD] 2.24",
                    "[BOLD] 1.32",
                    "[BOLD] 5.84",
                    "[BOLD] 2.20"
                ],
                [
                    "Latvian",
                    "11.1%",
                    "[BOLD] 2.87",
                    "[BOLD] 1.03",
                    "[BOLD] 7.29",
                    "[BOLD] 2.71"
                ],
                [
                    "Hungarian",
                    "11.6%",
                    "[BOLD] 2.62",
                    "[BOLD] 2.01",
                    "[BOLD] 5.76",
                    "[BOLD] 4.85"
                ],
                [
                    "Turkish",
                    "14.5%",
                    "[BOLD] 1.73",
                    "[BOLD] 1.69",
                    "[BOLD] 3.58",
                    "[BOLD] 2.71"
                ],
                [
                    "Tamil*",
                    "16.2%",
                    "[BOLD] 2.52",
                    "0.35",
                    "2.09",
                    "1.35"
                ],
                [
                    "Russian",
                    "16.5%",
                    "[BOLD] 2.17",
                    "[BOLD] 1.82",
                    "[BOLD] 4.55",
                    "[BOLD] 3.52"
                ],
                [
                    "Greek",
                    "17.5%",
                    "[BOLD] 1.07",
                    "0.34",
                    "[BOLD] 3.30",
                    "1.17"
                ],
                [
                    "Indonesian",
                    "19.1%",
                    "[BOLD] 0.46",
                    "0.25",
                    "[BOLD] 1.19",
                    "0.75"
                ],
                [
                    "Kazakh*",
                    "21.0%",
                    "2.01",
                    "1.24",
                    "[BOLD] 5.34",
                    "[BOLD] 4.20"
                ],
                [
                    "Vietnamese",
                    "21.9%",
                    "0.53",
                    "[BOLD] 1.18",
                    "1.07",
                    "[BOLD] 5.73"
                ],
                [
                    "Romanian",
                    "27.1%",
                    "[BOLD] 1.49",
                    "[BOLD] 0.47",
                    "[BOLD] 4.22",
                    "[BOLD] 1.24"
                ],
                [
                    "Arabic",
                    "27.1%",
                    "[BOLD] 1.23",
                    "[BOLD] 0.32",
                    "[BOLD] 2.15",
                    "0.22"
                ],
                [
                    "Basque",
                    "35.3%",
                    "[BOLD] 2.39",
                    "[BOLD] 1.06",
                    "[BOLD] 5.42",
                    "[BOLD] 1.68"
                ],
                [
                    "Chinese",
                    "69.9%",
                    "[BOLD] 4.19",
                    "[BOLD] 2.57",
                    "[BOLD] 9.52",
                    "[BOLD] 5.24"
                ]
            ],
            "title": "Table 7: Absolute gain in POS tagging accuracy from using mimick for 10,000-token datasets (all tokens for Tamil and Kazakh). Bold denotes statistical significance (McNemar\u2019s test,p<0.01)."
        },
        "insight": "Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR\u2192TAG concatenation, with 10,000 tokens of training data. We obtain statistically significant improvements in most languages, even when CHAR\u2192TAG is included. These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns. While test set OOVs are a strength of the CHAR\u2192TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization."
    },
    {
        "id": "128",
        "table": {
            "header": [
                "Scenario",
                "Human Model Accuracy",
                "Human Model Perplexity",
                "Script Model Accuracy",
                "Script Model Perplexity",
                "Linguistic Model Accuracy",
                "Linguistic Model Perplexity",
                "Tily Model Accuracy",
                "Tily Model Perplexity"
            ],
            "rows": [
                [
                    "Grocery Shopping",
                    "74.80",
                    "2.13",
                    "68.17",
                    "3.16",
                    "53.85",
                    "6.54",
                    "32.89",
                    "24.48"
                ],
                [
                    "Repairing a flat bicycle tyre",
                    "78.34",
                    "2.72",
                    "62.09",
                    "3.89",
                    "51.26",
                    "6.38",
                    "29.24",
                    "19.08"
                ],
                [
                    "Riding a public bus",
                    "72.19",
                    "2.28",
                    "64.57",
                    "3.67",
                    "52.65",
                    "6.34",
                    "32.78",
                    "23.39"
                ],
                [
                    "Getting a haircut",
                    "71.06",
                    "2.45",
                    "58.82",
                    "3.79",
                    "42.82",
                    "7.11",
                    "28.70",
                    "15.40"
                ],
                [
                    "Planting a tree",
                    "71.86",
                    "2.46",
                    "59.32",
                    "4.25",
                    "47.80",
                    "7.31",
                    "28.14",
                    "24.28"
                ],
                [
                    "Borrowing book from library",
                    "77.49",
                    "1.93",
                    "64.07",
                    "3.55",
                    "43.29",
                    "8.40",
                    "33.33",
                    "20.26"
                ],
                [
                    "Taking Bath",
                    "81.29",
                    "1.84",
                    "67.42",
                    "3.14",
                    "61.29",
                    "4.33",
                    "43.23",
                    "16.33"
                ],
                [
                    "Going on a train",
                    "70.79",
                    "2.39",
                    "58.73",
                    "4.20",
                    "47.62",
                    "7.68",
                    "30.16",
                    "35.11"
                ],
                [
                    "Baking a cake",
                    "76.43",
                    "2.16",
                    "61.79",
                    "5.11",
                    "46.40",
                    "9.16",
                    "24.07",
                    "23.67"
                ],
                [
                    "Flying in an airplane",
                    "62.04",
                    "3.08",
                    "61.31",
                    "4.01",
                    "48.18",
                    "7.27",
                    "30.90",
                    "30.18"
                ],
                [
                    "Average",
                    "73.63",
                    "2.34",
                    "62.63",
                    "3.88",
                    "49.52",
                    "7.05",
                    "31.34",
                    "23.22"
                ]
            ],
            "title": "Table 3: Accuracies (in %) and perplexities for different models and scenarios. The script model substantially outperforms linguistic and base models (with p<0.001, significance tested with McNemar\u2019s test [Everitt1992]). As expected, the human prediction model outperforms the script model (with p<0.001, significance tested by McNemar\u2019s test)."
        },
        "insight": "See Table 3 for the averages across 10 scenarios. [CONTINUE] As we can see from Table 3, the perplexity scores are consistent with the accuracies: the script model again outperforms other methods, and, as expected, all the models are weaker than humans."
    },
    {
        "id": "129",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Standard Decoder MRR",
                "[BOLD] Standard Decoder P@1",
                "[BOLD] Our Decoder MRR",
                "[BOLD] Our Decoder P@1"
            ],
            "rows": [
                [
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline"
                ],
                [
                    "Retrieval",
                    "81.08",
                    "65.45",
                    "-",
                    "-"
                ],
                [
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons"
                ],
                [
                    "Seq2seq",
                    "75.29",
                    "58.85",
                    "74.46",
                    "57.06"
                ],
                [
                    "+  [ITALIC] encode evd",
                    "83.73",
                    "71.59",
                    "88.24",
                    "78.76"
                ],
                [
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models"
                ],
                [
                    "Dec-shared",
                    "79.80",
                    "65.57",
                    "[BOLD] 95.18",
                    "[BOLD] 90.91"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "[BOLD] 94.33",
                    "[BOLD] 89.76",
                    "93.48",
                    "87.91"
                ],
                [
                    "Dec-separate",
                    "86.85",
                    "76.74",
                    "91.70",
                    "84.72"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "88.53",
                    "79.05",
                    "92.77",
                    "86.46"
                ]
            ],
            "title": "Table 4: Evaluation on topic relevance\u2014models that generate arguments highly related with OP should be ranked high by a separately trained relevance estimation model, i.e., higher Mean Reciprocal Rank (MRR) and Precision at 1 (P@1) scores. All models trained with evidence significantly outperform seq2seq trained without evidence (approximation randomization testing, p<0.0001)."
        },
        "insight": "Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in Table 4. The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model."
    },
    {
        "id": "130",
        "table": {
            "header": [
                "[EMPTY]",
                "Estimate Human",
                "Estimate Script",
                "Estimate Linguistic",
                "Estimate Base",
                "Std. Error Human",
                "Std. Error Script",
                "Std. Error Linguistic",
                "Std. Error Base",
                "Pr(>\u2223 [ITALIC] z\u2223) Human",
                "Pr(>\u2223 [ITALIC] z\u2223) Script",
                "Pr(>\u2223 [ITALIC] z\u2223) Linguistic",
                "Pr(>\u2223 [ITALIC] z\u2223) Base"
            ],
            "rows": [
                [
                    "(Intercept)",
                    "-3.4",
                    "-3.418",
                    "-3.245",
                    "-3.061",
                    "0.244",
                    "0.279",
                    "0.321",
                    "0.791",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "0.00011 ***"
                ],
                [
                    "recency",
                    "1.322",
                    "1.322",
                    "1.324",
                    "1.322",
                    "0.095",
                    "0.095",
                    "0.096",
                    "0.097",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***"
                ],
                [
                    "frequency",
                    "0.097",
                    "0.103",
                    "0.112",
                    "0.114",
                    "0.098",
                    "0.097",
                    "0.098",
                    "0.102",
                    "0.317",
                    "0.289",
                    "0.251",
                    "0.262"
                ],
                [
                    "pastObj",
                    "0.407",
                    "0.396",
                    "0.423",
                    "0.395",
                    "0.293",
                    "0.294",
                    "0.295",
                    "0.3",
                    "0.165",
                    "0.178",
                    "0.151",
                    "0.189"
                ],
                [
                    "pastSubj",
                    "-0.967",
                    "-0.973",
                    "-0.909",
                    "-0.926",
                    "0.559",
                    "0.564",
                    "0.562",
                    "0.565",
                    "0.0838 .",
                    "0.0846 .",
                    "0.106",
                    "0.101"
                ],
                [
                    "pastExpPronoun",
                    "1.603",
                    "1.619",
                    "1.616",
                    "1.602",
                    "0.21",
                    "0.207",
                    "0.208",
                    "0.245",
                    "2.19e-14 ***",
                    "5.48e-15 ***",
                    "7.59e-15 ***",
                    "6.11e-11 ***"
                ],
                [
                    "depTypeSubj",
                    "2.939",
                    "2.942",
                    "2.656",
                    "2.417",
                    "0.299",
                    "0.347",
                    "0.429",
                    "1.113",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "5.68e-10 ***",
                    "0.02994 *"
                ],
                [
                    "depTypeObj",
                    "1.199",
                    "1.227",
                    "0.977",
                    "0.705",
                    "0.248",
                    "0.306",
                    "0.389",
                    "1.109",
                    "1.35e-06 ***",
                    "6.05e-05 ***",
                    "0.0119 *",
                    "0.525"
                ],
                [
                    "surprisal",
                    "-0.04",
                    "-0.006",
                    "0.002",
                    "-0.131",
                    "0.099",
                    "0.097",
                    "0.117",
                    "0.387",
                    "0.684",
                    "0.951",
                    "0.988",
                    "0.735"
                ],
                [
                    "residualEntropy",
                    "-0.009",
                    "0.023",
                    "-0.141",
                    "-0.128",
                    "0.088",
                    "0.128",
                    "0.168",
                    "0.258",
                    "0.916",
                    "0.859",
                    "0.401",
                    "0.619"
                ]
            ],
            "title": "Table 5: Coefficients obtained from regression analysis for different models. Two NP types considered: full NP and Pronoun/ProperNoun, with base class full NP. Significance: \u2018***\u2019 <0.001, \u2018**\u2019 <0.01, \u2018*\u2019 <0.05, and \u2018.\u2019 <0.1."
        },
        "insight": "The results of all four logistic regression models are shown in Table 5. [CONTINUE] The results for the full dataset are fully consistent with the findings shown in Table 5: there was no significant effect of surprisal on referring expression type. [CONTINUE] In order to replicate their settings as closely as possible, we also included residualEntropy as a predictor in our model (see last predictor in Table 5); however, this did not change the results."
    },
    {
        "id": "131",
        "table": {
            "header": [
                "Model",
                "Downstream",
                "Keller dobj",
                "Keller amod",
                "Keller average",
                "SP-10K nsubj",
                "SP-10K dobj",
                "SP-10K amod",
                "SP-10K average"
            ],
            "rows": [
                [
                    "word2vec",
                    "Friendly",
                    "0.29",
                    "0.28",
                    "0.29",
                    "0.32",
                    "0.53",
                    "0.62",
                    "0.49"
                ],
                [
                    "GloVe",
                    "Friendly",
                    "0.37",
                    "0.32",
                    "0.35",
                    "0.57",
                    "0.60",
                    "0.68",
                    "0.62"
                ],
                [
                    "D-embeddings",
                    "Friendly",
                    "0.19",
                    "0.22",
                    "0.21",
                    "0.66",
                    "0.71",
                    "0.77",
                    "0.71"
                ],
                [
                    "ELMo",
                    "Friendly",
                    "0.23",
                    "0.06",
                    "0.15",
                    "0.09",
                    "0.29",
                    "0.38",
                    "0.25"
                ],
                [
                    "BERT (static)",
                    "Friendly",
                    "0.11",
                    "0.05",
                    "0.08",
                    "0.25",
                    "0.32",
                    "0.27",
                    "0.28"
                ],
                [
                    "BERT (dynamic)",
                    "Friendly",
                    "0.19",
                    "0.23",
                    "0.21",
                    "0.35",
                    "0.45",
                    "0.51",
                    "0.41"
                ],
                [
                    "PP",
                    "Unfriendly",
                    "[BOLD] 0.66",
                    "0.26",
                    "0.46",
                    "0.75",
                    "0.74",
                    "0.75",
                    "0.75"
                ],
                [
                    "DS",
                    "Unfriendly",
                    "0.53",
                    "0.32",
                    "0.43",
                    "0.59",
                    "0.65",
                    "0.67",
                    "0.64"
                ],
                [
                    "NN",
                    "Unfriendly",
                    "0.16",
                    "0.13",
                    "0.15",
                    "0.70",
                    "0.68",
                    "0.68",
                    "0.69"
                ],
                [
                    "MWE",
                    "Friendly",
                    "0.63",
                    "[BOLD] 0.43\u2020",
                    "[BOLD] 0.53\u2020",
                    "[BOLD] 0.76",
                    "[BOLD] 0.79\u2020",
                    "[BOLD] 0.78",
                    "[BOLD] 0.78\u2020"
                ]
            ],
            "title": "Table 1: Results on different SP acquisition evaluation sets. As Keller is created based on the PP distribution and has relatively small size while SP-10K is created based on random sampling and has a much larger size, we treat the performance on SP-10K as the main evaluation metric. Spearman\u2019s correlation between predicated plausibility and annotations are reported. The best performing models are denoted with bold font. \u2020 indicates statistical significant (p <0.005) overall baseline methods."
        },
        "insight": "The experimental results are shown in Table 1. As Keller is created based on the PP distribution and have relatively small size while SP-10K is created based on random sampling and has a much larger size, we treat the performance on SP-10K as the major evaluation. Our embeddings significantly outperform other baselines, especially embedding based baselines. The only exception is PP on the Keller dataset due to its biased distribution. In addition, there are other interesting observations. First, compared with 'dobj' and 'nsubj', 'amod' is simpler for word2vec and GloVe. The reason behind is that conventional embeddings only capture the co-occurrence information, which is enough to predict the selectional preference of"
    },
    {
        "id": "132",
        "table": {
            "header": [
                "Model word2vec",
                "Model word2vec",
                "noun 0.41",
                "verb 0.28",
                "adjective 0.44",
                "overall 0.38"
            ],
            "rows": [
                [
                    "Glove",
                    "Glove",
                    "0.40",
                    "0.22",
                    "0.53",
                    "0.37"
                ],
                [
                    "D-embedding",
                    "D-embedding",
                    "0.41",
                    "0.27",
                    "0.38",
                    "0.36"
                ],
                [
                    "nsubj",
                    "h",
                    "0.46",
                    "0.29",
                    "0.54",
                    "0.43"
                ],
                [
                    "nsubj",
                    "t",
                    "0.45",
                    "0.25",
                    "0.48",
                    "0.40"
                ],
                [
                    "nsubj",
                    "h+t",
                    "0.44",
                    "0.23",
                    "0.50",
                    "0.40"
                ],
                [
                    "nsubj",
                    "[h,t]",
                    "0.47",
                    "0.27",
                    "0.51",
                    "0.42"
                ],
                [
                    "dobj",
                    "h",
                    "0.46",
                    "0.27",
                    "0.45",
                    "0.41"
                ],
                [
                    "dobj",
                    "t",
                    "0.45",
                    "0.23",
                    "0.46",
                    "0.40"
                ],
                [
                    "dobj",
                    "h+t",
                    "0.45",
                    "0.20",
                    "0.45",
                    "0.38"
                ],
                [
                    "dobj",
                    "[h,t]",
                    "0.46",
                    "0.25",
                    "0.48",
                    "0.42"
                ],
                [
                    "amod",
                    "h",
                    "0.47",
                    "0.25",
                    "0.52",
                    "0.37"
                ],
                [
                    "amod",
                    "t",
                    "0.46",
                    "0.24",
                    "0.50",
                    "0.38"
                ],
                [
                    "amod",
                    "h+t",
                    "0.46",
                    "0.24",
                    "0.52",
                    "0.38"
                ],
                [
                    "amod",
                    "[h,t]",
                    "0.47",
                    "0.26",
                    "0.52",
                    "0.38"
                ],
                [
                    "center",
                    "h",
                    "0.51",
                    "[BOLD] 0.33",
                    "[BOLD] 0.57",
                    "[BOLD] 0.48"
                ],
                [
                    "center",
                    "t",
                    "0.51",
                    "0.30",
                    "0.56",
                    "0.47"
                ],
                [
                    "center",
                    "h+t",
                    "[BOLD] 0.52",
                    "0.31",
                    "0.54",
                    "0.46"
                ],
                [
                    "center",
                    "[h,t]",
                    "0.51",
                    "0.32",
                    "0.57",
                    "[BOLD] 0.48"
                ]
            ],
            "title": "Table 3: Spearman\u2019s correlation of different embeddings for the WS measurement. \u2018nsubj\u2019, \u2018dobj\u2019, \u2018amod\u2019 represents the embeddings of the corresponding relation and \u2018center\u2019 indicates the center embeddings. h, t, h+t, and [h,t] refer to the head, tail, sum of two embeddings, and the concatenation of them, respectively. The best scores are marked in bold fonts."
        },
        "insight": "Results are reported in Table 3 with several observations. First, our model achieves the best overall performance and significantly better on nouns, which can be explained by that nouns appear in all 9The only possible SP relation between nouns and adjectives is 'amod', while multiple SP relations could exist between nouns and verbs, and co occurrence information cannot effectively distinguish them. (2017) that three relations while most of the verbs and adjectives only appear in one or two relations. This result is promising since it is analyzed by Solovyev et al. two-thirds of the frequent words are nouns; thus there are potential benefits if our embeddings are used in downstream NLP tasks. Second, the center embeddings achieve the best performance against all the other relationdependent embeddings, which demonstrates the effectiveness of our model in learning relationdependent information over words and also enhancing their overall semantics."
    },
    {
        "id": "133",
        "table": {
            "header": [
                "Model",
                "WS",
                "Dimension",
                "Training Time"
            ],
            "rows": [
                [
                    "ELMo",
                    "0.434",
                    "512",
                    "\u224840"
                ],
                [
                    "BERT",
                    "0.486",
                    "768",
                    "\u2248300"
                ],
                [
                    "MWE",
                    "0.476",
                    "300",
                    "4.17"
                ]
            ],
            "title": "Table 4: Comparison of MWE against language models on the WS task. Overall performance, embedding dimension, and training time (days) on a single GPU are reported."
        },
        "insight": "We also compare MWE with pre-trained contextualized word embedding models in Table 4 for this task, with overall performance, embedding dimensions, and training times reported. It is observed that that MWE outperforms ELMo and achieves comparable results with BERT with smaller embedding dimension and much less training complexities."
    },
    {
        "id": "134",
        "table": {
            "header": [
                "Training Strategy",
                "Averaged SPA",
                "Overall WS"
            ],
            "rows": [
                [
                    "[ITALIC] \u03bb = 1",
                    "0.762",
                    "0.476"
                ],
                [
                    "[ITALIC] \u03bb = 0",
                    "0.073",
                    "0.018"
                ],
                [
                    "[ITALIC] \u03bb = 0.5",
                    "0.493",
                    "0.323"
                ],
                [
                    "Alternating optimization",
                    "0.775",
                    "0.476"
                ]
            ],
            "title": "Table 5: Comparisons of different training strategies."
        },
        "insight": "As shown in Table 5, we compare our model with several different strategies. The first one is to put all weights to the center embedding (fix \u03bb to 1), which never updates the local relational embeddings. As a result, it can achieve similar performance on word similarity measurement but is inferior in SP acquisition because no relationdependent information is preserved."
    },
    {
        "id": "135",
        "table": {
            "header": [
                "Representation",
                "Batches / update",
                "Learning rate",
                "Test BLEU"
            ],
            "rows": [
                [
                    "Plain BPE",
                    "1",
                    "0.025",
                    "27.5"
                ],
                [
                    "Plain BPE",
                    "1",
                    "0.2",
                    "27.2"
                ],
                [
                    "Plain BPE",
                    "8",
                    "0.2",
                    "28.9"
                ],
                [
                    "Linearized derivation",
                    "1",
                    "0.025",
                    "25.6"
                ],
                [
                    "Linearized derivation",
                    "1",
                    "0.2",
                    "25.6"
                ],
                [
                    "Linearized derivation",
                    "8",
                    "0.2",
                    "28.7"
                ]
            ],
            "title": "Table 3: Single Transformers trained to convergence on 1M WAT Ja-En, batch size 4096"
        },
        "insight": "results in Table 3 show that large batch training can significantly improve the performance of single Transformers, particularly when trained to produce longer sequences. Accumulating the gradient over 8 batches of size 4096 gives a 3 BLEU improvement for the linear derivation model. It has been suggested that decaying the learning rate can have a similar effect to large batch training (Smith et al., 2017), but reducing the initial learning rate by a factor of 8 alone did not give the same improvements."
    },
    {
        "id": "136",
        "table": {
            "header": [
                "Architecture",
                "Representation",
                "Dev BLEU",
                "Test BLEU"
            ],
            "rows": [
                [
                    "Seq2seq (8-model ensemble)",
                    "Best WAT17 result Morishita et\u00a0al. ( 2017 )",
                    "-",
                    "28.4"
                ],
                [
                    "Seq2seq",
                    "Plain BPE",
                    "21.6",
                    "21.2"
                ],
                [
                    "Seq2seq",
                    "Linearized derivation",
                    "21.9",
                    "21.2"
                ],
                [
                    "Transformer",
                    "Plain BPE",
                    "28.0",
                    "28.9"
                ],
                [
                    "Transformer",
                    "Linearized tree",
                    "28.2",
                    "28.4"
                ],
                [
                    "Transformer",
                    "Linearized derivation",
                    "28.5",
                    "28.7"
                ],
                [
                    "Transformer",
                    "POS/BPE",
                    "28.5",
                    "29.1"
                ]
            ],
            "title": "Table 4: Single models on Ja-En. Previous evaluation result included for comparison."
        },
        "insight": "Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017). Our syntax models achieve similar results despite producing much longer sequences."
    },
    {
        "id": "137",
        "table": {
            "header": [
                "External representation",
                "Internal representation",
                "Test BLEU"
            ],
            "rows": [
                [
                    "Plain BPE",
                    "Plain BPE",
                    "29.2"
                ],
                [
                    "Linearized derivation",
                    "Linearized derivation",
                    "28.8"
                ],
                [
                    "Linearized tree",
                    "Plain BPE",
                    "28.9"
                ],
                [
                    "Plain BPE",
                    "Linearized derivation",
                    "28.8"
                ],
                [
                    "Linearized derivation",
                    "Plain BPE",
                    "29.4\u2020"
                ],
                [
                    "POS/BPE",
                    "Plain BPE",
                    "29.3\u2020"
                ],
                [
                    "Plain BPE",
                    "POS/BPE",
                    "29.4\u2020"
                ]
            ],
            "title": "Table 5: Ja-En Transformer ensembles: \u2020 marks significant improvement on plain BPE baseline shown in Table 4 (p<0.05 using bootstrap resampling (Koehn et\u00a0al., 2007))."
        },
        "insight": "Ensembles of two identical models trained with different seeds only slightly improve over the single model (Table 5). However, an ensemble of models producing plain BPE and linearized derivations improves by 0.5 BLEU over the plain BPE baseline."
    },
    {
        "id": "138",
        "table": {
            "header": [
                "config",
                "English\u2192\u22c6 en-cs",
                "English\u2192\u22c6 en-de",
                "English\u2192\u22c6 en-fi",
                "English\u2192\u22c6 en-lv",
                "English\u2192\u22c6 en-ru",
                "English\u2192\u22c6 en-tr",
                "\u22c6\u2192English cs-en",
                "\u22c6\u2192English de-en",
                "\u22c6\u2192English fi-en",
                "\u22c6\u2192English lv-en",
                "\u22c6\u2192English ru-en",
                "\u22c6\u2192English tr-en"
            ],
            "rows": [
                [
                    "basic",
                    "20.7",
                    "25.8",
                    "22.2",
                    "16.9",
                    "33.3",
                    "18.5",
                    "26.8",
                    "31.2",
                    "26.6",
                    "21.1",
                    "36.4",
                    "24.4"
                ],
                [
                    "split",
                    "20.7",
                    "26.1",
                    "22.6",
                    "17.0",
                    "33.3",
                    "18.7",
                    "26.9",
                    "31.7",
                    "26.9",
                    "21.3",
                    "36.7",
                    "24.7"
                ],
                [
                    "unk",
                    "20.9",
                    "26.5",
                    "25.4",
                    "18.7",
                    "33.8",
                    "20.6",
                    "26.9",
                    "31.4",
                    "27.6",
                    "22.7",
                    "37.5",
                    "25.2"
                ],
                [
                    "metric",
                    "20.1",
                    "26.6",
                    "22.0",
                    "17.9",
                    "32.0",
                    "19.9",
                    "27.4",
                    "33.0",
                    "27.6",
                    "22.0",
                    "36.9",
                    "25.6"
                ],
                [
                    "[ITALIC] range",
                    "0.6",
                    "0.8",
                    "0.6",
                    "1.0",
                    "1.3",
                    "1.4",
                    "0.6",
                    "1.8",
                    "1.0",
                    "0.9",
                    "0.5",
                    "1.2"
                ],
                [
                    "basic [ITALIC] lc",
                    "21.2",
                    "26.3",
                    "22.5",
                    "17.4",
                    "33.3",
                    "18.9",
                    "27.7",
                    "32.5",
                    "27.5",
                    "22.0",
                    "37.3",
                    "25.2"
                ],
                [
                    "split [ITALIC] lc",
                    "21.3",
                    "26.6",
                    "22.9",
                    "17.5",
                    "33.4",
                    "19.1",
                    "27.8",
                    "32.9",
                    "27.8",
                    "22.2",
                    "37.5",
                    "25.4"
                ],
                [
                    "unk [ITALIC] lc",
                    "21.4",
                    "27.0",
                    "25.6",
                    "19.1",
                    "33.8",
                    "21.0",
                    "27.8",
                    "32.6",
                    "28.3",
                    "23.6",
                    "38.3",
                    "25.9"
                ],
                [
                    "metric [ITALIC] lc",
                    "20.6",
                    "27.2",
                    "22.4",
                    "18.5",
                    "32.8",
                    "20.4",
                    "28.4",
                    "34.2",
                    "28.5",
                    "23.0",
                    "37.8",
                    "26.4"
                ],
                [
                    "[ITALIC] rangelc",
                    "0.6",
                    "0.9",
                    "0.5",
                    "1.1",
                    "0.6",
                    "1.5",
                    "0.7",
                    "1.7",
                    "1.0",
                    "1.0",
                    "0.5",
                    "1.2"
                ]
            ],
            "title": "Table 1: BLEU score variation across WMT\u201917 language arcs for cased (top) and uncased (bottom) BLEU. Each column varies the processing of the \u201conline-B\u201d system output and its references. basic denotes basic user-supplied tokenization, split adds compound splitting, unk replaces words not appearing at least twice in the training data with UNK, and metric denotes the metric-supplied tokenization used by WMT. The range row lists the difference between the smallest and largest scores, excluding unk."
        },
        "insight": "Table 1 demonstrates the effect of computing BLEU scores with different reference tokenizations. [CONTINUE] The changes in each column show the effect these different schemes have, as high as 1.8 for one arc, and averaging around 1.0. The biggest is the treatment of case, which is well known, yet many papers are not clear about whether they report cased or case-insensitive BLEU. [CONTINUE] The variations in Table 1 are only some of the possible configurations, since there is no limit to the preprocessing that a group could apply."
    },
    {
        "id": "139",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] % perf",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F"
            ],
            "rows": [
                [
                    "[BOLD] SPMRL",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] baseline",
                    "69.65",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] UDPipe",
                    "89.65",
                    "93.52",
                    "68.82",
                    "79.29"
                ],
                [
                    "[ITALIC] yap",
                    "94.25",
                    "86.33",
                    "96.33",
                    "91.05"
                ],
                [
                    "[ITALIC] RF",
                    "[BOLD] 98.19",
                    "[BOLD] 97.59",
                    "[BOLD] 96.57",
                    "[BOLD] 97.08"
                ],
                [
                    "[ITALIC] DNN",
                    "97.27",
                    "95.90",
                    "95.01",
                    "95.45"
                ],
                [
                    "[BOLD] Wiki5K",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] baseline",
                    "67.61",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] UDPipe",
                    "87.39",
                    "92.03",
                    "64.88",
                    "76.11"
                ],
                [
                    "[ITALIC] yap",
                    "92.66",
                    "85.55",
                    "92.34",
                    "88.81"
                ],
                [
                    "[ITALIC] RF",
                    "[BOLD] 97.63",
                    "[BOLD] 97.41",
                    "[BOLD] 95.31",
                    "[BOLD] 96.35"
                ],
                [
                    "[ITALIC] DNN",
                    "95.72",
                    "94.95",
                    "92.22",
                    "93.56"
                ]
            ],
            "title": "Table 2: System performance on the SPMRL and Wiki5K datasets."
        },
        "insight": "Table 2 shows the results of several systems on both datasets.7 The column '% perf' indicates the proportion of perfectly segmented super-tokens, while the next three columns indicate precision, recall and F-score for boundary detection, not including the trivial final position characters. The first baseline strategy of not segmenting anything is given in the first row, and unsurprisingly gets many cases right, but performs badly overall. A more intelligent baseline is provided by UDPipe (Straka et al. 2016; retrained on the SPMRL data), which, for super-tokens in morphologically rich languages such as Hebrew, implements a 'most common segmentation' baseline [CONTINUE] (i.e. each super-token is given its most common segmentation from training data, forgoing segmentation for OOV items).8 Results for yap represent pure segmentation performance from the previous state of the art (More and Tsarfaty, 2016). The best two approaches in the present paper are represented next: the Extra Trees Random Forest variant,9 called RFTokenizer, is labeled RF and the DNN-based system is labeled DNN. Surprisingly, while the DNN is a close runner up, the best performance is achieved by the RFTokenizer, de [CONTINUE] spite not having access to word embeddings. Its high performance on the SPMRL dataset makes it difficult to converge to a better solution using the DNN, though it is conceivable that substantially more data, a better feature representation and/or more hyperparameter tuning could equal or surpass the RFTokenizer's performance. Coupled with a lower cost in system resources and external dependencies, and the ability to forgo large model files to store word embeddings, we consider the RFTokenizer solution to be better given the current training data size. Performance on the out of domain dataset is encouragingly nearly as good as on SPMRL, suggesting our features are robust. This is especially clear compared to UDPipe and yap, which degrade more substantially. A key advantage of the present approach is its comparatively high precision. While other approaches have good recall, and yap approaches RFTokenizer on recall for SPMRL, RFTokenizer's reduction in spurious segmentations boosts its F-score substantially. To see why, we examine some errors in the next section, and perform feature ablations in the following one."
    },
    {
        "id": "140",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] % perf",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F"
            ],
            "rows": [
                [
                    "[BOLD] SPMRL",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "FINAL",
                    "98.19",
                    "97.59",
                    "96.57",
                    "97.08"
                ],
                [
                    "-expansion",
                    "98.01",
                    "97.25",
                    "96.35",
                    "96.80"
                ],
                [
                    "-vowels",
                    "97.99",
                    "97.55",
                    "95.97",
                    "96.75"
                ],
                [
                    "-letters",
                    "97.77",
                    "96.98",
                    "95.73",
                    "96.35"
                ],
                [
                    "-letr-vowl",
                    "97.57",
                    "97.56",
                    "94.44",
                    "95.97"
                ],
                [
                    "-lexicon",
                    "94.79",
                    "92.08",
                    "91.46",
                    "91.77"
                ],
                [
                    "[BOLD] Wiki5K",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "FINAL",
                    "97.63",
                    "97.41",
                    "95.31",
                    "96.35"
                ],
                [
                    "-expansion",
                    "97.33",
                    "96.64",
                    "95.31",
                    "95.97"
                ],
                [
                    "-vowels",
                    "97.51",
                    "97.56",
                    "94.87",
                    "96.19"
                ],
                [
                    "-letters",
                    "97.27",
                    "96.89",
                    "94.71",
                    "95.79"
                ],
                [
                    "-letr-vowl",
                    "96.72",
                    "97.17",
                    "92.77",
                    "94.92"
                ],
                [
                    "-lexicon",
                    "94.72",
                    "92.53",
                    "91.51",
                    "92.01"
                ]
            ],
            "title": "Table 3: Effects of removing features on performance, ordered by descending F-score impact on SPMRL."
        },
        "insight": "Table 3 gives an overview of the impact on performance when specific features are removed: the entire lexicon, lexicon expansion, letter identity, 'vowel' features from Section 3.1, and both of the latter. Performance is high even in ablation scenarios, though we keep in mind that baselines for the task are high (e.g. 'most frequent lookup', the UDPipe strategy, achieves close to 90%). The results show the centrality of the lexicon: removing lexicon lookup features degrades performance by about 3.5% perfect accuracy, or 5.5 F-score points. All other ablations impact performance by less than 1% or 1.5 F-score points. Expanding the lexicon using Wikipedia data offers a contribution of 0.3\u20130.4 points, confirming the original lexicon's incompleteness.10 Looking more closely at the other features, it is surprising that identity of the letters is not crucial, as long as we have access to dictionary lookup using the letters. Nevertheless, removing letter identity impacts especially boundary recall, perhaps [CONTINUE] does not break down drastically. The impact on Wiki5k is stronger, possibly because the necessary memorization of familiar contexts is less effective out of domain. [CONTINUE] because some letters receive identical lookup values (e.g. single letter prepositions such as b 'in', l 'to') but have different segmentation likelihoods. The 'vowel' features, though ostensibly redundant with letter identity, help a little, causing 0.33 SPMRL F-score point degradation if removed. A"
    },
    {
        "id": "141",
        "table": {
            "header": [
                "Perturbation radius",
                "[ITALIC] \u03b4=1",
                "[ITALIC] \u03b4=2",
                "[ITALIC] \u03b4=3"
            ],
            "rows": [
                [
                    "SST-word",
                    "49",
                    "674",
                    "5,136"
                ],
                [
                    "SST-character",
                    "206",
                    "21,116",
                    "1,436,026"
                ],
                [
                    "AG-character",
                    "722",
                    "260,282",
                    "-"
                ]
            ],
            "title": "Table 3: Maximum perturbation space size in the SST and AG News test set using word / character substitutions, which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification."
        },
        "insight": "The allowable input perturbation space is much larger than for word-level synonym substitutions, as shown in Table 3. [CONTINUE] In Table 3, we show the maximum perturbation space size in the SST and AG News test set for different perturbation radii \u03b4. [CONTINUE] This number grows exponentially as \u03b4 increases."
    },
    {
        "id": "142",
        "table": {
            "header": [
                "[BOLD] Training",
                "[BOLD] Acc.",
                "[BOLD] SST-Char-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle",
                "[BOLD] Acc.",
                "[BOLD] SST-Word-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle",
                "[BOLD] Acc.",
                "[BOLD] AG-Char-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle"
            ],
            "rows": [
                [
                    "Normal",
                    "79.8",
                    "36.5",
                    "10.3",
                    "84.8",
                    "71.3",
                    "69.8",
                    "89.5",
                    "75.4",
                    "65.1"
                ],
                [
                    "Adversarial",
                    "79.0",
                    "[BOLD] 74.9",
                    "25.8",
                    "[BOLD] 85.0",
                    "76.8",
                    "74.6",
                    "[BOLD] 90.5",
                    "85.5",
                    "81.6"
                ],
                [
                    "Data aug.",
                    "79.8",
                    "37.8",
                    "13.7",
                    "85.4",
                    "72.7",
                    "71.6",
                    "88.4",
                    "77.5",
                    "72.0"
                ],
                [
                    "Verifiable (IBP)",
                    "74.2",
                    "73.1",
                    "[BOLD] 73.1",
                    "81.7",
                    "[BOLD] 77.2",
                    "[BOLD] 76.5",
                    "87.6",
                    "[BOLD] 87.1",
                    "[BOLD] 87.1"
                ]
            ],
            "title": "Table 1: Experimental results for changes up to \u03b4=3 and \u03b4=2 symbols on SST and AG dataset, respectively. We compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics on the test set: the nominal accuracy, adversarial accuracy, and exhaustively verified accuracy (Oracle) (%)."
        },
        "insight": "Table 1 shows the results of IBP training and baseline models under \u03b4 = 3 and \u03b4 = 24 perturbations on SST and AG News, respectively. [CONTINUE] In Table 1, comparing adversarial accuracy with exhaustive verification accuracy (oracle), we observe that although adversarial training is effective at defending against HotFlip attacks (74.9 / 76.8 / 85.5%), the oracle adversarial accuracy under exhaustive testing (25.8 / 74.6 / 81.6%) is much lower in SST-character / SST-word / AG-character level, respectively. [CONTINUE] In Table 1, when the perturbation space is larger (SST character-level vs. SST word-level), (a) across models, there is a larger gap in adversarial accuracy and true robustness (oracle); (b) the difference in oracle robustness between IBP and adversarial training is even larger (73.1% vs. 25.8% and 76.5% vs. 74.6%). [CONTINUE] The resulting models achieve the highest exhaustively verified accuracy at the cost of only moderate deterioration in nominal accuracy (Table 1)."
    },
    {
        "id": "143",
        "table": {
            "header": [
                "[EMPTY]",
                "P",
                "R",
                "F1"
            ],
            "rows": [
                [
                    "majority",
                    "32.9 \u00b1 0.0",
                    "50.0 \u00b1 0.0",
                    "39.7 \u00b1 0.0"
                ],
                [
                    "coin-toss",
                    "50.4 \u00b1 0.7",
                    "50.5 \u00b1 0.8",
                    "49.1 \u00b1 0.7"
                ],
                [
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized"
                ],
                [
                    "bow-svm",
                    "71.5 \u00b1 0.0",
                    "72.0 \u00b1 0.0",
                    "71.8 \u00b1 0.0"
                ],
                [
                    "bigru-att",
                    "87.1 \u00b1 1.0",
                    "77.2 \u00b1 3.4",
                    "79.5 \u00b1 2.7"
                ],
                [
                    "han",
                    "88.2 \u00b1 0.4",
                    "78.0 \u00b1 0.2",
                    "80.5 \u00b1 0.2"
                ],
                [
                    "bert",
                    "24.0 \u00b1 0.2",
                    "50.0 \u00b1 0.0",
                    "17.0 \u00b1 0.5"
                ],
                [
                    "hier-bert",
                    "[BOLD] 90.4 \u00b1 0.3",
                    "[BOLD] 79.3 \u00b1 0.9",
                    "[BOLD] 82.0 \u00b1 0.9"
                ],
                [
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized"
                ],
                [
                    "bow-svm",
                    "71.6 \u00b1 0.0",
                    "70.5 \u00b1 0.0",
                    "70.9 \u00b1 0.0"
                ],
                [
                    "bigru-att",
                    "[BOLD] 87.0 \u00b1 1.0",
                    "76.6 \u00b1 1.9",
                    "78.9 \u00b1 1.9"
                ],
                [
                    "han",
                    "85.2 \u00b1 4.9",
                    "[BOLD] 78.3 \u00b1 2.0",
                    "[BOLD] 80.2 \u00b1 2.7"
                ],
                [
                    "bert",
                    "17.0 \u00b1 3.0",
                    "50.0 \u00b1 0.0",
                    "25.4 \u00b1 0.4"
                ],
                [
                    "hier-bert",
                    "85.2 \u00b1 0.3",
                    "78.1 \u00b1 1.3",
                    "80.1 \u00b1 1.1"
                ]
            ],
            "title": "Table 2: Macro precision (P), recall (R), F1 for the binary violation prediction task (\u00b1 std. dev.)."
        },
        "insight": "Table 2 (upper part) shows the results for binary violation. We evaluate models using macroaveraged precision (P), recall (P), F1. The weak baselines (MAJORITY, COIN-TOSS) are widely outperformed by the rest of the methods. BIGRUATT outperforms in F1 (79.5 vs. 71.8) the previous best performing method (Aletras et al., 2016) in English judicial prediction. [CONTINUE] HAN slightly improves over BIGRU-ATT (80.5 vs. 79.5), while being more robust across runs (0.2% vs. 2.7% std. dev.). the results in Table 2 indicate that performance is comparable even when this information is masked, with the exception of HIER-BERT that has quite worse results (2%) compared to using non-anonymized data, suggesting model bias."
    },
    {
        "id": "144",
        "table": {
            "header": [
                "Seq. Tagger",
                "en",
                "de",
                "fr",
                "ru",
                "fa",
                "sw"
            ],
            "rows": [
                [
                    "brown",
                    "81.37",
                    "[BOLD] 81.28",
                    "84.81",
                    "[BOLD] 79.78",
                    "[BOLD] 86.94",
                    "87.35"
                ],
                [
                    "marlin",
                    "[BOLD] 81.53",
                    "81.25",
                    "[BOLD] 85.4",
                    "79.14",
                    "86.64",
                    "[BOLD] 88.81"
                ],
                [
                    "a-hmm",
                    "77.12",
                    "74.85",
                    "81.48",
                    "73.88",
                    "80.25",
                    "76.69"
                ],
                [
                    "e-kmeans",
                    "63.01",
                    "65.14",
                    "68.68",
                    "70.80",
                    "76.94",
                    "65.08"
                ]
            ],
            "title": "Table 1: Comparison of labeling strategies using many-to-one mapping for target languages with available test data, using 500 clusters or number of states. Accuracy is shown in percentage points."
        },
        "insight": "we evaluate our four labeling strategies using the many-to-one approach, as presented in Table 1. [CONTINUE] In all cases, clustering by type with Brown-based algorithms works better than using a sophisticated tagger such as A-HMM."
    },
    {
        "id": "145",
        "table": {
            "header": [
                "CL",
                "Parent Language (PL) en",
                "Parent Language (PL) de",
                "Parent Language (PL) fr",
                "Parent Language (PL) it",
                "Parent Language (PL) es",
                "Parent Language (PL) ja",
                "Parent Language (PL) cs",
                "Parent Language (PL) ru",
                "Parent Language (PL) ar",
                "Parent Language (PL) sw",
                "cipher-avg",
                "PL=CL"
            ],
            "rows": [
                [
                    "en",
                    "-",
                    "57.1",
                    "[BOLD] 60.4",
                    "59.9",
                    "59.4",
                    "25.1",
                    "52.8",
                    "49.0",
                    "30.7",
                    "28.4",
                    "56.4",
                    "73.9"
                ],
                [
                    "fr",
                    "58.1",
                    "56.0",
                    "-",
                    "68.6",
                    "[BOLD] 71.9",
                    "23.6",
                    "48.3",
                    "47.8",
                    "35.0",
                    "26.7",
                    "58.6",
                    "76.7"
                ],
                [
                    "fa",
                    "13.8",
                    "32.3",
                    "29.7",
                    "22.7",
                    "33.3",
                    "19.7",
                    "33.3",
                    "[BOLD] 43.5",
                    "37.0",
                    "38.2",
                    "37.4",
                    "73.3"
                ],
                [
                    "sw",
                    "24.9",
                    "14.3",
                    "37.3",
                    "21.2",
                    "35.9",
                    "21.3",
                    "25.8",
                    "27.9",
                    "[BOLD] 38.96",
                    "-",
                    "37.8",
                    "69.4"
                ]
            ],
            "title": "Table 2: Performance of cipher grounder using brown (|C|=500) as labeler. The best PL for each CL besides itself, is shown in bold. The artificial case where we have CL POS data (PL=CL) is shown for comparison, as is the ultimately used cipher-avg method."
        },
        "insight": "Table 2 presents the intrinsic performance of the cipher grounder over all PL-CL pairs considered. The difference between the best and the worst performing PL for each CL ranges from 24.62 percentage points for Swahili to 48.34 points for French, and an average difference of 34.5 points among all languages. [CONTINUE] The case when PL=CL is also presented in Table 2 as a reference and provides a reliable upper-bound under zero-resource conditions. [CONTINUE] It is worth noting the difference in accuracy when comparing the best performing PL for each CL with its corresponding PL=CL upper-bound. [CONTINUE] Among all CLs, the best cipher grounder for French (es-fr) gets the closest to its upper-bound with just 4.81 percentage points of difference, followed by the English grounder (fr-en) with 13.53 points of difference. [CONTINUE] As shown in Table 2, this model, CIPHER-AVG, obtains accuracy scores of 56.4, 58.6, 37.4, and 37.8 % for en, fr, fa, and sw, respectively. [CONTINUE] When compared to the best performing PL for each CL (see bold cells in Table 2), it can be noticed that the performance gap ranges from just 1.2 percentage points for Swahili to 13.3 points for French, with an average of 6.1 points among all target languages."
    },
    {
        "id": "146",
        "table": {
            "header": [
                "CL",
                "cipher-avg P",
                "cipher-avg R",
                "cipher-avg F1",
                "Supervised P",
                "Supervised R",
                "Supervised F1"
            ],
            "rows": [
                [
                    "en",
                    "47.70",
                    "64.4",
                    "54.81",
                    "94.04",
                    "90.44",
                    "92.20"
                ],
                [
                    "fr",
                    "56.26",
                    "78.82",
                    "65.65",
                    "96.15",
                    "93.72",
                    "94.92"
                ],
                [
                    "fa",
                    "64.94",
                    "51.23",
                    "57.27",
                    "96.48",
                    "97.77",
                    "97.12"
                ],
                [
                    "sw",
                    "53.46",
                    "51.82",
                    "52.63",
                    "98.88",
                    "97.50",
                    "98.18"
                ]
            ],
            "title": "Table 3: Comparison of performance over the NOUN tag, as measured by precision (P), recall (R), and F1 scores, between our combined cipher grounder (cipher-avg) and a supervised tagger."
        },
        "insight": "Let us now compare the performance of CIPHER-AVG with that of a vanilla supervised neural model.11 Table 3 shows precision, recall, and F1 scores for the NOUN tag. [CONTINUE] Even though CIPHER-AVG achieved mixed results (mid to low accuracy), the model robustly achieves mid-range performance according to F1-score for all CLs. [CONTINUE] The results are even more optimistic in terms of recall for English and French, and in terms of precision for Farsi and Swahili."
    },
    {
        "id": "147",
        "table": {
            "header": [
                "Test Tags",
                "de UAS",
                "de LAS",
                "fr UAS",
                "fr LAS",
                "es UAS",
                "es LAS",
                "it UAS",
                "it LAS",
                "pt UAS",
                "pt LAS",
                "sv UAS",
                "sv LAS"
            ],
            "rows": [
                [
                    "gold",
                    "65.57",
                    "52.37",
                    "71.27",
                    "59.80",
                    "73.26",
                    "63.13",
                    "71.46",
                    "59.66",
                    "63.28",
                    "54.93",
                    "77.50",
                    "64.90"
                ],
                [
                    "none",
                    "40.90",
                    "18.61",
                    "51.14",
                    "30.91",
                    "43.82",
                    "17.67",
                    "48.22",
                    "33.29",
                    "37.89",
                    "16.72",
                    "38.15",
                    "17.96"
                ],
                [
                    "cipher (this work)",
                    "38.31",
                    "[BOLD] 24.72",
                    "[BOLD] 54.46",
                    "[BOLD] 41.04",
                    "[BOLD] 55.56",
                    "[BOLD] 41.16",
                    "[BOLD] 54.05",
                    "[BOLD] 39.78",
                    "[BOLD] 46.97",
                    "[BOLD] 36.07",
                    "[BOLD] 55.06",
                    "[BOLD] 36.51"
                ]
            ],
            "title": "Table 4: Impact of grounded unsupervised POS tagging on MaLOPa\u2019s \u2018zero-resource\u2019 condition. Bold entries indicate an improvement over the baseline condition of having no POS tag information (beyond punctuation)"
        },
        "insight": "Likewise, the utility of CIPHER-AVG tags for dependency parsing under zero-resource scenarios is summarized in Table 4 and Table 5. [CONTINUE] We first analyze the effect of POS tag information at test time for the MALOPA setup in Table 4. [CONTINUE] First we remove all POS signal except trivial punctuation information (NONE row), and, predictably, the scores drop significantly across all target languages. Then, we use our cipher tags (CIPHER row) and see improvements for all languages in LAS and for all but one language in UAS (de)."
    },
    {
        "id": "148",
        "table": {
            "header": [
                "Embeddings",
                "Test Tags",
                "de UAS",
                "de LAS",
                "fr UAS",
                "fr LAS",
                "es UAS",
                "es LAS",
                "it UAS",
                "it LAS",
                "pt UAS",
                "pt LAS",
                "sv UAS",
                "sv LAS"
            ],
            "rows": [
                [
                    "guo",
                    "gold",
                    "65.57",
                    "52.37",
                    "71.27",
                    "59.80",
                    "73.26",
                    "63.13",
                    "71.46",
                    "59.66",
                    "63.28",
                    "54.93",
                    "[BOLD] 77.50",
                    "[BOLD] 64.90"
                ],
                [
                    "muse",
                    "gold",
                    "[BOLD] 66.19",
                    "[BOLD] 56.28",
                    "[BOLD] 80.86",
                    "[BOLD] 72.65",
                    "[BOLD] 81.06",
                    "[BOLD] 73.62",
                    "[BOLD] 82.08",
                    "[BOLD] 72.40",
                    "[BOLD] 81.17",
                    "[BOLD] 76.17",
                    "72.46",
                    "61.71"
                ],
                [
                    "muse",
                    "none",
                    "57.26",
                    "45.10",
                    "[ITALIC] 73.84",
                    "[ITALIC] 63.09",
                    "[ITALIC] 77.01",
                    "[ITALIC] 67.06",
                    "71.36",
                    "[ITALIC] 60.48",
                    "[ITALIC] 75.31",
                    "[ITALIC] 68.36",
                    "60.82",
                    "45.25"
                ],
                [
                    "muse",
                    "cipher",
                    "48.56",
                    "37.13",
                    "69.94",
                    "59.22",
                    "73.86",
                    "61.68",
                    "69.30",
                    "56.85",
                    "73.41",
                    "65.23",
                    "57.39",
                    "41.49"
                ]
            ],
            "title": "Table 5: Changing to unsupervised muse embeddings boosts MaLOPa\u2019s zero-resource performance significantly (bold entries), in many cases doing so even without any POS tag information (italic entries), however noisy decipherment-based POS tags are no longer helpful."
        },
        "insight": "Likewise, the utility of CIPHER-AVG tags for dependency parsing under zero-resource scenarios is summarized in Table 4 and Table 5. [CONTINUE] We then take the next logical step and remove the parallel data-grounded embeddings, replacing them with fully unsupervised MUSE embeddings. Table 5 summarizes these results. [CONTINUE] It can be observed that POS signal improves performance greatly for all languages when using MUSE embeddings. [CONTINUE] Here we note a mixed result: whilst de, sv, and it do benefit from POS information, the other languages do not, obtaining great improvements from MUSE embed [CONTINUE] dings instead. [CONTINUE] Finally, consider MUSE-CIPHER (gold POS tags during training, cipher tags during testing). When compared to MUSE-NONE setup, it can be observed that, unfortunately, the heuristic POS tagger is too noisy and gets in MUSE's way."
    },
    {
        "id": "149",
        "table": {
            "header": [
                "overall (all labels)",
                "overall (all labels)  [BOLD] P",
                "overall (all labels)  [BOLD] R",
                "overall (all labels)  [BOLD] F1"
            ],
            "rows": [
                [
                    "bow-svm",
                    "56.3 \u00b1 0.0",
                    "45.5 \u00b1 0.0",
                    "50.4 \u00b1 0.0"
                ],
                [
                    "bigru-att",
                    "62.6 \u00b1 1.2",
                    "50.9 \u00b1 1.5",
                    "56.2 \u00b1 1.3"
                ],
                [
                    "han",
                    "65.0 \u00b1 0.4",
                    "[BOLD] 55.5 \u00b1 0.7",
                    "59.9 \u00b1 0.5"
                ],
                [
                    "lwan",
                    "62.5 \u00b1 1.0",
                    "53.5 \u00b1 1.1",
                    "57.6 \u00b1 1.0"
                ],
                [
                    "hier-bert",
                    "[BOLD] 65.9 \u00b1 1.4",
                    "55.1 \u00b1 3.2",
                    "[BOLD] 60.0 \u00b1 1.3"
                ],
                [
                    "frequent (\u226550)",
                    "frequent (\u226550)",
                    "frequent (\u226550)",
                    "frequent (\u226550)"
                ],
                [
                    "bow-svm",
                    "56.3 \u00b1 0.0",
                    "45.6 \u00b1 0.0",
                    "50.4 \u00b1 0.0"
                ],
                [
                    "bigru-att",
                    "62.7 \u00b1 1.2",
                    "52.2 \u00b1 1.6",
                    "57.0 \u00b1 1.4"
                ],
                [
                    "han",
                    "65.1 \u00b1 0.3",
                    "[BOLD] 57.0 \u00b1 0.8",
                    "[BOLD] 60.8 \u00b1 1.3"
                ],
                [
                    "lwan",
                    "62.8 \u00b1 1.2",
                    "54.7 \u00b1 1.2",
                    "58.5 \u00b1 1.0"
                ],
                [
                    "hier-bert",
                    "[BOLD] 66.0 \u00b1 1.4",
                    "56.5 \u00b1 3.3",
                    "[BOLD] 60.8 \u00b1 1.3"
                ],
                [
                    "few ([1,50))",
                    "few ([1,50))",
                    "few ([1,50))",
                    "few ([1,50))"
                ],
                [
                    "bow-svm",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "bigru-att",
                    "36.3 \u00b1 13.8",
                    "03.2 \u00b1 23.1",
                    "05.6 \u00b1 03.8"
                ],
                [
                    "han",
                    "30.2 \u00b1 35.1",
                    "01.6 \u00b1 01.2",
                    "02.8 \u00b1 01.9"
                ],
                [
                    "lwan",
                    "24.9 \u00b1 06.3",
                    "[BOLD] 07.0 \u00b1 04.1",
                    "[BOLD] 10.6 \u00b1 05.2"
                ],
                [
                    "hier-bert",
                    "[BOLD] 43.6 \u00b1 14.5",
                    "05.0 \u00b1 02.8",
                    "08.9 \u00b1 04.9"
                ]
            ],
            "title": "Table 3: Micro precision, recall, F1 in multi-label violation for all, frequent, and few training instances."
        },
        "insight": "Table 3 reports micro-averaged precision (P), recall (R), and F1 results for all methods, now including LWAN, in multi-label violation prediction. The results are also grouped by label frequency for all (OVERALL), FREQUENT, and FEW labels (articles), counting frequencies on the training subset. [CONTINUE] We observe that predicting specific articles that have been violated is a much more difficult task than predicting if any article has been violated in a binary setup (cf. Table 2). Overall, HIER-BERT outperforms BIGRU-ATT and LWAN (60.0 vs. 57. [CONTINUE] micro-F1), which is tailored for multi-labeling tasks, while being comparable with HAN (60.0 vs. 59.9 micro-F1). All models under-perform in labels with FEW training examples, demonstrating the difficulty of few-shot learning in ECHR legal judgment prediction."
    },
    {
        "id": "150",
        "table": {
            "header": [
                "[BOLD] Dataset/Language",
                "[BOLD] Dataset/Language",
                "[BOLD] Time Period",
                "[BOLD] Genre",
                "[BOLD] Tokens Train",
                "[BOLD] Tokens Dev",
                "[BOLD] Tokens Test"
            ],
            "rows": [
                [
                    "DEA",
                    "German (Anselm)",
                    "14th\u201316th c.",
                    "Religious",
                    "233,947",
                    "45,996",
                    "45,999"
                ],
                [
                    "DER",
                    "German (RIDGES)",
                    "1482\u20131652",
                    "Science",
                    "41,857",
                    "9,712",
                    "9,587"
                ],
                [
                    "EN",
                    "English",
                    "1386\u20131698",
                    "Letters",
                    "147,826",
                    "16,334",
                    "17,644"
                ],
                [
                    "ES",
                    "Spanish",
                    "15th\u201319th c.",
                    "Letters",
                    "97,320",
                    "11,650",
                    "12,479"
                ],
                [
                    "HU",
                    "Hungarian",
                    "1440\u20131541",
                    "Religious",
                    "134,028",
                    "16,707",
                    "16,779"
                ],
                [
                    "IS",
                    "Icelandic",
                    "15th c.",
                    "Religious",
                    "49,633",
                    "6,109",
                    "6,037"
                ],
                [
                    "PT",
                    "Portuguese",
                    "15th\u201319th c.",
                    "Letters",
                    "222,525",
                    "26,749",
                    "27,078"
                ],
                [
                    "SLB",
                    "Slovene (Bohori\u010d)",
                    "1750\u20131840s",
                    "Diverse",
                    "50,023",
                    "5,841",
                    "5,969"
                ],
                [
                    "SLG",
                    "Slovene (Gaj)",
                    "1840s\u20131899",
                    "Diverse",
                    "161,211",
                    "20,878",
                    "21,493"
                ],
                [
                    "SV",
                    "Swedish",
                    "1527\u20131812",
                    "Diverse",
                    "24,458",
                    "2,245",
                    "29,184"
                ]
            ],
            "title": "Table 1: Historical datasets used in the experiments"
        },
        "insight": "Table 1 gives an overview of the historical datasets. [CONTINUE] covering eight languages from different language families\u2014English, German, Hungarian, Icelandic, Spanish, Portuguese, Slovene, and Swedish\u2014as well as different text genres and time periods."
    },
    {
        "id": "151",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Dataset DEA",
                "[BOLD] Dataset DER",
                "[BOLD] Dataset EN",
                "[BOLD] Dataset ES",
                "[BOLD] Dataset HU",
                "[BOLD] Dataset IS",
                "[BOLD] Dataset PT",
                "[BOLD] Dataset SLB",
                "[BOLD] Dataset SLG",
                "[BOLD] Dataset SV"
            ],
            "rows": [
                [
                    "[ITALIC] Identity",
                    "[ITALIC] 30.63",
                    "[ITALIC] 44.36",
                    "[ITALIC] 75.29",
                    "[ITALIC] 73.40",
                    "[ITALIC] 17.53",
                    "[ITALIC] 47.62",
                    "[ITALIC] 65.19",
                    "[ITALIC] 40.74",
                    "[ITALIC] 85.38",
                    "[ITALIC] 58.59"
                ],
                [
                    "[ITALIC] Maximum",
                    "[ITALIC] 94.64",
                    "[ITALIC] 96.46",
                    "[ITALIC] 98.57",
                    "[ITALIC] 97.40",
                    "[ITALIC] 98.70",
                    "[ITALIC] 93.46",
                    "[ITALIC] 97.65",
                    "[ITALIC] 98.71",
                    "[ITALIC] 98.96",
                    "[ITALIC] 98.97"
                ],
                [
                    "Norma, Lookup",
                    "83.86",
                    "82.15",
                    "92.45",
                    "92.51",
                    "74.58",
                    "82.84",
                    "91.67",
                    "81.76",
                    "93.90",
                    "83.80"
                ],
                [
                    "Norma, Rule-based",
                    "76.48",
                    "82.52",
                    "90.85",
                    "88.59",
                    "78.73",
                    "83.72",
                    "86.33",
                    "86.09",
                    "91.63",
                    "85.23"
                ],
                [
                    "Norma, Distance-based",
                    "58.92",
                    "73.30",
                    "83.92",
                    "84.41",
                    "62.38",
                    "69.95",
                    "77.28",
                    "71.02",
                    "88.20",
                    "76.03"
                ],
                [
                    "Norma (Combined)",
                    "88.02",
                    "86.55",
                    "94.60",
                    "94.41",
                    "86.83",
                    "*86.85",
                    "94.19",
                    "89.45",
                    "91.44",
                    "87.12"
                ],
                [
                    "cSMTiser",
                    "88.82",
                    "*88.06",
                    "*95.21",
                    "*95.01",
                    "*91.63",
                    "*87.10",
                    "*95.09",
                    "*93.18",
                    "*95.99",
                    "[BOLD] 91.13"
                ],
                [
                    "cSMTiser+LM",
                    "86.69",
                    "*88.19",
                    "[BOLD] 95.24",
                    "[BOLD] 95.02",
                    "[BOLD] 91.70",
                    "*86.83",
                    "[BOLD] 95.18",
                    "[BOLD] 93.30",
                    "[BOLD] 96.01",
                    "*91.11"
                ],
                [
                    "NMT (Bollmann,  2018 )",
                    "89.16",
                    "*88.07",
                    "94.80",
                    "*94.83",
                    "91.17",
                    "86.45",
                    "94.64",
                    "91.61",
                    "95.19",
                    "90.27"
                ],
                [
                    "NMT (Tang et\u00a0al.,  2018 )",
                    "[BOLD] 89.64",
                    "[BOLD] 88.22",
                    "94.95",
                    "*94.84",
                    "*91.65",
                    "[BOLD] 87.31",
                    "94.51",
                    "92.60",
                    "*95.85",
                    "90.39"
                ],
                [
                    "\u2020SMT (Pettersson et\u00a0al.,  2014 )",
                    "\u2013",
                    "\u2013",
                    "94.3\u2013",
                    "\u2013",
                    "80.1\u2013",
                    "71.8\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "92.9\u2013"
                ],
                [
                    "\u2020NMT (Tang et\u00a0al.,  2018 )",
                    "\u2013",
                    "\u2013",
                    "94.69",
                    "\u2013",
                    "91.69",
                    "87.59",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "91.56"
                ]
            ],
            "title": "Table 2: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent; best result for each dataset in bold; results marked with an asterisk\u00a0(*) are not significantly different from the best result using McNemar\u2019s test at p<0.05. \u2020\u00a0indicates scores that were not (re)produced here, but reported in previous work; they might not be strictly comparable due to differences in data preprocessing (cf.\u00a0Sec.\u20093). Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the theoretical maximum accuracy with purely token-level methods."
        },
        "insight": "Table 2 shows the results of this evaluation. The extent of spelling variation varies [CONTINUE] greatly between datasets, with less than 15% of tokens requiring normalization (SLG) to more than 80% (HU). The maximum accuracy is above 97% for most datasets, [CONTINUE] For the normalization systems, we observe significantly better word accuracy with SMT than NMT on four of the datasets, and non-significant differences on five others. There is only one dataset (DEA) where the NMT system by Tang et al. (2018) gets significantly better word accuracy than other systems. [CONTINUE] Overall, the deep NMT model by Tang et al. (2018) consistently outperforms the shallow one by Bollmann (2018). [CONTINUE] Finally, while Norma does produce competitive results on sev [CONTINUE] eral datasets (particularly in the \"combined\" setting), it is generally significantly behind the SMT and NMT methods."
    },
    {
        "id": "152",
        "table": {
            "header": [
                "[EMPTY]",
                "MAE",
                "spearman\u2019s  [ITALIC] \u03c1"
            ],
            "rows": [
                [
                    "majority",
                    "[BOLD] .369 \u00b1 .000",
                    "[ITALIC] N/ [ITALIC] A*"
                ],
                [
                    "bow-svr",
                    ".585 \u00b1 .000",
                    ".370 \u00b1 .000"
                ],
                [
                    "bigru-att",
                    ".539 \u00b1 .073",
                    ".459 \u00b1 .034"
                ],
                [
                    "han",
                    ".524 \u00b1 .049",
                    ".437 \u00b1 .018"
                ],
                [
                    "hier-bert",
                    ".437 \u00b1 .018",
                    "[BOLD] .527 \u00b1 .024"
                ]
            ],
            "title": "Table 4: Mean Absolute Error and Spearman\u2019s \u03c1 for case importance. Importance ranges from 1 (most important) to 4 (least). * Not Applicable."
        },
        "insight": "BOW-SVR performs worse than BIGRU-ATT, while HAN is 10% and 3% better, respectively. HIER-BERT further improves the results, outperforming HAN by 17%. [CONTINUE] HIER-BERT has the best \u03c1 (.527), indicating a moderate positive correlation (> 0.5), which is not the case for the rest of the methods."
    },
    {
        "id": "153",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] 2015",
                "[BOLD] 2017"
            ],
            "rows": [
                [
                    "Ours",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "local edge + projective decoder",
                    "70.2 \\scalebox 0.8$\u00b10.3$",
                    "[BOLD] 71.0 \u00b10.5"
                ],
                [
                    "local edge + fixed-tree decoder",
                    "69.4 \\scalebox 0.8$\u00b10.6$",
                    "70.2 \\scalebox 0.8$\u00b10.5$"
                ],
                [
                    "K&G\u00a0edge + projective decoder",
                    "68.6 \\scalebox 0.8$\u00b10.7$",
                    "69.4 \\scalebox 0.8$\u00b10.4$"
                ],
                [
                    "K&G\u00a0edge + fixed-tree decoder",
                    "69.6 \\scalebox 0.8$\u00b10.4$",
                    "69.9 \\scalebox 0.8$\u00b10.2$"
                ],
                [
                    "Baselines",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "fixed-tree (type-unaware)",
                    "26.0 \\scalebox 0.8$\u00b10.6$",
                    "27.9 \\scalebox 0.8$\u00b10.6$"
                ],
                [
                    "JAMR-style",
                    "66.1",
                    "66.2"
                ],
                [
                    "Previous work",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "CAMR Wang et\u00a0al. ( 2015 )",
                    "66.5",
                    "-"
                ],
                [
                    "JAMR Flanigan et\u00a0al. ( 2016 )",
                    "67",
                    "-"
                ],
                [
                    "E17-1051",
                    "64",
                    "-"
                ],
                [
                    "van2017neural",
                    "68.5",
                    "[BOLD] 71.0"
                ],
                [
                    "foland2017abstract",
                    "[BOLD] 70.7",
                    "-"
                ],
                [
                    "buys2017oxford",
                    "-",
                    "61.9"
                ]
            ],
            "title": "Table 1: 2015 & 2017 test set Smatch scores"
        },
        "insight": "Table 1 shows the Smatch scores (Cai and Knight, 2013) of our models, compared to a selection of previously published results. [CONTINUE] The fixed-tree decoder seems to work well with either edge model, but performance of the projective decoder drops with the K&G edge scores. [CONTINUE] As expected, the type-unaware baseline has low recall, [CONTINUE] The fact that our models outperform the JAMR-style baseline so clearly is an indication that"
    },
    {
        "id": "154",
        "table": {
            "header": [
                "[BOLD] Metric",
                "[BOLD] 2015 W\u201915",
                "[BOLD] 2015 F\u201916",
                "[BOLD] 2015 D\u201917",
                "[BOLD] 2015 PD",
                "[BOLD] 2015 FTD",
                "[BOLD] 2017 vN\u201917",
                "[BOLD] 2017 PD",
                "[BOLD] 2017 FTD"
            ],
            "rows": [
                [
                    "Smatch",
                    "67",
                    "67",
                    "64",
                    "[BOLD] 70",
                    "[BOLD] 70",
                    "[BOLD] 71",
                    "[BOLD] 71",
                    "70"
                ],
                [
                    "Unlabeled",
                    "69",
                    "69",
                    "69",
                    "[BOLD] 73",
                    "[BOLD] 73",
                    "[BOLD] 74",
                    "[BOLD] 74",
                    "[BOLD] 74"
                ],
                [
                    "No WSD",
                    "64",
                    "68",
                    "65",
                    "[BOLD] 71",
                    "70",
                    "[BOLD] 72",
                    "[BOLD] 72",
                    "70"
                ],
                [
                    "Named Ent.",
                    "75",
                    "79",
                    "[BOLD] 83",
                    "79",
                    "78",
                    "[BOLD] 79",
                    "78",
                    "77"
                ],
                [
                    "Wikification",
                    "0",
                    "[BOLD] 75",
                    "64",
                    "71",
                    "72",
                    "65",
                    "[BOLD] 71",
                    "[BOLD] 71"
                ],
                [
                    "Negations",
                    "18",
                    "45",
                    "48",
                    "[BOLD] 52",
                    "[BOLD] 52",
                    "[BOLD] 62",
                    "57",
                    "55"
                ],
                [
                    "Concepts",
                    "80",
                    "83",
                    "83",
                    "83",
                    "[BOLD] 84",
                    "82",
                    "[BOLD] 84",
                    "[BOLD] 84"
                ],
                [
                    "Reentrancies",
                    "41",
                    "42",
                    "41",
                    "[BOLD] 46",
                    "44",
                    "[BOLD] 52",
                    "49",
                    "46"
                ],
                [
                    "SRL",
                    "60",
                    "60",
                    "56",
                    "[BOLD] 63",
                    "61",
                    "[BOLD] 66",
                    "64",
                    "62"
                ]
            ],
            "title": "Table 2: Details for the LDC2015E86 and LDC2017T10 test sets"
        },
        "insight": "Table 2 analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail, [CONTINUE] and compares them to Wang's, Flanigan's, and Damonte's AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset. [CONTINUE] The good scores we achieve on reentrancy identification,"
    },
    {
        "id": "155",
        "table": {
            "header": [
                "System",
                "Dev",
                "Test"
            ],
            "rows": [
                [
                    "TSP-gen",
                    "21.12",
                    "22.44"
                ],
                [
                    "JAMR-gen",
                    "23.00",
                    "23.00"
                ],
                [
                    "All",
                    "[BOLD] 25.24",
                    "[BOLD] 25.62"
                ],
                [
                    "NoInducedRule",
                    "16.75",
                    "17.43"
                ],
                [
                    "NoConceptRule",
                    "23.99",
                    "24.86"
                ],
                [
                    "NoMovingDistance",
                    "23.48",
                    "24.06"
                ],
                [
                    "NoReorderModel",
                    "25.09",
                    "25.43"
                ]
            ],
            "title": "Table 2: Main results."
        },
        "insight": "Over the rules used on the 1-best result, more than 30% are non-terminal rules, [CONTINUE] On the other hand, 30% are glue rules. [CONTINUE] Finally, terminal rules take the largest percentage,"
    },
    {
        "id": "156",
        "table": {
            "header": [
                "[EMPTY]",
                "Glue",
                "Nonterminal",
                "Terminal"
            ],
            "rows": [
                [
                    "1-best",
                    "30.0%",
                    "30.1%",
                    "39.9%"
                ]
            ],
            "title": "Table 3: Rules used for decoding."
        },
        "insight": "Over the rules used on the 1-best result, more than 30% are non-terminal rules. [CONTINUE] On the other hand, 30% are glue rules. [CONTINUE] Finally, terminal rules take the largest percentage"
    },
    {
        "id": "157",
        "table": {
            "header": [
                "[EMPTY]",
                "happy",
                "angry",
                "sad",
                "others",
                "size"
            ],
            "rows": [
                [
                    "Train",
                    "14.07%",
                    "18.26%",
                    "18.11%",
                    "49.56%",
                    "30160"
                ],
                [
                    "Dev",
                    "5.15%",
                    "5.44%",
                    "4.54%",
                    "84.86%",
                    "2755"
                ],
                [
                    "Test",
                    "4.28%",
                    "5.57%",
                    "4.45%",
                    "85.70%",
                    "5509"
                ]
            ],
            "title": "Table 2: Label distribution of train, dev, and test set"
        },
        "insight": "According to the description in (CodaLab, 2019), the label distribution for dev and test sets are roughly 4% for each of the emotions. However, from the dev set (Table 2) we know that the proportions of each of the emotion categories are better described as %5 each, thereby we use %5 as the empirical estimation of distribution Pte(xtr i ). We did not use the exact proportion of dev set as the estimation to prevent the overfitting towards dev set. The sample distribution of the train set is used as Ptr(xtr i )."
    },
    {
        "id": "158",
        "table": {
            "header": [
                "[EMPTY]",
                "F1",
                "Happy",
                "Angry",
                "Sad",
                "Harm. Mean"
            ],
            "rows": [
                [
                    "SL",
                    "Dev",
                    "0.6430",
                    "0.7530",
                    "0.7180",
                    "0.7016"
                ],
                [
                    "SL",
                    "Test",
                    "0.6400",
                    "0.7190",
                    "0.7300",
                    "0.6939"
                ],
                [
                    "SLD",
                    "Dev",
                    "0.6470",
                    "0.7610",
                    "0.7360",
                    "0.7112"
                ],
                [
                    "SLD",
                    "Test",
                    "0.6350",
                    "0.7180",
                    "0.7360",
                    "0.6934"
                ],
                [
                    "HRLCE",
                    "Dev",
                    "0.7460",
                    "0.7590",
                    "0.8100",
                    "[BOLD] 0.7706"
                ],
                [
                    "HRLCE",
                    "Test",
                    "0.7220",
                    "0.766",
                    "0.8180",
                    "[BOLD] 0.7666"
                ],
                [
                    "BERT",
                    "Dev",
                    "0.7138",
                    "0.7736",
                    "0.8106",
                    "0.7638"
                ],
                [
                    "BERT",
                    "Test",
                    "0.7151",
                    "0.7654",
                    "0.8157",
                    "0.7631"
                ]
            ],
            "title": "Table 1: Macro-F1 scores and its harmonic means of the four models"
        },
        "insight": "The results are shown in Table 1. It shows that the proposed HRLCE model performs the best. The performance of SLD and SL are very close to each other, on the dev set, SLD performs better than SL but they have almost the same overall scores on the test set. The MacroF1 scores of each emotion category are very different from each other: the classification accuracy for emotion Sad is the highest in most of the cases, while the emotion Happy is the least accurately classified by all the models. We also noticed that the performance on the dev set is generally slightly better than that on the test set."
    },
    {
        "id": "159",
        "table": {
            "header": [
                "Aligner",
                "Alignment F1",
                "Oracle\u2019s Smatch"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "(on hand-align)",
                    "(on dev. dataset)"
                ],
                [
                    "JAMR",
                    "90.6",
                    "91.7"
                ],
                [
                    "Our",
                    "95.2",
                    "94.7"
                ]
            ],
            "title": "Table 3: The intrinsic evaluation results."
        },
        "insight": "Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser."
    },
    {
        "id": "160",
        "table": {
            "header": [
                "model",
                "newswire",
                "all"
            ],
            "rows": [
                [
                    "JAMR parser: Word, POS, NER, DEP",
                    "JAMR parser: Word, POS, NER, DEP",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "71.3",
                    "65.9"
                ],
                [
                    "+ Our aligner",
                    "73.1",
                    "67.6"
                ],
                [
                    "CAMR parser: Word, POS, NER, DEP",
                    "CAMR parser: Word, POS, NER, DEP",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.4",
                    "64.6"
                ],
                [
                    "+ Our aligner",
                    "68.8",
                    "65.1"
                ]
            ],
            "title": "Table 4: The parsing results."
        },
        "insight": "Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7."
    },
    {
        "id": "161",
        "table": {
            "header": [
                "model",
                "newswire",
                "all"
            ],
            "rows": [
                [
                    "Our single parser: Word only",
                    "Our single parser: Word only",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.6",
                    "63.9"
                ],
                [
                    "+ Our aligner",
                    "69.3",
                    "64.7"
                ],
                [
                    "Our single parser: Word, POS",
                    "Our single parser: Word, POS",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.8",
                    "64.6"
                ],
                [
                    "+ Our aligner",
                    "69.8",
                    "65.2"
                ],
                [
                    "Our ensemble: Word only + Our aligner",
                    "Our ensemble: Word only + Our aligner",
                    "[EMPTY]"
                ],
                [
                    "x3",
                    "71.9",
                    "67.4"
                ],
                [
                    "x10",
                    "72.5",
                    "68.1"
                ],
                [
                    "Our ensemble: Word, POS + Our aligner",
                    "Our ensemble: Word, POS + Our aligner",
                    "[EMPTY]"
                ],
                [
                    "x3",
                    "72.5",
                    "67.7"
                ],
                [
                    "x10",
                    "73.3",
                    "[BOLD] 68.4"
                ],
                [
                    "BA17: Word only\u2020",
                    "68",
                    "63"
                ],
                [
                    "+ POS",
                    "68",
                    "63"
                ],
                [
                    "+ POS, DEP",
                    "69",
                    "64"
                ],
                [
                    "Damonte et\u00a0al. ( 2017 )\u2021",
                    "-",
                    "66"
                ],
                [
                    "Artzi et\u00a0al. ( 2015 )",
                    "66.3",
                    "-"
                ],
                [
                    "Wang et\u00a0al. ( 2015a )",
                    "70",
                    "66"
                ],
                [
                    "Pust et\u00a0al. ( 2015 )",
                    "-",
                    "67.1"
                ],
                [
                    "Zhou et\u00a0al. ( 2016 )",
                    "71",
                    "66"
                ],
                [
                    "Goodman et\u00a0al. ( 2016 )",
                    "70",
                    "-"
                ],
                [
                    "Wang and Xue ( 2017 )",
                    "-",
                    "68.1"
                ]
            ],
            "title": "Table 6: The parsing results. xn denotes the ensemble of n differently initialized parsers. The difference in rounding is due to previous works report differently rounded results. \u2020 BA17 represents the result of Ballesteros and Al-Onaizan (2017), \u2021 Damonte et\u00a0al. (2017)\u2019s result is drawn from Ballesteros and Al-Onaizan (2017)."
        },
        "insight": "Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. [CONTINUE] The second block in Table 6 shows the results of our ensemble parser, in which ensemble significantly improves the performance and more parsers ensembled, more improvements are achieved."
    },
    {
        "id": "162",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] ALL",
                "[BOLD] SEQ",
                "[BOLD] POS1",
                "[BOLD] POS2",
                "[BOLD] POS3"
            ],
            "rows": [
                [
                    "FP\u00a0*",
                    "34.1",
                    "7.2",
                    "52.6",
                    "25.6",
                    "25.9"
                ],
                [
                    "NP\u00a0*",
                    "39.4",
                    "10.8",
                    "58.9",
                    "35.9",
                    "24.6"
                ],
                [
                    "DynSp",
                    "42.0",
                    "10.2",
                    "[BOLD] 70.9",
                    "35.8",
                    "20.1"
                ],
                [
                    "FP\u00a0\u2020\u00a0*",
                    "33.2",
                    "7.7",
                    "51.4",
                    "22.2",
                    "22.3"
                ],
                [
                    "NP\u00a0\u2020\u00a0*",
                    "40.2",
                    "11.8",
                    "60.0",
                    "35.9",
                    "25.5"
                ],
                [
                    "DynSp\u00a0\u2020",
                    "44.7",
                    "12.8",
                    "70.4",
                    "41.1",
                    "23.6"
                ],
                [
                    "Camp\u00a0\u2020\u00a0*",
                    "45.6",
                    "13.2",
                    "70.3",
                    "42.6",
                    "24.8"
                ],
                [
                    "Ours\u00a0*",
                    "45.1",
                    "13.3",
                    "67.2",
                    "42.4",
                    "26.4"
                ],
                [
                    "Ours\u00a0\u2020\u00a0*",
                    "[BOLD] 55.1",
                    "[BOLD] 28.1",
                    "67.2",
                    "[BOLD] 52.7",
                    "[BOLD] 46.8"
                ],
                [
                    "Ours\u00a0\u2020\u00a0* (RA)",
                    "61.7",
                    "28.1",
                    "67.2",
                    "60.1",
                    "57.7"
                ]
            ],
            "title": "Table 1: SQA test results. \u2020 marks contextual models using the previous question or the answer to the previous question. * marks the models that use the table content. RA denotes an oracle model that has access to the previous reference answer at test time. ALL is the average question accuracy, SEQ the sequence accuracy, and POS X, the accuracy of the X\u2019th question in a sequence."
        },
        "insight": "We compare our model to Float Parser (FP) (Pasupat and Liang, 2015), Neural Programmer (NP) (Neelakantan et al., 2016b), DYNSP (Iyyer et al., 2017) and CAMP (Sun et al., 2018b) in Table 1. [CONTINUE] We observe that our model improves the SOTA from 45.6% by CAMP to 55.1% in question accuracy (ALL), reducing the relative error rate by 18%. For the initial question (POS1), however, it is behind DYNSP by 3.7%. More interestingly, our model handles follow up questions especially well outperforming the previously best model FP by 20% on POS3, a 28% relative error reduction. [CONTINUE] We observe that our model effectively leverages the context information by improving the average question accuracy from 45.1% to 55.1% in comparison to the use of context in DYNSP yielding 2.7% improvement. [CONTINUE] If we provide the previous reference answers, the average question accuracy jumps to 61.7%, showing that 6.6% of the errors are due to error propagation."
    },
    {
        "id": "163",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "[BOLD] Location Eq. 5",
                "[BOLD] Location Eq. 6",
                "[BOLD] Location Eq. 7",
                "[BOLD] Person Eq. 5",
                "[BOLD] Person Eq. 6",
                "[BOLD] Person Eq. 7",
                "[BOLD] Gene Eq. 5",
                "[BOLD] Gene Eq. 6",
                "[BOLD] Gene Eq. 7",
                "[BOLD] Protein Eq. 5",
                "[BOLD] Protein Eq. 6",
                "[BOLD] Protein Eq. 7",
                "[BOLD] Cell Type Eq. 5",
                "[BOLD] Cell Type Eq. 6",
                "[BOLD] Cell Type Eq. 7",
                "[BOLD] Virus Eq. 5",
                "[BOLD] Virus Eq. 6",
                "[BOLD] Virus Eq. 7"
            ],
            "rows": [
                [
                    "[BOLD] Seed 1",
                    "Eq. 8",
                    "0.37",
                    "0.40",
                    "0.50",
                    "0.23",
                    "0.23",
                    "0.30",
                    "0.00",
                    "0.03",
                    "0.13",
                    "0.17",
                    "0.23",
                    "0.20",
                    "0.27",
                    "0.50",
                    "0.53",
                    "0.20",
                    "0.13",
                    "0.17"
                ],
                [
                    "[EMPTY]",
                    "Eq. 9",
                    "0.63",
                    "[BOLD] 0.73",
                    "0.73",
                    "0.03",
                    "[BOLD] 0.17",
                    "0.20",
                    "0.03",
                    "[BOLD] 0.07",
                    "0.07",
                    "0.43",
                    "[BOLD] 0.43",
                    "0.53",
                    "0.17",
                    "[BOLD] 0.23",
                    "0.23",
                    "0.07",
                    "[BOLD] 0.10",
                    "0.07"
                ],
                [
                    "[BOLD] Seed 2",
                    "Eq. 8",
                    "0.33",
                    "0.33",
                    "0.57",
                    "0.53",
                    "0.40",
                    "0.30",
                    "0.63",
                    "0.63",
                    "0.63",
                    "0.17",
                    "0.60",
                    "0.27",
                    "0.10",
                    "0.20",
                    "0.13",
                    "0.07",
                    "0.03",
                    "0.03"
                ],
                [
                    "[EMPTY]",
                    "Eq. 9",
                    "0.57",
                    "[BOLD] 0.70",
                    "0.63",
                    "0.47",
                    "[BOLD] 0.37",
                    "0.37",
                    "0.60",
                    "[BOLD] 0.57",
                    "0.60",
                    "0.07",
                    "[BOLD] 0.30",
                    "0.30",
                    "0.07",
                    "[BOLD] 0.07",
                    "0.07",
                    "0.03",
                    "[BOLD] 0.10",
                    "0.03"
                ]
            ],
            "title": "Table 2: ESE performance (p@k). Best performing combination is bold faced."
        },
        "insight": "We designed ESE to output thirty candidate entities (NPs) ranked based on the similarity to the seed term. Therefore, we calculated precision at k (p@k) where k is always 30. [CONTINUE] Table 2 shows the best results when using the feature ensemble method which is more stable than the non-ensemble one (due to lower standard deviation and non-zero precision). According to the results, the best combination in terms of the mean and standard deviation is obtained when using TFIDF (Eq.5) to weigh the edges and context-dependent similarity (Eq.8) to rank NPs. This shows that the uniqueness and the significant overlap of features between noun phrases were very important."
    },
    {
        "id": "164",
        "table": {
            "header": [
                "[BOLD] Dataset Name",
                "[BOLD] Entity Class",
                "[BOLD] EAL @ 1.0 F",
                "[BOLD] EAL @ 1.0 F",
                "[BOLD] EAA Annotation Mode  [BOLD] FA",
                "[BOLD] EAA Annotation Mode  [BOLD] HFA",
                "[BOLD] EAA Annotation Mode  [BOLD] UFA"
            ],
            "rows": [
                [
                    "[BOLD] Dataset Name",
                    "[BOLD] Entity Class",
                    "[ITALIC] \u03c3",
                    "% cut",
                    "[BOLD] F-Score (percentage cut)",
                    "[BOLD] F-Score (percentage cut)",
                    "[BOLD] F-Score (percentage cut)"
                ],
                [
                    "CoNLL-2003",
                    "Location",
                    "0.97",
                    "55%",
                    "0.99 (46%)",
                    "0.93 (83%)",
                    "0.82 (91%)"
                ],
                [
                    "CoNLL-2003",
                    "Person",
                    "0.97",
                    "59%",
                    "0.99 (48%)",
                    "0.95 (81%)",
                    "0.85 (90%)"
                ],
                [
                    "BioCreAtIvE II",
                    "Gene",
                    "0.94",
                    "35%",
                    "1.00 (35%)",
                    "0.96 (50%)",
                    "0.89 (69%)"
                ],
                [
                    "GENIA 3.02",
                    "Protein Molecule",
                    "0.99",
                    "33%",
                    "0.98 (36%)",
                    "0.87 (71%)",
                    "0.74 (85%)"
                ],
                [
                    "GENIA 3.02",
                    "Cell Type",
                    "0.99",
                    "62%",
                    "0.94 (70%)",
                    "0.82 (86%)",
                    "0.74 (91%)"
                ],
                [
                    "GENIA 3.02",
                    "Virus",
                    "0.94",
                    "24%",
                    "0.97 (79%)",
                    "0.89 (94%)",
                    "0.84 (96%)"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] Average",
                    "0.97",
                    "45%",
                    "0.98 (52%)",
                    "0.90 (78%)",
                    "0.81 (87%)"
                ]
            ],
            "title": "Table 3: Pipeline testing results of EAL and EAA annotation modes showing the model confidence (\u03c3), F-Scores, and percentage cut from the pool of sentences."
        },
        "insight": "Finally, for the last setting, we tested the system using the three auto-annotation modes (i.e., FA, HFA, [CONTINUE] and UFA) as shown in Table 3. While the auto-annotation mode can allow us to reduce up to 87% of the data pool, this drastic saving also reduces the accuracy of the learned model, achieving, on average, around 81% F-Score. Overall, our framework presents a trade off between coverage and annotation cost. The HFA auto-annotation mode shows the benefit, especially in a realistic enterprise setting, when we need to annotate 33% of the data to increase F-Score by only 10% (when comparing the on average performance of HFA with ESA) is unreasonable. Table 3 appears to show FA being inferior to EAL in terms of the percentage cut for the Location class, for example. In reality FA reduced sentence annotation by 65% to reach 0.99 F-Score. But as our testing criteria demanded that we either reach 1.0 F-Score or finish all sentences from the pool, FA tried to finish the pool without any further performance improvement on the 0.99 F-Score."
    },
    {
        "id": "165",
        "table": {
            "header": [
                "[BOLD] Spoken bc.conv",
                "[BOLD] Spoken 137,223",
                "[BOLD] Written news",
                "[BOLD] Written 68,6455"
            ],
            "rows": [
                [
                    "bc.news",
                    "244,425",
                    "bible",
                    "243,040"
                ],
                [
                    "phone",
                    "110,132",
                    "trans.",
                    "98,143"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "web",
                    "71467"
                ],
                [
                    "[BOLD] total",
                    "491,780",
                    "[BOLD] total",
                    "1,099,105"
                ],
                [
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885"
                ]
            ],
            "title": "Table 1: Coarse text types in OntoNotes"
        },
        "insight": "The coreference annotated portion of the corpus contains 1.59 million tokens from multiple genres, presented in Table 1. [CONTINUE] Written data constitutes the large bulk of material, primarily from newswire (Wall Street Journal data), as well as some data from the Web and the New Testament, and some translations of news and online discussions in Arabic and Chinese. The translated data has been placed in its own category: it behaves more conservatively in preferring strict agreement than non-translated language (see Section 4.2), perhaps due to translators' editorial practices. The spoken data comes primarily from television broadcasts, including dialogue data from MSNBC, Phoenix and other broadcast sources (bc.conv), or news, from CNN, ABC and others (bc.news), as well as phone conversations."
    },
    {
        "id": "166",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Predicted Sg",
                "Predicted Pl",
                "Total"
            ],
            "rows": [
                [
                    "Actual",
                    "Sg",
                    "222",
                    "39",
                    "261"
                ],
                [
                    "Actual",
                    "Pl",
                    "7",
                    "81",
                    "88"
                ],
                [
                    "[EMPTY]",
                    "Total",
                    "229",
                    "120",
                    "349"
                ]
            ],
            "title": "Table 2: Confusion matrix for test data classification"
        },
        "insight": "Looking at the actual classifications obtained by the classifier produces the confusion matrix in Table 2. The matrix makes it clear that the classifier is very good at avoiding errors against the majority class: it almost never guesses 'notional' when it shouldn't. Conversely, about 1/3 of actual notional cases are misclassified, predicted to be 'strict'. Among the erroneous cases, only 6 belong to Type III (about 15% of errors) , showing that the classifier largely handles this type quite well next to the other types, since Type III covers about 20% of plural-to-singular agreement cases."
    },
    {
        "id": "167",
        "table": {
            "header": [
                "[BOLD] genre  [ITALIC] written",
                "[BOLD] agreement  [ITALIC] notional",
                "[BOLD] agreement  [ITALIC] strict",
                "[BOLD] agreement  [ITALIC] % notional"
            ],
            "rows": [
                [
                    "bible",
                    "169",
                    "487",
                    "25.76"
                ],
                [
                    "newswire",
                    "344",
                    "843",
                    "28.98"
                ],
                [
                    "translations",
                    "55",
                    "210",
                    "20.75"
                ],
                [
                    "web",
                    "48",
                    "71",
                    "40.33"
                ],
                [
                    "[BOLD] total written",
                    "616",
                    "1611",
                    "27.66"
                ],
                [
                    "[ITALIC] spoken",
                    "[ITALIC] notional",
                    "[ITALIC] strict",
                    "[ITALIC] % notional"
                ],
                [
                    "bc.conv",
                    "237",
                    "201",
                    "54.11"
                ],
                [
                    "bc.news",
                    "296",
                    "378",
                    "43.91"
                ],
                [
                    "phone",
                    "60",
                    "89",
                    "40.26"
                ],
                [
                    "[BOLD] total spoken",
                    "593",
                    "668",
                    "47.02"
                ]
            ],
            "title": "Table 3: Agreement patterns across genres"
        },
        "insight": "Next we can consider the effect of genre, and expectations that speech promotes notional agreement. This is confirmed in Table 3. However we note that individual genres do behave differently: data from the Web is closer to spoken language. The most restrictive genre in avoiding notional agreement is translations. Both of these facts may reflect a combination of modality, genre and editorial practice effects. However the strong differences suggest that genre is likely crucial to any model attempting to predict this phenomenon."
    },
    {
        "id": "168",
        "table": {
            "header": [
                "Eval",
                "Req",
                "Fact",
                "Ref",
                "Quot",
                "Non-A",
                "Total"
            ],
            "rows": [
                [
                    "3,982",
                    "1,911",
                    "3,786",
                    "207",
                    "161",
                    "339",
                    "10,386"
                ]
            ],
            "title": "Table 3: Number of propositions per type in AMPERE."
        },
        "insight": "We also show the number of propositions in each category in Table 3. The most frequent types are evaluation (38.3%) and fact (36.5%)."
    },
    {
        "id": "169",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Prec.",
                "[BOLD] \u00a0Rec.",
                "[BOLD]  F1"
            ],
            "rows": [
                [
                    "FullSent",
                    "73.68",
                    "56.00",
                    "63.64"
                ],
                [
                    "PDTB-conn",
                    "51.11",
                    "49.71",
                    "50.40"
                ],
                [
                    "RST-parser",
                    "30.28",
                    "43.00",
                    "35.54"
                ],
                [
                    "CRF",
                    "66.53",
                    "52.92",
                    "58.95"
                ],
                [
                    "BiLSTM-CRF",
                    "[BOLD] 82.25",
                    "[BOLD] 79.96",
                    "[BOLD] 81.09\u2217"
                ],
                [
                    "CRF-joint",
                    "74.99",
                    "63.33",
                    "68.67"
                ],
                [
                    "BiLSTM-CRF-joint",
                    "81.12",
                    "78.42",
                    "79.75"
                ]
            ],
            "title": "Table 4: Proposition segmentation results. Result that is significantly better than all comparisons is marked with \u2217 (p<10\u22126, McNemar test)."
        },
        "insight": "Table 4 shows that BiLSTM-CRF outperforms other methods in F1. More importantly, the perfor [CONTINUE] mance on reviews is lower than those reached on existing datasets, e.g., an F1 of 86.7 is obtained by CRF for essays (Stab and Gurevych, 2017)."
    },
    {
        "id": "170",
        "table": {
            "header": [
                "[EMPTY]",
                "FactBank MAE",
                "FactBank r",
                "UW MAE",
                "UW r",
                "Meantime MAE",
                "Meantime r",
                "UDS-IH2 MAE",
                "UDS-IH2 r"
            ],
            "rows": [
                [
                    "All-3.0",
                    "0.8",
                    "NAN",
                    "0.78",
                    "NAN",
                    "0.31",
                    "NAN",
                    "2.255",
                    "NAN"
                ],
                [
                    "Lee et al. 2015",
                    "-",
                    "-",
                    "0.511",
                    "0.708",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Stanovsky et al. 2017",
                    "0.59",
                    "0.71",
                    "[BOLD] 0.42\u2020",
                    "0.66",
                    "0.34",
                    "0.47",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-S",
                    "[BOLD] 0.427",
                    "[BOLD] 0.826",
                    "0.508",
                    "[BOLD] 0.719",
                    "0.427",
                    "0.335",
                    "[BOLD] 0.960\u2020",
                    "[BOLD] 0.768"
                ],
                [
                    "T-biLSTM(2)-S",
                    "[BOLD] 0.577",
                    "[BOLD] 0.752",
                    "0.600",
                    "0.645",
                    "0.428",
                    "0.094",
                    "[BOLD] 1.101",
                    "[BOLD] 0.704"
                ],
                [
                    "L-biLSTM(2)-G",
                    "[BOLD] 0.412",
                    "[BOLD] 0.812",
                    "0.523",
                    "0.703",
                    "0.409",
                    "0.462",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-G",
                    "[BOLD] 0.455",
                    "[BOLD] 0.809",
                    "0.567",
                    "0.688",
                    "0.396",
                    "0.368",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-S+lexfeats",
                    "[BOLD] 0.429",
                    "[BOLD] 0.796",
                    "0.495",
                    "[BOLD] 0.730",
                    "0.427",
                    "0.322",
                    "[BOLD] 1.000",
                    "[BOLD] 0.755"
                ],
                [
                    "T-biLSTM(2)-S+lexfeats",
                    "[BOLD] 0.542",
                    "[BOLD] 0.744",
                    "0.567",
                    "0.676",
                    "0.375",
                    "0.242",
                    "[BOLD] 1.087",
                    "[BOLD] 0.719"
                ],
                [
                    "L-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.353",
                    "[BOLD] 0.843",
                    "0.503",
                    "[BOLD] 0.725",
                    "0.345",
                    "[BOLD] 0.540",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.482",
                    "[BOLD] 0.803",
                    "0.599",
                    "0.645",
                    "0.545",
                    "0.237",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiBal",
                    "[BOLD] 0.391",
                    "[BOLD] 0.821",
                    "0.496",
                    "[BOLD] 0.724",
                    "[BOLD] 0.278",
                    "[BOLD] 0.613\u2020",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiBal",
                    "[BOLD] 0.517",
                    "[BOLD] 0.788",
                    "0.573",
                    "0.659",
                    "0.400",
                    "0.405",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(1)-MultiFoc",
                    "[BOLD] 0.343",
                    "[BOLD] 0.823",
                    "0.516",
                    "0.698",
                    "[BOLD] 0.229\u2020",
                    "[BOLD] 0.599",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiFoc",
                    "[BOLD] 0.314",
                    "[BOLD] 0.846",
                    "0.502",
                    "[BOLD] 0.710",
                    "[BOLD] 0.305",
                    "0.377",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiFoc",
                    "1.100",
                    "0.234",
                    "0.615",
                    "0.616",
                    "0.395",
                    "0.300",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "[BOLD] 0.377",
                    "[BOLD] 0.828",
                    "0.508",
                    "[BOLD] 0.722",
                    "0.367",
                    "0.469",
                    "[BOLD] 0.965",
                    "[BOLD] 0.771\u2020"
                ],
                [
                    "T-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "0.595",
                    "[BOLD] 0.716",
                    "0.598",
                    "0.609",
                    "0.467",
                    "0.345",
                    "[BOLD] 1.072",
                    "[BOLD] 0.723"
                ],
                [
                    "H-biLSTM(2)-S",
                    "0.488",
                    "[BOLD] 0.775",
                    "0.526",
                    "[BOLD] 0.714",
                    "0.442",
                    "0.255",
                    "[BOLD] 0.967",
                    "[BOLD] 0.768"
                ],
                [
                    "H-biLSTM(1)-MultiSimp",
                    "[BOLD] 0.313\u2020",
                    "[BOLD] 0.857\u2020",
                    "0.528",
                    "0.704",
                    "0.314",
                    "0.545",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.431",
                    "[BOLD] 0.808",
                    "0.514",
                    "[BOLD] 0.723",
                    "0.401",
                    "0.461",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiBal",
                    "[BOLD] 0.386",
                    "[BOLD] 0.825",
                    "0.502",
                    "[BOLD] 0.713",
                    "0.352",
                    "[BOLD] 0.564",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "[BOLD] 0.393",
                    "[BOLD] 0.820",
                    "0.481",
                    "[BOLD] 0.749\u2020",
                    "0.374",
                    "[BOLD] 0.495",
                    "[BOLD] 0.969",
                    "[BOLD] 0.760"
                ]
            ],
            "title": "Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art results in bold; \u2020 indicates best in column (corresponding row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced, MultiFoc=multi-task focused, w/UDS-IH2=trained on all data including UDS-IH2. All-3.0 is a constant baseline, always predicting 3.0."
        },
        "insight": "Table 4 reports the results for all of the 2-layer L-, T-, and H-biLSTMs.7 The best-performing system for each dataset and metric are highlighted in purple, and when the best-performing system for a particular dataset was a 1-layer model, that system is included in Table 4. [CONTINUE] The highest-performing system for each is reported in Table 4. [CONTINUE] On its own, the biLSTM with linear topology (L-biLSTM) performs [CONTINUE] consistently better than the biLSTM with tree topology (T-biLSTM). However, the hybrid topology (H-biLSTM), consisting of both a L- and TbiLSTM is the top-performing system on UW for correlation (Table 4). [CONTINUE] Though our methods achieve state of the art in the single-task setting, the best performing systems are mostly multi-task (Table 4 and Supplementary Materials)."
    },
    {
        "id": "171",
        "table": {
            "header": [
                "[EMPTY]",
                "Overall",
                "Eval",
                "Req",
                "Fact",
                "Ref",
                "Quot"
            ],
            "rows": [
                [
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments"
                ],
                [
                    "Majority",
                    "40.75",
                    "57.90",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "PropLexicon",
                    "36.83",
                    "40.42",
                    "36.07",
                    "32.23",
                    "59.57",
                    "31.28"
                ],
                [
                    "SVM",
                    "60.98",
                    "63.88",
                    "[BOLD] 69.02",
                    "54.74",
                    "[BOLD] 69.47",
                    "7.69"
                ],
                [
                    "CNN",
                    "[BOLD] 66.56\u2217",
                    "[BOLD] 69.02",
                    "63.26",
                    "[BOLD] 66.17",
                    "67.44",
                    "[BOLD] 52.94"
                ],
                [
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments"
                ],
                [
                    "Majority",
                    "33.30",
                    "47.60",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "PropLexicon",
                    "23.21",
                    "22.45",
                    "23.97",
                    "23.73",
                    "35.96",
                    "16.67"
                ],
                [
                    "SVM",
                    "51.46",
                    "54.05",
                    "48.16",
                    "52.77",
                    "52.27",
                    "4.71"
                ],
                [
                    "CNN",
                    "55.48",
                    "57.75",
                    "53.71",
                    "55.19",
                    "48.78",
                    "33.33"
                ],
                [
                    "CRF-joint",
                    "50.69",
                    "46.78",
                    "55.74",
                    "52.27",
                    "[BOLD] 55.77",
                    "26.47"
                ],
                [
                    "BiLSTM-CRF-joint",
                    "[BOLD] 62.64\u2217",
                    "[BOLD] 62.36\u2217",
                    "[BOLD] 67.31\u2217",
                    "[BOLD] 61.86",
                    "54.74",
                    "[BOLD] 37.36"
                ]
            ],
            "title": "Table 5: Proposition classification F1 scores. Results that are significant better than other methods are marked with \u2217 (p<10\u22126, McNemar test)."
        },
        "insight": "F1 scores for all propositions and each type are reported in Table 5. [CONTINUE] CNN performs better for types with significantly more training samples, i.e., evaluation and fact, indicating the effect of data size on neural model's performance. Joint models (CRF-joint and BiLSTM-CRF-joint) yield the best F1 scores for all categories when gold-standard segmentation is unavailable."
    },
    {
        "id": "172",
        "table": {
            "header": [
                "# encoders",
                "2"
            ],
            "rows": [
                [
                    "encoder type",
                    "B-LSTM"
                ],
                [
                    "encoder layers",
                    "2"
                ],
                [
                    "encoder hidden dim",
                    "500"
                ],
                [
                    "# decoders",
                    "1"
                ],
                [
                    "decoder type",
                    "LSTM"
                ],
                [
                    "decoder layers",
                    "2"
                ],
                [
                    "decoder hidden dim",
                    "500"
                ],
                [
                    "word vector dim",
                    "300"
                ],
                [
                    "attention type",
                    "[ITALIC] general"
                ],
                [
                    "dropout",
                    "0.3"
                ],
                [
                    "beam size",
                    "5"
                ]
            ],
            "title": "Table 1: The model and its hyper-parameters."
        },
        "insight": "Table 1 lists all hyper-parameters which have all been chosen using only training and validation data. The two encoders have been implemented using a Bidirectional Long Short-Term Memory (B-LSTM) (Hochreiter and Schmidhuber, 1997) while the decoder uses a unidirectional LSTM. Both the encoders and the decoder use two hidden layers. For the attention network, we have used the OpenNMT's general option (Luong et al., 2015)."
    },
    {
        "id": "173",
        "table": {
            "header": [
                "Model",
                "TER",
                "BLEU"
            ],
            "rows": [
                [
                    "MT Bojar et\u00a0al. ( 2017 )",
                    "24.48",
                    "62.49"
                ],
                [
                    "SPE Bojar et\u00a0al. ( 2017 )",
                    "24.69",
                    "62.97"
                ],
                [
                    "Varis and Bojar ( 2017 )",
                    "24.03",
                    "64.28"
                ],
                [
                    "B\u00e9rard et\u00a0al. ( 2017 )",
                    "22.81",
                    "65.91"
                ],
                [
                    "train 11K",
                    "41.58",
                    "43.05"
                ],
                [
                    "train 23K",
                    "30.23",
                    "57.14"
                ],
                [
                    "train 23K + 500K",
                    "[BOLD] 22.60",
                    "[BOLD] 66.21"
                ]
            ],
            "title": "Table 2: Results on the WMT17 IT domain English-German APE test set."
        },
        "insight": "Table 2 compares the accuracy of our model on the test data with two baselines and two state-of-theart comparable systems. The MT baseline simply consists of the accuracy of the mt sentences with respect to the pe ground truth. The other baseline is given by a statistical PE (SPE) system (Simard et al., 2007) chosen by the WMT17 organizers. Table 2 shows that when our model is trained with only the 11K WMT17 official training sentences, it cannot even approach the baselines. Even when the 12K WMT16 sentences are added, its accuracy is still well below that of the baselines. However, when the 500K artificial data are added, it reports a major improvement and it outperforms them both significantly. In addition, we have compared our model with two recent systems that have used our [CONTINUE] same training settings (500K artificial triplets + 23K manual triplets oversampled 10 times), reporting a slightly higher accuracy than both (1.43 TER and 1.93 BLEU p.p. over (Varis and Bojar, 2017) and 0.21 TER and 0.30 BLEU p.p. over (B\u00b4erard et al., 2017)). Since their models explicitly predicts edit operations rather than post-edited sentences, we speculate that these two tasks are of comparable intrinsic complexity."
    },
    {
        "id": "174",
        "table": {
            "header": [
                "[BOLD] Feature",
                "[BOLD] Representation",
                "[BOLD] Embedding",
                "[BOLD] Window",
                "[BOLD] Dimension",
                "[BOLD] Precision",
                "[BOLD] Recall",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Used for transportation",
                    "Co-occurrence",
                    "word2vec SG",
                    "10",
                    "300",
                    "74.5",
                    "78.8",
                    "76.6"
                ],
                [
                    "Is a weapon",
                    "Backtranslation",
                    "word2vec CBOW",
                    "2",
                    "300",
                    "71.4",
                    "88.2",
                    "78.9"
                ],
                [
                    "Is round",
                    "Co-occurrence",
                    "word2vec CBOW",
                    "10",
                    "300",
                    "56.2",
                    "87.1",
                    "68.4"
                ],
                [
                    "Has various colors",
                    "Co-occurrence",
                    "GloVe",
                    "2",
                    "200",
                    "70.6",
                    "76.6",
                    "73.5"
                ],
                [
                    "Made of metal",
                    "Matrix",
                    "word2vec SG",
                    "5",
                    "300",
                    "78.6",
                    "61.1",
                    "68.8"
                ]
            ],
            "title": "Table 3: The performance of the best setting for each property."
        },
        "insight": "Table 3 displays the best performing DSMfor each property. There is a preference to word2vec and to a higher embedding dimension."
    },
    {
        "id": "175",
        "table": {
            "header": [
                "Relation",
                "Mean Label",
                "L-biLSTM",
                "T-biLSTM",
                "#"
            ],
            "rows": [
                [
                    "root",
                    "1.07",
                    "1.03",
                    "0.96",
                    "949"
                ],
                [
                    "conj",
                    "0.37",
                    "0.44",
                    "0.46",
                    "316"
                ],
                [
                    "advcl",
                    "0.46",
                    "0.53",
                    "0.45",
                    "303"
                ],
                [
                    "xcomp",
                    "-0.42",
                    "-0.57",
                    "-0.49",
                    "234"
                ],
                [
                    "acl:relcl",
                    "1.28",
                    "1.40",
                    "1.31",
                    "193"
                ],
                [
                    "ccomp",
                    "0.11",
                    "0.31",
                    "0.34",
                    "191"
                ],
                [
                    "acl",
                    "0.77",
                    "0.59",
                    "0.58",
                    "159"
                ],
                [
                    "parataxis",
                    "0.44",
                    "0.63",
                    "0.79",
                    "127"
                ],
                [
                    "amod",
                    "1.92",
                    "1.88",
                    "1.81",
                    "76"
                ],
                [
                    "csubj",
                    "0.36",
                    "0.38",
                    "0.27",
                    "37"
                ]
            ],
            "title": "Table 6: Mean predictions for linear (L-biLSTM-S(2)) and tree models (T-biLSTM-S(2)) on UDS-IH2-dev, grouped by governing dependency relation. Only the 10 most frequent governing dependency relations in UDS-IH2-dev are shown."
        },
        "insight": "Evidence of this complementarity can be seen in Table 6, which contains a breakdown of system performance by governing dependency relation, for both linear and tree models, on UDS-IH2-dev. In most cases, the L-biLSTM's mean prediction is closer to the true mean. This appears to arise in part because the T-biLSTM is less confident in its predictions \u2013 i.e. its mean prediction tends to be closer to 0. This results in the L-biLSTM being too confident in certain cases \u2013 e.g. in the case of the xcomp governing relation, where the T-biLSTM mean prediction is closer to the true mean."
    },
    {
        "id": "176",
        "table": {
            "header": [
                "Lexicons",
                "Annotations",
                "# dim.",
                "# words"
            ],
            "rows": [
                [
                    "LIWC",
                    "psycho-linguistic",
                    "73",
                    "18,504"
                ],
                [
                    "Bing Liu",
                    "valence",
                    "1",
                    "2,477"
                ],
                [
                    "AFINN",
                    "sentiment",
                    "1",
                    "6,786"
                ],
                [
                    "MPQA",
                    "sentiment",
                    "4",
                    "6,886"
                ],
                [
                    "SemEval15",
                    "sentiment",
                    "1",
                    "1,515"
                ],
                [
                    "Emolex",
                    "emotion",
                    "19",
                    "14,182"
                ]
            ],
            "title": "Table 1: The lexicons used as external knowledge."
        },
        "insight": "we augment our models with existing linguistic and affective knowledge from human experts. Specifically, we leverage lexica containing psycho-linguistic, sentiment and emotion annotations. [CONTINUE] the word's annotations from the lexicons shown in Table 1. [CONTINUE] As prior knowledge, we leverage the lexicons presented in Table 1. We selected widely-used lexicons that represent different facets of affective and psycho-linguistic features, namely;"
    },
    {
        "id": "177",
        "table": {
            "header": [
                "Model",
                "SST-5",
                "Sent17",
                "PhychExp",
                "Irony18",
                "SCv1",
                "SCv2"
            ],
            "rows": [
                [
                    "baseline",
                    "43.5\u00b10.5",
                    "68.3\u00b10.2",
                    "53.2\u00b10.8",
                    "46.3\u00b11.4",
                    "64.1\u00b10.5",
                    "74.0\u00b10.7"
                ],
                [
                    "emb. conc.",
                    "43.3\u00b10.6",
                    "68.4\u00b10.2",
                    "57.1\u00b11.2",
                    "48.1\u00b11.2",
                    "64.2\u00b10.7",
                    "74.2\u00b10.7"
                ],
                [
                    "conc.",
                    "44.0\u00b10.7",
                    "68.6\u00b10.3",
                    "54.3\u00b10.6",
                    "47.4\u00b10.9",
                    "[BOLD] 65.1\u00b10.6",
                    "74.3\u00b11.2"
                ],
                [
                    "gate",
                    "44.2\u00b10.4",
                    "68.7\u00b10.3",
                    "53.4\u00b11.0",
                    "[BOLD] 48.5\u00b10.7",
                    "64.7\u00b10.7",
                    "74.3\u00b11.2"
                ],
                [
                    "affine",
                    "43.2\u00b10.7",
                    "68.5\u00b10.3",
                    "53.1\u00b10.9",
                    "45.3\u00b11.5",
                    "60.3\u00b10.8",
                    "74.0\u00b11.0"
                ],
                [
                    "gate+emb.conc.",
                    "[BOLD] 46.2\u00b10.5",
                    "[BOLD] 68.9\u00b10.3",
                    "[BOLD] 57.2\u00b11.1",
                    "[BOLD] 48.4\u00b11.0",
                    "[BOLD] 64.9\u00b10.6",
                    "[BOLD] 74.4\u00b10.9"
                ],
                [
                    "state-of-the-art",
                    "51.7",
                    "68.5",
                    "57.0",
                    "53.6",
                    "69.0",
                    "76.0"
                ],
                [
                    "state-of-the-art",
                    "Shen et\u00a0al. ( 2018 )",
                    "Cliche ( 2017 )",
                    "Felbo et\u00a0al. ( 2017 )",
                    "Baziotis et\u00a0al. ( 2018 )",
                    "Felbo et\u00a0al. ( 2017 )",
                    "Ili\u0107 et\u00a0al. ( 2018 )"
                ]
            ],
            "title": "Table 3: Comparison across benchmark datasets. Reported values are averaged across ten runs. All reported measures are F1 scores, apart from SST\u22125 which is evaluated with Accuracy."
        },
        "insight": "We present our results in Section 5 (Table 3) [CONTINUE] In Table 3 we use the abbreviations \"baseline\" and \"emb. conc.\" for the two baseline models respectively. [CONTINUE] We compare the performance of the three proposed conditioning methods with the two baselines and the state-of-the-art in Table 3. [CONTINUE] The results show that incorporating external knowledge in RNN-based architectures consistently improves performance over the baseline for all datasets. Furthermore, feature-based gating improves upon baseline concatenation in the embedding layer across benchmarks, with the exception of PsychExp dataset. [CONTINUE] For the Sent17 dataset we achieve state-ofthe-art F1 score using the feature-based gating method; we further improve performance when combining gating with the emb. conc. method. For SST-5, we observe a significant performance boost with combined attentional gating and embedding conditioning (gate + emb. conc.). For PsychExp, we marginally outperform the state-ofthe-art also with the combined method, while for Irony18, feature-based gating yields the best results. Finally, concatenation based conditioning is the top method for SCv1, and the combination method for SCv2."
    },
    {
        "id": "178",
        "table": {
            "header": [
                "Modal",
                "Negated",
                "Mean Label",
                "Linear MAE",
                "Tree MAE",
                "#"
            ],
            "rows": [
                [
                    "none",
                    "no",
                    "1.00",
                    "0.93",
                    "1.03",
                    "2244"
                ],
                [
                    "none",
                    "yes",
                    "-0.19",
                    "1.40",
                    "1.69",
                    "98"
                ],
                [
                    "may",
                    "no",
                    "-0.38",
                    "1.00",
                    "0.99",
                    "14"
                ],
                [
                    "would",
                    "no",
                    "-0.61",
                    "0.85",
                    "0.99",
                    "39"
                ],
                [
                    "ca(n\u2019t)",
                    "yes",
                    "-0.72",
                    "1.28",
                    "1.55",
                    "11"
                ],
                [
                    "can",
                    "yes",
                    "-0.75",
                    "0.99",
                    "0.86",
                    "6"
                ],
                [
                    "(wi)\u2019ll",
                    "no",
                    "-0.94",
                    "1.47",
                    "1.14",
                    "8"
                ],
                [
                    "could",
                    "no",
                    "-1.03",
                    "0.97",
                    "1.32",
                    "20"
                ],
                [
                    "can",
                    "no",
                    "-1.25",
                    "1.02",
                    "1.21",
                    "73"
                ],
                [
                    "might",
                    "no",
                    "-1.25",
                    "0.66",
                    "1.06",
                    "6"
                ],
                [
                    "would",
                    "yes",
                    "-1.27",
                    "0.40",
                    "0.86",
                    "5"
                ],
                [
                    "should",
                    "no",
                    "-1.31",
                    "1.20",
                    "1.01",
                    "22"
                ],
                [
                    "will",
                    "no",
                    "-1.88",
                    "0.75",
                    "0.86",
                    "75"
                ]
            ],
            "title": "Table 5: Mean gold labels, counts, and MAE for L-biLSTM(2)-S and T-biLSTM(2)-S model predictions on UDS-IH2-dev, grouped by modals and negation."
        },
        "insight": "Table 5 illustrates the influence of modals and negation on the factuality of the events they have direct scope over. The context with the highest factuality on average is no direct modal and no negation (first row); all other modal contexts have varying degrees of negative mean factuality scores, with will as the most negative."
    },
    {
        "id": "179",
        "table": {
            "header": [
                "[EMPTY]",
                "Metric",
                "Amazon",
                "Newsgroups",
                "New York Times"
            ],
            "rows": [
                [
                    "Global",
                    "SigVac",
                    "0.6960",
                    "0.6081",
                    "0.6063"
                ],
                [
                    "Global",
                    "SigUni",
                    "0.6310",
                    "0.4839",
                    "0.4935"
                ],
                [
                    "Global",
                    "Coherence",
                    "0.4907",
                    "0.4463",
                    "0.3799"
                ]
            ],
            "title": "Table 2: Coefficient of determination (r2) between global metrics and crowdsourced topic-word matching annotations."
        },
        "insight": "As seen in Table 2, we report the coefficient of determination (r2) for each global metric and dataset. [CONTINUE] Note that global metrics do correlate somewhat with human judgment of local topic quality. However, the correlation is moderate to poor, especially in the case of coherence"
    },
    {
        "id": "180",
        "table": {
            "header": [
                "[EMPTY]",
                "Metric",
                "Amazon",
                "Newsgroups",
                "New York Times"
            ],
            "rows": [
                [
                    "Local",
                    "SwitchP",
                    "0.9077",
                    "0.8737",
                    "0.7022"
                ],
                [
                    "Local",
                    "SwitchVI",
                    "0.8485",
                    "0.8181",
                    "0.6977"
                ],
                [
                    "Local",
                    "AvgRank",
                    "0.5103",
                    "0.5089",
                    "0.4473"
                ],
                [
                    "Local",
                    "Window",
                    "0.4884",
                    "0.3024",
                    "0.1127"
                ],
                [
                    "Local",
                    "WordDiv",
                    "0.3112",
                    "0.2197",
                    "0.0836"
                ],
                [
                    "Global",
                    "SigVac",
                    "0.6960",
                    "0.6081",
                    "0.6063"
                ],
                [
                    "Global",
                    "SigUni",
                    "0.6310",
                    "0.4839",
                    "0.4935"
                ],
                [
                    "Global",
                    "Coherence",
                    "0.4907",
                    "0.4463",
                    "0.3799"
                ]
            ],
            "title": "Table 3: Coefficient of determination (r2) between automated metrics and crowdsourced topic-word matching annotations. We include metrics measuring both local topic quality and global topic quality."
        },
        "insight": "Humans agree more often with models trained on Amazon reviews than on New York Times. [CONTINUE] As seen in Table 3, we report the coefficient of determination (r2) for each metric and dataset. [CONTINUE] SWITCHP most closely approximates human judgments of local topic quality, with an r2 which indicates a strong correlation. [CONTINUE] As evidenced by the lower r2 for SWITCHVI, even switching between related topics does not seem to line up with human judgments of local topic quality."
    },
    {
        "id": "181",
        "table": {
            "header": [
                "[BOLD] Method Location",
                "[BOLD] P@01 0.3555",
                "[BOLD] P@01 \u2013",
                "[BOLD] P@05 0.3077",
                "[BOLD] P@05 \u2013",
                "[BOLD] P@10 0.2505",
                "[BOLD] P@10 \u2013",
                "[BOLD] AUC 0.5226",
                "[BOLD] AUC \u2013"
            ],
            "rows": [
                [
                    "PageRank",
                    "0.3628",
                    "\u2013",
                    "0.3438",
                    "\u2013",
                    "0.3007",
                    "\u2013",
                    "0.5866",
                    "\u2013"
                ],
                [
                    "Frequency",
                    "0.4542",
                    "\u2013",
                    "0.4024",
                    "\u2013",
                    "0.3445",
                    "\u2013",
                    "0.5732",
                    "\u2013"
                ],
                [
                    "LeToR",
                    "0.4753\u2020",
                    "+4.64%",
                    "0.4099\u2020",
                    "+1.87%",
                    "0.3517\u2020",
                    "+2.10%",
                    "0.6373\u2020",
                    "+11.19%"
                ],
                [
                    "KCE (-EF)",
                    "0.4420",
                    "\u22122.69%",
                    "0.4038",
                    "+0.34%",
                    "0.3464\u2020",
                    "+0.54%",
                    "0.6089\u2020",
                    "+6.23%"
                ],
                [
                    "KCE (-E)",
                    "0.4861\u2020\u2021",
                    "+7.01%",
                    "0.4227\u2020\u2021",
                    "+5.04%",
                    "0.3603\u2020\u2021",
                    "+4.58%",
                    "0.6541\u2020\u2021",
                    "+14.12%"
                ],
                [
                    "KCE",
                    "0.5049\u2020\u2021",
                    "+11.14%",
                    "0.4277\u2020\u2021",
                    "+6.29%",
                    "0.3638\u2020\u2021",
                    "+5.61%",
                    "0.6557\u2020\u2021",
                    "+14.41%"
                ],
                [
                    "[BOLD] Method",
                    "[BOLD] R@01",
                    "[BOLD] R@01",
                    "[BOLD] R@05",
                    "[BOLD] R@05",
                    "[BOLD] R@10",
                    "[BOLD] R@10",
                    "[BOLD] W/T/L",
                    "[BOLD] W/T/L"
                ],
                [
                    "Location",
                    "0.0807",
                    "\u2013",
                    "0.2671",
                    "\u2013",
                    "0.3792",
                    "\u2013",
                    "\u2013/\u2013/\u2013",
                    "\u2013/\u2013/\u2013"
                ],
                [
                    "PageRank",
                    "0.0758",
                    "\u2013",
                    "0.2760",
                    "\u2013",
                    "0.4163",
                    "\u2013",
                    "\u2013/\u2013/\u2013",
                    "\u2013/\u2013/\u2013"
                ],
                [
                    "Frequency",
                    "0.0792",
                    "\u2013",
                    "0.2846",
                    "\u2013",
                    "0.4270",
                    "\u2013",
                    "\u2013/\u2013/\u2013",
                    "\u2013/\u2013/\u2013"
                ],
                [
                    "LeToR",
                    "0.0836\u2020",
                    "+5.61%",
                    "0.2980\u2020",
                    "+4.70%",
                    "0.4454\u2020",
                    "+4.31%",
                    "8037 / 48493 / 6770",
                    "8037 / 48493 / 6770"
                ],
                [
                    "KCE (-EF)",
                    "0.0714",
                    "\u22129.77%",
                    "0.2812",
                    "\u22121.18%",
                    "0.4321\u2020",
                    "+1.20%",
                    "6936 / 48811 / 7553",
                    "6936 / 48811 / 7553"
                ],
                [
                    "KCE (-E)",
                    "0.0925\u2020\u2021",
                    "+16.78%",
                    "0.3172\u2020\u2021",
                    "+11.46%",
                    "0.4672\u2020\u2021",
                    "+9.41%",
                    "11676 / 43294 / 8330",
                    "11676 / 43294 / 8330"
                ],
                [
                    "KCE",
                    "0.0946\u2020\u2021",
                    "+19.44%",
                    "0.3215\u2020\u2021",
                    "+12.96%",
                    "0.4719\u2020\u2021",
                    "+10.51%",
                    "12554 / 41461 / 9285",
                    "12554 / 41461 / 9285"
                ]
            ],
            "title": "Table 3: Event salience performance. (-E) and (-F) marks removing Entity information and Features from the full KCM model. The relative performance differences are computed against Frequency. W/T/L are the number of documents a method wins, ties, and loses compared to Frequency. \u2020 and \u2021 mark the statistically significant improvements over Frequency\u2020, LeToR\u2021 respectively."
        },
        "insight": "We summarize the main results in Table 3. [CONTINUE] Frequency is the best performing baseline. Its precision at 1 and 5 are higher than 40%. PageRank performs worse than Frequency on all [CONTINUE] the precision and recall metrics. Location performs the worst. [CONTINUE] LeToR outperforms the baselines significantly on all metrics. Particularly, its P@1 value outperforms the Frequency baseline the most (4.64%), indicating a much better estimation on the most salient event. In terms of AUC, LeToR outperforms Frequency by a large margin (11.19% relative gain). [CONTINUE] The KCE model further beats LeToR significantly on all metrics, by around 5% on AUC and precision values, and by around 10% on the recall values. Notably, the P@1 score is much higher, reaching 50%. The large relative gain on all the recall metrics and the high performance on precision show that KCE works really well on the top of the rank list. [CONTINUE] To understand the source of performance gain of KCE, we conduct an ablation study by removing its components: -E removes of entity kernels; -EF removes the entity kernels and the features. We observe a performance drop in both cases."
    },
    {
        "id": "182",
        "table": {
            "header": [
                "[BOLD] Feature Groups",
                "P@1",
                "P@5",
                "P@10",
                "R@1",
                "R@5",
                "R@10",
                "AUC"
            ],
            "rows": [
                [
                    "Loc",
                    "0.3548",
                    "0.3069",
                    "0.2497",
                    "0.0807",
                    "0.2671",
                    "0.3792",
                    "0.5226"
                ],
                [
                    "Frequency",
                    "0.4536",
                    "0.4018",
                    "0.3440",
                    "0.0792",
                    "0.2846",
                    "0.4270",
                    "0.5732"
                ],
                [
                    "+ Loc",
                    "0.4734",
                    "0.4097",
                    "0.3513",
                    "0.0835",
                    "0.2976",
                    "0.4436",
                    "0.6354"
                ],
                [
                    "+ Loc + Event",
                    "0.4726",
                    "0.4101\u2020",
                    "0.3516",
                    "0.0831",
                    "0.2969",
                    "0.4431",
                    "0.6365\u2020"
                ],
                [
                    "+ Loc + Entity",
                    "0.4739",
                    "0.4100",
                    "0.3518",
                    "0.0812",
                    "0.2955",
                    "0.4418",
                    "0.6374"
                ],
                [
                    "+ Loc + Entity + Event",
                    "0.4739",
                    "0.4100",
                    "0.3518\u2020",
                    "0.0832",
                    "0.2974",
                    "0.4452\u2020",
                    "0.6374\u2020"
                ],
                [
                    "+ Loc + Entity + Event + Local",
                    "0.4754\u2020",
                    "0.4100",
                    "0.3517\u2020",
                    "0.0837",
                    "0.2981",
                    "0.4454\u2020",
                    "0.6373\u2020"
                ]
            ],
            "title": "Table 4: Feature Ablation Results. + sign indicates the additional features to Frequency. Loc is the sentence location feature. Event is the event voting feature. Entity is the entity voting feature. Local is the local entity voting feature. \u2020 marks the statistically significant improvements over + Loc."
        },
        "insight": "We gradually add feature groups to the Frequency baseline. The combination of Location (sentence location) and Frequency almost sets the performance for the whole model. Adding each voting feature individually produces mixed results. However, adding all voting features improves all metrics. Though the margin is small, 4 of them are statistically significant over Frequency+Location. [CONTINUE] To understand the contribution of individual features, we conduct an ablation study of various feature settings in Table 4."
    },
    {
        "id": "183",
        "table": {
            "header": [
                "Attribute",
                "#"
            ],
            "rows": [
                [
                    "Grammatical error present, incl. run-ons",
                    "16"
                ],
                [
                    "Is an auxiliary or light verb",
                    "14"
                ],
                [
                    "Annotation is incorrect",
                    "13"
                ],
                [
                    "Future event",
                    "12"
                ],
                [
                    "Is a question",
                    "5"
                ],
                [
                    "Is an imperative",
                    "3"
                ],
                [
                    "Is not an event or state",
                    "2"
                ],
                [
                    "One or more of the above",
                    "43"
                ]
            ],
            "title": "Table 7: Notable attributes of 50 instances from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)-MultiSim w/UDS-IH2)."
        },
        "insight": "Table 7 shows results from a manual error analysis on 50 events from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)MultiSim w/UDS-IH2). Grammatical errors (such as run-on sentences) in the underlying text of UDS-IH2 appear to pose a particular challenge for these models;"
    },
    {
        "id": "184",
        "table": {
            "header": [
                "attack",
                "kill",
                "[BOLD] Word2Vec 0.69",
                "[BOLD] KCE 0.3"
            ],
            "rows": [
                [
                    "arrest",
                    "charge",
                    "0.53",
                    "0.3"
                ],
                [
                    "USA (E)",
                    "war",
                    "0.46",
                    "0.3"
                ],
                [
                    "911 attack (E)",
                    "attack",
                    "0.72",
                    "0.3"
                ],
                [
                    "attack",
                    "trade",
                    "0.42",
                    "0.9"
                ],
                [
                    "hotel (E)",
                    "travel",
                    "0.49",
                    "0.9"
                ],
                [
                    "charge",
                    "murder",
                    "0.49",
                    "0.7"
                ],
                [
                    "business (E)",
                    "increase",
                    "0.43",
                    "0.7"
                ],
                [
                    "attack",
                    "walk",
                    "0.44",
                    "-0.3"
                ],
                [
                    "people (E)",
                    "work",
                    "0.40",
                    "-0.3"
                ]
            ],
            "title": "Table 5: Similarities between event entity pairs. Word2vec shows the cosine similarity in pre-trained embeddings. KCE lists their closest kernel mean after training. (E) marks entities."
        },
        "insight": "We inspect some pairs of events and entities in different kernels and list some examples in Table 5. [CONTINUE] The pairs in Table 3 exhibit interesting types of relations: e.g.,\"arrest-charge\" and \"attack-kill\" form script-like chains; \"911 attack\" forms a quasiidentity relation (Recasens et al., 2010) with \"attack\"; \"business\" and \"increase\" are candidates as frame-argument structure."
    },
    {
        "id": "185",
        "table": {
            "header": [
                "Model",
                "Self-BLEU (\u2193)",
                "% Unique  [ITALIC] n-grams (\u2191) Self",
                "% Unique  [ITALIC] n-grams (\u2191) Self",
                "% Unique  [ITALIC] n-grams (\u2191) Self",
                "% Unique  [ITALIC] n-grams (\u2191) WT103",
                "% Unique  [ITALIC] n-grams (\u2191) WT103",
                "% Unique  [ITALIC] n-grams (\u2191) WT103",
                "% Unique  [ITALIC] n-grams (\u2191) TBC",
                "% Unique  [ITALIC] n-grams (\u2191) TBC",
                "% Unique  [ITALIC] n-grams (\u2191) TBC"
            ],
            "rows": [
                [
                    "Model",
                    "Self-BLEU (\u2193)",
                    "n=2",
                    "n=3",
                    "n=4",
                    "n=2",
                    "n=3",
                    "n=4",
                    "n=2",
                    "n=3",
                    "n=4"
                ],
                [
                    "BERT (large)",
                    "9.43",
                    "63.15",
                    "92.38",
                    "98.01",
                    "59.91",
                    "91.86",
                    "98.43",
                    "64.59",
                    "93.27",
                    "98.59"
                ],
                [
                    "BERT (base)",
                    "10.06",
                    "60.76",
                    "91.76",
                    "98.14",
                    "57.90",
                    "91.72",
                    "98.55",
                    "60.94",
                    "92.04",
                    "98.56"
                ],
                [
                    "GPT",
                    "40.02",
                    "31.13",
                    "67.01",
                    "87.28",
                    "33.71",
                    "72.86",
                    "91.12",
                    "25.74",
                    "65.04",
                    "88.42"
                ],
                [
                    "WT103",
                    "9.80",
                    "70.29",
                    "94.36",
                    "99.05",
                    "56.19",
                    "88.05",
                    "97.44",
                    "68.35",
                    "94.20",
                    "99.23"
                ],
                [
                    "TBC",
                    "12.51",
                    "62.19",
                    "92.70",
                    "98.73",
                    "55.30",
                    "91.08",
                    "98.81",
                    "44.75",
                    "82.06",
                    "96.31"
                ]
            ],
            "title": "Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText-103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets."
        },
        "insight": "We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3. [CONTINUE] This observation is further bolstered by the fact that the GPT generations have a higher corpus-BLEU with TBC than TBC has with itself. [CONTINUE] The corpusBLEU between BERT models and the datasets is low, particularly with WT103. [CONTINUE] We find that BERT generations are more diverse than GPT generations. GPT has high n-gram overlap (smaller percent of unique n-grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. Furthermore, GPT generations have greater n-gram overlap with these datasets than these datasets have with themselves, further suggesting that GPT is relying significantly on generic sentences. BERT has lower n-gram overlap with both corpora, with similar degrees of n-gram overlap as the samples of the data."
    },
    {
        "id": "186",
        "table": {
            "header": [
                "Model",
                "Corpus-BLEU (\u2191) WT103",
                "Corpus-BLEU (\u2191) TBC",
                "PPL (\u2193)"
            ],
            "rows": [
                [
                    "BERT (large)",
                    "5.05",
                    "7.60",
                    "331.47"
                ],
                [
                    "BERT (base)",
                    "7.80",
                    "7.06",
                    "279.10"
                ],
                [
                    "GPT",
                    "10.81",
                    "30.75",
                    "154.29"
                ],
                [
                    "WT103",
                    "17.48",
                    "6.57",
                    "54.00"
                ],
                [
                    "TBC",
                    "10.05",
                    "23.05",
                    "314.28"
                ]
            ],
            "title": "Table 3: Quality metrics of model generations. Perplexity (PPL) is measured using an additional language model (Dauphin et\u00a0al., 2016). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets."
        },
        "insight": "We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3. [CONTINUE] We find that, compared to GPT, the BERT generations are of worse quality, but are more diverse. Surprisingly, the outside language model, which was trained on Wikipedia, is less perplexed by the GPT generations than the BERT generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as perplexed as on the BERT generations, which suggests that domain shift is an issue in using a trained language [CONTINUE] model for evaluating generations and that the GPT generations might have collapsed to fairly generic and simple sentences. [CONTINUE] The perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent."
    },
    {
        "id": "187",
        "table": {
            "header": [
                "Methods",
                "AIDA-B",
                "MSNBC",
                "AQUAINT",
                "ACE2004",
                "CWEB",
                "WIKI",
                "Avg"
            ],
            "rows": [
                [
                    "[ITALIC] Wikipedia",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Milne and Witten ( 2008 )",
                    "-",
                    "78",
                    "85",
                    "81",
                    "64.1",
                    "[BOLD] 81.7",
                    "77.96"
                ],
                [
                    "Ratinov et\u00a0al. ( 2011a )",
                    "-",
                    "75",
                    "83",
                    "82",
                    "56.2",
                    "67.2",
                    "72.68"
                ],
                [
                    "Hoffart et\u00a0al. ( 2011 )",
                    "-",
                    "79",
                    "56",
                    "80",
                    "58.6",
                    "63",
                    "67.32"
                ],
                [
                    "Cheng and Roth ( 2013 )",
                    "-",
                    "90",
                    "90",
                    "86",
                    "67.5",
                    "73.4",
                    "81.38"
                ],
                [
                    "Chisholm and Hachey ( 2015 )",
                    "84.9",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[ITALIC] Wiki + unlab",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Lazic et\u00a0al. ( 2015 )",
                    "86.4",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[EMPTY]"
                ],
                [
                    "Our model",
                    "[BOLD] 89.66 \u00b10.16",
                    "[BOLD] 92.2 \u00b10.2",
                    "[BOLD] 90.7 \u00b10.2",
                    "[BOLD] 88.1 \u00b10.0",
                    "[BOLD] 78.2 \u00b10.2",
                    "[BOLD] 81.7 \u00b10.1",
                    "[BOLD] 86.18"
                ],
                [
                    "[ITALIC] Wiki + Extra supervision",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chisholm and Hachey ( 2015 )",
                    "88.7",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[ITALIC] Fully-supervised (Wiki +",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] AIDA CoNLL train)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Guo and Barbosa ( 2016 )",
                    "89.0",
                    "92",
                    "87",
                    "88",
                    "77",
                    "84.5",
                    "85.7"
                ],
                [
                    "Globerson et\u00a0al. ( 2016 )",
                    "91.0",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Yamada et\u00a0al. ( 2016 )",
                    "91.5",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Ganea and Hofmann ( 2017 )",
                    "92.22 \u00b10.14",
                    "93.7 \u00b10.1",
                    "88.5 \u00b10.4",
                    "88.5 \u00b10.3",
                    "77.9 \u00b10.1",
                    "77.5 \u00b10.1",
                    "85.22"
                ],
                [
                    "Le and Titov ( 2018 )",
                    "93.07 \u00b10.27",
                    "93.9 \u00b10.2",
                    "88.3 \u00b10.6",
                    "89.9 \u00b10.8",
                    "77.5 \u00b10.1",
                    "78.0 \u00b10.1",
                    "85.5"
                ]
            ],
            "title": "Table 1: F1 scores on six test sets. The last column, Avg, shows the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI."
        },
        "insight": "First, we compare to systems which relied on Wikipedia and those which used Wikipedia along with unlabeled data ('Wikipedia + unlab'), i.e. the top half of Table 1. These methods are comparable to ours, as they use the same type of information as supervision. Our model outperformed all of them on all test sets. [CONTINUE] When evaluated on AIDA-B, their scores are still lower than ours, though significantly higher that those of the previous systems suggesting that web links are indeed valuable. [CONTINUE] Second, we compare to fully-supervised systems, which were estimated on AIDA-CoNLL documents. [CONTINUE] We distinguish results on a test set taken from AIDA-CoNLL (AIDA-B) and the other standard test sets not directly corresponding to the AIDA-CoNLL domain. When tested on the latter, our approach is very effective, on average outperforming fully-supervised techniques. [CONTINUE] As expected, on the in-domain test set (AIDA-B), the majority of recent fully-supervised methods are more accurate than our model. However, even on this test set our model is not as far behind, for example, outperforming the system of Guo and Barbosa (2016)."
    },
    {
        "id": "188",
        "table": {
            "header": [
                "Our model",
                "AIDA-A",
                "AIDA-B",
                "Avg"
            ],
            "rows": [
                [
                    "weakly-supervised",
                    "88.05",
                    "89.66",
                    "86.18"
                ],
                [
                    "fully-supervised",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "on Wikipedia",
                    "87.23",
                    "87.83",
                    "85.84"
                ],
                [
                    "on AIDA CoNLL",
                    "91.34",
                    "91.87",
                    "84.55"
                ]
            ],
            "title": "Table 2: F1 scores of our model when it is weakly-supervised and when it is fully-supervised on Wikipedia and on AIDA CoNLL. AIDA-A is our development set. Avg is the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI. Each F1 is the mean of five runs."
        },
        "insight": "The results are shown in Table 2 ('Wikipedia'). The resulting model is significantly less accurate than the one which used unlabeled documents. The score difference is larger [CONTINUE] for AIDA-CoNLL test set than for the other 5 test sets. [CONTINUE] Additionally we train our model on AIDA-CoNLL, producing its fully-supervised version ('AIDA CoNLL' row in Table 2). Though, as expected, this version is more accurate on AIDA test set, similarly to other fully-supervised methods, it overfits and does not perform that well on the 5 out-of-domain test sets."
    },
    {
        "id": "189",
        "table": {
            "header": [
                "Model",
                "AIDA-A"
            ],
            "rows": [
                [
                    "Our model",
                    "88.05"
                ],
                [
                    "without local",
                    "82.41"
                ],
                [
                    "without attention",
                    "86.82"
                ],
                [
                    "No disambiguation model ( [ITALIC] sc)",
                    "86.42"
                ]
            ],
            "title": "Table 3: Ablation study on AIDA CoNLL development set. Each F1 score is the mean of five runs."
        },
        "insight": "As we do not want to test multiple systems on the final test set, we report the remaining ablations on the development set (AIDA-A), Table 3. [CONTINUE] we constructed a baseline which only relies on link statistics in Wikipedia as well as string similarity (we refereed to its scoring function as sc). It appears surprisingly strong, however, we still outperform it by 1.6% (see Table 3). [CONTINUE] When we use only global coherence (i.e. only second term in expression (1)) and drop any modeling of local context on the disambiguation stage, the performance drops very substantially (to 82.4% F1, see Table 3). [CONTINUE] Without using local scores the disambiguation model appears to be even less accurate than our 'no-statisticaldisambiguation' baseline. It is also important to have an accurate global model: not using global attention results in a 1.2% drop in performance."
    },
    {
        "id": "190",
        "table": {
            "header": [
                "Type",
                "Our model",
                "Fully-supervised learning"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "on AIDA CoNLL"
                ],
                [
                    "LOC",
                    "85.53",
                    "89.41"
                ],
                [
                    "MISC",
                    "75.71",
                    "83.27"
                ],
                [
                    "ORG",
                    "89.51",
                    "92.70"
                ],
                [
                    "PER",
                    "97.20",
                    "97.73"
                ]
            ],
            "title": "Table 4: Accuracy (%) by NER type on AIDA-A."
        },
        "insight": "Figure 4 shows the accuracy of two systems for different NER (named entity recognition) types. We consider four types: location (LOC), organization (ORG), person (PER), and miscellany (MICS). These types are given in CoNLL 2003 dataset, which was used as a basis for AIDA CoNLL. Our model is accurate for PER, achieving accuracy of about 97%, only 0.53% lower than the supervised model."
    },
    {
        "id": "191",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "All Token-",
                "All MWE-",
                "Discontinuous",
                "Discontinuous",
                "Discontinuous",
                "Discontinuous"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "based",
                    "based",
                    "[EMPTY]",
                    "MWE-based",
                    "MWE-based",
                    "MWE-based"
                ],
                [
                    "L",
                    "model",
                    "F",
                    "F",
                    "%",
                    "P",
                    "R",
                    "F"
                ],
                [
                    "EN",
                    "baseline",
                    "41.37",
                    "35.38",
                    "32",
                    "24.44",
                    "10.48",
                    "14.67"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "39.78",
                    "39.11",
                    "32",
                    "39.53",
                    "16.19",
                    "22.97"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "33.33",
                    "31.79",
                    "32",
                    "46.88",
                    "14.29",
                    "21.90"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "41.63",
                    "[BOLD] 40.76",
                    "32",
                    "63.33",
                    "18.10",
                    "[BOLD] 28.15"
                ],
                [
                    "DE",
                    "baseline",
                    "62.27",
                    "57.17",
                    "43",
                    "69.50",
                    "45.37",
                    "54.90"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "65.48",
                    "[BOLD] 61.17",
                    "43",
                    "65.19",
                    "47.69",
                    "55.08"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "61.20",
                    "58.19",
                    "43",
                    "67.86",
                    "43.98",
                    "53.37"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "63.80",
                    "60.71",
                    "43",
                    "68.59",
                    "49.54",
                    "[BOLD] 57.53"
                ],
                [
                    "FR",
                    "baseline",
                    "76.62",
                    "72.16",
                    "43",
                    "75.27",
                    "52.04",
                    "61.54"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "79.59",
                    "75.15",
                    "43",
                    "79.58",
                    "56.51",
                    "66.09"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "78.21",
                    "74.23",
                    "43",
                    "71.49",
                    "60.59",
                    "65.59"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "80.25",
                    "[BOLD] 76.56",
                    "43",
                    "77.94",
                    "59.11",
                    "[BOLD] 67.23"
                ],
                [
                    "FA",
                    "baseline",
                    "88.45",
                    "86.50",
                    "14",
                    "67.76",
                    "55.88",
                    "61.29"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "87.78",
                    "86.42",
                    "14",
                    "78.72",
                    "54.41",
                    "64.35"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "87.55",
                    "84.20",
                    "14",
                    "62.32",
                    "63.24",
                    "62.77"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "88.76",
                    "[BOLD] 87.15",
                    "14",
                    "75.44",
                    "63.24",
                    "[BOLD] 68.80"
                ]
            ],
            "title": "Table 1: Model performance (P, R and F) for development sets for all MWE and only discontinuous ones (%: proportion of discontinuous MWES)"
        },
        "insight": "We perform hyperparameter optimisation and make comparisons among our systems, including GCN + Bi-LSTM (GCN-based), CNN + attention + Bi-LSTM (Attbased), and their combination using a highway layer (H-combined) in Table 1. [CONTINUE] Systems are evaluated using two types of precision, recall and F-score measures: strict MWEbased scores (every component of an MWE should be correctly tagged to be considered as true positive), and token-based scores (a partial match between a predicted and a gold MWE would be considered as true positive). We report results for all MWEs as well as discontinuous ones specifically. [CONTINUE] GCN-based outperforms Att-based and they both outperform the strong baseline in terms of MWE-based F-score in three out of four languages. Combining GCN with attention using highway networks results in further improvements for EN, FR and FA. The Hcombined model consistently exceeds the baseline for all languages. [CONTINUE] GCN and H-combined models each show significant improvement with regard to discontinuous MWEs, regardless of the proportion of such expressions. [CONTINUE] The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths."
    },
    {
        "id": "192",
        "table": {
            "header": [
                "Verb",
                "L-biLSTM(2)-S",
                "+lexfeats",
                "#"
            ],
            "rows": [
                [
                    "decide to",
                    "3.28",
                    "2.66",
                    "2"
                ],
                [
                    "forget to",
                    "0.67",
                    "0.48",
                    "2"
                ],
                [
                    "get to",
                    "1.55",
                    "1.43",
                    "9"
                ],
                [
                    "hope to",
                    "1.35",
                    "1.23",
                    "5"
                ],
                [
                    "intend to",
                    "1.18",
                    "0.61",
                    "1"
                ],
                [
                    "promise to",
                    "0.40",
                    "0.49",
                    "1"
                ],
                [
                    "try to",
                    "1.14",
                    "1.42",
                    "12"
                ],
                [
                    "want to",
                    "1.22",
                    "1.17",
                    "24"
                ]
            ],
            "title": "Table 9: MAE of L-biLSTM(2)-S and L-biLSTM(2)-S+lexfeats, for predictions on events in UDS-IH2-dev that are xcomp-governed by an infinitival-taking verb."
        },
        "insight": "Despite the underperformance of these features overall, Table 9 shows that they may still improve performance in the subset of instances where they appear."
    },
    {
        "id": "193",
        "table": {
            "header": [
                "[EMPTY]",
                "All | Discontinuous EN",
                "All | Discontinuous DE",
                "All | Discontinuous FR",
                "All | Discontinuous FA"
            ],
            "rows": [
                [
                    "baseline",
                    "33.01 | 16.53",
                    "54.12 | 53.94",
                    "67.66 | 58.70",
                    "[BOLD] 81.62 | 61.73"
                ],
                [
                    "GCN-based",
                    "36.27 |  [BOLD] 24.15",
                    "56.96 | 54.87",
                    "70.79 | 59.95",
                    "81.00 |  [BOLD] 62.35"
                ],
                [
                    "H-combined",
                    "[BOLD] 41.91 | 22.73",
                    "[BOLD] 59.29 |  [BOLD] 55.00",
                    "[BOLD] 70.97 |  [BOLD] 63.90",
                    "80.04 | 61.90"
                ],
                [
                    "ATILF-LLF",
                    "31.58 | 09.91",
                    "54.43 | 40.34",
                    "58.60 | 51.96",
                    "77.48 | 53.85"
                ],
                [
                    "SHOMA",
                    "26.42 | 01.90",
                    "48.71 | 40.12",
                    "62.00 | 51.43",
                    "78.35 | 56.10"
                ]
            ],
            "title": "Table 2: Comparing the performance of the systems on test data in terms of MWE-based F-score"
        },
        "insight": "we show the superior performance (in terms of MWE-based F-score) of our top systems on the test data compared to the baseline and stateof-the-art systems, namely, ATILF-LLF (Al Saied et al., 2017) and SHOMA (Taslimipoor and Rohanian, 2018). GCN works the best for discontinuous MWEs in EN and FA, while H-combined outperforms based on results for all MWEs except for FA. [CONTINUE] The overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths."
    },
    {
        "id": "194",
        "table": {
            "header": [
                "[BOLD] Pretr. Baselines",
                "[BOLD] Avg Baselines",
                "[BOLD] CoLA Baselines",
                "[BOLD] SST Baselines",
                "[BOLD] MRPC Baselines",
                "[BOLD] MRPC Baselines",
                "[BOLD] QQP Baselines",
                "[BOLD] QQP Baselines",
                "[BOLD] STS Baselines",
                "[BOLD] STS Baselines",
                "[BOLD] MNLI Baselines",
                "[BOLD] QNLI Baselines",
                "[BOLD] RTE Baselines",
                "[BOLD] WNLI Baselines"
            ],
            "rows": [
                [
                    "[BOLD] Random",
                    "68.2",
                    "16.9",
                    "84.3",
                    "77.7/",
                    "85.6",
                    "83.0/",
                    "80.6",
                    "81.7/",
                    "82.6",
                    "73.9",
                    "[BOLD] 79.6",
                    "57.0",
                    "31.0*"
                ],
                [
                    "[BOLD] Single-Task",
                    "69.1",
                    "21.3",
                    "89.0",
                    "77.2/",
                    "84.7",
                    "84.7/",
                    "81.9",
                    "81.4/",
                    "82.2",
                    "74.8",
                    "78.8",
                    "56.0",
                    "11.3*"
                ],
                [
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks"
                ],
                [
                    "[BOLD] CoLA",
                    "68.2",
                    "21.3",
                    "85.7",
                    "75.0/",
                    "83.7",
                    "85.7/",
                    "82.4",
                    "79.0/",
                    "80.3",
                    "72.7",
                    "78.4",
                    "56.3",
                    "15.5*"
                ],
                [
                    "[BOLD] SST",
                    "68.6",
                    "16.4",
                    "89.0",
                    "76.0/",
                    "84.2",
                    "84.4/",
                    "81.6",
                    "80.6/",
                    "81.4",
                    "73.9",
                    "78.5",
                    "58.8",
                    "19.7*"
                ],
                [
                    "[BOLD] MRPC",
                    "68.2",
                    "16.4",
                    "85.6",
                    "77.2/",
                    "84.7",
                    "84.4/",
                    "81.8",
                    "81.2/",
                    "82.2",
                    "73.6",
                    "79.3",
                    "56.7",
                    "22.5*"
                ],
                [
                    "[BOLD] QQP",
                    "68.0",
                    "14.7",
                    "86.1",
                    "77.2/",
                    "84.5",
                    "84.7/",
                    "81.9",
                    "81.1/",
                    "82.0",
                    "73.7",
                    "78.2",
                    "57.0",
                    "45.1*"
                ],
                [
                    "[BOLD] STS",
                    "67.7",
                    "14.1",
                    "84.6",
                    "77.9/",
                    "85.3",
                    "81.7/",
                    "79.2",
                    "81.4/",
                    "82.2",
                    "73.6",
                    "79.3",
                    "57.4",
                    "43.7*"
                ],
                [
                    "[BOLD] MNLI",
                    "69.1",
                    "16.7",
                    "88.2",
                    "78.9/",
                    "85.2",
                    "84.5/",
                    "81.5",
                    "81.8/",
                    "82.6",
                    "74.8",
                    "[BOLD] 79.6",
                    "58.8",
                    "36.6*"
                ],
                [
                    "[BOLD] QNLI",
                    "67.9",
                    "15.6",
                    "84.2",
                    "76.5/",
                    "84.2",
                    "84.3/",
                    "81.4",
                    "80.6/",
                    "81.8",
                    "73.4",
                    "78.8",
                    "58.8",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] RTE",
                    "68.1",
                    "18.1",
                    "83.9",
                    "77.5/",
                    "85.4",
                    "83.9/",
                    "81.2",
                    "81.2/",
                    "82.2",
                    "74.1",
                    "79.1",
                    "56.0",
                    "39.4*"
                ],
                [
                    "[BOLD] WNLI",
                    "68.0",
                    "16.3",
                    "84.3",
                    "76.5/",
                    "84.6",
                    "83.0/",
                    "80.5",
                    "81.6/",
                    "82.5",
                    "73.6",
                    "78.8",
                    "58.1",
                    "11.3*"
                ],
                [
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks"
                ],
                [
                    "[BOLD] DisSent WT",
                    "68.6",
                    "18.3",
                    "86.6",
                    "79.9/",
                    "86.0",
                    "85.3/",
                    "82.0",
                    "79.5/",
                    "80.5",
                    "73.4",
                    "79.1",
                    "56.7",
                    "42.3*"
                ],
                [
                    "[BOLD] LM WT",
                    "70.1",
                    "30.8",
                    "85.7",
                    "76.2/",
                    "84.2",
                    "86.2/",
                    "82.9",
                    "79.2/",
                    "80.2",
                    "74.0",
                    "79.4",
                    "60.3",
                    "25.4*"
                ],
                [
                    "[BOLD] LM BWB",
                    "[BOLD] 70.4",
                    "30.7",
                    "86.8",
                    "79.9/",
                    "86.2",
                    "[BOLD] 86.3/",
                    "[BOLD] 83.2",
                    "80.7/",
                    "81.4",
                    "74.2",
                    "79.0",
                    "57.4",
                    "47.9*"
                ],
                [
                    "[BOLD] MT En-De",
                    "68.1",
                    "16.7",
                    "85.4",
                    "77.9/",
                    "84.9",
                    "83.8/",
                    "80.5",
                    "82.4/",
                    "82.9",
                    "73.5",
                    "[BOLD] 79.6",
                    "55.6",
                    "22.5*"
                ],
                [
                    "[BOLD] MT En-Ru",
                    "68.4",
                    "16.8",
                    "85.1",
                    "79.4/",
                    "86.2",
                    "84.1/",
                    "81.2",
                    "82.7/",
                    "83.2",
                    "74.1",
                    "79.1",
                    "56.0",
                    "26.8*"
                ],
                [
                    "[BOLD] Reddit",
                    "66.9",
                    "15.3",
                    "82.3",
                    "76.5/",
                    "84.6",
                    "81.9/",
                    "79.2",
                    "81.5/",
                    "81.9",
                    "72.7",
                    "76.8",
                    "55.6",
                    "53.5*"
                ],
                [
                    "[BOLD] SkipThought",
                    "68.7",
                    "16.0",
                    "84.9",
                    "77.5/",
                    "85.0",
                    "83.5/",
                    "80.7",
                    "81.1/",
                    "81.5",
                    "73.3",
                    "79.1",
                    "[BOLD] 63.9",
                    "49.3*"
                ],
                [
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining"
                ],
                [
                    "[BOLD] MTL GLUE",
                    "68.9",
                    "15.4",
                    "[BOLD] 89.9",
                    "78.9/",
                    "86.3",
                    "82.6/",
                    "79.9",
                    "[BOLD] 82.9/",
                    "[BOLD] 83.5",
                    "[BOLD] 74.9",
                    "78.9",
                    "57.8",
                    "38.0*"
                ],
                [
                    "[BOLD] MTL Non-GLUE",
                    "69.9",
                    "30.6",
                    "87.0",
                    "[BOLD] 81.1/",
                    "[BOLD] 87.6",
                    "86.0/",
                    "82.2",
                    "79.9/",
                    "80.6",
                    "72.8",
                    "78.9",
                    "54.9",
                    "22.5*"
                ],
                [
                    "[BOLD] MTL All",
                    "[BOLD] 70.4",
                    "[BOLD] 33.2",
                    "88.2",
                    "78.9/",
                    "85.9",
                    "85.5/",
                    "81.8",
                    "79.7/",
                    "80.0",
                    "73.9",
                    "78.7",
                    "57.4",
                    "33.8*"
                ],
                [
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results"
                ],
                [
                    "[BOLD] LM BWB",
                    "66.5",
                    "29.1",
                    "86.9",
                    "75.0/",
                    "82.1",
                    "82.7/",
                    "63.3",
                    "74.0/",
                    "73.1",
                    "73.4",
                    "68.0",
                    "51.3",
                    "65.1"
                ],
                [
                    "[BOLD] MTL All",
                    "68.5",
                    "36.3",
                    "88.9",
                    "77.7/",
                    "84.8",
                    "82.7/",
                    "63.6",
                    "77.8/",
                    "76.7",
                    "75.3",
                    "66.2",
                    "53.2",
                    "65.1"
                ]
            ],
            "title": "Table 2: Results for pretraining experiments on development sets except where noted. Bold denotes best result overall. Underlining denotes an average score surpassing the Random baseline. See Section 6 for discussion of WNLI results (*)."
        },
        "insight": "Tables 2 and 3 respectively show results for our pretraining and intermediate training experiments. [CONTINUE] From Table 2, among target tasks, we find the grammar-related CoLA task benefits dramatically from LM pretraining: The results achieved with LM pretraining are significantly better than the results achieved without. In contrast, the meaningoriented STS sees good results with several kinds of pretraining, but does not benefit substantially from LM pretraining. Among pretraining tasks, language modeling performs best, followed by MNLI. The remaining pretraining tasks yield performance near that of the random baseline. Even our single-task baseline gets less than a one point gain over this simple baseline. The multitask models are tied or outperformed by models trained on one of their constituent tasks, suggesting that our approach to multitask learning does not reliably produce models that productively combine the knowledge taught by each task. However, of the two models that perform best on the development data, the multitask model generalizes better than the single-task model on test data for tasks like STS and MNLI where the test set contains out-of-domain data."
    },
    {
        "id": "195",
        "table": {
            "header": [
                "[BOLD] Intermediate Task ELMo with Intermediate Task Training",
                "[BOLD] Avg ELMo with Intermediate Task Training",
                "[BOLD] CoLA ELMo with Intermediate Task Training",
                "[BOLD] SST ELMo with Intermediate Task Training",
                "[BOLD] MRPC ELMo with Intermediate Task Training",
                "[BOLD] MRPC ELMo with Intermediate Task Training",
                "[BOLD] QQP ELMo with Intermediate Task Training",
                "[BOLD] QQP ELMo with Intermediate Task Training",
                "[BOLD] STS ELMo with Intermediate Task Training",
                "[BOLD] STS ELMo with Intermediate Task Training",
                "[BOLD] MNLI ELMo with Intermediate Task Training",
                "[BOLD] QNLI ELMo with Intermediate Task Training",
                "[BOLD] RTE ELMo with Intermediate Task Training",
                "[BOLD] WNLI ELMo with Intermediate Task Training"
            ],
            "rows": [
                [
                    "[BOLD] Random [ITALIC] E",
                    "70.5",
                    "38.5",
                    "87.7",
                    "79.9/",
                    "86.5",
                    "86.7/",
                    "83.4",
                    "80.8/",
                    "82.1",
                    "75.6",
                    "79.6",
                    "[BOLD] 61.7",
                    "33.8*"
                ],
                [
                    "[BOLD] Single-Task [ITALIC] E",
                    "71.2",
                    "39.4",
                    "[BOLD] 90.6",
                    "77.5/",
                    "84.4",
                    "86.4/",
                    "82.4",
                    "79.9/",
                    "80.6",
                    "75.6",
                    "78.0",
                    "55.6",
                    "11.3*"
                ],
                [
                    "[BOLD] CoLA [ITALIC] E",
                    "71.1",
                    "39.4",
                    "87.3",
                    "77.5/",
                    "85.2",
                    "86.5/",
                    "83.0",
                    "78.8/",
                    "80.2",
                    "74.2",
                    "78.2",
                    "59.2",
                    "33.8*"
                ],
                [
                    "[BOLD] SST [ITALIC] E",
                    "71.2",
                    "38.8",
                    "[BOLD] 90.6",
                    "80.4/",
                    "86.8",
                    "87.0/",
                    "83.5",
                    "79.4/",
                    "81.0",
                    "74.3",
                    "77.8",
                    "53.8",
                    "43.7*"
                ],
                [
                    "[BOLD] MRPC [ITALIC] E",
                    "71.3",
                    "40.0",
                    "88.4",
                    "77.5/",
                    "84.4",
                    "86.4/",
                    "82.7",
                    "79.5/",
                    "80.6",
                    "74.9",
                    "78.4",
                    "58.1",
                    "[BOLD] 54.9*"
                ],
                [
                    "[BOLD] QQP [ITALIC] E",
                    "70.8",
                    "34.3",
                    "88.6",
                    "79.4/",
                    "85.7",
                    "86.4/",
                    "82.4",
                    "81.1/",
                    "82.1",
                    "74.3",
                    "78.1",
                    "56.7",
                    "38.0*"
                ],
                [
                    "[BOLD] STS [ITALIC] E",
                    "71.6",
                    "39.9",
                    "88.4",
                    "79.9/",
                    "86.4",
                    "86.7/",
                    "83.3",
                    "79.9/",
                    "80.6",
                    "74.3",
                    "78.6",
                    "58.5",
                    "26.8*"
                ],
                [
                    "[BOLD] MNLI [ITALIC] E",
                    "72.1",
                    "38.9",
                    "89.0",
                    "80.9/",
                    "86.9",
                    "86.1/",
                    "82.7",
                    "81.3/",
                    "82.5",
                    "75.6",
                    "79.7",
                    "58.8",
                    "16.9*"
                ],
                [
                    "[BOLD] QNLI [ITALIC] E",
                    "71.2",
                    "37.2",
                    "88.3",
                    "81.1/",
                    "86.9",
                    "85.5/",
                    "81.7",
                    "78.9/",
                    "80.1",
                    "74.7",
                    "78.0",
                    "58.8",
                    "22.5*"
                ],
                [
                    "[BOLD] RTE [ITALIC] E",
                    "71.2",
                    "38.5",
                    "87.7",
                    "81.1/",
                    "87.3",
                    "86.6/",
                    "83.2",
                    "80.1/",
                    "81.1",
                    "74.6",
                    "78.0",
                    "55.6",
                    "32.4*"
                ],
                [
                    "[BOLD] WNLI [ITALIC] E",
                    "70.9",
                    "38.4",
                    "88.6",
                    "78.4/",
                    "85.9",
                    "86.3/",
                    "82.8",
                    "79.1/",
                    "80.0",
                    "73.9",
                    "77.9",
                    "57.0",
                    "11.3*"
                ],
                [
                    "[BOLD] DisSent WT [ITALIC] E",
                    "71.9",
                    "39.9",
                    "87.6",
                    "[BOLD] 81.9/",
                    "[BOLD] 87.2",
                    "85.8/",
                    "82.3",
                    "79.0/",
                    "80.7",
                    "74.6",
                    "79.1",
                    "61.4",
                    "23.9*"
                ],
                [
                    "[BOLD] MT En-De [ITALIC] E",
                    "72.1",
                    "40.1",
                    "87.8",
                    "79.9/",
                    "86.6",
                    "86.4/",
                    "83.2",
                    "81.8/",
                    "82.4",
                    "75.9",
                    "79.4",
                    "58.8",
                    "31.0*"
                ],
                [
                    "[BOLD] MT En-Ru [ITALIC] E",
                    "70.4",
                    "[BOLD] 41.0",
                    "86.8",
                    "76.5/",
                    "85.0",
                    "82.5/",
                    "76.3",
                    "81.4/",
                    "81.5",
                    "70.1",
                    "77.3",
                    "60.3",
                    "45.1*"
                ],
                [
                    "[BOLD] Reddit [ITALIC] E",
                    "71.0",
                    "38.5",
                    "87.7",
                    "77.2/",
                    "85.0",
                    "85.4/",
                    "82.1",
                    "80.9/",
                    "81.7",
                    "74.2",
                    "79.3",
                    "56.7",
                    "21.1*"
                ],
                [
                    "[BOLD] SkipThought [ITALIC] E",
                    "71.7",
                    "40.6",
                    "87.7",
                    "79.7/",
                    "86.5",
                    "85.2/",
                    "82.1",
                    "81.0/",
                    "81.7",
                    "75.0",
                    "79.1",
                    "58.1",
                    "52.1*"
                ],
                [
                    "[BOLD] MTL GLUE [ITALIC] E",
                    "72.1",
                    "33.8",
                    "90.5",
                    "81.1/",
                    "87.4",
                    "86.6/",
                    "83.0",
                    "82.1/",
                    "83.3",
                    "[BOLD] 76.2",
                    "79.2",
                    "61.4",
                    "42.3*"
                ],
                [
                    "[BOLD] MTL Non-GLUE [ITALIC] E",
                    "[BOLD] 72.4",
                    "39.4",
                    "88.8",
                    "80.6/",
                    "86.8",
                    "[BOLD] 87.1/",
                    "[BOLD] 84.1",
                    "[BOLD] 83.2/",
                    "[BOLD] 83.9",
                    "75.9",
                    "[BOLD] 80.9",
                    "57.8",
                    "22.5*"
                ],
                [
                    "[BOLD] MTL All [ITALIC] E",
                    "72.2",
                    "37.9",
                    "89.6",
                    "79.2/",
                    "86.4",
                    "86.0/",
                    "82.8",
                    "81.6/",
                    "82.5",
                    "76.1",
                    "80.2",
                    "60.3",
                    "31.0*"
                ],
                [
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training"
                ],
                [
                    "[BOLD] Single-Task [ITALIC] B",
                    "78.8",
                    "56.6",
                    "90.9",
                    "88.5/",
                    "91.8",
                    "89.9/",
                    "86.4",
                    "86.1/",
                    "86.0",
                    "83.5",
                    "[BOLD] 87.9",
                    "69.7",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] CoLA [ITALIC] B",
                    "78.3",
                    "[BOLD] 61.3",
                    "91.1",
                    "87.7/",
                    "91.4",
                    "89.7/",
                    "86.3",
                    "85.0/",
                    "85.0",
                    "83.3",
                    "85.9",
                    "64.3",
                    "43.7*"
                ],
                [
                    "[BOLD] SST [ITALIC] B",
                    "78.4",
                    "57.4",
                    "[BOLD] 92.2",
                    "86.3/",
                    "90.0",
                    "89.6/",
                    "86.1",
                    "85.3/",
                    "85.1",
                    "83.2",
                    "87.4",
                    "67.5",
                    "43.7*"
                ],
                [
                    "[BOLD] MRPC [ITALIC] B",
                    "78.3",
                    "60.3",
                    "90.8",
                    "87.0/",
                    "91.1",
                    "89.7/",
                    "86.3",
                    "86.6/",
                    "86.4",
                    "[BOLD] 83.8",
                    "83.9",
                    "66.4",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] QQP [ITALIC] B",
                    "79.1",
                    "56.8",
                    "91.3",
                    "88.5/",
                    "91.7",
                    "[BOLD] 90.5/",
                    "[BOLD] 87.3",
                    "88.1/",
                    "87.8",
                    "83.4",
                    "87.2",
                    "69.7",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] STS [ITALIC] B",
                    "79.4",
                    "61.1",
                    "92.3",
                    "88.0/",
                    "91.5",
                    "89.3/",
                    "85.5",
                    "86.2/",
                    "86.0",
                    "82.9",
                    "87.0",
                    "71.5",
                    "50.7*"
                ],
                [
                    "[BOLD] MNLI [ITALIC] B",
                    "[BOLD] 79.6",
                    "56.0",
                    "91.3",
                    "88.0/",
                    "91.3",
                    "90.0/",
                    "86.7",
                    "87.8/",
                    "87.7",
                    "82.9",
                    "87.0",
                    "[BOLD] 76.9",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] QNLI [ITALIC] B",
                    "78.4",
                    "55.4",
                    "91.2",
                    "[BOLD] 88.7/",
                    "[BOLD] 92.1",
                    "89.9/",
                    "86.4",
                    "86.5/",
                    "86.3",
                    "82.9",
                    "86.8",
                    "68.2",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] RTE [ITALIC] B",
                    "77.7",
                    "59.3",
                    "91.2",
                    "86.0/",
                    "90.4",
                    "89.2/",
                    "85.9",
                    "85.9/",
                    "85.7",
                    "82.0",
                    "83.3",
                    "65.3",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] WNLI [ITALIC] B",
                    "76.2",
                    "53.2",
                    "92.1",
                    "85.5/",
                    "90.0",
                    "89.1/",
                    "85.5",
                    "85.6/",
                    "85.4",
                    "82.4",
                    "82.5",
                    "58.5",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] DisSent WT [ITALIC] B",
                    "78.1",
                    "58.1",
                    "91.9",
                    "87.7/",
                    "91.2",
                    "89.2/",
                    "85.9",
                    "84.2/",
                    "84.1",
                    "82.5",
                    "85.5",
                    "67.5",
                    "43.7*"
                ],
                [
                    "[BOLD] MT En-De [ITALIC] B",
                    "73.9",
                    "47.0",
                    "90.5",
                    "75.0/",
                    "83.4",
                    "89.6/",
                    "86.1",
                    "84.1/",
                    "83.9",
                    "81.8",
                    "83.8",
                    "54.9",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] MT En-Ru [ITALIC] B",
                    "74.3",
                    "52.4",
                    "89.9",
                    "71.8/",
                    "81.3",
                    "89.4/",
                    "85.6",
                    "82.8/",
                    "82.8",
                    "81.5",
                    "83.1",
                    "58.5",
                    "43.7*"
                ],
                [
                    "[BOLD] Reddit [ITALIC] B",
                    "75.6",
                    "49.5",
                    "91.7",
                    "84.6/",
                    "89.2",
                    "89.4/",
                    "85.8",
                    "83.8/",
                    "83.6",
                    "81.8",
                    "84.4",
                    "58.1",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] SkipThought [ITALIC] B",
                    "75.2",
                    "53.9",
                    "90.8",
                    "78.7/",
                    "85.2",
                    "89.7/",
                    "86.3",
                    "81.2/",
                    "81.5",
                    "82.2",
                    "84.6",
                    "57.4",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL GLUE [ITALIC] B",
                    "[BOLD] 79.6",
                    "56.8",
                    "91.3",
                    "88.0/",
                    "91.4",
                    "90.3/",
                    "86.9",
                    "[BOLD] 89.2/",
                    "[BOLD] 89.0",
                    "83.0",
                    "86.8",
                    "74.7",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL Non-GLUE [ITALIC] B",
                    "76.7",
                    "54.8",
                    "91.1",
                    "83.6/",
                    "88.7",
                    "89.2/",
                    "85.6",
                    "83.2/",
                    "83.2",
                    "82.4",
                    "84.4",
                    "64.3",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL All [ITALIC] B",
                    "79.3",
                    "53.1",
                    "91.7",
                    "88.0/",
                    "91.3",
                    "90.4/",
                    "87.0",
                    "88.1/",
                    "87.9",
                    "83.5",
                    "87.6",
                    "75.1",
                    "45.1*"
                ],
                [
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results"
                ],
                [
                    "[BOLD] Non-GLUE [ITALIC] E",
                    "69.7",
                    "34.5",
                    "89.5",
                    "78.2/",
                    "84.8",
                    "83.6/",
                    "64.3",
                    "77.5/",
                    "76.0",
                    "75.4",
                    "74.8",
                    "55.6",
                    "65.1"
                ],
                [
                    "[BOLD] MNLI [ITALIC] B",
                    "77.1",
                    "49.6",
                    "93.2",
                    "88.5/",
                    "84.7",
                    "70.6/",
                    "88.3",
                    "86.0/",
                    "85.5",
                    "82.7",
                    "78.7",
                    "72.6",
                    "65.1"
                ],
                [
                    "[BOLD] GLUE [ITALIC] B",
                    "77.3",
                    "49.0",
                    "93.5",
                    "89.0/",
                    "85.3",
                    "70.6/",
                    "88.6",
                    "85.8/",
                    "84.9",
                    "82.9",
                    "81.0",
                    "71.7",
                    "34.9"
                ],
                [
                    "[BOLD] BERT Base",
                    "78.4",
                    "52.1",
                    "93.5",
                    "88.9/",
                    "84.8",
                    "71.2/",
                    "89.2",
                    "87.1/",
                    "85.8",
                    "84.0",
                    "91.1",
                    "66.4",
                    "65.1"
                ]
            ],
            "title": "Table 3: Results for intermediate training experiments on development sets except where noted. E and B respectively denote ELMo and BERT experiments. Bold denotes best scores by section. Underlining denotes average scores better than the single-task baseline. See Section 6 for discussion of WNLI results (*). BERT Base numbers are from Devlin et\u00a0al. (2019)."
        },
        "insight": "Tables 2 and 3 respectively show results for our pretraining and intermediate training experiments. [CONTINUE] Looking to Table 3, using ELMo uniformly improves over training the encoder from scratch. The ELMo-augmented random baseline is strong, lagging behind the single-task baseline by less than a point. Most intermediate tasks beat the random baseline, but several fail to significantly outperform the single-task baseline. MNLI and English\u2013German translation perform best with ELMo, with SkipThought and DisSent also beating the single-task baseline. Intermediate multitask training on all the non-GLUE tasks produces our best-performing ELMo model. Using BERT consistently outperforms ELMo and pretraining from scratch. We find that intermediate training on each of MNLI, QQP, and STS leads to improvements over no intermediate training, while intermediate training on the other tasks harms transfer performance. The improve [CONTINUE] ments gained via STS, a small-data task, versus the negative impact of fairly large-data tasks (e.g. QNLI), suggests that the benefit of intermediate training is not solely due to additional training, but that the signal provided by the intermediate task complements the original language modeling objective. Intermediate training on generation tasks such as MT and SkipThought significantly impairs BERT's transfer ability. We speculate that this degradation may be due to catastrophic forgetting in fine-tuning for a task substantially different from the tasks BERT was originally trained on. This phenomenon might be mitigated in our ELMo models via the frozen encoder and skip connection. On the test set, we lag slightly behind the BERT base results from Devlin et al. (2019), likely due in part to our limited hyperparameter tuning."
    },
    {
        "id": "196",
        "table": {
            "header": [
                "[BOLD] Task",
                "[BOLD] Avg",
                "[BOLD] CoLA",
                "[BOLD] SST",
                "[BOLD] STS",
                "[BOLD] QQP",
                "[BOLD] MNLI",
                "[BOLD] QNLI"
            ],
            "rows": [
                [
                    "[BOLD] CoLA",
                    "0.86",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] SST",
                    "0.60",
                    "0.25",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] MRPC",
                    "0.39",
                    "0.21",
                    "0.34",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] STS",
                    "-0.36",
                    "-0.60",
                    "0.01",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] QQP",
                    "0.61",
                    "0.61",
                    "0.27",
                    "-0.58",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] MNLI",
                    "0.54",
                    "0.16",
                    "0.66",
                    "0.40",
                    "0.08",
                    "1.00",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] QNLI",
                    "0.43",
                    "0.13",
                    "0.26",
                    "0.04",
                    "0.27",
                    "0.56",
                    "1.00"
                ],
                [
                    "[BOLD] RTE",
                    "0.34",
                    "0.08",
                    "0.16",
                    "-0.10",
                    "0.04",
                    "0.14",
                    "0.32"
                ],
                [
                    "[BOLD] WNLI",
                    "-0.21",
                    "-0.21",
                    "-0.37",
                    "0.31",
                    "-0.37",
                    "-0.07",
                    "-0.26"
                ]
            ],
            "title": "Table 4: Pearson correlations between performances on a subset of all pairs of target tasks, measured over all runs reported in Table 2. The Avg column shows the correlation between performance on a target task and the overall GLUE score. For QQP and STS, the correlations are computed respectively using F1 and Pearson correlation. Negative correlations are underlined."
        },
        "insight": "Table 4 presents an alternative view of the results of the pretraining experiment (Table 2): The table shows correlations between pairs of target tasks over the space of pretrained encoders. The correlations reflect the degree to which the performance on one target task with some encoder predicts performance on another target task with the same encoder. See Appendix D for the full table and similar tables for intermediate ELMo and BERT experiments. Many correlations are low, suggesting that different tasks benefit from different forms of pretraining to a substantial degree, and bolstering the observation that no single pretraining task yields good performance on all target tasks. For reasons noted earlier, the models that tended to perform best overall also tended to overfit the WNLI training set most, leading to a negative correlation between WNLI and overall GLUE score. STS also shows a negative correlation, likely due to the observation that it does not benefit from LM pretraining. In contrast, CoLA shows a strong correlation with the overall GLUE scores, but has weak or negative correlations with many tasks: The use of LM pretraining dramatically improves CoLA performance, but most other forms of pretraining have little effect."
    },
    {
        "id": "197",
        "table": {
            "header": [
                "NE Tag",
                "Frequency"
            ],
            "rows": [
                [
                    "date, time, duration, set (temporal)",
                    "54.28%"
                ],
                [
                    "number",
                    "40.13%"
                ],
                [
                    "[ITALIC]  Relation cardinality",
                    "[ITALIC] 18.86%"
                ],
                [
                    "percent",
                    "2.92%"
                ],
                [
                    "money",
                    "2.25%"
                ],
                [
                    "person, location, organization",
                    "0.26%"
                ],
                [
                    "ordinal",
                    "0.16%"
                ]
            ],
            "title": "Table 1: NE-tags of numbers in Wikipedia."
        },
        "insight": "The distribution of their named-entity (NE) tags, according to Stanford NE-tagger, is shown in Table 1. While temporal-related numbers are the most frequent, around 40% are classified only as unspecific NUMBER. By manually checking 100 random NUMBERs, we observed that 47 are relation cardinalities,1 i.e., approximately 18.86% of all numbers in Wikipedia are relation cardinalities."
    },
    {
        "id": "198",
        "table": {
            "header": [
                "[ITALIC] p",
                "# [ITALIC] s",
                "[ITALIC] baseline P",
                "[ITALIC] vanilla P",
                "[ITALIC] vanilla R",
                "[ITALIC] vanilla F1",
                "[ITALIC] only-nummod P",
                "[ITALIC] only-nummod R",
                "[ITALIC] only-nummod F1"
            ],
            "rows": [
                [
                    "has part (creative work series)",
                    "261",
                    ".050",
                    ".333",
                    ".316",
                    ".324",
                    ".353",
                    ".316",
                    ".333"
                ],
                [
                    "contains admin. terr. entity",
                    "18,000",
                    ".034",
                    ".390",
                    ".188",
                    ".254",
                    ".548",
                    ".200",
                    ".293"
                ],
                [
                    "spouse",
                    "45,917",
                    "0",
                    ".014",
                    ".011",
                    ".013",
                    ".028",
                    ".017",
                    ".021"
                ],
                [
                    "child",
                    "35,057",
                    ".112",
                    ".151",
                    ".129",
                    ".139",
                    ".320",
                    ".219",
                    ".260"
                ],
                [
                    "child (manual ground truth)",
                    "6,408",
                    "[EMPTY]",
                    "0.374",
                    "0.309",
                    "0.338",
                    "0.452",
                    "0.315",
                    "0.371"
                ]
            ],
            "title": "Table 2: Number of Wikidata entities as subjects (#s) of each predicate (p), and evaluation results on manually annotated randomly selected subjects that have at least an object."
        },
        "insight": "Table 2 shows the performance of our CRF-based method in finding the correct relation cardinality, evaluated on manually annotated 20 (has part), 100 (admin. terr. entity) and 200 (child and spouse) randomly selected subjects that have at least one object. [CONTINUE] The random-number baseline achieves a precision of 5% (has part), 3.5% (admin. territ. entity), 0% (spouse) and 11.2% (child). Compared to that, especially using only-nummod, our method gives encouraging results for has part, admin. territ. entity and child, with 30-50% precision and around 30% F1-score. [CONTINUE] As shown by the last row of Table 2, higher quality of training data can considerably boost the performance of cardinality extraction."
    },
    {
        "id": "199",
        "table": {
            "header": [
                "Model",
                "Model",
                "Ara",
                "Baq",
                "Fre",
                "Ger",
                "Heb",
                "Hun",
                "Kor",
                "Pol",
                "Swe",
                "Avg"
            ],
            "rows": [
                [
                    "Int",
                    "WORD",
                    "84.50",
                    "77.87",
                    "82.20",
                    "85.35",
                    "[BOLD] 74.68",
                    "76.17",
                    "84.62",
                    "80.71",
                    "79.14",
                    "80.58"
                ],
                [
                    "Int",
                    "W2V",
                    "[BOLD] 85.11",
                    "79.07",
                    "[BOLD] 82.73",
                    "[BOLD] 86.60",
                    "74.55",
                    "78.21",
                    "85.30",
                    "82.37",
                    "79.67",
                    "81.51"
                ],
                [
                    "Int",
                    "LSTM",
                    "83.42",
                    "82.97",
                    "81.35",
                    "85.34",
                    "74.03",
                    "83.06",
                    "86.56",
                    "80.13",
                    "77.44",
                    "81.48"
                ],
                [
                    "Int",
                    "CNN",
                    "84.65",
                    "[BOLD] 83.91",
                    "82.41",
                    "85.61",
                    "74.23",
                    "[BOLD] 83.68",
                    "[BOLD] 86.99",
                    "[BOLD] 83.28",
                    "[BOLD] 80.00",
                    "[BOLD] 82.75"
                ],
                [
                    "Int",
                    "LSTM+WORD",
                    "[BOLD] 84.75",
                    "83.43",
                    "[BOLD] 82.25",
                    "85.56",
                    "74.62",
                    "83.43",
                    "86.85",
                    "82.30",
                    "79.85",
                    "82.56"
                ],
                [
                    "Int",
                    "CNN+WORD",
                    "84.58",
                    "[BOLD] 84.22",
                    "81.79",
                    "[BOLD] 85.85",
                    "[BOLD] 74.79",
                    "[BOLD] 83.51",
                    "[BOLD] 87.21",
                    "[BOLD] 83.66",
                    "[BOLD] 80.52",
                    "[BOLD] 82.90"
                ],
                [
                    "Int",
                    "LSTM+W2V",
                    "85.35",
                    "83.94",
                    "83.04",
                    "86.38",
                    "[BOLD] 75.15",
                    "83.30",
                    "87.35",
                    "83.00",
                    "79.38",
                    "82.99"
                ],
                [
                    "Int",
                    "CNN+W2V",
                    "[BOLD] 85.67",
                    "[BOLD] 84.37",
                    "[BOLD] 83.09",
                    "[BOLD] 86.81",
                    "74.95",
                    "[BOLD] 84.08",
                    "[BOLD] 87.72",
                    "[BOLD] 84.44",
                    "[BOLD] 80.35",
                    "[BOLD] 83.50"
                ],
                [
                    "Ext",
                    "B15-WORD",
                    "[BOLD] 83.46",
                    "73.56",
                    "[BOLD] 82.03",
                    "[BOLD] 84.62",
                    "[BOLD] 72.70",
                    "69.31",
                    "83.37",
                    "[BOLD] 79.83",
                    "[BOLD] 76.40",
                    "78.36"
                ],
                [
                    "Ext",
                    "B15-LSTM",
                    "83.40",
                    "[BOLD] 78.61",
                    "81.08",
                    "84.49",
                    "72.26",
                    "[BOLD] 76.34",
                    "[BOLD] 86.21",
                    "78.24",
                    "74.47",
                    "[BOLD] 79.46"
                ],
                [
                    "Ext",
                    "BestPub",
                    "86.21",
                    "85.70",
                    "85.66",
                    "89.65",
                    "81.65",
                    "86.13",
                    "87.27",
                    "87.07",
                    "82.75",
                    "85.79"
                ]
            ],
            "title": "Table 1: LAS on the test sets, the best LAS in each group is marked in bold face."
        },
        "insight": "The experimental results are shown in Table 1, with Int denoting internal comparisons (with three groups) and Ext denoting external comparisons, the highest LAS in each group is marked in bold face. [CONTINUE] In the first group, we compare the LAS of the four single models WORD, W2V, LSTM, and CNN. In macro average of all languages, the CNN model performs 2.17% higher than the WORD model, and 1.24% higher than the W2V model. The LSTM model, however, performs only 0.9% higher than the WORD model and 1.27% lower than the CNN model. [CONTINUE] In the second group, we observe that the additional word-lookup model does not significantly improve the CNN moodel (from 82.75% in CNN to 82.90% in CNN+WORD on average) while the LSTM model is improved by a much larger margin (from 81.48% in LSTM to 82.56% in LSTM+WORD on average). This suggests that the CNN model has already learned the most important information from the the word forms, while the LSTM model has not. Also, the combined CNN+WORD model is still better than the LSTM+WORD model, despite the large improvement in the latter. [CONTINUE] While comparing to the best published results (Bj\u00f6rkelund et al., 2013, 2014), we have to note that their approach uses explicit morphological features, ensemble, ranking, etc., which all can boost parsing performance. We only use a greedy parser with much fewer features, but bridge the 6 points gap between the previous best greedy parser and the best published result by more than one half. [CONTINUE] On average, the B15-LSTM model improves their own baseline by 1.1%, similar to the 0.9% improvement of our LSTM model, which is much smaller than the 2.17% improvement of the CNN model. Furthermore, the CNN model is improved from a strong baseline: our WORD model performs already 2.22% higher than the B15-WORD model. [CONTINUE] Comparing the individual performances on each language, we observe that the CNN model almost always outperforms the WORD model except for Hebrew. However, both LSTM and B15-LSTM perform higher than baseline only on the three agglutinative languages (Basque, Hungarian, and Korean), and lower than baseline on the other six."
    },
    {
        "id": "200",
        "table": {
            "header": [
                "Model",
                "Case",
                "Ara",
                "Baq",
                "Fre",
                "Ger",
                "Heb",
                "Hun",
                "Kor",
                "Pol",
                "Swe",
                "Avg"
            ],
            "rows": [
                [
                    "CNN",
                    "\u0394IV",
                    "0.12",
                    "2.72",
                    "-0.44",
                    "0.13",
                    "-0.35",
                    "1.48",
                    "1.30",
                    "0.98",
                    "1.39",
                    "0.81"
                ],
                [
                    "CNN",
                    "\u0394OOV",
                    "0.03",
                    "5.78",
                    "0.33",
                    "0.10",
                    "-1.04",
                    "5.04",
                    "2.17",
                    "2.34",
                    "0.95",
                    "1.74"
                ],
                [
                    "LSTM",
                    "\u0394IV",
                    "-0.58",
                    "1.98",
                    "-0.55",
                    "-0.08",
                    "-1.23",
                    "1.62",
                    "1.12",
                    "-0.49",
                    "0.21",
                    "0.22"
                ],
                [
                    "LSTM",
                    "\u0394OOV",
                    "-0.32",
                    "5.09",
                    "0.12",
                    "-0.21",
                    "-1.99",
                    "4.74",
                    "1.51",
                    "0.10",
                    "0.38",
                    "1.05"
                ]
            ],
            "title": "Table 2: LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets."
        },
        "insight": "Table 2 shows the results, where the two cases are denoted as \u0394IV and \u0394OOV. The general trend in the results is that the improvements of both models in the OOV case are larger than in the IV case, which means that the character composition models indeed alleviates the OOV problem. Also, CNN improves on seven languages in the IV case and eight languages in the OOV case, and it performs consistently better than LSTM in both cases."
    },
    {
        "id": "201",
        "table": {
            "header": [
                "train dataset",
                "test dataset Belarusian",
                "test dataset Russian",
                "test dataset Ukrai-nian"
            ],
            "rows": [
                [
                    "Belarusian",
                    "647",
                    "326",
                    "373"
                ],
                [
                    "Russian",
                    "495",
                    "738",
                    "516"
                ],
                [
                    "Ukrainian",
                    "556",
                    "553",
                    "683"
                ],
                [
                    "Ukrainian, Belarusian",
                    "769",
                    "597",
                    "701"
                ],
                [
                    "Russian, Belarusian",
                    "740",
                    "740",
                    "563"
                ],
                [
                    "Russian, Ukrainian",
                    "627",
                    "756",
                    "[BOLD] 700"
                ],
                [
                    "Russian, Ukrainian, Belarusian",
                    "[BOLD] 772",
                    "[BOLD] 760",
                    "698"
                ]
            ],
            "title": "Table 3: Accuracy scores \u00d7 1000 for different train and test dataset combinations"
        },
        "insight": "averaged. The Table 3 presents the results of these experiments. The Table 3 shows, that: 1. in monolingual setting, we can get highquality results. The scores are significantly lower than the scores of the same model on the standard dataset, due to the smaller sizes of the training datasets. Nevertheless, one can see, that our approach to word stress detection applies not only to the Russian language data, but also to the data in the Belarusian and Ukrainian languages; 2. cross-lingual setting (1): the Belarusian training dataset, being the smallest one among the three datasets, is not a good source for training word stress detection models in other languages, while the Ukrainian dataset stands [CONTINUE] out as a good source for training word stress detection systems both for the Russian and Belarusian languages; 3. cross-lingual setting (2): adding one or two datasets to the other languages improves the quality. For example, around 10% of accuracy is gained by adding the Russian training dataset to the Belarusian training dataset, while testing on Belarusian."
    },
    {
        "id": "202",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] BLEU  [BOLD] (All)",
                "[BOLD] BLEU  [BOLD] (WSJ)",
                "[BOLD] BLEU  [BOLD] (All overlap)",
                "[BOLD] Exact  [BOLD] Match%",
                "[BOLD] Coverage%"
            ],
            "rows": [
                [
                    "Neural MRS (gold)",
                    "66.11",
                    "73.12",
                    "69.27",
                    "24.09",
                    "100"
                ],
                [
                    "Neural MRS (silver)",
                    "75.43",
                    "81.76",
                    "77.13",
                    "25.82",
                    "100"
                ],
                [
                    "Neural MRS (gold + silver)",
                    "77.17",
                    "83.37",
                    "79.15",
                    "32.07",
                    "100"
                ],
                [
                    "ACE (ERG)",
                    "\u2013",
                    "\u2013",
                    "62.05",
                    "15.08",
                    "78"
                ],
                [
                    "DAG transducer Ye et\u00a0al. ( 2018 )",
                    "\u2013",
                    "68.07",
                    "\u2013",
                    "\u2013",
                    "100"
                ]
            ],
            "title": "Table 1: BLEU and exact-match scores over held-out test set"
        },
        "insight": "We compare the performance of our neural generator when trained on either gold, silver, or gold and silver data (Table 1). Generation quality is primarily evaluated with BLEU [CONTINUE] Semi-supervised training [CONTINUE] leads to an 11 BLEU point improvement [CONTINUE] compar [CONTINUE] train [CONTINUE] In addition to BLEU, we also report exact match accuracy on the overlapping subset. Results show that our neural models outperform the grammar-based generator by a large margin."
    },
    {
        "id": "203",
        "table": {
            "header": [
                "[BOLD] Parameter char emb size",
                "[BOLD] Value 50",
                "[BOLD] Parameter bigram emb size",
                "[BOLD] Value 50"
            ],
            "rows": [
                [
                    "lattice emb size",
                    "50",
                    "LSTM hidden",
                    "200"
                ],
                [
                    "char dropout",
                    "0.5",
                    "lattice dropout",
                    "0.5"
                ],
                [
                    "LSTM layer",
                    "1",
                    "regularization  [ITALIC] \u03bb",
                    "1e-8"
                ],
                [
                    "learning rate  [ITALIC] lr",
                    "0.015",
                    "[ITALIC] lr decay",
                    "0.05"
                ]
            ],
            "title": "Table 3: Hyper-parameter values."
        },
        "insight": "Table 3 shows the values of hyper-parameters for our models, [CONTINUE] In particular, the embedding sizes are set to 50 and the hidden size of LSTM models to 200. Dropout (Srivastava et al., 2014) is applied to both word and character embeddings with a rate of 0.5. Stochastic gradient descent (SGD) is used for optimization, with an initial learning rate of 0.015 and a decay rate of 0.05."
    },
    {
        "id": "204",
        "table": {
            "header": [
                "[BOLD] Input",
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Auto seg",
                    "Word baseline",
                    "73.20",
                    "57.05",
                    "64.12"
                ],
                [
                    "Auto seg",
                    "+char LSTM",
                    "71.98",
                    "65.41",
                    "68.54"
                ],
                [
                    "Auto seg",
                    "+char LSTM\u2032",
                    "71.08",
                    "65.83",
                    "68.35"
                ],
                [
                    "Auto seg",
                    "+char+bichar LSTM",
                    "72.63",
                    "67.60",
                    "70.03"
                ],
                [
                    "Auto seg",
                    "+char CNN",
                    "73.06",
                    "66.29",
                    "69.51"
                ],
                [
                    "Auto seg",
                    "+char+bichar CNN",
                    "72.01",
                    "65.50",
                    "68.60"
                ],
                [
                    "No seg",
                    "Char baseline",
                    "67.12",
                    "58.42",
                    "62.47"
                ],
                [
                    "No seg",
                    "+softword",
                    "69.30",
                    "62.47",
                    "65.71"
                ],
                [
                    "No seg",
                    "+bichar",
                    "71.67",
                    "64.02",
                    "67.63"
                ],
                [
                    "No seg",
                    "+bichar+softword",
                    "72.64",
                    "66.89",
                    "69.64"
                ],
                [
                    "No seg",
                    "Lattice",
                    "[BOLD] 74.64",
                    "[BOLD] 68.83",
                    "[BOLD] 71.62"
                ]
            ],
            "title": "Table 4: Development results."
        },
        "insight": "As shown in Table 4, without using word segmentation, a characterbased LSTM-CRF model gives a development F1score of 62.47%. Adding character-bigram and softword representations as described in Section 3.1 increases the F1-score to 67.63% and 65.71%, respectively, demonstrating the usefulness of both sources of information. In addition, a combination of both gives a 69.64% F1-score, which is the best [CONTINUE] among various character representations. [CONTINUE] Table 4 shows a variety of different settings for word-based Chinese NER. With automatic segmentation, a word-based LSTM CRF baseline gives a 64.12% F1-score, which is higher compared to the character-based baseline. This demonstrates that both word information and character information are useful for Chinese NER. The two methods of [CONTINUE] word+char LSTM and word+char LSTM(cid:48), lead to similar improvements. [CONTINUE] A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations. On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN. [CONTINUE] As shown in Table 4, the lattice LSTM-CRF model gives a development F1-score of 71.62%, which is significantly7 higher compared with both the word-based and character-based methods, despite that it does not use character bigrams or word segmentation information."
    },
    {
        "id": "205",
        "table": {
            "header": [
                "[BOLD] Input",
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Gold seg",
                    "Yang et\u00a0al. ( 2016 )",
                    "65.59",
                    "71.84",
                    "68.57"
                ],
                [
                    "Gold seg",
                    "Yang et\u00a0al. ( 2016 )*\u2020",
                    "72.98",
                    "[BOLD] 80.15",
                    "[BOLD] 76.40"
                ],
                [
                    "Gold seg",
                    "Che et\u00a0al. ( 2013 )*",
                    "77.71",
                    "72.51",
                    "75.02"
                ],
                [
                    "Gold seg",
                    "Wang et\u00a0al. ( 2013 )*",
                    "76.43",
                    "72.32",
                    "74.32"
                ],
                [
                    "Gold seg",
                    "Word baseline",
                    "76.66",
                    "63.60",
                    "69.52"
                ],
                [
                    "Gold seg",
                    "+char+bichar LSTM",
                    "[BOLD] 78.62",
                    "73.13",
                    "75.77"
                ],
                [
                    "Auto seg",
                    "Word baseline",
                    "72.84",
                    "59.72",
                    "65.63"
                ],
                [
                    "Auto seg",
                    "+char+bichar LSTM",
                    "73.36",
                    "70.12",
                    "71.70"
                ],
                [
                    "No seg",
                    "Char baseline",
                    "68.79",
                    "60.35",
                    "64.30"
                ],
                [
                    "No seg",
                    "+bichar+softword",
                    "74.36",
                    "69.43",
                    "71.81"
                ],
                [
                    "No seg",
                    "Lattice",
                    "[BOLD] 76.35",
                    "[BOLD] 71.56",
                    "[BOLD] 73.88"
                ]
            ],
            "title": "Table 5: Main results on OntoNotes."
        },
        "insight": "The OntoNotes test results are shown in Table 5. With gold-standard segmentation, our word-based methods give competitive results to the state-of-the-art on the dataset (Che et al., 2013; Wang et al., 2013), [CONTINUE] In addition, the results show [CONTINUE] that our word-based models can serve as highly competitive baselines. With automatic segmentation, the F1-score of word+char+bichar LSTM decreases from 75.77% to 71.70%, showing the influence of segmentation to NER. Consistent with observations on the development set, adding lattice word information leads to an 88.81% \u2192 93.18% increasement of F1-score over the character baseline, as compared with 88.81% \u2192 91.87% by adding bichar+softword. The lattice model gives significantly the best F1-score on automatic segmentation."
    },
    {
        "id": "206",
        "table": {
            "header": [
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Chen et\u00a0al. ( 2006a )",
                    "91.22",
                    "81.71",
                    "86.20"
                ],
                [
                    "Zhang et\u00a0al. ( 2006 )*",
                    "92.20",
                    "90.18",
                    "91.18"
                ],
                [
                    "Zhou et\u00a0al. ( 2013 )",
                    "91.86",
                    "88.75",
                    "90.28"
                ],
                [
                    "Lu et\u00a0al. ( 2016 )",
                    "\u2013",
                    "\u2013",
                    "87.94"
                ],
                [
                    "Dong et\u00a0al. ( 2016 )",
                    "91.28",
                    "90.62",
                    "90.95"
                ],
                [
                    "Word baseline",
                    "90.57",
                    "83.06",
                    "86.65"
                ],
                [
                    "+char+bichar LSTM",
                    "91.05",
                    "89.53",
                    "90.28"
                ],
                [
                    "Char baseline",
                    "90.74",
                    "86.96",
                    "88.81"
                ],
                [
                    "+bichar+softword",
                    "92.97",
                    "90.80",
                    "91.87"
                ],
                [
                    "Lattice",
                    "[BOLD] 93.57",
                    "[BOLD] 92.79",
                    "[BOLD] 93.18"
                ]
            ],
            "title": "Table 6: Main results on MSRA."
        },
        "insight": "Results on the MSRA dataset are shown in Table 6. [CONTINUE] Our chosen segmentor gives 95.93% accuracy on 5-fold cross-validated training set. [CONTINUE] Compared with the existing methods, our wordbased and character-based LSTM-CRF models give competitive accuracies. The lattice model significantly outperforms both the best characterbased and word-based models (p < 0.01), achieving the best result on this standard benchmark."
    },
    {
        "id": "207",
        "table": {
            "header": [
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "Word baseline",
                    "93.72",
                    "93.44",
                    "93.58"
                ],
                [
                    "+char+bichar LSTM",
                    "94.07",
                    "94.42",
                    "94.24"
                ],
                [
                    "Char baseline",
                    "93.66",
                    "93.31",
                    "93.48"
                ],
                [
                    "+bichar+softword",
                    "94.53",
                    "[BOLD] 94.29",
                    "94.41"
                ],
                [
                    "Lattice",
                    "[BOLD] 94.81",
                    "94.11",
                    "[BOLD] 94.46"
                ]
            ],
            "title": "Table 8: Main results on resume NER."
        },
        "insight": "Results on the resume NER test data are shown in Table 8. [CONTINUE] the lattice model significantly outperforms both the word-based mode and the character-based model for Weibo and resume (p < 0.01), giving state-of-the-art results."
    },
    {
        "id": "208",
        "table": {
            "header": [
                "[EMPTY]",
                "chrF-1.0",
                "BLEU%"
            ],
            "rows": [
                [
                    "en-et",
                    "dev",
                    "dev"
                ],
                [
                    "BPE",
                    "56.52",
                    "17.93"
                ],
                [
                    "monolingual",
                    "53.44",
                    "15.82"
                ],
                [
                    "Cognate Morfessor",
                    "57.05",
                    "18.40"
                ],
                [
                    "+finetuned",
                    "57.23",
                    "18.45"
                ],
                [
                    "+ensemble-of-5",
                    "[BOLD] 57.75",
                    "[BOLD] 19.09"
                ],
                [
                    "+ensemble-of-3",
                    "57.64",
                    "18.96"
                ],
                [
                    "+linked embeddings",
                    "56.20",
                    "17.48"
                ],
                [
                    "\u2212LM filtering",
                    "52.94",
                    "14.65"
                ],
                [
                    "6+6 layers",
                    "57.35",
                    "18.84"
                ]
            ],
            "title": "Table 3: Development set results for English\u2013Estonian. character-F and BLEU scores in percentages. +/\u2212 stands for adding/removing a component. Multiple modifications are indicated by increasing the indentation."
        },
        "insight": "Table 3 shows the development set results for Estonian. Cognate Morfessor outperforms the comparable BPE system according to both measures for Estonian. The cross-lingual segmentation is particularly beneficial for Estonian. For Estonian, we have two ensemble configurations one combining 3 monolingually finetuned independent runs, and one combining 5 monolingually finetuned savepoints from 4 independent runs."
    },
    {
        "id": "209",
        "table": {
            "header": [
                "[EMPTY]",
                "Sestorain et\u00a0al. ( 2018 )\u2020 PBSMT",
                "Sestorain et\u00a0al. ( 2018 )\u2020 NMT-0",
                "Sestorain et\u00a0al. ( 2018 )\u2020 Dual-0",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "rows": [
                [
                    "\\En \u2192 \\Es",
                    "61.26",
                    "51.93",
                    "\u2014",
                    "56.58",
                    "56.58",
                    "56.36"
                ],
                [
                    "\\En \u2192 \\Fr",
                    "50.09",
                    "40.56",
                    "\u2014",
                    "44.27",
                    "44.27",
                    "44.80"
                ],
                [
                    "\\Es \u2192 \\En",
                    "59.89",
                    "51.58",
                    "\u2014",
                    "55.70",
                    "55.70",
                    "55.24"
                ],
                [
                    "\\Fr \u2192 \\En",
                    "52.22",
                    "43.33",
                    "\u2014",
                    "46.46",
                    "46.46",
                    "46.17"
                ],
                [
                    "Supervised (avg.)",
                    "55.87",
                    "46.85",
                    "\u2014",
                    "50.75",
                    "50.75",
                    "50.64"
                ],
                [
                    "\\Es \u2192 \\Fr",
                    "52.44",
                    "20.29",
                    "36.68",
                    "34.75",
                    "[BOLD] 38.10",
                    "37.54"
                ],
                [
                    "\\Fr \u2192 \\Es",
                    "49.79",
                    "19.01",
                    "39.19",
                    "37.67",
                    "[BOLD] 40.84",
                    "40.02"
                ],
                [
                    "Zero-shot (avg.)",
                    "51.11",
                    "19.69",
                    "37.93",
                    "36.21",
                    "[BOLD] 39.47",
                    "38.78"
                ]
            ],
            "title": "Table 1: Results on UNCorpus-1."
        },
        "insight": "Tables 1 and 2 show results on the UNCorpus datasets. [CONTINUE] Our approach consistently outperforms Basic and Dual-0, despite the latter being trained with additional monolingual data (Sestorain et al., 2018)."
    },
    {
        "id": "210",
        "table": {
            "header": [
                "[BOLD] Ablation",
                "[BOLD] BLEU"
            ],
            "rows": [
                [
                    "All attributes",
                    "72.06"
                ],
                [
                    "No node attributes",
                    "59.37"
                ],
                [
                    "No node attr except num, tense",
                    "67.34"
                ],
                [
                    "No edge features",
                    "71.27"
                ]
            ],
            "title": "Table 3: Results of semantic feature ablation, model trained with gold data only"
        },
        "insight": "edge properties are removed, which Table 3 shows has an effect of less than 1 BLEU point). [CONTINUE] we ablate node (predicate) and edge attributes [CONTINUE] number and tense [CONTINUE] have the largest effect on the reported BLEU score. [CONTINUE] has only a small impact on performance."
    },
    {
        "id": "211",
        "table": {
            "header": [
                "[EMPTY]",
                "Sestorain et\u00a0al. ( 2018 ) PBSMT",
                "Sestorain et\u00a0al. ( 2018 ) NMT-0",
                "Sestorain et\u00a0al. ( 2018 ) Dual-0",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "rows": [
                [
                    "\\En \u2192 \\Es",
                    "61.26",
                    "47.51",
                    "44.30",
                    "55.15",
                    "55.15",
                    "54.30"
                ],
                [
                    "\\En \u2192 \\Fr",
                    "50.09",
                    "36.70",
                    "34.34",
                    "43.42",
                    "43.42",
                    "42.57"
                ],
                [
                    "\\En \u2192 \\Ru",
                    "43.25",
                    "30.45",
                    "29.47",
                    "36.26",
                    "36.26",
                    "35.89"
                ],
                [
                    "\\Es \u2192 \\En",
                    "59.89",
                    "48.56",
                    "45.55",
                    "54.35",
                    "54.35",
                    "54.33"
                ],
                [
                    "\\Fr \u2192 \\En",
                    "52.22",
                    "40.75",
                    "37.75",
                    "45.55",
                    "45.55",
                    "45.87"
                ],
                [
                    "\\Ru \u2192 \\En",
                    "52.59",
                    "39.35",
                    "37.96",
                    "45.52",
                    "45.52",
                    "44.67"
                ],
                [
                    "Supervised (avg.)",
                    "53.22",
                    "40.55",
                    "36.74",
                    "46.71",
                    "46.71",
                    "46.27"
                ],
                [
                    "\\Es \u2192 \\Fr",
                    "52.44",
                    "25.85",
                    "34.51",
                    "34.73",
                    "35.93",
                    "[BOLD] 36.02"
                ],
                [
                    "\\Fr \u2192 \\Es",
                    "49.79",
                    "22.68",
                    "37.71",
                    "38.20",
                    "39.51",
                    "[BOLD] 39.94"
                ],
                [
                    "\\Es \u2192 \\Ru",
                    "39.69",
                    "9.36",
                    "24.55",
                    "26.29",
                    "27.15",
                    "[BOLD] 28.08"
                ],
                [
                    "\\Ru \u2192 \\Es",
                    "49.61",
                    "26.26",
                    "33.23",
                    "33.43",
                    "[BOLD] 37.17",
                    "35.01"
                ],
                [
                    "\\Fr \u2192 \\Ru",
                    "36.48",
                    "9.35",
                    "22.76",
                    "23.88",
                    "24.99",
                    "[BOLD] 25.13"
                ],
                [
                    "\\Ru \u2192 \\Fr",
                    "43.37",
                    "22.43",
                    "26.49",
                    "28.52",
                    "[BOLD] 30.06",
                    "29.53"
                ],
                [
                    "Zero-shot (avg.)",
                    "45.23",
                    "26.26",
                    "29.88",
                    "30.84",
                    "[BOLD] 32.47",
                    "32.29"
                ]
            ],
            "title": "Table 2: Results on UNCorpus-2."
        },
        "insight": "We see that models trained with agreement perform comparably to Pivot, outperforming it in some cases, e.g., when the target is Russian, perhaps because it is quite different linguistically from the English pivot. [CONTINUE] unlike Dual-0, Agree maintains high performance in the supervised directions (within 1 BLEU point compared to Basic),"
    },
    {
        "id": "212",
        "table": {
            "header": [
                "[EMPTY]",
                "Previous work Soft\u2021",
                "Previous work Distill\u2020",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "rows": [
                [
                    "\\En \u2192 \\Es",
                    "\u2014",
                    "\u2014",
                    "34.69",
                    "34.69",
                    "33.80"
                ],
                [
                    "\\En \u2192 \\De",
                    "\u2014",
                    "\u2014",
                    "23.06",
                    "23.06",
                    "22.44"
                ],
                [
                    "\\En \u2192 \\Fr",
                    "31.40",
                    "\u2014",
                    "33.87",
                    "33.87",
                    "32.55"
                ],
                [
                    "\\Es \u2192 \\En",
                    "31.96",
                    "\u2014",
                    "34.77",
                    "34.77",
                    "34.53"
                ],
                [
                    "\\De \u2192 \\En",
                    "26.55",
                    "\u2014",
                    "29.06",
                    "29.06",
                    "29.07"
                ],
                [
                    "\\Fr \u2192 \\En",
                    "\u2014",
                    "\u2014",
                    "33.67",
                    "33.67",
                    "33.30"
                ],
                [
                    "Supervised (avg.)",
                    "\u2014",
                    "\u2014",
                    "31.52",
                    "31.52",
                    "30.95"
                ],
                [
                    "\\Es \u2192 \\De",
                    "\u2014",
                    "\u2014",
                    "18.23",
                    "20.14",
                    "[BOLD] 20.70"
                ],
                [
                    "\\De \u2192 \\Es",
                    "\u2014",
                    "\u2014",
                    "20.28",
                    "[BOLD] 26.50",
                    "22.45"
                ],
                [
                    "\\Es \u2192 \\Fr",
                    "30.57",
                    "[BOLD] 33.86",
                    "27.99",
                    "32.56",
                    "30.94"
                ],
                [
                    "\\Fr \u2192 \\Es",
                    "\u2014",
                    "\u2014",
                    "27.12",
                    "[BOLD] 32.96",
                    "29.91"
                ],
                [
                    "\\De \u2192 \\Fr",
                    "23.79",
                    "[BOLD] 27.03",
                    "21.36",
                    "25.67",
                    "24.45"
                ],
                [
                    "\\Fr \u2192 \\De",
                    "\u2014",
                    "\u2014",
                    "18.57",
                    "[BOLD] 19.86",
                    "19.15"
                ],
                [
                    "Zero-shot (avg.)",
                    "\u2014",
                    "\u2014",
                    "22.25",
                    "26.28",
                    "24.60"
                ]
            ],
            "title": "Table 3: Zero-shot results on Europarl. Note that Soft and Distill are not multilingual systems."
        },
        "insight": "Table 3 shows the results on the Europarl corpus. [CONTINUE] our approach consistently outperforms Basic by 2-3 BLEU points but lags a bit behind Pivot on average (except on De where it is better). Es"
    },
    {
        "id": "213",
        "table": {
            "header": [
                "[EMPTY]",
                "Previous work SOTA\u2020",
                "Previous work CPG\u2021",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "rows": [
                [
                    "Supervised (avg.)",
                    "24.10",
                    "19.75",
                    "24.63",
                    "24.63",
                    "23.97"
                ],
                [
                    "Zero-shot (avg.)",
                    "20.55",
                    "11.69",
                    "19.86",
                    "19.26",
                    "[BOLD] 20.58"
                ]
            ],
            "title": "Table 4: Results on the official IWSLT17 multilingual task."
        },
        "insight": "Table 4 presents results on the original IWSLT17 task. [CONTINUE] the vanilla training method (Johnson et al., 2016) achieves very high zero shot performance, even outperforming Pivot."
    },
    {
        "id": "214",
        "table": {
            "header": [
                "[EMPTY]",
                "Basic",
                "Pivot",
                "Agree"
            ],
            "rows": [
                [
                    "Supervised (avg.)",
                    "28.72",
                    "28.72",
                    "[BOLD] 29.17"
                ],
                [
                    "Zero-shot (avg.)",
                    "12.61",
                    "[BOLD] 17.68",
                    "15.23"
                ]
            ],
            "title": "Table 5: Results on our proposed IWSLT17?"
        },
        "insight": "on our proposed preprocessed IWSLT17(cid:63) that eliminates the overlap and reduces the number of supervised directions (8), there is a considerable gap between the supervised and zeroshot performance of Basic. [CONTINUE] Agree performs better than Basic and is slightly worse than Pivot."
    },
    {
        "id": "215",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Subtitles  [BOLD] En-Ru",
                "[BOLD] Subtitles  [BOLD] En-Ru",
                "[BOLD] Subtitles  [BOLD] En-Ru"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Overall",
                    "En\u2192Fr",
                    "Fr\u2192En",
                    "Overall",
                    "En\u2192Et",
                    "Et\u2192En",
                    "Overall",
                    "En\u2192De",
                    "De\u2192En",
                    "Overall",
                    "En\u2192Ru",
                    "Ru\u2192En"
                ],
                [
                    "[ITALIC] Base Model",
                    "37.36",
                    "38.13",
                    "36.03",
                    "20.68",
                    "18.64",
                    "26.65",
                    "24.74",
                    "21.80",
                    "27.74",
                    "19.05",
                    "14.90",
                    "23.04"
                ],
                [
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "InitDec",
                    "38.40\u2020",
                    "39.19\u2020",
                    "36.86\u2020",
                    "[BOLD] 21.79\u2020",
                    "19.54\u2020",
                    "[BOLD] 28.33\u2020",
                    "[BOLD] 26.34\u2020",
                    "[BOLD] 23.31\u2020",
                    "29.39\u2020",
                    "18.88",
                    "14.89",
                    "22.56"
                ],
                [
                    "AddDec",
                    "38.50\u2020",
                    "[BOLD] 39.35\u2020",
                    "36.98\u2020",
                    "21.65\u2020",
                    "[BOLD] 19.66\u2020",
                    "27.48\u2020",
                    "26.30\u2020",
                    "23.09\u2020",
                    "[BOLD] 29.52\u2020",
                    "19.34",
                    "15.16",
                    "23.12"
                ],
                [
                    "InitDec+AddDec",
                    "[BOLD] 38.55\u2020",
                    "39.34\u2020",
                    "[BOLD] 37.14\u2020",
                    "21.49\u2020",
                    "19.43\u2020",
                    "27.55\u2020",
                    "26.25\u2020",
                    "23.18\u2020",
                    "29.30\u2020",
                    "[BOLD] 19.35",
                    "[BOLD] 15.16",
                    "[BOLD] 23.14"
                ],
                [
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Direct Tranformation",
                    "38.35\u2020",
                    "39.13\u2020",
                    "36.96\u2020",
                    "21.75\u2020",
                    "[BOLD] 19.59\u2020",
                    "28.07\u2020",
                    "26.29\u2020",
                    "23.34\u2020",
                    "29.22\u2020",
                    "19.09",
                    "14.89",
                    "22.76"
                ],
                [
                    "Hierarchical Gating",
                    "38.33\u2020",
                    "39.14\u2020",
                    "36.89\u2020",
                    "21.62\u2020",
                    "19.55\u2020",
                    "27.64\u2020",
                    "26.31\u2020",
                    "23.17\u2020",
                    "29.45\u2020",
                    "19.20",
                    "15.10",
                    "22.73"
                ],
                [
                    "Lang-Specific Attention",
                    "38.40\u2020",
                    "39.19\u2020",
                    "36.86\u2020",
                    "21.79\u2020",
                    "19.54\u2020",
                    "28.33\u2020",
                    "26.34\u2020",
                    "23.31\u2020",
                    "29.39\u2020",
                    "[BOLD] 19.35",
                    "[BOLD] 15.16",
                    "[BOLD] 23.14"
                ],
                [
                    "Combined Attention",
                    "[BOLD] 38.50\u2020",
                    "[BOLD] 39.36\u2020",
                    "36.94\u2020",
                    "21.66\u2020",
                    "19.52\u2020",
                    "27.90\u2020",
                    "26.38\u2020",
                    "23.31\u2020",
                    "29.44\u2020",
                    "18.96",
                    "14.82",
                    "22.92"
                ],
                [
                    "Lang-Specific S-Attention",
                    "38.46\u2020",
                    "39.24\u2020",
                    "[BOLD] 37.06\u2020",
                    "[BOLD] 21.84\u2020",
                    "19.58\u2020",
                    "[BOLD] 28.43\u2020",
                    "[BOLD] 26.49\u2020",
                    "[BOLD] 23.49\u2020",
                    "[BOLD] 29.49\u2020",
                    "19.09",
                    "14.59",
                    "22.98"
                ],
                [
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Source Context",
                    "38.46\u2020",
                    "39.24\u2020",
                    "37.06\u2020",
                    "[BOLD] 21.84\u2020",
                    "19.58\u2020",
                    "[BOLD] 28.43\u2020",
                    "[BOLD] 26.49\u2020",
                    "[BOLD] 23.49\u2020",
                    "29.49\u2020",
                    "19.09",
                    "14.59",
                    "22.98"
                ],
                [
                    "Target Context",
                    "38.76\u2020",
                    "[BOLD] 39.57\u2020",
                    "37.35\u2020",
                    "21.77\u2020",
                    "[BOLD] 19.68\u2020",
                    "27.86\u2020",
                    "26.21\u2020",
                    "23.16\u2020",
                    "29.26\u2020",
                    "19.23",
                    "14.77",
                    "[BOLD] 23.23"
                ],
                [
                    "Dual Context Src-Tgt",
                    "[BOLD] 38.80\u2020",
                    "39.51\u2020",
                    "[BOLD] 37.50\u2020",
                    "21.74\u2020",
                    "19.60\u2020",
                    "27.98\u2020",
                    "26.39\u2020",
                    "23.28\u2020",
                    "[BOLD] 29.50\u2020",
                    "18.89",
                    "14.52",
                    "23.06"
                ],
                [
                    "Dual Context Src-Tgt-Mix",
                    "38.76\u2020",
                    "39.52\u2020",
                    "37.43\u2020",
                    "21.68\u2020",
                    "19.63\u2020",
                    "27.71\u2020",
                    "26.37\u2020",
                    "23.26\u2020",
                    "29.48\u2020",
                    "[BOLD] 19.26",
                    "[BOLD] 14.86",
                    "23.01"
                ]
            ],
            "title": "Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, \u2020: Statistically significantly better than the base model, based on bootstrap resampling Clark et\u00a0al. (2011) with p < 0.05."
        },
        "insight": "For the Europarl data, we see decent improvements with InitDec for En-Et (+1.11 BLEU) and En-De (+1.60 BLEU), and with InitDec+AddDec for En-Fr (+1.19 BLEU). We also observe that, for all language-pairs, both translation directions benefit from context, [CONTINUE] On the other hand, for the Subtitles data, we see a maximum improvement of +0.30 BLEU for InitDec+AddDec. [CONTINUE] The next set of experiments evaluates the five different approaches for computing the sourceside context. from Table 2 that for English-Estonian and English-German, our model indeed benefits from using is evident [CONTINUE] Finally, our results with source, target and dual contexts are reported. Interestingly, just using the source context is sufficient for English-Estonian and English-German. For English-French, on the other hand, we see significant improvements for the models using the target-side conversation history over using only the source-side. [CONTINUE] Unlike Europarl, for Subtitles, we see improvements for our Src-TgtMix dual context variant over the Src-Tgt one for En\u2192Ru, [CONTINUE] To summarise, for majority of the cases our Language-Specific Sentence-level Attention is a winner or a close second. Using the Target Context is useful when the base model generates reasonable-quality translations; otherwise, using the Source Context should suffice."
    },
    {
        "id": "216",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Subtitles  [BOLD] En-Ru"
            ],
            "rows": [
                [
                    "[ITALIC] Prev Sent",
                    "38.15",
                    "21.70",
                    "26.09",
                    "[BOLD] 19.13"
                ],
                [
                    "Our Model",
                    "[BOLD] 38.46\u2020",
                    "[BOLD] 21.84",
                    "[BOLD] 26.49\u2020",
                    "19.09"
                ]
            ],
            "title": "Table 3: BLEU scores for the bilingual test sets. bold: Best performance, \u2020: Statistically significantly better than the contextual baseline."
        },
        "insight": "From Table 3, it can be seen that our model surpasses the local-context baseline for Europarl showing that the wider context is indeed beneficial [CONTINUE] For En-Ru, it can be seen that using previous sentence is sufficient"
    },
    {
        "id": "217",
        "table": {
            "header": [
                "[BOLD] Type of Context",
                "[BOLD] BLEU"
            ],
            "rows": [
                [
                    "No context (Base Model)",
                    "24.74"
                ],
                [
                    "Current Turn",
                    "26.39"
                ],
                [
                    "Current Language from Previous Turns",
                    "26.21"
                ],
                [
                    "Other Language from Previous Turns",
                    "26.32"
                ],
                [
                    "Complete Context",
                    "[BOLD] 26.49"
                ]
            ],
            "title": "Table 4: BLEU scores for En-De bilingual test set."
        },
        "insight": "We conduct an ablation study to validate our hypothesis of using the complete context versus using only one of the three types of contexts in a bilingual multi-speaker conversation: (i) current turn, (ii) previous turns in current language, and (iii) previous turns in the other language. The results for En-De are reported in Table 4. We see decrease in BLEU for all types of contexts with significant decrease when considering only current language from previous turns.The results show that the current turn has the most influence on translating a sentence,"
    },
    {
        "id": "218",
        "table": {
            "header": [
                "Method",
                "AvgSimC",
                "MaxSimC"
            ],
            "rows": [
                [
                    "Liu et\u00a0al. ( 2015a )",
                    "67.3",
                    "68.1"
                ],
                [
                    "Liu et\u00a0al. ( 2015b )",
                    "69.5",
                    "67.9"
                ],
                [
                    "Amiri et\u00a0al. ( 2016 )",
                    "70.9",
                    "-"
                ],
                [
                    "Lee and Chen ( 2017 )",
                    "68.7",
                    "67.9"
                ],
                [
                    "Guo et\u00a0al. ( 2018 )",
                    "69.3",
                    "68.2"
                ],
                [
                    "[ITALIC] 300-dimensions",
                    "[ITALIC] 300-dimensions",
                    "[ITALIC] 300-dimensions"
                ],
                [
                    "Global-DSM",
                    "67.1",
                    "67.1"
                ],
                [
                    "UTDSM Random",
                    "69.1\u00b10.1",
                    "66.4\u00b10.2"
                ],
                [
                    "UTDSM",
                    "[BOLD] 69.6",
                    "67.1"
                ],
                [
                    "UTDSM + GMM (1)",
                    "67.4",
                    "67.4"
                ],
                [
                    "UTDSM + GMM (2)",
                    "68.4",
                    "[BOLD] 68.3"
                ],
                [
                    "UTDSM + GMM (3)",
                    "68.9",
                    "[BOLD] 68.3"
                ],
                [
                    "UTDSM + GMM (8)",
                    "69.1",
                    "68.0"
                ],
                [
                    "UTDSM + GMM (10)",
                    "69.0",
                    "67.8"
                ],
                [
                    "[ITALIC] 500-dimensions",
                    "[ITALIC] 500-dimensions",
                    "[ITALIC] 500-dimensions"
                ],
                [
                    "Global-DSM",
                    "67.6",
                    "67.6"
                ],
                [
                    "UTDSM Random",
                    "69.4\u00b10.1",
                    "66.5\u00b10.3"
                ],
                [
                    "UTDSM",
                    "[BOLD] 70.2",
                    "68.0"
                ],
                [
                    "UTDSM + GMM (1)",
                    "67.6",
                    "67.6"
                ],
                [
                    "UTDSM + GMM (2)",
                    "68.8",
                    "[BOLD] 68.6"
                ],
                [
                    "UTDSM + GMM (3)",
                    "69.0",
                    "68.5"
                ],
                [
                    "UTDSM + GMM (8)",
                    "69.5",
                    "68.5"
                ],
                [
                    "UTDSM + GMM (10)",
                    "69.2",
                    "68.0"
                ]
            ],
            "title": "Table 1: Performance comparison between different state-of-the-art approaches on SCWS, in terms of Spearman\u2019s correlation. UTDSM refers to the projected cross-topic representation, UTDSM Random refers to the case when random words served as anchors and GMM (c) corresponds to GMM smoothing with c components."
        },
        "insight": "In Table 1 we compare our model (UTDSM) with our baseline (Global-DSM) and other state-ofthe-art multi-prototype approaches for the contextual semantic similarity task. It is clear that all different setups of UTDSM perform better than the baseline for both contextual semantic similarity metrics. Using a single Gaussian distribution (UTDSM + GMM (1)) at the smoothing step of our method produces similar results to the baseline model. [CONTINUE] We also observe that random anchoring performs slightly worse than UTDSM with respect to AvgSimC. [CONTINUE] Furthermore, we observe that GMM smoothing has a different effect on the MaxSimC and AvgSimC metrics. Specifically, for AvgSimC we consistently report lower results when GMM smoothing is applied for different number of components. [CONTINUE] At the same time, our smoothing technique highly improves the performance of MaxSimC for all possible configurations. [CONTINUE] Overall, the performance of our model is highly competitive to the state-of-the-art models in terms of AvgSimC, for 500-dimensional topic embeddings. We also achieve state-of-the-art performance for the MaxSimC metric, using smoothed topic embeddings of 300 or 500 dimensions with 2 or 3 Gaussian components."
    },
    {
        "id": "219",
        "table": {
            "header": [
                "[BOLD] Test domain",
                "[BOLD] Training Data  [BOLD] WSJ",
                "[BOLD] Training Data  [BOLD] WSJ + Giga"
            ],
            "rows": [
                [
                    "WSJ",
                    "65.78",
                    "83.42"
                ],
                [
                    "Brown",
                    "45.00",
                    "76.99"
                ],
                [
                    "Wikipedia",
                    "35.90",
                    "62.26"
                ]
            ],
            "title": "Table 2: BLEU scores for domain match experiments"
        },
        "insight": "We evaluate the in- and out-of-domain performance of our approach by training models on either WSJ gold data only, or both WSJ gold data and Gigaword silver data, and evaluating on different domains. The results in Table 2 show that while the generator performs best on test data which matches the training domain (news), semisupervised training leads to substantial out-ofdomain improvements on the Wikipedia and the Brown corpus portions of the test set."
    },
    {
        "id": "220",
        "table": {
            "header": [
                "Method",
                "Precision",
                "Recall",
                "F1-score",
                "Accuracy"
            ],
            "rows": [
                [
                    "LDA",
                    "39.7",
                    "41.8",
                    "38.8",
                    "41.8"
                ],
                [
                    "Global-DSM",
                    "62.9",
                    "63.3",
                    "62.9",
                    "63.3"
                ],
                [
                    "MaxCD",
                    "61.9",
                    "63.0",
                    "62.0",
                    "63.0"
                ],
                [
                    "AvgD",
                    "63.5",
                    "64.6",
                    "63.3",
                    "64.3"
                ],
                [
                    "AvgCD",
                    "[BOLD] 64.6",
                    "[BOLD] 65.5",
                    "[BOLD] 64.5",
                    "[BOLD] 65.5"
                ]
            ],
            "title": "Table 2: Evaluation results of multi-class text classification."
        },
        "insight": "Evaluation results on text classification are presented in Table 2. We observe that our model performs better than the baseline across all metrics for both averaging approaches (AvgCD, AvgD), while the usage of dominant topics appears to have lower performance (MaxCD). Specifically, we get an improvement of 2 \u2212 2.5% on topic-based average and 0.5 \u2212 1% on simple average combination compared to using Global-DSM."
    },
    {
        "id": "221",
        "table": {
            "header": [
                "Method",
                "Precision",
                "Recall",
                "F1-score",
                "Accuracy"
            ],
            "rows": [
                [
                    "Global-DSM",
                    "68.6",
                    "69.2",
                    "62.0",
                    "69.2"
                ],
                [
                    "MaxCD",
                    "[BOLD] 69.0",
                    "69.3",
                    "62.1",
                    "69.3"
                ],
                [
                    "AvgD",
                    "67.7",
                    "[BOLD] 69.4",
                    "[BOLD] 64.0",
                    "[BOLD] 69.4"
                ],
                [
                    "AvgCD",
                    "68.8",
                    "69.4",
                    "62.6",
                    "69.4"
                ]
            ],
            "title": "Table 3: Evaluation results on paraphrase detection task."
        },
        "insight": "Results for the paraphrase identification task are presented in Table 3. AvgD yields the best results especially in F1 metric showing that cross-topic representations are semantically richer than single embeddings baseline (Global-DSM)."
    },
    {
        "id": "222",
        "table": {
            "header": [
                "% of known intents",
                "SNIPS 25%",
                "50%",
                "75%",
                "ATIS 25%",
                "50%",
                "75%"
            ],
            "rows": [
                [
                    "MSP",
                    "0.0",
                    "6.2",
                    "8.3",
                    "8.1",
                    "15.3",
                    "17.2"
                ],
                [
                    "DOC",
                    "72.5",
                    "67.9",
                    "63.9",
                    "61.6",
                    "62.8",
                    "37.7"
                ],
                [
                    "DOC (Softmax)",
                    "72.8",
                    "65.7",
                    "61.8",
                    "63.6",
                    "63.3",
                    "38.7"
                ],
                [
                    "LOF (Softmax)",
                    "76.0",
                    "69.4",
                    "65.8",
                    "67.3",
                    "61.8",
                    "38.9"
                ],
                [
                    "LOF (LMCL)",
                    "[BOLD] 79.2",
                    "[BOLD] 84.1",
                    "[BOLD] 78.8",
                    "[BOLD] 69.6",
                    "[BOLD] 63.4",
                    "[BOLD] 39.6"
                ]
            ],
            "title": "Table 2: Macro f1-score of unknown intent detection with different proportion (25%, 50% and 75%) of classes are treated as known intents on SNIPS and ATIS dataset."
        },
        "insight": "We show the experiment results in Table 2. [CONTINUE] our method consistently performs better than all baselines in all settings. [CONTINUE] our method is also better than LOF (Softmax). [CONTINUE] we observe that on the ATIS dataset, the performance of unknown intent detection dramatically drops as the known intent increases."
    },
    {
        "id": "223",
        "table": {
            "header": [
                "Statistics  [ITALIC] i",
                "Statistics 1",
                "Statistics 2",
                "Statistics 3",
                "Statistics 4",
                "Scores  [ITALIC] \u03c4=2",
                "Scores  [ITALIC] \u03c4=| [BOLD] y|"
            ],
            "rows": [
                [
                    "[ITALIC] gi",
                    "3",
                    "4",
                    "4",
                    "4",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "AL [ITALIC] i",
                    "3",
                    "3",
                    "2",
                    "1",
                    "AL = 3",
                    "AL = 2.25"
                ]
            ],
            "title": "Table 1: Comparing AL with and without its truncated average, tracking time-indexed lag ALi=gi\u2212i\u22121\u03b3 when |x|=|y|=4 for a wait-3 system."
        },
        "insight": "Table 1 shows the time-indexed lags that are averaged to calculate AL for a wait-3 system. The lags make the problem clear: each position beyond the point where all source tokens have been read (gi = |x|) has its lag reduced by [CONTINUE] , pulling the average lag below k."
    },
    {
        "id": "224",
        "table": {
            "header": [
                "[BOLD] Representation",
                "[BOLD] Train on  [BOLD] Gold",
                "[BOLD] Train on  [BOLD] Gold+Silver"
            ],
            "rows": [
                [
                    "AMR",
                    "22.0",
                    "33.8"
                ],
                [
                    "DMRS - no attributes",
                    "40.1",
                    "63.6"
                ],
                [
                    "DMRS - all attributes",
                    "56.9",
                    "75.8"
                ]
            ],
            "title": "Table 4: BLEU scores for evaluating AMR and DMRS generators on an AMR test set"
        },
        "insight": "We compare our approach to AMR-to-text generation by evaluating our generator on a standard AMR test set [CONTINUE] for models trained on gold as well as gold plus silver data.9 We evaluate DMRS models both with and without predicate and edge attributes, [CONTINUE] The results in Table 4 show that our MRS generator performs better than the AMR generator by a large margin, even when the additional MRS attributes are excluded."
    },
    {
        "id": "225",
        "table": {
            "header": [
                "[BOLD] Complexity",
                "[BOLD] Ent.",
                "[BOLD] Neutral",
                "[BOLD] Cont.",
                "[BOLD] All"
            ],
            "rows": [
                [
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set"
                ],
                [
                    "[BOLD] All",
                    "83.56",
                    "84.12",
                    "86.37",
                    "[BOLD] 84.66"
                ],
                [
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation"
                ],
                [
                    "[BOLD] Simple",
                    "100",
                    "-",
                    "4.22",
                    "[BOLD] 52.63"
                ],
                [
                    "[BOLD] Medium",
                    "100",
                    "-",
                    "2.16",
                    "[BOLD] 49.27"
                ],
                [
                    "[BOLD] Complex",
                    "99.77",
                    "-",
                    "0.36",
                    "[BOLD] 50.45"
                ],
                [
                    "[BOLD] All",
                    "99.92",
                    "-",
                    "2.25",
                    "[BOLD] 50.78"
                ],
                [
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning"
                ],
                [
                    "[BOLD] Simple",
                    "38.14",
                    "0.66",
                    "69.53",
                    "[BOLD] 45.04"
                ],
                [
                    "[BOLD] Medium",
                    "57.14",
                    "1.36",
                    "50.14",
                    "[BOLD] 38.11"
                ],
                [
                    "[BOLD] Complex",
                    "55.48",
                    "3.04",
                    "46.26",
                    "[BOLD] 36.15"
                ],
                [
                    "[BOLD] All",
                    "50.25",
                    "1.69",
                    "55.31",
                    "[BOLD] 39.77"
                ]
            ],
            "title": "Table 3: Accuracy of the model trained only on MultiNLI on our datasets, which are used as probing datasets. The Complexity column refers to the syntactic complexity of the sentences."
        },
        "insight": "The model has relatively low accuracy on the entailment and contradiction examples while close to zero accuracy on the neutral ones. [CONTINUE] the accuracy of the model on each test set. [CONTINUE] On our dative alternation dataset, the accuracy on our test sets is substantially lower than on the MultiNLI development set (50.78% versus 84.66% respectively), [CONTINUE] The model has very high accuracy on the entailment examples, while close to zero on the contradiction ones. [CONTINUE] On the numerical reasoning dataset, the model also seems to fail on this inference type with test set accuracy much lower than on the MultiNLI development set,"
    },
    {
        "id": "226",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Fr\u2192En",
                    "De\u2192En",
                    "De\u2192En",
                    "Et\u2192En",
                    "Fr\u2192En",
                    "De\u2192En",
                    "De\u2192En",
                    "Et\u2192En",
                    "Fr\u2192En",
                    "De\u2192En",
                    "De\u2192En",
                    "Et\u2192En",
                    "Fr\u2192En",
                    "De\u2192En",
                    "De\u2192En",
                    "Et\u2192En"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] S-NMT",
                    "20.85",
                    "5.24",
                    "9.18",
                    "20.42",
                    "23.27",
                    "10.90",
                    "14.35",
                    "24.65",
                    "20.85",
                    "5.24",
                    "9.18",
                    "20.42",
                    "23.27",
                    "10.90",
                    "14.35",
                    "24.65"
                ],
                [
                    "[ITALIC]  +src",
                    "21.91\u2020",
                    "6.26\u2020",
                    "10.20\u2020",
                    "22.10\u2020",
                    "24.04\u2020",
                    "11.52\u2020",
                    "15.45\u2020",
                    "25.92\u2020",
                    "[BOLD] 21.80\u2020",
                    "6.10\u2020",
                    "9.98\u2020",
                    "21.50\u2020",
                    "23.99\u2020",
                    "11.53\u2020",
                    "15.29\u2020",
                    "25.44\u2020"
                ],
                [
                    "[ITALIC]  +trg",
                    "21.74\u2020",
                    "6.24\u2020",
                    "9.97\u2020",
                    "21.94\u2020",
                    "23.98\u2020",
                    "11.58\u2020",
                    "15.32\u2020",
                    "25.89\u2020",
                    "21.76\u2020",
                    "[BOLD] 6.31\u2020",
                    "10.04\u2020",
                    "21.82\u2020",
                    "24.06\u2020",
                    "[BOLD] 12.10\u2020",
                    "15.75\u2020",
                    "25.93\u2020"
                ],
                [
                    "[ITALIC]  +both",
                    "[BOLD] 22.00\u2020",
                    "[BOLD] 6.57\u2020",
                    "[BOLD] 10.54\u2020",
                    "[BOLD] 22.32\u2020",
                    "[BOLD] 24.40\u2020",
                    "[BOLD] 12.24\u2020",
                    "[BOLD] 16.18\u2020",
                    "[BOLD] 26.34\u2020",
                    "21.77\u2020",
                    "6.20\u2020",
                    "[BOLD] 10.23\u2020",
                    "[BOLD] 22.20\u2020",
                    "[BOLD] 24.27\u2020",
                    "11.84\u2020",
                    "[BOLD] 15.82\u2020",
                    "[BOLD] 26.10\u2020"
                ]
            ],
            "title": "Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Document NMT model. bold: Best performance, \u2020: Statistically significantly better than the baseline."
        },
        "insight": "We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2). Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs. [CONTINUE] From Table 2, we consistently see +.95/+1.00 BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan [CONTINUE] For German\u2192English, guage pairs. For French\u2192English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best. the target memory variants give comparable results, whereas for Estonian\u2192English, the dual memory variant proves to be the best. Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts."
    },
    {
        "id": "227",
        "table": {
            "header": [
                "Model",
                "[BOLD] Surface SentLen",
                "[BOLD] Surface WC",
                "[BOLD] Syntactic TreeDepth",
                "[BOLD] Syntactic TopConst",
                "[BOLD] Syntactic BShift",
                "[BOLD] Semantic Tense",
                "[BOLD] Semantic SubjNum",
                "[BOLD] Semantic ObjNum",
                "[BOLD] Semantic SOMO",
                "[BOLD] Semantic CoordInv"
            ],
            "rows": [
                [
                    "Majority",
                    "20.0",
                    "0.5",
                    "17.9",
                    "5.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0"
                ],
                [
                    "Human",
                    "100",
                    "100",
                    "84.0",
                    "84.0",
                    "98.0",
                    "85.0",
                    "88.0",
                    "86.5",
                    "81.2",
                    "85.0"
                ],
                [
                    "Length",
                    "100",
                    "0.2",
                    "18.1",
                    "9.3",
                    "50.6",
                    "56.5",
                    "50.3",
                    "50.1",
                    "50.2",
                    "50.0"
                ],
                [
                    "AVG",
                    "64.12",
                    "82.1",
                    "36.38",
                    "68.04",
                    "50.16",
                    "87.9",
                    "80.89",
                    "80.24",
                    "50.39",
                    "51.95"
                ],
                [
                    "MAX",
                    "62.67",
                    "88.97",
                    "33.02",
                    "62.63",
                    "50.31",
                    "85.66",
                    "77.11",
                    "76.04",
                    "51.86",
                    "52.33"
                ],
                [
                    "[ITALIC] c[0]",
                    "[BOLD] 98.67",
                    "[BOLD] 91.11",
                    "38.6",
                    "70.54",
                    "50.42",
                    "88.25",
                    "80.88",
                    "80.56",
                    "[BOLD] 55.6",
                    "55"
                ],
                [
                    "[ITALIC] c[0:1]",
                    "97.18",
                    "89.16",
                    "40.41",
                    "78.34",
                    "52.25",
                    "88.58",
                    "86.59",
                    "84.36",
                    "54.62",
                    "70.42"
                ],
                [
                    "[ITALIC] c[0:2]",
                    "95.84",
                    "86.77",
                    "43.01",
                    "80.41",
                    "54.84",
                    "88.87",
                    "88.06",
                    "86.26",
                    "53.07",
                    "71.87"
                ],
                [
                    "[ITALIC] c[0:3]",
                    "94.63",
                    "84.96",
                    "[BOLD] 43.35",
                    "81.01",
                    "57.29",
                    "88.88",
                    "88.36",
                    "86.51",
                    "53.79",
                    "[BOLD] 72.01"
                ],
                [
                    "[ITALIC] c[0:4]",
                    "93.25",
                    "83.24",
                    "43.26",
                    "81.49",
                    "60.31",
                    "[BOLD] 88.91",
                    "[BOLD] 88.65",
                    "87.15",
                    "52.77",
                    "71.91"
                ],
                [
                    "[ITALIC] c[0:5]",
                    "92.29",
                    "81.84",
                    "42.75",
                    "[BOLD] 81.60",
                    "62.01",
                    "88.82",
                    "88.44",
                    "87.98",
                    "52.38",
                    "70.96"
                ],
                [
                    "[ITALIC] c[0:6]",
                    "91.56",
                    "79.83",
                    "43.05",
                    "81.41",
                    "[BOLD] 62.59",
                    "88.87",
                    "[BOLD] 88.65",
                    "[BOLD] 88.28",
                    "52.07",
                    "70.63"
                ]
            ],
            "title": "Table 2: Probing tasks performance of vector averaging (AVG) and max pooling (MAX) vs. DCT embeddings with various K. Majority (baseline), Human (human-bound), and a linear classifier with sentence length as sole feature (Length) as reported in Conneau et al. (2018), respectively."
        },
        "insight": "We report the performance in probing tasks in Table 2. [CONTINUE] In general, DCT yields better performance compared to averaging on all tasks, and larger K often yields improved performance in syntactic and semantic tasks. For the surface information tasks, SentLen and Word content (WC), c significantly outperforms AVG. [CONTINUE] The performance decreased with increasing K in c[0 : K], [CONTINUE] While increasing K has no positive effect on surface information tasks, syntactic and semantic tasks demonstrate performance gains with larger K. This trend is clearly observed in all syntactic tasks and three of the semantic tasks, where DCT performs well above AVG and the performance improves with increasing K. The only exception is SOMO, where increasing K actually results in lower performance, although all DCT results are still higher than AVG."
    },
    {
        "id": "228",
        "table": {
            "header": [
                "Model",
                "[BOLD] Sentiment Analysis MR",
                "[BOLD] Sentiment Analysis SST2",
                "[BOLD] Sentiment Analysis SST5",
                "[BOLD] Sentiment Analysis CR",
                "[BOLD] Sentiment Analysis MPQA",
                "[BOLD] SUBJ",
                "[BOLD] Relatedness/Paraphrase SICK-R",
                "[BOLD] Relatedness/Paraphrase STSB",
                "[BOLD] Relatedness/Paraphrase MRPC",
                "[BOLD] Inference SICK-E",
                "[BOLD] TREC"
            ],
            "rows": [
                [
                    "AVG",
                    "78.3",
                    "[BOLD] 84.13",
                    "44.16",
                    "79.6",
                    "87.94",
                    "92.33",
                    "81.95",
                    "69.26",
                    "74.43",
                    "79.5",
                    "83.2"
                ],
                [
                    "MAX",
                    "73.31",
                    "79.24",
                    "41.86",
                    "73.35",
                    "86.54",
                    "88.02",
                    "81.93",
                    "[BOLD] 71.57",
                    "72.5",
                    "77.98",
                    "76.2"
                ],
                [
                    "[ITALIC] c[0]",
                    "[BOLD] 78.45",
                    "83.53",
                    "44.57",
                    "79.81",
                    "[BOLD] 88.36",
                    "[BOLD] 92.79",
                    "82.61",
                    "71.11",
                    "72.93",
                    "78.91",
                    "84.8"
                ],
                [
                    "[ITALIC] c[0:1]",
                    "78.15",
                    "83.47",
                    "[BOLD] 46.06",
                    "79.84",
                    "87.76",
                    "92.61",
                    "82.73",
                    "70.82",
                    "72.81",
                    "79.64",
                    "88.2"
                ],
                [
                    "[ITALIC] c[0:2]",
                    "78.02",
                    "82.98",
                    "45.16",
                    "79.68",
                    "87.62",
                    "92.5",
                    "[BOLD] 82.95",
                    "70.36",
                    "72.87",
                    "79.76",
                    "[BOLD] 89.8"
                ],
                [
                    "[ITALIC] c[0:3]",
                    "77.81",
                    "83.8",
                    "45.79",
                    "79.66",
                    "87.54",
                    "92.4",
                    "82.93",
                    "69.79",
                    "73.57",
                    "[BOLD] 80.56",
                    "88.2"
                ],
                [
                    "[ITALIC] c[0:4]",
                    "77.72",
                    "83.75",
                    "44.03",
                    "[BOLD] 80.08",
                    "87.4",
                    "92.61",
                    "82.53",
                    "69.31",
                    "72.35",
                    "79.72",
                    "[BOLD] 89.8"
                ],
                [
                    "[ITALIC] c[0:5]",
                    "77.42",
                    "82.43",
                    "43.3",
                    "78.6",
                    "87.21",
                    "92.19",
                    "82.36",
                    "68.9",
                    "73.91",
                    "79.89",
                    "88.8"
                ],
                [
                    "[ITALIC] c[0:6]",
                    "77.47",
                    "82.81",
                    "42.99",
                    "78.78",
                    "87.06",
                    "92.15",
                    "81.86",
                    "68.17",
                    "[BOLD] 75.07",
                    "79.76",
                    "86.4"
                ]
            ],
            "title": "Table 3: DCT embedding Performance in SentEval downstream tasks compared to vector averaging (AVG) and max pooling (MAX)."
        },
        "insight": "Our results in Table 3 are consistent with these observations, where we see improvements in most tasks, but the difference is not as significant as the probing tasks, except in TREC question classification where increasing K leads to much better performance."
    },
    {
        "id": "229",
        "table": {
            "header": [
                "Model",
                "[BOLD] 20-NG P",
                "[BOLD] 20-NG R",
                "[BOLD] 20-NG F1",
                "[BOLD] R-8 P",
                "[BOLD] R-8 R",
                "[BOLD] R-8 F1",
                "[BOLD] SST-5 P",
                "[BOLD] SST-5 R",
                "[BOLD] SST-5 F1"
            ],
            "rows": [
                [
                    "PCA",
                    "55.43",
                    "54.67",
                    "54.77",
                    "83.83",
                    "83.42",
                    "83.41",
                    "26.47",
                    "25.08",
                    "25.23"
                ],
                [
                    "DCT*",
                    "61.07",
                    "59.16",
                    "59.78",
                    "90.41",
                    "90.78",
                    "90.38",
                    "30.11",
                    "30.09",
                    "29.53"
                ],
                [
                    "Avg. vec.",
                    "68.72",
                    "68.19",
                    "68.25",
                    "96.34",
                    "96.30",
                    "96.27",
                    "27.88",
                    "26.44",
                    "24.81"
                ],
                [
                    "p-means",
                    "[ITALIC] 72.20",
                    "[ITALIC] 71.65",
                    "[BOLD] 71.79",
                    "96.69",
                    "96.67",
                    "96.65",
                    "33.77",
                    "33.41",
                    "33.26"
                ],
                [
                    "ELMo",
                    "71.20",
                    "[BOLD] 71.79",
                    "71.36",
                    "94.54",
                    "91.32",
                    "91.32",
                    "[ITALIC] 42.35",
                    "[ITALIC] 41.51",
                    "[ITALIC] 41.54"
                ],
                [
                    "BERT",
                    "70.89",
                    "70.79",
                    "70.88",
                    "95.52",
                    "95.39",
                    "95.39",
                    "39.92",
                    "39.38",
                    "39.35"
                ],
                [
                    "EigenSent",
                    "66.98",
                    "66.40",
                    "66.54",
                    "95.91",
                    "95.80",
                    "95.76",
                    "35.32",
                    "33.69",
                    "33.91"
                ],
                [
                    "EigenSent\u2295Avg",
                    "[BOLD] 72.24",
                    "71.62",
                    "[ITALIC] 71.78",
                    "[BOLD] 97.18",
                    "[BOLD] 97.13",
                    "[BOLD] 97.14",
                    "[BOLD] 42.77",
                    "[BOLD] 41.67",
                    "[BOLD] 41.81"
                ],
                [
                    "c[k]",
                    "[ITALIC] 72.20",
                    "71.58",
                    "71.73",
                    "[ITALIC] 96.98",
                    "[ITALIC] 96.98",
                    "[ITALIC] 96.94",
                    "37.67",
                    "34.47",
                    "34.54"
                ]
            ],
            "title": "Table 4: Performance in text classification (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported in Kayal and Tsatsaronis (2019), where DCT* refers to the implementation in Kayal and Tsatsaronis (2019). Our DCT embeddings are denoted as c[k] in the bottom row. Bold indicates the best result, and italic indicates second-best."
        },
        "insight": "Table 4 shows the best results for the various models as reported in Kayal and Tsat [CONTINUE] saronis (2019), in addition to the best performance of our model denoted as c[k]. [CONTINUE] Note that the DCT-based model, DCT*, described in Kayal and Tsatsaronis (2019) performed relatively poorly in all tasks, while our model achieved close to state-of-the-art performance in both the 20-NG and R-8 tasks. Our model outperformed EignSent on all tasks and generally performed better than or on par with p-means, ELMo, BERT, and EigenSent\u2295Avg on both the 20-NG and R-8. On the other hand, both EigenSent\u2295Avg and ELMo performed better than all other models on SST-5."
    },
    {
        "id": "230",
        "table": {
            "header": [
                "[EMPTY]",
                "1-best",
                "10-best",
                "cnet",
                "pruned cnet"
            ],
            "rows": [
                [
                    "all words",
                    "69.3",
                    "78.6",
                    "85.7",
                    "83.1"
                ],
                [
                    "slots/values",
                    "69.8",
                    "75.6",
                    "82.4",
                    "80.6"
                ]
            ],
            "title": "Table 1: Coverage of words from the manual transcripts in the DSTC2 development set of different batch\u00a0ASR output types\u00a0(%). In the pruned cnet interjections and hypotheses with scores below 0.001 were removed."
        },
        "insight": "As shown in Table 1, this does not discard too many correct hypotheses but markedly reduces the size of the cnet to an average of seven timesteps with two hypotheses."
    },
    {
        "id": "231",
        "table": {
            "header": [
                "[BOLD] method",
                "[BOLD] goals",
                "[BOLD] requests"
            ],
            "rows": [
                [
                    "1-best baseline",
                    "63.6\u200466.658.7",
                    "96.8\u200497.196.5"
                ],
                [
                    "[ITALIC] cnet - no pruning",
                    "[ITALIC] cnet - no pruning",
                    "[ITALIC] cnet - no pruning"
                ],
                [
                    "weighted pooling",
                    "63.7\u200465.661.6",
                    "96.7\u200497.096.3"
                ],
                [
                    "[ITALIC] cnet - score threshold 0.001",
                    "[ITALIC] cnet - score threshold 0.001",
                    "[ITALIC] cnet - score threshold 0.001"
                ],
                [
                    "average pooling",
                    "63.7\u200466.460.0",
                    "96.6\u200496.896.0"
                ],
                [
                    "weighted pooling",
                    "[BOLD] 65.2\u200468.559.1",
                    "97.0\u200497.496.6"
                ],
                [
                    "[ITALIC] cnet - score threshold 0.01",
                    "[ITALIC] cnet - score threshold 0.01",
                    "[ITALIC] cnet - score threshold 0.01"
                ],
                [
                    "average pooling",
                    "64.6\u200467.959.7",
                    "96.9\u22c6\u200497.296.5"
                ],
                [
                    "weighted pooling",
                    "64.7\u200468.462.2",
                    "[BOLD] 97.1\u22c6\u200497.396.9"
                ],
                [
                    "[ITALIC] ensemble models",
                    "[ITALIC] ensemble models",
                    "[ITALIC] ensemble models"
                ],
                [
                    "baseline",
                    "69.7",
                    "96.7"
                ],
                [
                    "cnet",
                    "[BOLD] 71.4",
                    "[BOLD] 97.2"
                ],
                [
                    "[ITALIC] results from related work",
                    "[ITALIC] results from related work",
                    "[ITALIC] results from related work"
                ],
                [
                    "Vodol\u00e1n et\u00a0al. ( 2017 )",
                    "80.0",
                    "-"
                ],
                [
                    "Williams ( 2014 )",
                    "78.4",
                    "98.0"
                ],
                [
                    "Mrksic et\u00a0al. ( 2017 )",
                    "73.4",
                    "96.5"
                ]
            ],
            "title": "Table 3: DSTC2 test set accuracy of ten runs with different random seeds in the format average\u2004maximumminimum. \u22c6\u00a0denotes a statistically significant higher result than the baseline (p<0.05, Wilcoxon signed-rank test with Bonferroni correction for ten repeated comparisons). The cnet ensemble corresponds to the best cnet model with pruning threshold\u00a00.001 and weighted pooling."
        },
        "insight": "Table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels (discarding only interjections, additionally discarding hypotheses with scores below 0.001 and 0.01, respectively). As can be seen, using the full cnet except for interjections does not improve over the baseline. [CONTINUE] However, when pruning low-probability hypotheses both pooling strategies improve over the baseline. Yet, average pooling performs worse for the lower pruning threshold, which shows that the model is still affected by noise among the hypotheses. [CONTINUE] Weighted pooling performs better for the lower pruning threshold of 0.001 with which we obtain the highest result overall, improving the joint goals accuracy by 1.6 percentage points compared to the baseline. [CONTINUE] Moreover, we see that an ensemble model that averages the predictions of ten cnet models trained with different random seeds also outperforms an ensemble of ten baseline models. [CONTINUE] Our ensemble models outperform Mrksic et al. (2017) for the joint requests but are a bit worse for the joint goals."
    },
    {
        "id": "232",
        "table": {
            "header": [
                "[BOLD] test data",
                "[BOLD] goals",
                "[BOLD] requests"
            ],
            "rows": [
                [
                    "[ITALIC] train on transcripts + batch\u00a0ASR (baseline)",
                    "[ITALIC] train on transcripts + batch\u00a0ASR (baseline)",
                    "[ITALIC] train on transcripts + batch\u00a0ASR (baseline)"
                ],
                [
                    "[ITALIC] batch\u00a0ASR",
                    "63.6\u200466.658.7",
                    "96.8\u200497.196.5"
                ],
                [
                    "[ITALIC] train on transcripts + live\u00a0ASR",
                    "[ITALIC] train on transcripts + live\u00a0ASR",
                    "[ITALIC] train on transcripts + live\u00a0ASR"
                ],
                [
                    "[ITALIC] live\u00a0ASR",
                    "63.8\u200467.060.2",
                    "97.5\u200497.797.2"
                ],
                [
                    "transcripts",
                    "78.3\u200482.474.3",
                    "98.7\u200499.098.0"
                ]
            ],
            "title": "Table 2: DSTC2 test set accuracy for 1-best ASR outputs of ten runs with different random seeds in the format average\u2004maximumminimum."
        },
        "insight": "As can be seen from Table 2, the DST accuracy slightly increases for the higher-quality live ASR outputs. More importantly, the DST performance drastically increases, when we evaluate on the manual transcripts that reflect the true user utterances nearly perfectly."
    },
    {
        "id": "233",
        "table": {
            "header": [
                "[BOLD] Gold Class",
                "[BOLD] Train",
                "[BOLD] Dev",
                "[BOLD] Test"
            ],
            "rows": [
                [
                    "None",
                    "15,401",
                    "3,905",
                    "4,141"
                ],
                [
                    "Obligation",
                    "11,005",
                    "2,860",
                    "970"
                ],
                [
                    "Prohibition",
                    "1,172",
                    "314",
                    "108"
                ],
                [
                    "Obligation List Intro",
                    "828",
                    "203",
                    "70"
                ],
                [
                    "Obligation List Item",
                    "2888",
                    "726",
                    "255"
                ],
                [
                    "Prohibition List Item",
                    "251",
                    "28",
                    "19"
                ],
                [
                    "[BOLD] Total",
                    "[BOLD] 31,545",
                    "[BOLD] 8,036",
                    "[BOLD] 5,563"
                ]
            ],
            "title": "Table 2: Sentences/clauses after sentence splitting."
        },
        "insight": "Fourth, we introduce finer classes (Tables 1\u20132), which fit better the target task, where nested clauses are frequent. [CONTINUE] The splitter produced 31,545 training, 8,036 development, and 5,563 test sentences/clauses.3 Table 2 shows their distribution in the six gold (correct) classes. [CONTINUE] cf. Table 2),"
    },
    {
        "id": "234",
        "table": {
            "header": [
                "Ru",
                "Ja",
                "En",
                "#sent.",
                "Usage test",
                "Usage development"
            ],
            "rows": [
                [
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "913",
                    "600",
                    "313"
                ],
                [
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "173",
                    "-",
                    "173"
                ],
                [
                    "[EMPTY]",
                    "\u2713",
                    "\u2713",
                    "276",
                    "-",
                    "276"
                ],
                [
                    "\u2713",
                    "[EMPTY]",
                    "\u2713",
                    "0",
                    "-",
                    "-"
                ],
                [
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "4",
                    "-",
                    "-"
                ],
                [
                    "[EMPTY]",
                    "\u2713",
                    "[EMPTY]",
                    "287",
                    "-",
                    "-"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2713",
                    "1",
                    "-",
                    "-"
                ],
                [
                    "Total",
                    "Total",
                    "Total",
                    "1,654",
                    "-",
                    "-"
                ]
            ],
            "title": "Table 1: Manually aligned News Commentary data."
        },
        "insight": "As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly sentences) as summarized in Table 1. Finally, for the sake of comparability, we randomly chose 600 trilingual sentences to create a test set, and concatenated the rest of them and bilingual sentences to form development sets."
    },
    {
        "id": "235",
        "table": {
            "header": [
                "[BOLD] Gold Class",
                "[BOLD] bilstm P",
                "[BOLD] bilstm R",
                "[BOLD] bilstm F1",
                "[BOLD] bilstm AUC",
                "[BOLD] bilstm-att P",
                "[BOLD] bilstm-att R",
                "[BOLD] bilstm-att F1",
                "[BOLD] bilstm-att AUC",
                "[BOLD] x-bilstm-att P",
                "[BOLD] x-bilstm-att R",
                "[BOLD] x-bilstm-att F1",
                "[BOLD] x-bilstm-att AUC",
                "[BOLD] h-bilstm-att P",
                "[BOLD] h-bilstm-att R",
                "[BOLD] h-bilstm-att F1",
                "[BOLD] h-bilstm-att AUC"
            ],
            "rows": [
                [
                    "None",
                    "0.95",
                    "0.91",
                    "0.93",
                    "0.98",
                    "0.97",
                    "0.90",
                    "0.93",
                    "[BOLD] 0.99",
                    "0.96",
                    "0.90",
                    "0.93",
                    "0.98",
                    "[BOLD] 0.98",
                    "[BOLD] 0.96",
                    "[BOLD] 0.97",
                    "[BOLD] 0.99"
                ],
                [
                    "Obligation",
                    "0.75",
                    "0.85",
                    "0.79",
                    "0.86",
                    "0.75",
                    "0.88",
                    "0.81",
                    "0.86",
                    "0.75",
                    "0.87",
                    "0.81",
                    "0.88",
                    "[BOLD] 0.87",
                    "[BOLD] 0.92",
                    "[BOLD] 0.90",
                    "[BOLD] 0.96"
                ],
                [
                    "Prohibition",
                    "0.67",
                    "0.62",
                    "0.64",
                    "0.75",
                    "0.74",
                    "0.75",
                    "0.74",
                    "0.80",
                    "0.65",
                    "0.75",
                    "0.70",
                    "0.74",
                    "[BOLD] 0.84",
                    "[BOLD] 0.83",
                    "[BOLD] 0.84",
                    "[BOLD] 0.90"
                ],
                [
                    "Obl.\u00a0List Begin",
                    "0.70",
                    "0.86",
                    "0.77",
                    "0.81",
                    "0.71",
                    "0.85",
                    "0.77",
                    "0.83",
                    "0.72",
                    "0.75",
                    "0.74",
                    "0.80",
                    "[BOLD] 0.90",
                    "[BOLD] 0.89",
                    "[BOLD] 0.89",
                    "[BOLD] 0.93"
                ],
                [
                    "Obl.\u00a0List Item",
                    "0.53",
                    "0.66",
                    "0.59",
                    "0.64",
                    "0.48",
                    "0.70",
                    "0.57",
                    "0.60",
                    "0.49",
                    "0.78",
                    "0.60",
                    "0.66",
                    "[BOLD] 0.85",
                    "[BOLD] 0.94",
                    "[BOLD] 0.89",
                    "[BOLD] 0.94"
                ],
                [
                    "Proh.\u00a0List Item",
                    "0.59",
                    "0.35",
                    "0.43",
                    "0.50",
                    "0.61",
                    "0.55",
                    "0.59",
                    "0.62",
                    "[BOLD] 0.83",
                    "0.50",
                    "0.62",
                    "0.67",
                    "0.80",
                    "[BOLD] 0.84",
                    "[BOLD] 0.82",
                    "[BOLD] 0.92"
                ],
                [
                    "[BOLD] Macro-average",
                    "0.70",
                    "0.70",
                    "0.70",
                    "0.74",
                    "0.73",
                    "0.78",
                    "0.74",
                    "0.78",
                    "0.73",
                    "0.76",
                    "0.73",
                    "0.79",
                    "[BOLD] 0.87",
                    "[BOLD] 0.90",
                    "[BOLD] 0.89",
                    "[BOLD] 0.94"
                ],
                [
                    "[BOLD] Micro-average",
                    "0.90",
                    "0.88",
                    "0.88",
                    "0.94",
                    "0.90",
                    "0.88",
                    "0.89",
                    "0.96",
                    "0.90",
                    "0.88",
                    "0.89",
                    "0.94",
                    "[BOLD] 0.95",
                    "[BOLD] 0.95",
                    "[BOLD] 0.95",
                    "[BOLD] 0.98"
                ]
            ],
            "title": "Table 3: Precision, recall, f1, and auc scores, with the best results in bold and gray background."
        },
        "insight": "Table 3 reports the precision, recall, F1 score, area under the precision-recall curve (AUC) per class, as well as micro- and macro-averages. [CONTINUE] The self-attention mechanism (BILSTM-ATT) leads to clear overall improvements (in macro and micro F1 and AUC, Table 3) comparing to the plain BILSTM, supporting the hypothesis that selfattention allows the classifier to focus on indicative tokens. Allowing the BILSTM to consider tokens of neighboring sentences (X-BILSTM-ATT) does not lead to any clear overall improvements. [CONTINUE] The hierarchical H-BILSTM-ATT clearly outperforms the other three methods, supporting the hypothesis that considering entire sections and allowing the sentence embeddings to interact in the upper BILSTM (Fig. 3) is beneficial."
    },
    {
        "id": "236",
        "table": {
            "header": [
                "ID",
                "Pseudo-parallel data involved Ja\u2217\u2192Ru",
                "Pseudo-parallel data involved Ru\u2217\u2192Ja",
                "Pseudo-parallel data involved Ja\u2217\u2192En",
                "Pseudo-parallel data involved En\u2217\u2192Ja",
                "Pseudo-parallel data involved Ru\u2217\u2192En",
                "Pseudo-parallel data involved En\u2217\u2192Ru",
                "BLEU score Ja\u2192Ru",
                "BLEU score Ru\u2192Ja",
                "BLEU score Ja\u2192En",
                "BLEU score En\u2192Ja",
                "BLEU score Ru\u2192En",
                "BLEU score En\u2192Ru"
            ],
            "rows": [
                [
                    "(b3)",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "3.72",
                    "8.35",
                    "10.24",
                    "12.43",
                    "22.10",
                    "16.92"
                ],
                [
                    "#1",
                    "\u2713",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "\u2219 [BOLD] 4.59",
                    "[BOLD] 8.63",
                    "[BOLD] 10.64",
                    "[BOLD] 12.94",
                    "[BOLD] 22.21",
                    "[BOLD] 17.30"
                ],
                [
                    "#2",
                    "-",
                    "\u2713",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 3.74",
                    "\u2219 [BOLD] 8.85",
                    "10.13",
                    "[BOLD] 13.05",
                    "[BOLD] 22.48",
                    "[BOLD] 17.20"
                ],
                [
                    "#3",
                    "\u2713",
                    "\u2713",
                    "-",
                    "-",
                    "-",
                    "-",
                    "\u2219 [BOLD] 4.56",
                    "\u2219 [BOLD] 9.09",
                    "[BOLD] 10.57",
                    "\u2219 [BOLD] 13.23",
                    "[BOLD] 22.48",
                    "\u2219 [BOLD] 17.89"
                ],
                [
                    "2-13 #4",
                    "-",
                    "-",
                    "\u2713",
                    "-",
                    "-",
                    "-",
                    "3.71",
                    "8.05",
                    "\u2219 [BOLD] 11.00",
                    "[BOLD] 12.66",
                    "[BOLD] 22.17",
                    "16.76"
                ],
                [
                    "#5",
                    "-",
                    "-",
                    "-",
                    "\u2713",
                    "-",
                    "-",
                    "3.62",
                    "8.10",
                    "9.92",
                    "\u2219 [BOLD] 14.06",
                    "21.66",
                    "16.68"
                ],
                [
                    "#6",
                    "-",
                    "-",
                    "\u2713",
                    "\u2713",
                    "-",
                    "-",
                    "3.61",
                    "7.94",
                    "\u2219 [BOLD] 11.51",
                    "\u2219 [BOLD] 14.38",
                    "[BOLD] 22.22",
                    "16.80"
                ],
                [
                    "2-13 #7",
                    "-",
                    "-",
                    "-",
                    "-",
                    "\u2713",
                    "-",
                    "[BOLD] 3.80",
                    "[BOLD] 8.37",
                    "[BOLD] 10.67",
                    "[BOLD] 13.00",
                    "[BOLD] 22.51",
                    "\u2219 [BOLD] 17.73"
                ],
                [
                    "#8",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "\u2713",
                    "[BOLD] 3.77",
                    "8.04",
                    "[BOLD] 10.52",
                    "12.43",
                    "\u2219 [BOLD] 22.85",
                    "[BOLD] 17.13"
                ],
                [
                    "#9",
                    "-",
                    "-",
                    "-",
                    "-",
                    "\u2713",
                    "\u2713",
                    "3.37",
                    "8.03",
                    "10.19",
                    "[BOLD] 12.79",
                    "[BOLD] 22.77",
                    "[BOLD] 17.26"
                ],
                [
                    "2-13 #10",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2219 [BOLD] 4.43",
                    "\u2219 [BOLD] 9.38",
                    "\u2219 [BOLD] 12.06",
                    "\u2219 [BOLD] 14.43",
                    "\u2219 [BOLD] 23.09",
                    "[BOLD] 17.30"
                ]
            ],
            "title": "Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six \u201cX\u2217\u2192Y\u201d columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and \u201c\u2219\u201d indicates statistical significance of the improvement."
        },
        "insight": "Table 8 shows the BLEU scores achieved by several reasonable combinations of six-way pseudo-parallel data. We observed that the use of all six-way pseudo-parallel data (#10) significantly improved the base model for all the translation directions, except En\u2192Ru. A translation direction often benefited when the pseudo-parallel data for that specific direction was used. [CONTINUE] However, the resulting BLEU scores for Ja\u2192Ru and Ru\u2192Ja tasks do not exceed 10 BLEU points, implying the inherent limitation of the in-domain data as well as the difficulty of these translation directions. [CONTINUE] models of our multistage fine-tuning, i.e., V and VII, achieved significantly higher BLEU scores than (b3) in Table 5, a weak baseline without using any monolingual data, and #10 in Table 8, a strong baseline established with monolingual data."
    },
    {
        "id": "237",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] 5-Way 1-Shot  [BOLD] On 1.0",
                "[BOLD] 5-Way 1-Shot  [BOLD] On 2.0",
                "[BOLD] 5-Way 5-Shot  [BOLD] On 1.0",
                "[BOLD] 5-Way 5-Shot  [BOLD] On 2.0"
            ],
            "rows": [
                [
                    "GNN (CNN)",
                    "66.23\u00b10.75",
                    "27.94\u00b10.03",
                    "81.28\u00b10.62",
                    "29.33\u00b10.11"
                ],
                [
                    "Proto (CNN)",
                    "74.52\u00b10.07",
                    "35.09\u00b10.10",
                    "88.40\u00b10.06",
                    "49.37\u00b10.10"
                ],
                [
                    "Proto-ADV (CNN)",
                    "70.28\u00b10.15",
                    "42.21\u00b10.09",
                    "84.63\u00b10.07",
                    "58.71\u00b10.06"
                ],
                [
                    "Proto (BERT)",
                    "80.68\u00b10.28",
                    "40.12\u00b10.19",
                    "89.60\u00b10.09",
                    "51.50\u00b10.29"
                ],
                [
                    "Proto-ADV (BERT)",
                    "73.35\u00b10.95",
                    "41.90\u00b10.44",
                    "82.30\u00b10.53",
                    "54.74\u00b10.22"
                ],
                [
                    "BERT-PAIR",
                    "88.32\u00b10.64",
                    "56.25\u00b10.40",
                    "93.22\u00b10.13",
                    "67.44\u00b10.54"
                ],
                [
                    "[BOLD] Model",
                    "[BOLD] 10-Way 1-Shot",
                    "[BOLD] 10-Way 1-Shot",
                    "[BOLD] 10-Way 5-Shot",
                    "[BOLD] 10-Way 5-Shot"
                ],
                [
                    "[BOLD] Model",
                    "[BOLD] On 1.0",
                    "[BOLD] On 2.0",
                    "[BOLD] On 1.0",
                    "[BOLD] On 2.0"
                ],
                [
                    "GNN (CNN)",
                    "46.27\u00b10.80",
                    "16.44\u00b10.04",
                    "64.02\u00b10.77",
                    "18.26\u00b10.03"
                ],
                [
                    "Proto (CNN)",
                    "62.38\u00b10.06",
                    "22.98\u00b10.05",
                    "80.45\u00b10.08",
                    "35.22\u00b10.06"
                ],
                [
                    "Proto-ADV (CNN)",
                    "56.34\u00b10.08",
                    "28.91\u00b10.10",
                    "74.67\u00b10.12",
                    "44.35\u00b10.09"
                ],
                [
                    "Proto (BERT)",
                    "71.48\u00b10.15",
                    "26.45\u00b10.10",
                    "82.89\u00b10.11",
                    "36.93\u00b10.01"
                ],
                [
                    "Proto-ADV (BERT)",
                    "61.49\u00b10.69",
                    "27.36\u00b10.50",
                    "72.60\u00b10.38",
                    "37.40\u00b10.36"
                ],
                [
                    "BERT-PAIR",
                    "80.63\u00b10.17",
                    "43.64\u00b10.46",
                    "87.02\u00b10.12",
                    "53.17\u00b10.09"
                ]
            ],
            "title": "Table 2: Accuracies (%) on few-shot DA. \u201cOn 1.0\u201d represents the results on the original FewRel dataset and \u201cOn 2.0\u201d represents the results on the new test set. The models with \u201c-ADV\u201d use adversarial training described in Section 3."
        },
        "insight": "Table 2 demonstrates the evaluation results of few-shot DA on the existing FewRel test set and the new test set. [CONTINUE] (1) All few-shot models suffer dramatic perfor mance falls when tested on a different domain. (2) Adversarial training does improve the results on the new test domain, yet still has large space for growth. (3) BERT-PAIR outperforms all other few-shot models on both 1.0 and 2.0 test set."
    },
    {
        "id": "238",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] 5-Way-1-Shot  [BOLD] 0% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 15% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 30% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 50% NOTA"
            ],
            "rows": [
                [
                    "Proto (CNN)*",
                    "74.52\u00b10.07",
                    "62.18\u00b10.22",
                    "53.38\u00b10.14",
                    "37.26\u00b10.04"
                ],
                [
                    "Proto (CNN)",
                    "69.17\u00b10.07",
                    "60.59\u00b10.05",
                    "53.18\u00b10.12",
                    "40.00\u00b10.10"
                ],
                [
                    "Proto (BERT)*",
                    "80.68\u00b10.28",
                    "67.92\u00b10.31",
                    "58.22\u00b10.20",
                    "40.64\u00b10.14"
                ],
                [
                    "Proto (BERT)",
                    "81.65\u00b10.97",
                    "70.02\u00b10.23",
                    "61.08\u00b10.28",
                    "45.94\u00b10.50"
                ],
                [
                    "BERT-PAIR*",
                    "88.32\u00b10.64",
                    "73.60\u00b10.51",
                    "63.00\u00b10.47",
                    "43.99\u00b10.09"
                ],
                [
                    "BERT-PAIR",
                    "76.73\u00b10.55",
                    "77.67\u00b10.14",
                    "78.49\u00b10.21",
                    "80.31\u00b10.12"
                ]
            ],
            "title": "Table 3: Accuracies (%) on few-shot NOTA. Models with * simply ignore the NOTA setting and assume all queries can be classified as one of the given relations."
        },
        "insight": "All models are trained given 50% NOTA queries and tested under four different NOTA rates: 0%, 15%, 30%, 50%. [CONTINUE] For detailed numbers of results on fewshot NOTA, please refer to Table 3. [CONTINUE] (1) Treating NOTA as the [CONTINUE] + 1 relation is beneficial for handling Few-Shot NOTA, though the results still fall fast when the NOTA rate increases. (2) BERT-PAIR works better under the NOTA setting for its binary-classification style model, and stays stable with rising NOTA rate. (3) Though BERT-PAIR achieves promising results, huge gaps still exist between the conventional (0% NOTA rate) and NOTA settings (gaps of 8 points for 5-way 1-shot and 7 points for 5way 5-shot with 50% NOTA rate), which calls for further research to address the challenge."
    },
    {
        "id": "239",
        "table": {
            "header": [
                "Typing methods",
                "Wiki/Figer(gold) Acc.",
                "Wiki/Figer(gold) Ma-F1",
                "Wiki/Figer(gold) Mi-F1",
                "OntoNotes Acc.",
                "OntoNotes Ma-F1",
                "OntoNotes Mi-F1",
                "BBN Acc.",
                "BBN Ma-F1",
                "BBN Mi-F1"
            ],
            "rows": [
                [
                    "[BOLD] FIGER*\u00a0",
                    "0.474",
                    "0.692",
                    "0.655",
                    "0.369",
                    "0.578",
                    "0.516",
                    "0.467",
                    "0.672",
                    "0.612"
                ],
                [
                    "[BOLD] HYENA*\u00a0",
                    "0.288",
                    "0.528",
                    "0.506",
                    "0.249",
                    "0.497",
                    "0.446",
                    "0.523",
                    "0.576",
                    "0.587"
                ],
                [
                    "[BOLD] AFET-NoCo*\u00a0",
                    "0.526",
                    "0.693",
                    "0.654",
                    "0.486",
                    "0.652",
                    "0.594",
                    "0.655",
                    "0.711",
                    "0.716"
                ],
                [
                    "[BOLD] AFET-CoH*\u00a0",
                    "0.433",
                    "0.583",
                    "0.551",
                    "0.521",
                    "0.680",
                    "0.609",
                    "0.657",
                    "0.703",
                    "0.712"
                ],
                [
                    "[BOLD] AFET*\u00a0",
                    "0.533",
                    "0.693",
                    "0.664",
                    "0.551",
                    "0.711",
                    "0.647",
                    "0.670",
                    "0.727",
                    "0.735"
                ],
                [
                    "[BOLD] AFET\u2020\u2021\u00a0",
                    "0.509",
                    "0.689",
                    "0.653",
                    "[BOLD] 0.553",
                    "[BOLD] 0.712",
                    "[BOLD] 0.646",
                    "0.683",
                    "0.744",
                    "0.747"
                ],
                [
                    "[BOLD] Attentive\u2020\u00a0",
                    "0.581",
                    "0.780",
                    "0.744",
                    "0.473",
                    "0.655",
                    "0.586",
                    "0.484",
                    "0.732",
                    "0.724"
                ],
                [
                    "[BOLD] our-AllC\u2020",
                    "[BOLD] 0.662",
                    "0.805",
                    "0.770",
                    "0.514",
                    "0.672",
                    "0.626",
                    "0.655",
                    "0.736",
                    "0.752"
                ],
                [
                    "[BOLD] our-NoM\u2020",
                    "0.646",
                    "0.808",
                    "0.768",
                    "0.521",
                    "0.683",
                    "0.626",
                    "0.615",
                    "0.742",
                    "0.755"
                ],
                [
                    "[BOLD] our\u2020",
                    "0.658",
                    "[BOLD] 0.812",
                    "[BOLD] 0.774",
                    "0.522",
                    "0.685",
                    "0.633",
                    "0.604",
                    "0.741",
                    "0.757"
                ],
                [
                    "[BOLD] model level transfer-learning\u2020",
                    "-",
                    "-",
                    "-",
                    "0.531",
                    "0.684",
                    "0.637",
                    "0.645",
                    "0.784",
                    "[BOLD] 0.795"
                ],
                [
                    "[BOLD] feature level transfer-learning\u2020",
                    "-",
                    "-",
                    "-",
                    "0.471",
                    "0.689",
                    "0.635",
                    "[BOLD] 0.733",
                    "[BOLD] 0.791",
                    "0.792"
                ]
            ],
            "title": "Table 2: Performance analysis of entity classification methods on the three datasets."
        },
        "insight": "These results are shown in table 2 as feature level transfer-learning. [CONTINUE] These results are shown in table 2 as model level transfer learning. [CONTINUE] Table 2 shows the results of the proposed method, its variants and the baseline methods."
    },
    {
        "id": "240",
        "table": {
            "header": [
                "Label type",
                "Support",
                "our Prec.",
                "our Rec.",
                "our F-1",
                "AFET Prec.",
                "AFET Rec.",
                "AFET F-1"
            ],
            "rows": [
                [
                    "[ITALIC] /other",
                    "42.6%",
                    "0.838",
                    "0.809",
                    "0.823",
                    "0.774",
                    "0.962",
                    "[BOLD] 0.858"
                ],
                [
                    "[ITALIC] /organization",
                    "11.0%",
                    "0.588",
                    "0.490",
                    "[BOLD] 0.534",
                    "0.903",
                    "0.273",
                    "0.419"
                ],
                [
                    "[ITALIC] /person",
                    "9.9%",
                    "0.559",
                    "0.467",
                    "[BOLD] 0.508",
                    "0.669",
                    "0.352",
                    "0.461"
                ],
                [
                    "[ITALIC] /organization/company",
                    "7.8%",
                    "0.932",
                    "0.166",
                    "[BOLD] 0.282",
                    "1.0",
                    "0.127",
                    "0.225"
                ],
                [
                    "[ITALIC] /location",
                    "7.5%",
                    "0.687",
                    "0.796",
                    "[BOLD] 0.737",
                    "0.787",
                    "0.609",
                    "0.687"
                ],
                [
                    "[ITALIC] /organization/government",
                    "2.1%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] /location/country",
                    "2.0%",
                    "0.783",
                    "0.614",
                    "[BOLD] 0.688",
                    "0.838",
                    "0.498",
                    "0.625"
                ],
                [
                    "[ITALIC] /other/legal",
                    "1.8%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] /location/city",
                    "1.8%",
                    "0.919",
                    "0.610",
                    "[BOLD] 0.733",
                    "0.816",
                    "0.637",
                    "0.715"
                ],
                [
                    "[ITALIC] /person/political_figure",
                    "1.6%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ]
            ],
            "title": "Table 4: Performance analysis of the proposed model and AFET on top 10 (in terms of type frequency) types present in OntoNotes dataset."
        },
        "insight": "Results are shown in Table 4. [CONTINUE] Compared to AFET, the proposed model performs better in all types except other in the top-10 frequent types."
    },
    {
        "id": "241",
        "table": {
            "header": [
                "feature",
                "cos",
                "f1-neigh",
                "f1-lr",
                "f1-net",
                "type"
            ],
            "rows": [
                [
                    "is_heavy",
                    "0.15",
                    "0.15",
                    "0.17",
                    "0.21",
                    "op"
                ],
                [
                    "is_strong",
                    "0.15",
                    "0.13",
                    "0.13",
                    "0.34",
                    "e"
                ],
                [
                    "is_thin",
                    "0.16",
                    "0",
                    "0.05",
                    "0.1",
                    "vp"
                ],
                [
                    "is_hard",
                    "0.16",
                    "0.15",
                    "0.08",
                    "0.26",
                    "op"
                ],
                [
                    "is_expensive",
                    "0.16",
                    "0",
                    "0.28",
                    "0.37",
                    "e"
                ],
                [
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "[EMPTY]"
                ],
                [
                    "is_black",
                    "0.2",
                    "0.29",
                    "0.23",
                    "0.24",
                    "vp"
                ],
                [
                    "is_electric",
                    "0.21",
                    "0.48",
                    "0.5",
                    "0.69",
                    "vp"
                ],
                [
                    "is_dangerous",
                    "0.21",
                    "0.53",
                    "0.57",
                    "0.59",
                    "e"
                ],
                [
                    "is_colourful",
                    "0.21",
                    "0.14",
                    "0.25",
                    "0.32",
                    "vp"
                ],
                [
                    "is_brown",
                    "0.21",
                    "0.13",
                    "0.22",
                    "0.33",
                    "vp"
                ],
                [
                    "has_a_handle _handles",
                    "0.22",
                    "0.44",
                    "0.57",
                    "0.58",
                    "p"
                ],
                [
                    "has_a_seat _seats",
                    "0.22",
                    "0.43",
                    "0.3",
                    "0.48",
                    "p"
                ],
                [
                    "does_smell _is_smelly",
                    "0.22",
                    "0.08",
                    "0.15",
                    "0.37",
                    "op"
                ],
                [
                    "made_of_glass",
                    "0.22",
                    "0.29",
                    "0",
                    "0.28",
                    "vp"
                ],
                [
                    "has_a_point",
                    "0.23",
                    "0.38",
                    "0.23",
                    "0.47",
                    "p"
                ],
                [
                    "does_protect",
                    "0.24",
                    "0.38",
                    "0.26",
                    "0.37",
                    "f"
                ],
                [
                    "is_yellow",
                    "0.24",
                    "0.22",
                    "0",
                    "0.23",
                    "vp"
                ],
                [
                    "is_soft",
                    "0.24",
                    "0.12",
                    "0",
                    "0.16",
                    "op"
                ],
                [
                    "is_red",
                    "0.25",
                    "0.34",
                    "0.13",
                    "0.27",
                    "vp"
                ],
                [
                    "is_fast",
                    "0.25",
                    "0.3",
                    "0.31",
                    "0.48",
                    "vp"
                ],
                [
                    "is_tall",
                    "0.25",
                    "0.43",
                    "0.57",
                    "0.65",
                    "vp"
                ],
                [
                    "is_a_tool",
                    "0.26",
                    "0.5",
                    "0.51",
                    "0.47",
                    "t"
                ],
                [
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "[EMPTY]"
                ],
                [
                    "is_a_weapon",
                    "0.3",
                    "0.74",
                    "0.56",
                    "0.63",
                    "t"
                ],
                [
                    "is_green",
                    "0.31",
                    "0.45",
                    "0.45",
                    "0.45",
                    "vp"
                ],
                [
                    "has_a_ blade_blades",
                    "0.32",
                    "0.68",
                    "0.65",
                    "0.74",
                    "p"
                ],
                [
                    "is_worn",
                    "0.32",
                    "0.47",
                    "0.86",
                    "0.9",
                    "f"
                ],
                [
                    "has_wheels",
                    "0.32",
                    "0.82",
                    "0.83",
                    "0.87",
                    "p"
                ],
                [
                    "is_found _in_kitchens",
                    "0.33",
                    "0.56",
                    "0.73",
                    "0.76",
                    "e"
                ],
                [
                    "does_fly",
                    "0.33",
                    "0.57",
                    "0.76",
                    "0.76",
                    "f"
                ],
                [
                    "has_a_tail",
                    "0.33",
                    "0.53",
                    "0.68",
                    "0.69",
                    "p"
                ],
                [
                    "is_an_animal",
                    "0.33",
                    "0.64",
                    "0.76",
                    "0.78",
                    "t"
                ],
                [
                    "is_eaten_edible",
                    "0.33",
                    "0.37",
                    "0.88",
                    "0.85",
                    "f"
                ],
                [
                    "has_four_legs",
                    "0.34",
                    "0.67",
                    "0.66",
                    "0.66",
                    "p"
                ],
                [
                    "is_a_vehicle",
                    "0.34",
                    "0.76",
                    "0.69",
                    "0.79",
                    "t"
                ],
                [
                    "does_eat",
                    "0.34",
                    "0.68",
                    "0.71",
                    "0.68",
                    "f"
                ],
                [
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "[EMPTY]"
                ],
                [
                    "has_a_beak",
                    "0.37",
                    "0.63",
                    "0.83",
                    "0.87",
                    "p"
                ],
                [
                    "made_of_cotton",
                    "0.37",
                    "0.68",
                    "0.56",
                    "0.64",
                    "vp"
                ],
                [
                    "has_roots",
                    "0.37",
                    "0.3",
                    "0.65",
                    "0.72",
                    "p"
                ],
                [
                    "is_a_mammal",
                    "0.37",
                    "0.69",
                    "0.85",
                    "0.86",
                    "t"
                ],
                [
                    "does_grow",
                    "0.37",
                    "0.52",
                    "0.81",
                    "0.81",
                    "e"
                ],
                [
                    "is_a_plant",
                    "0.37",
                    "0.43",
                    "0.63",
                    "0.64",
                    "t"
                ],
                [
                    "has_leaves",
                    "0.37",
                    "0.41",
                    "0.71",
                    "0.78",
                    "p"
                ],
                [
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "\u2026",
                    "[EMPTY]"
                ],
                [
                    "has_pips_seeds",
                    "0.47",
                    "0.5",
                    "0.08",
                    "0.46",
                    "p"
                ],
                [
                    "is_juicy",
                    "0.5",
                    "0.71",
                    "0.48",
                    "0.56",
                    "op"
                ],
                [
                    "is_a_vegetable",
                    "0.52",
                    "0.78",
                    "0.75",
                    "0.81",
                    "t"
                ],
                [
                    "is_played _does_play",
                    "0.53",
                    "0.9",
                    "0.98",
                    "0.98",
                    "f"
                ],
                [
                    "does_make_music",
                    "0.55",
                    "0.89",
                    "0.95",
                    "0.92",
                    "f"
                ],
                [
                    "spearman-r",
                    "[EMPTY]",
                    "0.72",
                    "0.52",
                    "0.59",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 2: Performance of different approaches in relation to the average cosine similarity of words associated with a property (cos). The last row shows the Spearman Rank correlation between f1-scores and average cosine similarity. Property types are listed under type (p = part, vp = visual-perceptual, op = other-perceptual, e = encyclopaedic, f = functional, t = taxonomic)."
        },
        "insight": "Table 2 shows a selection of the f1-scores achieved on properties in the CSLB dataset in relation to the average cosine similarity of the associated words. A high average cosine similarity means that the concepts overall have similar vector representations and can thus be seen as having a low diversity. The results of the Spearman Rank correlation clearly indicate that scores achieved by nearest neighbors correlate more strongly with the average cosine than the two supervised classification approaches."
    },
    {
        "id": "242",
        "table": {
            "header": [
                "[BOLD] Network",
                "[BOLD] Training Time",
                "[BOLD] Parameters"
            ],
            "rows": [
                [
                    "[BOLD] bilstm",
                    "5h 30m",
                    "1,278M"
                ],
                [
                    "[BOLD] bilstm-att",
                    "8h 30m",
                    "1,279M"
                ],
                [
                    "[BOLD] x-bilstm-att",
                    "25h 40m",
                    "1,279M"
                ],
                [
                    "[BOLD] h-bilstm-att",
                    "2h 30m",
                    "1,837M"
                ]
            ],
            "title": "Table 4: Training times and parameters to learn."
        },
        "insight": "H-BILSTM-ATT is also much faster to train than BILSTM and BILSTM-ATT (Table 4), even though it has more parameters, because it converges faster (5-7 epochs vs. 12-15)."
    },
    {
        "id": "243",
        "table": {
            "header": [
                "property",
                "av-cos",
                "neigh",
                "lr",
                "net1",
                "net2"
            ],
            "rows": [
                [
                    "full_is_yellow",
                    "0.23",
                    "0.19",
                    "0.47",
                    "0.64",
                    "0.64"
                ],
                [
                    "full_is_used_in _cooking",
                    "0.37",
                    "0.29",
                    "0.98",
                    "0.98",
                    "0.98"
                ],
                [
                    "full_is_black",
                    "0.19",
                    "0.35",
                    "0.75",
                    "0.77",
                    "0.77"
                ],
                [
                    "full_is_red",
                    "0.23",
                    "0.36",
                    "0.51",
                    "0.54",
                    "0.52"
                ],
                [
                    "full_is_dangerous",
                    "0.24",
                    "0.58",
                    "0.88",
                    "0.88",
                    "0.87"
                ],
                [
                    "crowd_is_dangerous",
                    "0.26",
                    "0.61",
                    "0.86",
                    "0.86",
                    "0.86"
                ],
                [
                    "full_has_wheels",
                    "0.38",
                    "0.90",
                    "0.96",
                    "0.96",
                    "0.95"
                ],
                [
                    "full_is_found_in_seas",
                    "0.44",
                    "0.87",
                    "0.97",
                    "0.98",
                    "0.98"
                ],
                [
                    "crowd_is_found _in_seas",
                    "0.50",
                    "0.87",
                    "0.94",
                    "0.96",
                    "0.96"
                ],
                [
                    "full_does_kill",
                    "0.27",
                    "0.67",
                    "0.83",
                    "0.86",
                    "0.82"
                ],
                [
                    "crowd_does_kill",
                    "0.30",
                    "0.70",
                    "0.82",
                    "0.84",
                    "0.80"
                ],
                [
                    "full_made_of_wood",
                    "0.17",
                    "0.14",
                    "0.84",
                    "0.85",
                    "0.85"
                ],
                [
                    "full_is_food_test",
                    "0.37",
                    "0.00",
                    "0.36",
                    "0.36",
                    "0.36"
                ],
                [
                    "full_is_an _animal_test",
                    "0.37",
                    "0.52",
                    "0.88",
                    "0.88",
                    "0.88"
                ]
            ],
            "title": "Table 4: F1 scores achieved by logistic regression (lr) two runs of a neural net classifier (net1 and net2 and the n-best nearest neighbors evaluated with leave-one-out on the full datasets (marked as full_ and the crow-only sets (marked as crowd_)."
        },
        "insight": "Table 4 shows the f1-scores on the full clean datasets. [CONTINUE] For polysemy between food and animals (Table 4), we observe that when trained on pure animal and food words and tested on polysemous animal and food words, the classifiers perform highly with a large difference to nearest neighbors."
    },
    {
        "id": "244",
        "table": {
            "header": [
                "SNLI",
                "w",
                "[BOLD] MEN 71.78",
                "[BOLD] MTurk287 35.40",
                "[BOLD] MTurk771 49.05",
                "[BOLD] RG65 61.80",
                "[BOLD] RW 18.43",
                "[BOLD] SimLex999 19.17",
                "[BOLD] SimVerb3500 10.32",
                "[BOLD] WS353 39.27",
                "[BOLD] WS353R 28.01",
                "[BOLD] WS353S 53.42"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "c",
                    "9.85",
                    "-5.65",
                    "0.82",
                    "-5.28",
                    "17.81",
                    "0.86",
                    "2.76",
                    "-2.20",
                    "0.20",
                    "-3.87"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "71.91",
                    "[BOLD] 35.52",
                    "48.84",
                    "62.12",
                    "18.46",
                    "19.10",
                    "10.21",
                    "39.35",
                    "28.16",
                    "53.40"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "70.49",
                    "34.49",
                    "46.15",
                    "59.75",
                    "18.24",
                    "17.20",
                    "8.73",
                    "35.86",
                    "23.48",
                    "50.83"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "[BOLD] 80.00",
                    "32.54",
                    "[BOLD] 62.09",
                    "[BOLD] 68.90",
                    "[BOLD] 20.76",
                    "[BOLD] 37.70",
                    "[BOLD] 20.45",
                    "[BOLD] 54.72",
                    "[BOLD] 47.24",
                    "[BOLD] 65.60"
                ],
                [
                    "MNLI",
                    "w",
                    "68.76",
                    "50.15",
                    "68.81",
                    "65.83",
                    "18.43",
                    "42.21",
                    "25.18",
                    "61.10",
                    "58.21",
                    "70.17"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "4.84",
                    "0.06",
                    "1.95",
                    "-0.06",
                    "12.18",
                    "3.01",
                    "1.52",
                    "-4.68",
                    "-3.63",
                    "-3.65"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "68.77",
                    "50.40",
                    "68.77",
                    "65.92",
                    "18.35",
                    "42.22",
                    "25.12",
                    "61.15",
                    "58.26",
                    "70.21"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "67.66",
                    "49.58",
                    "68.29",
                    "64.84",
                    "18.36",
                    "41.81",
                    "24.57",
                    "60.13",
                    "57.09",
                    "69.41"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "[BOLD] 76.69",
                    "[BOLD] 56.06",
                    "[BOLD] 70.13",
                    "[BOLD] 69.00",
                    "[BOLD] 25.35",
                    "[BOLD] 48.40",
                    "[BOLD] 35.12",
                    "[BOLD] 68.91",
                    "[BOLD] 64.70",
                    "[BOLD] 77.23"
                ]
            ],
            "title": "Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models initialized with different random seeds. Correlations were scaled to the [\u2212100;100] range for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. For each task and dataset, every best-performing method was significantly different to other methods (p<0.05), except for w trained in SNLI at the MTurk287 task. Statistical significance was obtained with a two-sided Welch\u2019s t-test for two independent samples without assuming equal variance\u00a0(Welch, 1947)."
        },
        "insight": "Table 1 shows the quality of word representations in terms of the correlation between word similarity [CONTINUE] scores obtained by the proposed models and word similarity scores defined by humans. [CONTINUE] for that can see First, we each task, character only models had significantly worse performance than every other model trained on the same dataset. [CONTINUE] Further, bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset. [CONTINUE] Additionally, results from the MNLI row in general, and underlined results in particular, show that training on MultiNLI produces word representations better at capturing word similarity. [CONTINUE] Exceptions to the previous rule are models evaluated in MEN and RW. [CONTINUE] More notably, in the RareWords dataset (Luthe word only, concat, ong et al., 2013), and scalar gate methods performed equally, despite having been trained in different datasets (p > 0.1), and the char only method performed significantly worse when trained in MultiNLI. [CONTINUE] The vector gate, however, performed significantly better than its counterpart trained in SNLI."
    },
    {
        "id": "245",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Classification  [BOLD] CR",
                "Classification  [BOLD] MPQA",
                "Classification  [BOLD] MR",
                "Classification  [BOLD] SST2",
                "Classification  [BOLD] SST5",
                "Classification  [BOLD] SUBJ",
                "Classification  [BOLD] TREC",
                "Entailment  [BOLD] SICKE",
                "Relatedness  [BOLD] SICKR\u2020",
                "Semantic Textual Similarity  [BOLD] STS16\u2020",
                "Semantic Textual Similarity  [BOLD] STSB\u2020"
            ],
            "rows": [
                [
                    "SNLI",
                    "w",
                    "80.50",
                    "84.59",
                    "74.18",
                    "78.86",
                    "42.33",
                    "[BOLD] 90.38",
                    "[BOLD] 86.83",
                    "86.37",
                    "88.52",
                    "59.90\u2217",
                    "71.29\u2217"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "74.90\u2217",
                    "78.86\u2217",
                    "65.93\u2217",
                    "69.42\u2217",
                    "35.56\u2217",
                    "82.97\u2217",
                    "83.31\u2217",
                    "84.13\u2217",
                    "83.89\u2217",
                    "59.33\u2217",
                    "67.20\u2217"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "80.44",
                    "84.66",
                    "74.31",
                    "78.37",
                    "41.34\u2217",
                    "90.28",
                    "85.80\u2217",
                    "[BOLD] 86.40",
                    "88.44",
                    "59.90\u2217",
                    "71.24\u2217"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "[BOLD] 80.59",
                    "84.60",
                    "[BOLD] 74.49",
                    "[BOLD] 79.04",
                    "41.63\u2217",
                    "90.16",
                    "86.00",
                    "86.10\u2217",
                    "[BOLD] 88.57",
                    "60.05\u2217",
                    "71.34\u2217"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "80.42",
                    "[BOLD] 84.66",
                    "74.26",
                    "78.87",
                    "[BOLD] 42.38",
                    "90.07",
                    "85.97",
                    "85.67",
                    "88.31\u2217",
                    "[BOLD] 60.92",
                    "[BOLD] 71.99"
                ],
                [
                    "MNLI",
                    "w",
                    "83.80",
                    "[BOLD] 89.13",
                    "79.05",
                    "83.38",
                    "45.21",
                    "91.79",
                    "89.23",
                    "84.92",
                    "86.33",
                    "66.08",
                    "71.96\u2217"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "70.23\u2217",
                    "72.19\u2217",
                    "62.83\u2217",
                    "64.55\u2217",
                    "32.47\u2217",
                    "79.49\u2217",
                    "74.74\u2217",
                    "81.53\u2217",
                    "75.92\u2217",
                    "51.47\u2217",
                    "61.74\u2217"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "[BOLD] 83.96",
                    "89.12",
                    "[BOLD] 79.23",
                    "83.70",
                    "45.08\u2217",
                    "[BOLD] 91.92",
                    "[BOLD] 90.03",
                    "[BOLD] 85.06",
                    "86.45",
                    "[BOLD] 66.17",
                    "71.82\u2217"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "83.88",
                    "89.06",
                    "79.22",
                    "83.71",
                    "45.26",
                    "91.66\u2217",
                    "88.83\u2217",
                    "84.96",
                    "86.40",
                    "65.49\u2217",
                    "71.87\u2217"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "83.45\u2217",
                    "89.05",
                    "79.13",
                    "[BOLD] 83.87",
                    "[BOLD] 45.88",
                    "91.55\u2217",
                    "89.49",
                    "84.82",
                    "[BOLD] 86.50",
                    "65.75",
                    "[BOLD] 72.82"
                ]
            ],
            "title": "Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by \u2020, in which case they represent Pearson correlation scaled to the range [\u2212100,100] for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. Values marked with an asterisk (\u2217) are significantly different to the average performance of the best model trained on the same dataset (p<0.05). Results for every best-performing method trained on one dataset are significantly different to the best-performing method trained on the other. Statistical significance was obtained in the same way as described in table 1."
        },
        "insight": "Table 2 shows the impact that different methods for combining character and word-level word representations have in the quality of the sentence representations produced by our models. [CONTINUE] We can observe the same trend mentioned in section 4.1, and highlighted by the difference between bold values, that models trained in MultiNLI performed better than those trained in [CONTINUE] SNLI at a statistically significant level, [CONTINUE] The two exceptions to the previous trend, SICKE and SICKR, benefited more from models trained on SNLI. [CONTINUE] Additionally, there was no method that significantly outperformed the word only baseline in classification tasks. [CONTINUE] On the other hand, the vector gate significantly outperformed every other method in the STSB task when trained in both datasets, and in the STS16 task when trained in SNLI."
    },
    {
        "id": "246",
        "table": {
            "header": [
                "System",
                "\u201cGotcha\u201d?",
                "Female",
                "Male"
            ],
            "rows": [
                [
                    "Rule",
                    "no",
                    "38.3",
                    "51.7"
                ],
                [
                    "Rule",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB10.0",
                    "[HTML]CBCEFB37.5"
                ],
                [
                    "Stat",
                    "no",
                    "50.8",
                    "61.7"
                ],
                [
                    "Stat",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB45.8",
                    "[HTML]CBCEFB40.0"
                ],
                [
                    "Neural",
                    "no",
                    "50.8",
                    "49.2"
                ],
                [
                    "Neural",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB36.7",
                    "[HTML]CBCEFB46.7"
                ]
            ],
            "title": "Table 2: System accuracy (%) bucketed by gender and difficulty (so-called \u201cgotchas,\u201d shaded in purple). For female pronouns, a \u201cgotcha\u201d sentence is one where either (1) the correct answer is occupation but the occupation is <50% female (according to BLS); or (2) the occupation is \u226550% female but the correct answer is participant; this is reversed for male pronouns. Systems do uniformly worse on \u201cgotchas.\u201d"
        },
        "insight": "We also identify so-called \"gotcha\" sentences in which pronoun gender does not match the occupation's majority gender (BLS) if OCCUPATION is the correct answer; all systems perform worse on these \"gotchas.\"8 (See Table 2.)"
    },
    {
        "id": "247",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] B-1",
                "[BOLD] B-2",
                "[BOLD] B-3",
                "[BOLD] B-4",
                "[BOLD] ROUGE-L",
                "[BOLD] METEOR",
                "[BOLD] CIDEr"
            ],
            "rows": [
                [
                    "Facts-to-seq",
                    "0.404",
                    "0.324",
                    "0.274",
                    "0.242",
                    "0.433",
                    "0.214",
                    "1.627"
                ],
                [
                    "Facts-to-seq w. Attention",
                    "0.491",
                    "0.414",
                    "0.366",
                    "0.335",
                    "0.512",
                    "0.257",
                    "2.207"
                ],
                [
                    "Static Memory",
                    "0.374",
                    "0.298",
                    "0.255",
                    "0.223",
                    "0.383",
                    "0.185",
                    "1.328"
                ],
                [
                    "DMN+",
                    "0.281",
                    "0.234",
                    "0.236",
                    "0.234",
                    "0.275",
                    "0.139",
                    "0.912"
                ],
                [
                    "Our Model",
                    "[BOLD] 0.611",
                    "[BOLD] 0.535",
                    "[BOLD] 0.485",
                    "[BOLD] 0.461",
                    "[BOLD] 0.641",
                    "[BOLD] 0.353",
                    "[BOLD] 3.295"
                ]
            ],
            "title": "Table 1: Automatic evaluation results of different models. For a detailed explanation of the baseline models, please refer to Section 3.2. The best performing model for each column is highlighted in boldface."
        },
        "insight": "The results of the experiments are reported in Table 1. [CONTINUE] A facts-to-seq model exploiting our positional fact encoding performs adequately. With an additional attention mechanism (Facts-to-seq w. Attention), the results are even better. [CONTINUE] The analysis of the Static Memory approach amounts to an ablation study, [CONTINUE] the DMN+ is even outperformed by our Facts-to-seq baseline."
    },
    {
        "id": "248",
        "table": {
            "header": [
                "Model",
                "Val",
                "Test",
                "Hard"
            ],
            "rows": [
                [
                    "Baseline",
                    "84.25",
                    "84.22",
                    "68.02"
                ],
                [
                    "AdvCls",
                    "84.58",
                    "83.56",
                    "66.27"
                ],
                [
                    "AdvDat",
                    "78.45",
                    "78.30",
                    "55.60"
                ]
            ],
            "title": "Table 2: Accuracies for the approaches. Baseline refers to the unmodified, non-adversarial InferSent."
        },
        "insight": "Table 2 reports the results on SNLI, with the configurations that performed best on the validation set for each of the adversarial methods. [CONTINUE] both training methods perform worse than our unmodified, non-adversarial InferSent baseline on SNLI's test set, since they remove biases that may be useful for performing this task. The difference for AdvCls is minimal, and it even slightly outperforms InferSent on the validation set. While AdvDat's results are noticeably lower than the non-adversarial InferSent, the drops are still less than 6% points."
    },
    {
        "id": "249",
        "table": {
            "header": [
                "Word",
                "Count",
                "Score ^ [ITALIC] p( [ITALIC] l| [ITALIC] w)",
                "Score Baseline",
                "Percentage decrease from baseline AdvCls (1,1)",
                "Percentage decrease from baseline AdvDat (0.4,1)",
                "Percentage decrease from baseline AdvDat (1,1)"
            ],
            "rows": [
                [
                    "sleeping",
                    "108",
                    "0.88",
                    "0.24",
                    "15.63",
                    "53.13",
                    "-81.25"
                ],
                [
                    "driving",
                    "53",
                    "0.81",
                    "0.32",
                    "-8.33",
                    "50",
                    "-66.67"
                ],
                [
                    "Nobody",
                    "52",
                    "1",
                    "0.42",
                    "14.29",
                    "42.86",
                    "14.29"
                ],
                [
                    "alone",
                    "50",
                    "0.9",
                    "0.32",
                    "0",
                    "83.33",
                    "0"
                ],
                [
                    "cat",
                    "49",
                    "0.84",
                    "0.31",
                    "7.14",
                    "57.14",
                    "-85.71"
                ],
                [
                    "asleep",
                    "43",
                    "0.91",
                    "0.39",
                    "-18.75",
                    "50",
                    "12.5"
                ],
                [
                    "no",
                    "31",
                    "0.84",
                    "0.36",
                    "0",
                    "52.94",
                    "-52.94"
                ],
                [
                    "empty",
                    "28",
                    "0.93",
                    "0.3",
                    "-16.67",
                    "83.33",
                    "-16.67"
                ],
                [
                    "eats",
                    "24",
                    "0.83",
                    "0.3",
                    "37.5",
                    "87.5",
                    "-25"
                ],
                [
                    "naked",
                    "20",
                    "0.95",
                    "0.46",
                    "0",
                    "83.33",
                    "-33.33"
                ]
            ],
            "title": "Table 3: Indicator words and how correlated they are with CONTRADICTION predictions. The parentheses indicate hyper-parameter values: (\u03bbLoss,\u03bbEnc) for AdvCls and (\u03bbRand,\u03bbEnc) for AdvDat. Baseline refers to the unmodified InferSent."
        },
        "insight": "For each of the most biased words in SNLI associated with the CONTRADICTION label, we computed the probability that a model predicts an example as a contradiction, given that the hypothesis contains the word. Table 3 shows the top 10 examples in the training set. For each word w, we give its frequency in SNLI, its empirical correlation with the label and with InferSent's prediction, and the percentage decrease in correlations with CONTRADICTION predictions by three configurations of our methods. Generally, the baseline correlations are more uniform than the empirical ones (\u02c6p(l|w)), [CONTINUE] However, we still observed small skews towards CONTRADICTION. Thus, we investigate whether our methods reduce the probability of predicting CONTRADICTION when a hypothesis contains an indicator word. The model trained with AdvDat (where \u03bbRand = 0.4, \u03bbEnc = 1) predicts contradiction much less frequently than InferSent on examples with these words."
    },
    {
        "id": "250",
        "table": {
            "header": [
                "Model",
                "amh",
                "ara",
                "ben",
                "fas",
                "hin",
                "hun",
                "orm",
                "rus",
                "som",
                "tgl",
                "tir",
                "uig",
                "yor",
                "avg"
            ],
            "rows": [
                [
                    "Cap.",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SRILM",
                    "43.9",
                    "30.9",
                    "59.8",
                    "38.2",
                    "50.1",
                    "64.3",
                    "50.1",
                    "53.1",
                    "68.3",
                    "68.0",
                    "62.5",
                    "35.5",
                    "66.5",
                    "53.1"
                ],
                [
                    "Skip-gram",
                    "25.9",
                    "10.5",
                    "20.4",
                    "15.4",
                    "16.1",
                    "46.8",
                    "31.5",
                    "36.4",
                    "36.2",
                    "44.3",
                    "35.3",
                    "13.4",
                    "43.0",
                    "28.9"
                ],
                [
                    "CBOW",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BiLSTM",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 1: NEI F1 scores. There are two language models: one for entity, and one for non-entity. All LMs are trained on the standard Train split. At test time, the tag associated with the language model which gives the lowest perplexity is used as prediction. Consecutive tokens with the same tag are joined into a phrase. Scores are Phrase-level F1."
        },
        "insight": "Our datasets vary in size of entity and non-entity tokens, as shown in Table 1. The smallest, Farsi, has 4.5K entity and 50K non-entity tokens; the largest, English, has 29K entity and 170K nonentity tokens."
    },
    {
        "id": "251",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] #D",
                "[BOLD] #L",
                "[BOLD] #T",
                "[BOLD] #S"
            ],
            "rows": [
                [
                    "MUC 4",
                    "1700",
                    "400",
                    "4",
                    "4"
                ],
                [
                    "ACE 2005",
                    "599",
                    "599",
                    "33",
                    "36"
                ],
                [
                    "ERE",
                    "562",
                    "562",
                    "38",
                    "27"
                ],
                [
                    "ASTRE",
                    "1038",
                    "100",
                    "12",
                    "18"
                ],
                [
                    "[BOLD] GNBusiness",
                    "12,985",
                    "680",
                    "\u2013",
                    "8"
                ]
            ],
            "title": "Table 2: Comparison with existing datasets. (D documents or news clusters; L labeled documents or news clusters; T event types; S slots.)"
        },
        "insight": "a comparison with existing event extraction and event schema induction datasets, including ASTRE (Nguyen et al., 2016a), MUC 4, ACE 20052 and ERE3, is shown in Table 2. Compared with the other datasets, GNBusiness has a much larger number of documents (i.e., news clusters in GNBusiness), and a comparable number of labeled documents."
    },
    {
        "id": "252",
        "table": {
            "header": [
                "[ITALIC] \u03bb",
                "unpreserved BLEU",
                "unpreserved DAL",
                "preserved BLEU",
                "preserved DAL"
            ],
            "rows": [
                [
                    "0.0",
                    "27.7",
                    "21.0",
                    "27.7",
                    "27.9"
                ],
                [
                    "0.1",
                    "27.0",
                    "13.6",
                    "27.6",
                    "10.5"
                ],
                [
                    "0.2",
                    "25.7",
                    "11.6",
                    "27.5",
                    "8.7"
                ]
            ],
            "title": "Table 3: Varying MILk\u2019s \u03bb with and without mass preservation on the DeEn development set."
        },
        "insight": "Before preservation, MILk with a latency weight \u03bb = 0 still showed a substantial reduction in latency from the maximum value of 27.9, indicating an intrinsic latency incentive. Furthermore, training quickly destabilized, resulting in very poor trade-offs for \u03bbs as low as 0.2."
    },
    {
        "id": "253",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Schema Matching (%)  [ITALIC] P",
                "[BOLD] Schema Matching (%)  [ITALIC] R",
                "[BOLD] Schema Matching (%)  [ITALIC] F1"
            ],
            "rows": [
                [
                    "DBLP:conf/acl/NguyenTFB15",
                    "41.5",
                    "53.4",
                    "46.7"
                ],
                [
                    "Clustering",
                    "41.2",
                    "50.6",
                    "45.4"
                ],
                [
                    "ODEE-F",
                    "41.7",
                    "53.2",
                    "46.8"
                ],
                [
                    "ODEE-FE",
                    "42.4",
                    "56.1",
                    "48.3"
                ],
                [
                    "ODEE-FER",
                    "[BOLD] 43.4",
                    "[BOLD] 58.3",
                    "[BOLD] 49.8"
                ]
            ],
            "title": "Table 4: Overall performance of schema matching."
        },
        "insight": "Table 4 shows the overall performance of schema matching on GNBusinessTest. From the table, we can see that ODEE-FER achieves the best F1 scores among all the methods. By comparing Nguyen et al. (2015) and ODEEF (p = 0.01), we can see that using continuous contextual features gives better performance than discrete features. [CONTINUE] Among ODEE models, ODEE-FE gives a 2% gain in F1 score against ODEE-F, [CONTINUE] there is a 1% gain in F"
    },
    {
        "id": "254",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Ave Slot Coherence"
            ],
            "rows": [
                [
                    "DBLP:conf/acl/NguyenTFB15",
                    "0.10"
                ],
                [
                    "ODEE-F",
                    "0.10"
                ],
                [
                    "ODEE-FE",
                    "0.16"
                ],
                [
                    "ODEE-FER",
                    "[BOLD] 0.18"
                ]
            ],
            "title": "Table 5: Averaged slot coherence results."
        },
        "insight": "Table 5 shows the comparison of averaged slot coherence results over all the slots in the schemas. [CONTINUE] The averaged slot coherence of ODEE-FER is the highest, [CONTINUE] The averaged slot coherence [CONTINUE] of ODEE-F is comparable to that of Nguyen et al. (2015) (p = 0.3415), [CONTINUE] The scores of ODEE-FE (p = 0.06) and ODEE-FER (p = 10\u22125) are both higher than that of ODEE-F,"
    },
    {
        "id": "255",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] METEOR",
                "[BOLD] ROUGE-L",
                "[BOLD] HUMAN  [BOLD] PREFER-"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[BOLD] ENCE"
                ],
                [
                    "LSTM-LM",
                    "8.7",
                    "15.1",
                    "0"
                ],
                [
                    "Seq2seq",
                    "13.5",
                    "19.2",
                    "22"
                ],
                [
                    "ED(1)",
                    "13.3",
                    "[BOLD] 20.3",
                    "30"
                ],
                [
                    "ED(2)",
                    "[BOLD] 14.0",
                    "19.8",
                    "[BOLD] 48"
                ]
            ],
            "title": "Table 2: Method Comparison (%)."
        },
        "insight": "Table 2 presents METEOR (Denkowski and Lavie, 2014) and ROUGE-L (Lin, 2004) scores for each method, where we can see score gains on [CONTINUE] both metrics from the Editing Mechanism. [CONTINUE] Table 2 shows that the human judges strongly favor the abstracts from our ED(2) method."
    },
    {
        "id": "256",
        "table": {
            "header": [
                "[ITALIC] n",
                "[BOLD] 1",
                "[BOLD] 2",
                "[BOLD] 3",
                "[BOLD] 4",
                "[BOLD] 5",
                "[BOLD] 6"
            ],
            "rows": [
                [
                    "[BOLD] System",
                    "100",
                    "94.4",
                    "67.3",
                    "35.0",
                    "15.9",
                    "6.6"
                ],
                [
                    "[BOLD] Human",
                    "98.2",
                    "78.5",
                    "42.2",
                    "17.9",
                    "7.7",
                    "4.1"
                ]
            ],
            "title": "Table 3: Plagiarism Check: Percentage (%) of n-grams in test abstracts generated by system/human which appeared in training data."
        },
        "insight": "We also conduct a plagiarism check in Table 3, which shows that 93.4% of 6-grams generated by ED(2) did not appear in the training data, indicating that our model is not simply copying."
    },
    {
        "id": "257",
        "table": {
            "header": [
                "[ITALIC] n",
                "[BOLD] 1",
                "[BOLD] 2",
                "[BOLD] 3",
                "[BOLD] 4",
                "[BOLD] 5",
                "[BOLD] 6"
            ],
            "rows": [
                [
                    "METEOR",
                    "13.3",
                    "[BOLD] 14.0",
                    "13.6",
                    "13.9",
                    "13.8",
                    "13.5"
                ],
                [
                    "ROUGE-L",
                    "[BOLD] 20.3",
                    "19.8",
                    "18.6",
                    "19.2",
                    "18.9",
                    "18.8"
                ]
            ],
            "title": "Table 5: Iteration comparison (%)"
        },
        "insight": "We trained and evaluated our editing approach with 1-6 iterations and the experimental results (Table 5) showed that the second iteration produced the best results."
    },
    {
        "id": "258",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] # Tests",
                "[BOLD] # Choices  [BOLD] per Test",
                "[BOLD] Non-expert  [BOLD] Non-CS",
                "[BOLD] Non-expert  [BOLD] CS",
                "[BOLD] NLP Expert  [BOLD] Junior",
                "[BOLD] NLP Expert  [BOLD] Senior"
            ],
            "rows": [
                [
                    "Different Titles",
                    "50",
                    "2",
                    "30%",
                    "15%",
                    "12%",
                    "0%"
                ],
                [
                    "Different Titles",
                    "20",
                    "5",
                    "60%",
                    "20%",
                    "30%",
                    "20%"
                ],
                [
                    "Different Titles",
                    "10",
                    "10",
                    "80%",
                    "30%",
                    "30%",
                    "20%"
                ],
                [
                    "Same Title",
                    "50",
                    "2",
                    "54%",
                    "10%",
                    "4%",
                    "0%"
                ],
                [
                    "Same Title",
                    "20",
                    "5",
                    "75%",
                    "25%",
                    "5%",
                    "5%"
                ]
            ],
            "title": "Table 4: Turing Test Passing Rates."
        },
        "insight": "As expected, Table 4 shows that people with less domain knowledge are more easily deceived. Specifically, non-CS human judges fail at more than half of the 1-to-1 sets for the same titles, which suggests that most of our system generated abstracts follow correct grammar and consistent writing style. Domain experts fail on 1 or 2 sets, mostly because the human written abstracts in those sets don't seem very topically relevant. Additionally, the more abstracts that we provided to human judges, the easier it is to conceal the system generated abstract amongst human generated ones."
    },
    {
        "id": "259",
        "table": {
            "header": [
                "[BOLD] CC",
                "[BOLD] S",
                "[BOLD] S+P",
                "[BOLD] S+I",
                "[BOLD] S+P+I"
            ],
            "rows": [
                [
                    "Random",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "Vocal",
                    "0.125",
                    "0.149",
                    "0.119",
                    "[BOLD] 0.153"
                ],
                [
                    "Visual",
                    "0.092",
                    "0.109",
                    "[BOLD] 0.116",
                    "0.106"
                ],
                [
                    "Verbal",
                    "0.404",
                    "[BOLD] 0.455",
                    "0.434",
                    "0.417"
                ],
                [
                    "Human",
                    "0.820",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[BOLD] MAE",
                    "[BOLD] S",
                    "[BOLD] S+P",
                    "[BOLD] S+I",
                    "[BOLD] S+P+I"
                ],
                [
                    "Random",
                    "1.880",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "Vocal",
                    "1.456",
                    "1.471",
                    "1.444",
                    "[BOLD] 1.431"
                ],
                [
                    "Visual",
                    "1.442",
                    "[BOLD] 1.439",
                    "1.453",
                    "1.460"
                ],
                [
                    "Verbal",
                    "1.196",
                    "[BOLD] 1.156",
                    "1.181",
                    "1.206"
                ],
                [
                    "Human",
                    "0.710",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ]
            ],
            "title": "Table 2: Unimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results on each modality."
        },
        "insight": "The results of unimodal sentiment prediction experiments are shown in Table 2. [CONTINUE] The verbal models have the best performance here, [CONTINUE] On each modality, the best performance is achieved by a multi-task learning model. [CONTINUE] All unimodal models have significantly different performance. p = 0.009 for S+P and S+P+I Visual models, p << 0.001 for Visual and Vocal S+I models. [CONTINUE] In multi-task learning, the main task gains additional information from the auxillary tasks. Compared to the S model, the S+P model has increased focus on the polarity of sentiment, while the S+I model has increased focus on the intensity of sentiment. On the verbal modality, the S+P model achieved the best performance, while on the visual modality the S+I model achieved the best performance. [CONTINUE] For the vocal modality, the S+P+I model achieved the best performance, and the S+P model yielded improved performance over that of the S model."
    },
    {
        "id": "260",
        "table": {
            "header": [
                "[BOLD] CC",
                "[BOLD] S",
                "[BOLD] S+P",
                "[BOLD] S+I",
                "[BOLD] S+P+I"
            ],
            "rows": [
                [
                    "Random",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "EF",
                    "0.471",
                    "0.472",
                    "0.476",
                    "[BOLD] 0.482"
                ],
                [
                    "TFN",
                    "0.448",
                    "[BOLD] 0.461",
                    "0.446",
                    "0.429"
                ],
                [
                    "LF",
                    "[BOLD] 0.454",
                    "0.413",
                    "0.428",
                    "0.428"
                ],
                [
                    "HF",
                    "[BOLD] 0.469",
                    "0.424",
                    "0.458",
                    "0.432"
                ],
                [
                    "Human",
                    "0.820",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[BOLD] MAE",
                    "[BOLD] S",
                    "[BOLD] S+P",
                    "[BOLD] S+I",
                    "[BOLD] S+P+I"
                ],
                [
                    "Random",
                    "1.880",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "EF",
                    "1.197",
                    "1.181",
                    "1.193",
                    "[BOLD] 1.172"
                ],
                [
                    "TFN",
                    "1.186",
                    "1.181",
                    "[BOLD] 1.178",
                    "1.205"
                ],
                [
                    "LF",
                    "[BOLD] 1.179",
                    "1.211",
                    "1.204",
                    "1.201"
                ],
                [
                    "HF",
                    "[BOLD] 1.155",
                    "1.211",
                    "1.164",
                    "1.187"
                ],
                [
                    "Human",
                    "0.710",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ]
            ],
            "title": "Table 3: Multimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results for each fusion strategy in each row."
        },
        "insight": "of results the multimodal The experiments are shown in Table 3. [CONTINUE] We find that EF>HF>TFN>LF. [CONTINUE] 4 [CONTINUE] Unlike Zadeh et al. (2017), here the EF model outperforms the TFN model. However, the TFN model achieved the best performance on the training and validation sets. [CONTINUE] Compared to the feature concatenation used in EF, the Cartesian product used in TFN results in higher dimensionality of the multimodal input vector,5 which in turn increases the complexity of the model. Similarly, the HF model has worse performance than the EF model here, unlike in Tian et al. (2016). [CONTINUE] In general, the multimodal models have better performance than the unimodal models. [CONTINUE] In fact, the HF and LF models have better performance using single-task learning. For the TFN models, only the S+P model outperforms the S model, although the improvement is not significant.7 For the EF models, multi-task learning results in better performance. [CONTINUE] Dimension of the EF input is 420, for TFN is 65,536. 6Except that the LF models often have worse performance than the verbal S+P model. p << 0.001 for TFN S+P and verbal S+P, p = 0.017 for verbal S+P and LF S. 7p = 0.105 for S TFN and S+P TFN. 8p = 0.888 for S EF and S+P EF, p = 0.029 for S EF and S+I EF, p = 0.009 for S EF and S+P+I EF."
    },
    {
        "id": "261",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Accuracy dev",
                "[BOLD] Accuracy test"
            ],
            "rows": [
                [
                    "+PMC",
                    "80.50",
                    "78.97"
                ],
                [
                    "+PubMedd",
                    "81.14",
                    "78.83"
                ],
                [
                    "+PubMed+PMC",
                    "[BOLD] 82.15",
                    "[BOLD] 79.04"
                ]
            ],
            "title": "Table 2: The BioBERT performance on the MedNLI task. Each model is trained on three different combinations of PMC and PubMed datasets (top score marked as bold)."
        },
        "insight": "As shown in table 2, BioBERT trained on PubMed+PMC performs the best."
    },
    {
        "id": "262",
        "table": {
            "header": [
                "Topic",
                "BiLSTM",
                "DeAT",
                "BiMPM",
                "RCN (Our)"
            ],
            "rows": [
                [
                    "CC",
                    "68.1\u00b10.6",
                    "70.9\u00b10.7",
                    "71.5\u00b10.6",
                    "[BOLD] 73.0\u00b10.5\u2217"
                ],
                [
                    "HC",
                    "52.5\u00b10.6",
                    "56.9\u00b10.4",
                    "56.4\u00b10.7",
                    "[BOLD] 58.6\u00b10.4\u2217\u2217"
                ],
                [
                    "FM",
                    "58.3\u00b10.6",
                    "60.6\u00b10,7",
                    "59.8\u00b10.7",
                    "[BOLD] 64.4\u00b10.5\u2217\u2217"
                ],
                [
                    "AT",
                    "67.5\u00b10.4",
                    "69.5\u00b10.5",
                    "70.3\u00b10.6",
                    "[BOLD] 72.2\u00b10.4\u2217"
                ],
                [
                    "LA",
                    "61.3\u00b10.3",
                    "63.2\u00b10.6",
                    "62.4\u00b10.4",
                    "[BOLD] 64.5\u00b10.4\u2217\u2217"
                ],
                [
                    "Two tailed t-test: \u2217\u2217\u00a0 [ITALIC] p<0.01; \u2217\u00a0 [ITALIC] p<0.05",
                    "Two tailed t-test: \u2217\u2217\u00a0 [ITALIC] p<0.01; \u2217\u00a0 [ITALIC] p<0.05",
                    "Two tailed t-test: \u2217\u2217\u00a0 [ITALIC] p<0.01; \u2217\u00a0 [ITALIC] p<0.05",
                    "Two tailed t-test: \u2217\u2217\u00a0 [ITALIC] p<0.01; \u2217\u00a0 [ITALIC] p<0.05",
                    "Two tailed t-test: \u2217\u2217\u00a0 [ITALIC] p<0.01; \u2217\u00a0 [ITALIC] p<0.05"
                ]
            ],
            "title": "Table 2: Classification performance of the compared methods on various topics, measured by the averaged macro F1-score over ten runs on the test data."
        },
        "insight": "Table 2 shows the results of our method and all the baselines on tasks with different topics. [CONTINUE] We can first observe that the proposed RCN consistently outperformed all the baselines across all topics. Despite being modest, all the improvements of RCN over the baselines are statistically significant at p < 0.05 with a two-tailed t-test. [CONTINUE] BiLSTM performed the worst, showing that only using the RNN encoder for sequence encoding is not sufficient for obtaining optimal results. [CONTINUE] DeAT and BiMPM performed similarly well; [CONTINUE] RCN performed the best, with relative improvements from 2.1% to 10.4% over the second best."
    },
    {
        "id": "263",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Accuracy (\u00b10.01)"
            ],
            "rows": [
                [
                    "LSTM+Feed-Forward",
                    "0.518"
                ],
                [
                    "LSTM+Gated CNN+Feed-Forward",
                    "0.524"
                ],
                [
                    "BERT Features(512 tokens)+Feed-Forward",
                    "0.639"
                ],
                [
                    "BERT Classifier(30 tokens / 15 tokens from each paragraph)",
                    "0.681"
                ],
                [
                    "BERT Classifier(128 tokens / 64 tokens from each paragraph)",
                    "0.717"
                ],
                [
                    "BERT Classifier(256 tokens / 128 tokens from each paragraph)",
                    "0.843"
                ]
            ],
            "title": "Table 3: Accuracy on Test set."
        },
        "insight": "Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around 84% accuracy on test set which outperforms other models significantly. [CONTINUE] First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN (Dauphin et al., 2017) for extraction of single encoding for each paragraph. The accuracy is barely above 50%, which depicts that this method is not very promising. [CONTINUE] We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. [CONTINUE] In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task. [CONTINUE] we increased the number of tokens and accuracy respectively increases. [CONTINUE] We found this method very promising and the accuracy significantly increases with respect to previous methods (Table 3). This result reveals fine-tuning pre-trained BERT can approximately learn the order of the paragraphs and arrow of the time in the stories."
    },
    {
        "id": "264",
        "table": {
            "header": [
                "[BOLD] Window position",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "rows": [
                [
                    "OS left",
                    "0.40",
                    "0.35"
                ],
                [
                    "OS right",
                    "0.43",
                    "0.35"
                ],
                [
                    "OS symmetric",
                    "0.43",
                    "[BOLD] 0.45"
                ],
                [
                    "GW left",
                    "0.43",
                    "0.64"
                ],
                [
                    "GW right",
                    "0.44",
                    "0.65"
                ],
                [
                    "GW symmetric",
                    "0.45",
                    "[BOLD] 0.68"
                ]
            ],
            "title": "Table 2: Average performance across all models depending on the window position."
        },
        "insight": "Table 2 shows how the position of the context window influences the average model performance. Note that symmetric windows of, for instance, 10 are in fact 2 times larger than the 'left' or 'right' [CONTINUE] windows of the same size, as they consider 10 words both to the left and to the right of the focus word. This is most likely why symmetric windows consistently outperform 'single-sided' ones on the analogy task, as they are able to include twice as much contextual input. However, the average performance on the semantic similarity task (as indicated by the Spearman correlation with the SimLex999 test set) does not exhibit the same trend. 'Left' windows are indeed worse than symmetric ones, but 'right' windows are on par with the symmetric windows for OpenSubtitles and only one percent point behind them for Gigaword. It means that in many cases (at least with English texts) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with SimLex999 as by using a model which additionally considers n words to the left, and thus requires significantly more training time."
    },
    {
        "id": "265",
        "table": {
            "header": [
                "[BOLD] Cross-sentential",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "rows": [
                [
                    "OS False",
                    "[BOLD] 0.44",
                    "0.34"
                ],
                [
                    "OS True",
                    "0.40",
                    "[BOLD] 0.43"
                ],
                [
                    "GW False",
                    "0.44",
                    "0.66"
                ],
                [
                    "GW True",
                    "0.44",
                    "0.65"
                ]
            ],
            "title": "Table 3: Average performance across all models with and without cross-sentential contexts."
        },
        "insight": "For similarity tasks, cross-sentential contexts do not seem useful, and can even be detrimental for large window sizes. However, for analogy tasks, crosssentential contexts lead to improved results thanks to the increased window it provides. This is especially pronounced for corpora with short sentences such as OpenSubtitles (see Table 3)."
    },
    {
        "id": "266",
        "table": {
            "header": [
                "[BOLD] Stop words removal",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "rows": [
                [
                    "OS no removal",
                    "0.41",
                    "0.34"
                ],
                [
                    "OS with removal",
                    "0.42",
                    "[BOLD] 0.43"
                ],
                [
                    "GW no removal",
                    "0.44",
                    "0.64"
                ],
                [
                    "GW with removal",
                    "0.44",
                    "[BOLD] 0.68"
                ]
            ],
            "title": "Table 4: Average performance across all models depending on the removal of stop words."
        },
        "insight": "As shown in Table 4, the removal of stop words does not really influence the average model performance for the semantic similarity task. The analogy task, however, benefits substantially from this filtering, for both corpora."
    },
    {
        "id": "267",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] w/o. chars",
                "[BOLD] w. chars"
            ],
            "rows": [
                [
                    "[ITALIC] mean",
                    "71.3 \u00b1 1.2",
                    "71.3 \u00b1 0.7"
                ],
                [
                    "[ITALIC] sum",
                    "70.7 \u00b1 1.0",
                    "70.9 \u00b1 0.8"
                ],
                [
                    "[ITALIC] last",
                    "70.9 \u00b1 0.6",
                    "71.0 \u00b1 1.2"
                ],
                [
                    "[ITALIC] max",
                    "70.6 \u00b1 1.1",
                    "71.0 \u00b1 1.1"
                ]
            ],
            "title": "Table 1: Mean matched validation accuracies (%) broken down by type of pooling method and presence or absence of character embeddings. Confidence intervals are calculated at 95% confidence over 10 runs for each method."
        },
        "insight": "Table 1 presents the results of using different pooling strategies for generating a raw sentence representation vector from the word vectors. [CONTINUE] We can observe that that both the mean method, and picking the last hidden state for both directions performed slightly better than the two other strategies, however at 95% confidence we cannot assert that any of these methods is statistically different from one another. [CONTINUE] Another interesting result, as shown by Table 1 and Table 2, is that the model seemed to be insensitive to the usage of character embeddings,"
    },
    {
        "id": "268",
        "table": {
            "header": [
                "[BOLD] Genre",
                "[BOLD] CBOW",
                "[BOLD] ESIM",
                "[BOLD] InnerAtt"
            ],
            "rows": [
                [
                    "Fiction",
                    "67.5",
                    "73.0",
                    "73.2"
                ],
                [
                    "Government",
                    "67.5",
                    "74.8",
                    "75.2"
                ],
                [
                    "Slate",
                    "60.6",
                    "67.9",
                    "67.2"
                ],
                [
                    "Telephone",
                    "63.7",
                    "72.2",
                    "73.0"
                ],
                [
                    "Travel",
                    "64.6",
                    "73.7",
                    "72.8"
                ],
                [
                    "9/11",
                    "63.2",
                    "71.9",
                    "70.5"
                ],
                [
                    "Face-to-face",
                    "66.3",
                    "71.2",
                    "74.5"
                ],
                [
                    "Letters",
                    "68.3",
                    "74.7",
                    "75.4"
                ],
                [
                    "Oup",
                    "62.8",
                    "71.7",
                    "71.5"
                ],
                [
                    "Verbatim",
                    "62.7",
                    "71.9",
                    "69.5"
                ],
                [
                    "[BOLD] MultiNLI Overall",
                    "[BOLD] 64.7",
                    "[BOLD] 72.2",
                    "[BOLD] 72.3"
                ]
            ],
            "title": "Table 3: Validation accuracies (%) for our best model broken down by genre. Both CBOW and ESIM results are reported as in (Williams et\u00a0al., 2017)."
        },
        "insight": "In Table 3 we report the accuracies obtained by our best model in both matched (first 5 genres) and mismatched (last 5 genres) development sets. [CONTINUE] We can observe that our implementation performed like ESIM overall,"
    },
    {
        "id": "269",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Pre-trained",
                "[BOLD] Fine-tuned, initialized with  [BOLD] normal distr.",
                "[BOLD] Fine-tuned, initialized with  [BOLD] pre-trained",
                "[BOLD] Metric",
                "[BOLD] Size"
            ],
            "rows": [
                [
                    "MRPC",
                    "0/31.6",
                    "81.2/68.3",
                    "87.9/82.3",
                    "F1/Acc",
                    "5.8K"
                ],
                [
                    "STS-B",
                    "33.1",
                    "2.9",
                    "82.7",
                    "Acc",
                    "8.6K"
                ],
                [
                    "SST-2",
                    "49.1",
                    "80.5",
                    "92",
                    "Acc",
                    "70K"
                ],
                [
                    "QQP",
                    "0/60.9",
                    "0/63.2",
                    "65.2/78.6",
                    "F1/Acc",
                    "400K"
                ],
                [
                    "RTE",
                    "52.7",
                    "52.7",
                    "64.6",
                    "Acc",
                    "2.7K"
                ],
                [
                    "QNLI",
                    "52.8",
                    "49.5",
                    "84.4",
                    "Acc",
                    "130K"
                ],
                [
                    "MNLI-m",
                    "31.7",
                    "61.0",
                    "78.6",
                    "Acc",
                    "440K"
                ]
            ],
            "title": "Table 1: GLUE task performance of BERT models with different initialization. We report the scores on the validation, rather than test data, so these results differ from the original BERT paper."
        },
        "insight": "Table 1 shows that finetuned BERT outperforms pre-trained BERT by a significant margin on all the tasks (with an average of 35.9 points of absolute difference). [CONTINUE] BERT with weights initialized from normal distribution and further fine-tuned for a given task consistently produces lower scores than the ones achieved with pre-trained BERT. In fact, for some tasks (STS-B and QNLI), initialization with random weights yields worse performance than pre-trained BERT without fine-tuning."
    },
    {
        "id": "270",
        "table": {
            "header": [
                "Comparison Point Chen2018",
                "Comparison Point BPE",
                "Comparison Point EnFr",
                "Ref 41.0",
                "Ours 38.8"
            ],
            "rows": [
                [
                    "Wu2016",
                    "BPE",
                    "EnFr",
                    "39.0",
                    "38.8"
                ],
                [
                    "Lee2017",
                    "Char",
                    "CsEn",
                    "22.5",
                    "25.9"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "DeEn",
                    "25.8",
                    "31.6"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "FiEn",
                    "13.1",
                    "19.3"
                ]
            ],
            "title": "Table 3: Comparisons with some recent points in the literature. Scores are tokenized BLEU."
        },
        "insight": "First, from the EnFr results in Table 3, we are in line with GNMT (Wu et al., 2016), and within 2 BLEU points of the RNN and Transformer models investigated by Chen et al. (2018). [CONTINUE] we compare quite favorably with Lee et al. (2017), exceeding their reported scores by 3-6 points,"
    },
    {
        "id": "271",
        "table": {
            "header": [
                "Language",
                "Tokenized BLEU BPE",
                "Tokenized BLEU Char",
                "Tokenized BLEU Delta",
                "SacreBLEU Char"
            ],
            "rows": [
                [
                    "EnFr",
                    "38.8",
                    "39.2",
                    "0.4",
                    "38.1"
                ],
                [
                    "CsEn",
                    "24.8",
                    "25.9",
                    "1.1",
                    "25.6"
                ],
                [
                    "DeEn",
                    "29.7",
                    "31.6",
                    "1.9",
                    "31.6"
                ],
                [
                    "FiEn",
                    "17.5",
                    "19.3",
                    "1.8",
                    "19.5"
                ]
            ],
            "title": "Table 2: Character versus BPE translation."
        },
        "insight": "Table 2 clearly shows the characterlevel systems outperforming BPE for all language pairs."
    },
    {
        "id": "272",
        "table": {
            "header": [
                "Error Type",
                "BPE",
                "Char"
            ],
            "rows": [
                [
                    "Lexical Choice",
                    "19",
                    "8"
                ],
                [
                    "Compounds",
                    "13",
                    "1"
                ],
                [
                    "Proper Names",
                    "2",
                    "1"
                ],
                [
                    "Morphological",
                    "2",
                    "2"
                ],
                [
                    "Other lexical",
                    "2",
                    "4"
                ],
                [
                    "Dropped Content",
                    "7",
                    "0"
                ]
            ],
            "title": "Table 4: Error counts out of 100 randomly sampled examples from the DeEn test set."
        },
        "insight": "BPE and character sys [CONTINUE] tems differ most in the number of lexical choice errors, and in the extent to which they drop content. [CONTINUE] Regarding lexical choice, the two systems differ not only in the number of errors, but in the nature of those errors. In particular, the BPE model had more trouble handling German compound nouns. [CONTINUE] We also found that both systems occasionally mistranslate proper names."
    },
    {
        "id": "273",
        "table": {
            "header": [
                "Encoder",
                "BPE Size",
                "BLEU",
                "Comp."
            ],
            "rows": [
                [
                    "BiLSTM",
                    "Char",
                    "31.6",
                    "1.00"
                ],
                [
                    "BiLSTM",
                    "1k",
                    "30.5",
                    "0.44"
                ],
                [
                    "BiLSTM",
                    "2k",
                    "30.4",
                    "0.35"
                ],
                [
                    "BiLSTM",
                    "4k",
                    "30.0",
                    "0.29"
                ],
                [
                    "BiLSTM",
                    "8k",
                    "29.6",
                    "0.25"
                ],
                [
                    "BiLSTM",
                    "16k",
                    "30.0",
                    "0.22"
                ],
                [
                    "BiLSTM",
                    "32k",
                    "29.7",
                    "0.20"
                ],
                [
                    "Lee et. al. reimpl",
                    "Char",
                    "28.0",
                    "0.20"
                ],
                [
                    "BiLSTM + pooling",
                    "Char",
                    "30.0",
                    "0.47"
                ],
                [
                    "HM, 3-layer",
                    "Char",
                    "31.2",
                    "0.77"
                ],
                [
                    "HM, 2-layer",
                    "Char",
                    "30.9",
                    "0.89"
                ]
            ],
            "title": "Table 6: Compression results on WMT15 DeEn. The Comp. column shows the ratio of total computations carried out in the encoder."
        },
        "insight": "Unfortunately, even at just 1k vocabulary items, BPE has already lost a BLEU point with respect to the character model. [CONTINUE] Comparing the performance of our Pooled BiLSTM model against BPE, we notice that for a comparable level of compression (BPE size of 1k), BPE out-performs the pooled model by around 0.5 BLEU points. At a similar level of performance (BPE size of 4k), BPE has significantly shorter sequences. [CONTINUE] As shown in table 6, the 3-HM configuration achieves much better compression even when this is accounted for, and also gives slightly better performance than 2-HM. In general, HM gating results in less compression but better performance than the fixed-stride techniques."
    },
    {
        "id": "274",
        "table": {
            "header": [
                "Method",
                "[ITALIC] \u03bcP",
                "[ITALIC] \u03bcR",
                "[ITALIC] \u03bcF1",
                "eP",
                "eR",
                "eF1"
            ],
            "rows": [
                [
                    "Zubiaga et\u00a0al.  2017",
                    "0.667",
                    "0.556",
                    "0.607",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BiLSTM",
                    "0.623",
                    "0.564",
                    "0.590",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "0.699 \u00b1 0.0165",
                    "0.608 \u00b1 0.0257",
                    "0.650 \u00b1 0.0134",
                    "0.713",
                    "0.619",
                    "0.663"
                ],
                [
                    "BERT + Wiki",
                    "0.693 \u00b1 0.0159",
                    "0.614 \u00b1 0.0263",
                    "0.651 \u00b1 0.0121",
                    "0.707",
                    "0.622",
                    "0.662"
                ],
                [
                    "BERT + Wiki + PU",
                    "0.699 \u00b1 0.0133",
                    "[BOLD] 0.625 \u00b1 0.0164",
                    "[BOLD] 0.660 \u00b1 0.0109",
                    "[BOLD] 0.722",
                    "[BOLD] 0.646",
                    "[BOLD] 0.682"
                ],
                [
                    "BERT + Wiki +  [ITALIC] PUC",
                    "[BOLD] 0.701 \u00b1 0.0108",
                    "0.618 \u00b1 0.0176",
                    "0.657 \u00b1 0.0097",
                    "0.715",
                    "0.627",
                    "0.668"
                ]
            ],
            "title": "Table 2: micro-F1 (\u03bcF1) and ensembled F1 (eF1) performance of each system on the PHEME dataset. Performance is averaged across the five splits of\u00a0Zubiaga et\u00a0al. (2017). Results show the mean, standard deviation, and ensembled score across 15 seeds. Bold indicates best performance, underline indicates second best."
        },
        "insight": "The results for the tested systems are given in Table 2. Again we saw large gains from the BERT [CONTINUE] based models over the baseline from (Zubiaga et al., 2017) and the 2-layer BiLSTM. Compared to training solely on PHEME, fine tuning from basic citation needed detection saw very little improvement (0.1 F1 points). However, fine tuning with a model trained using PU learning led to an increase of 1 F1 point over the non-PU learning model, indicating that PU learning enables the Wikipedia data to be useful for transferring to rumour detection. For PUC, we saw an improvement of 0.7 F1 points over the baseline and lower overall variance than vanilla PU learning, meaning that the results with PUC are more consistent across runs. When models are ensembled, pretraining with vanilla PU learning improved over no pretraining by almost 2 F1 points."
    },
    {
        "id": "275",
        "table": {
            "header": [
                "Method",
                "MAP"
            ],
            "rows": [
                [
                    "Konstantinovskiy et\u00a0al.  2018",
                    "0.267"
                ],
                [
                    "Hansen et\u00a0al.  2019",
                    "0.302"
                ],
                [
                    "BERT",
                    "[BOLD] 0.346 \u00b1 0.024"
                ],
                [
                    "BERT + Wiki",
                    "0.339 \u00b1 0.025"
                ],
                [
                    "BERT + Wiki + PU",
                    "0.328 \u00b1 0.027"
                ],
                [
                    "BERT + Wiki +  [ITALIC] PUC",
                    "0.321 \u00b1 0.031"
                ]
            ],
            "title": "Table 3: Mean average precision (MAP) of models on political speeches. Bold indicates best performance, underline indicates second best."
        },
        "insight": "The results for political speech check-worthiness detection are given in Table 3. We found that the vanilla BERT model performed the best of all models. As we added transfer learning and PU learning, the performance steadily dropped, with the worst performing model being the one using PUC."
    },
    {
        "id": "276",
        "table": {
            "header": [
                "Dataset",
                "P",
                "R",
                "F1"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "0.817",
                    "0.870",
                    "0.843"
                ],
                [
                    "Wikipedia",
                    "0.848",
                    "0.870",
                    "0.859"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.833",
                    "[ITALIC] 0.870",
                    "[ITALIC] 0.851"
                ],
                [
                    "[EMPTY]",
                    "0.875",
                    "0.824",
                    "0.848"
                ],
                [
                    "Twitter",
                    "0.863",
                    "0.812",
                    "0.836"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.869",
                    "[ITALIC] 0.818",
                    "[ITALIC] 0.842"
                ],
                [
                    "[EMPTY]",
                    "0.338",
                    "0.893",
                    "0.490"
                ],
                [
                    "Politics",
                    "0.311",
                    "1.0",
                    "0.475"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.325",
                    "[ITALIC] 0.947",
                    "[ITALIC] 0.483"
                ]
            ],
            "title": "Table 4: F1 score comparing manual relabelling of the top 100 predictions by PUC model with the original labels in each dataset by two different annotators. Italics are average value between the two annotators."
        },
        "insight": "Our results are given in Table 4. [CONTINUE] We found that the Wikipedia and Twitter datasets contained labels which were more general, evidenced by similar high F1 scores from both annotators (> 0.8). For political speeches, we observed that the human annotators both found many more examples to be check-worthy than were labelled in the dataset."
    },
    {
        "id": "277",
        "table": {
            "header": [
                "[BOLD] dataset",
                "[BOLD] tool",
                "[BOLD] # correct prediction",
                "[BOLD] positive  [BOLD] precision",
                "[BOLD] positive  [BOLD] recall",
                "[BOLD] positive  [BOLD] F1",
                "[BOLD] neutral  [BOLD] precision",
                "[BOLD] neutral  [BOLD] recall",
                "[BOLD] neutral  [BOLD] F1",
                "[BOLD] negative  [BOLD] precision",
                "[BOLD] negative  [BOLD] recall",
                "[BOLD] negative  [BOLD] F1"
            ],
            "rows": [
                [
                    "[BOLD] Stack Overflow",
                    "SentiStrength",
                    "1043",
                    "0.200",
                    "[BOLD] 0.359",
                    "0.257",
                    "0.858",
                    "0.772",
                    "0.813",
                    "0.397",
                    "0.433",
                    "0.414"
                ],
                [
                    "positive: 178",
                    "NLTK",
                    "1168",
                    "0.317",
                    "0.244",
                    "0.276",
                    "0.815",
                    "[BOLD] 0.941",
                    "0.873",
                    "[BOLD] 0.625",
                    "0.084",
                    "0.148"
                ],
                [
                    "neutral: 1,191",
                    "Standford CoreNLP",
                    "604",
                    "0.231",
                    "0.344",
                    "0.276",
                    "[BOLD] 0.884",
                    "0.344",
                    "0.495",
                    "0.177",
                    "[BOLD] 0.837",
                    "0.292"
                ],
                [
                    "negative: 131",
                    "SentiStrength-SE",
                    "1170",
                    "0.312",
                    "0.221",
                    "0.259",
                    "0.826",
                    "0.930",
                    "0.875",
                    "0.500",
                    "0.185",
                    "0.270"
                ],
                [
                    "sum: 1,500",
                    "Stanford CoreNLP SO",
                    "1139",
                    "0.317",
                    "0.145",
                    "0.199",
                    "0.836",
                    "0.886",
                    "0.860",
                    "0.365",
                    "0.365",
                    "0.365"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 1317",
                    "[BOLD] 0.667",
                    "0.316",
                    "[BOLD] 0.418",
                    "0.871",
                    "0.939",
                    "[BOLD] 0.904",
                    "0.600",
                    "0.472",
                    "[BOLD] 0.514"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE\u2020",
                    "-",
                    "0.680",
                    "0.005",
                    "0.009",
                    "0.344",
                    "0.930",
                    "0.499",
                    "0.657",
                    "0.160",
                    "0.251"
                ],
                [
                    "[BOLD] App reviews",
                    "SentiStrength",
                    "213",
                    "0.745",
                    "0.866",
                    "0.801",
                    "0.113",
                    "0.320",
                    "0.167",
                    "0.815",
                    "0.338",
                    "0.478"
                ],
                [
                    "positive: 186",
                    "NLTK",
                    "184",
                    "0.751",
                    "0.812",
                    "0.780",
                    "0.093",
                    "[BOLD] 0.440",
                    "0.154",
                    "[BOLD] 1.000",
                    "0.169",
                    "0.289"
                ],
                [
                    "neutral: 25",
                    "Standford CoreNLP",
                    "237",
                    "0.831",
                    "0.715",
                    "0.769",
                    "[BOLD] 0.176",
                    "0.240",
                    "[BOLD] 0.203",
                    "0.667",
                    "0.754",
                    "0.708"
                ],
                [
                    "negative: 130",
                    "SentiStrength-SE",
                    "201",
                    "0.741",
                    "0.817",
                    "0.777",
                    "0.106",
                    "0.400",
                    "0.168",
                    "0.929",
                    "0.300",
                    "0.454"
                ],
                [
                    "sum: 341",
                    "Stanford CoreNLP SO",
                    "142",
                    "0.770",
                    "0.253",
                    "0.381",
                    "0.084",
                    "0.320",
                    "0.133",
                    "0.470",
                    "0.669",
                    "0.552"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 293",
                    "[BOLD] 0.822",
                    "[BOLD] 0.894",
                    "[BOLD] 0.853",
                    "0.083",
                    "0.066",
                    "0.073",
                    "0.823",
                    "[BOLD] 0.808",
                    "[BOLD] 0.807"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE\u2020",
                    "-",
                    "0.520",
                    "0.885",
                    "0.641",
                    "0.100",
                    "0.058",
                    "0.073",
                    "0.648",
                    "0.622",
                    "0.607"
                ],
                [
                    "[BOLD] Jira issues",
                    "SentiStrength",
                    "714",
                    "0.850",
                    "[BOLD] 0.921",
                    "0.884",
                    "-",
                    "-",
                    "-",
                    "0.993",
                    "0.703",
                    "0.823"
                ],
                [
                    "positive: 290",
                    "NLTK",
                    "276",
                    "0.840",
                    "0.362",
                    "0.506",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 1.000",
                    "0.269",
                    "0.424"
                ],
                [
                    "neutral: 0",
                    "Standford CoreNLP",
                    "626",
                    "0.726",
                    "0.621",
                    "0.669",
                    "-",
                    "-",
                    "-",
                    "0.945",
                    "0.701",
                    "0.805"
                ],
                [
                    "negative: 636",
                    "SentiStrength-SE",
                    "704",
                    "0.948",
                    "0.883",
                    "0.914",
                    "-",
                    "-",
                    "-",
                    "0.996",
                    "0.704",
                    "0.825"
                ],
                [
                    "sum: 926",
                    "Stanford CoreNLP SO",
                    "333",
                    "0.635",
                    "0.252",
                    "0.361",
                    "-",
                    "-",
                    "-",
                    "0.724",
                    "0.409",
                    "0.523"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 884",
                    "[BOLD] 0.960",
                    "0.839",
                    "[BOLD] 0.893",
                    "-",
                    "-",
                    "-",
                    "0.932",
                    "[BOLD] 0.982",
                    "[BOLD] 0.956"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE\u2020",
                    "-",
                    "0.986",
                    "0.704",
                    "0.809",
                    "-",
                    "-",
                    "-",
                    "0.781",
                    "0.988",
                    "0.872"
                ],
                [
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method.",
                    "\u2020 Applying SMOTE, a oversampling technique, for our method."
                ]
            ],
            "title": "TABLE I: The comparison result of the number of corrected prediction, precision, recall, and f1-score"
        },
        "insight": "Table [CONTINUE] shows the number of correct predictions, precision, recall, and F1 values with all tools including our method (n-gram auto-sklearn). [CONTINUE] We can see that the number of correct predictions are higher with our method in all three datasets, and our method achieved the highest F1 values for all three positive, all three negative, and one neutral. [CONTINUE] In summary, our method using n-gram IDF and automated machine learning (auto-sklearn) largely outperformed existing sentiment analysis tools."
    },
    {
        "id": "278",
        "table": {
            "header": [
                "[EMPTY]",
                "rg  [BOLD] p",
                "rg  [BOLD] s",
                "rg  [BOLD] avg.",
                "wordsim  [BOLD] p",
                "wordsim  [BOLD] s",
                "wordsim  [BOLD] avg.",
                "mc  [BOLD] p",
                "mc  [BOLD] s",
                "mc  [BOLD] avg.",
                "semeval17  [BOLD] p",
                "semeval17  [BOLD] s",
                "semeval17  [BOLD] avg."
            ],
            "rows": [
                [
                    "cosine",
                    "77.2",
                    "76.0",
                    "76.6",
                    "64.9",
                    "69.4",
                    "67.1",
                    "79.2",
                    "80.0",
                    "79.6",
                    "69.4",
                    "70.0",
                    "69.7"
                ],
                [
                    "10rv [ITALIC] w",
                    "78.1",
                    "77.0",
                    "77.5",
                    "66.0",
                    "69.6",
                    "67.8",
                    "79.7",
                    "80.7",
                    "[BOLD] 80.2",
                    "70.2",
                    "70.8",
                    "70.5"
                ],
                [
                    "10rv [ITALIC] r",
                    "77.4",
                    "75.5",
                    "76.4",
                    "65.8",
                    "69.5",
                    "67.6",
                    "78.8",
                    "77.9",
                    "78.3",
                    "70.0",
                    "70.7",
                    "70.3"
                ],
                [
                    "1800rv [ITALIC] w",
                    "79.5",
                    "80.6",
                    "[BOLD] 80.0",
                    "67.4",
                    "69.8",
                    "68.6",
                    "79.4",
                    "79.0",
                    "79.2",
                    "71.4",
                    "71.8",
                    "71.6"
                ],
                [
                    "1800rv [ITALIC] r",
                    "78.9",
                    "80.2",
                    "79.5",
                    "68.1",
                    "70.1",
                    "[BOLD] 69.1",
                    "79.2",
                    "79.7",
                    "79.4",
                    "72.2",
                    "73.0",
                    "[BOLD] 72.6"
                ]
            ],
            "title": "Table 3: Correlation results for different configurations of our proposed approach and a competitor baseline based on cosine similarity of word embeddings."
        },
        "insight": "Table 3 shows that the 10rvw variant consistently outperforms the word-level baseline."
    },
    {
        "id": "279",
        "table": {
            "header": [
                "Architecture",
                "Zh\u21d2En",
                "Params",
                "Emb.",
                "Red.",
                "Dev.",
                "MT02",
                "MT03",
                "MT04",
                "MT08",
                "All"
            ],
            "rows": [
                [
                    "SMT*",
                    "-",
                    "-",
                    "-",
                    "-",
                    "34.00",
                    "35.81",
                    "34.70",
                    "37.15",
                    "25.28",
                    "33.39"
                ],
                [
                    "RNNsearch*",
                    "Vanilla",
                    "74.8M",
                    "55.8M",
                    "0%",
                    "35.92",
                    "37.88",
                    "36.21",
                    "38.83",
                    "26.30",
                    "34.81"
                ],
                [
                    "RNNsearch*",
                    "Source bridging",
                    "78.5M",
                    "55.8M",
                    "0%",
                    "36.79",
                    "38.71",
                    "37.24",
                    "40.28",
                    "27.40",
                    "35.91"
                ],
                [
                    "RNNsearch*",
                    "Target bridging",
                    "76.6M",
                    "55.8M",
                    "0%",
                    "36.69",
                    "39.04",
                    "37.63",
                    "40.41",
                    "27.98",
                    "36.27"
                ],
                [
                    "RNNsearch*",
                    "Direct bridging",
                    "78.9M",
                    "55.8M",
                    "0%",
                    "36.97",
                    "39.77",
                    "38.02",
                    "40.83",
                    "27.85",
                    "36.62"
                ],
                [
                    "Transformer",
                    "Vanilla",
                    "90.2M",
                    "46.1M",
                    "0%",
                    "41.37",
                    "42.53",
                    "40.25",
                    "43.58",
                    "32.89",
                    "40.33"
                ],
                [
                    "Transformer",
                    "Direct bridging",
                    "90.5M",
                    "46.1M",
                    "0%",
                    "41.67",
                    "42.89",
                    "41.34",
                    "43.56",
                    "32.69",
                    "40.54"
                ],
                [
                    "Transformer",
                    "Decoder WT",
                    "74.9M",
                    "30.7M",
                    "33.4%",
                    "41.90",
                    "43.02",
                    "41.89",
                    "43.87",
                    "32.62",
                    "40.82"
                ],
                [
                    "Transformer",
                    "[ITALIC] Shared-private",
                    "62.8M",
                    "18.7M",
                    "59.4%",
                    "42.57\u2191",
                    "43.73\u2191",
                    "41.99\u2191",
                    "44.53\u2191",
                    "33.81\u21d1",
                    "41.61\u21d1"
                ]
            ],
            "title": "Table 1: Results on the NIST Chinese-English translation task. \u201cParams\u201d denotes the number of model parameters. \u201cEmb.\u201d represents the number of parameters used for word representation. \u201cRed.\u201d represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et\u00a0al. Kuang et\u00a0al. (2018) with the same datasets and vocabulary settings. \u201c\u2191\u201d indicates the result is significantly better than that of the vanilla Transformer (p<0.01), while \u201c\u21d1\u201d indicates the result is significantly better than that of all other Transformer models (p<0.01). All significance tests are measured by paired bootstrap resampling\u00a0Koehn (2004)."
        },
        "insight": "Table 1: Results on the NIST Chinese-English translation task. \"Params\" denotes the number of model parameters. \"Emb.\" represents the number of parameters used for word representation. \"Red.\" represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. (2018) with the same datasets and vocabulary settings. \"\u2191\" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01), while \"\u21d1\" indicates the result is significantly better than that of all other Transformer models (p < 0.01). All significance tests are measured by paired bootstrap resampling (Koehn, 2004). [CONTINUE] Table 1 reports the results on the NIST ChineseEnglish test sets. It is observed that the Transformer models significantly outperform SMT and RNNsearch models. Therefore, we decide to implement all of our experiments based on Transformer architecture. The direct bridging model can further improve the translation quality of the Transformer baseline. The decoder WT model improves the translation quality while reducing the number of parameters for the word representation. This improved performance happens because there are fewer model parameters, which prevents over-fitting (Press and Wolf, 2017). Finally, the performance is further improved by the proposed method while using even fewer parameters than other models."
    },
    {
        "id": "280",
        "table": {
            "header": [
                "En\u21d2De",
                "Params",
                "Emb.",
                "Red.",
                "BLEU"
            ],
            "rows": [
                [
                    "Vanilla",
                    "98.7M",
                    "54.5M",
                    "0%",
                    "27.62"
                ],
                [
                    "Direct bridging",
                    "98.9M",
                    "54.5M",
                    "0%",
                    "27.79"
                ],
                [
                    "Decoder WT",
                    "80.4M",
                    "36.2M",
                    "33.6%",
                    "27.51"
                ],
                [
                    "Three-way WT",
                    "63.1M",
                    "18.9M",
                    "65.3%",
                    "27.39"
                ],
                [
                    "[ITALIC] Shared-private",
                    "65.0M",
                    "20.9M",
                    "63.1%",
                    "28.06\u2021"
                ]
            ],
            "title": "Table 2: Results on the WMT English-German translation task. \u201c\u2021\u201d indicates the result is significantly better than the vanilla Transformer model (p<0.05)."
        },
        "insight": "Table 2: Results on the WMT English-German translation task. \"\u2021\" indicates the result is significantly better than the vanilla Transformer model (p < 0.05). [CONTINUE] Similar observations are obtained on the English-German translation task, as shown in Table 2."
    },
    {
        "id": "281",
        "table": {
            "header": [
                "[EMPTY]",
                "Model",
                "Emb.",
                "Red.",
                "BLEU"
            ],
            "rows": [
                [
                    "Ar\u21d2 En",
                    "Vanilla",
                    "23.6M",
                    "0%",
                    "28.36"
                ],
                [
                    "Ar\u21d2 En",
                    "[ITALIC] Shared-private",
                    "11.8M",
                    "50%",
                    "29.71\u2191"
                ],
                [
                    "Ja\u21d2 En",
                    "Vanilla",
                    "25.6M",
                    "0%",
                    "10.94"
                ],
                [
                    "Ja\u21d2 En",
                    "[ITALIC] Shared-private",
                    "13.3M",
                    "48.0%",
                    "12.35\u2191"
                ],
                [
                    "Ko\u21d2 En",
                    "Vanilla",
                    "25.1M",
                    "0%",
                    "16.48"
                ],
                [
                    "Ko\u21d2 En",
                    "[ITALIC] Shared-private",
                    "13.2M",
                    "47.4%",
                    "17.84\u2191"
                ],
                [
                    "Zh\u21d2 En",
                    "Vanilla",
                    "27.4M",
                    "0%",
                    "19.36"
                ],
                [
                    "Zh\u21d2 En",
                    "[ITALIC] Shared-private",
                    "13.8M",
                    "49.6%",
                    "21.00\u2191"
                ]
            ],
            "title": "Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.\u201c\u2191\u201d indicates the result is significantly better than that of the vanilla Transformer (p<0.01)."
        },
        "insight": "Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.\"\u2191\" indicates the result is significantly better than that of the vanilla Transformer (p < 0.01). [CONTINUE] Table 3 shows the results on the small-scale IWSLT translation tasks. We observe that the proposed method stays consistently better than the vanilla model on these distant language pairs."
    },
    {
        "id": "282",
        "table": {
            "header": [
                "Zh-En",
                "[ITALIC] \u03bblm",
                "[ITALIC] \u03bbwf",
                "[ITALIC] \u03bbur",
                "Emb.",
                "BLEU"
            ],
            "rows": [
                [
                    "Vanilla",
                    "-",
                    "-",
                    "-",
                    "46.1M",
                    "41.37"
                ],
                [
                    "Decoder WT",
                    "0",
                    "0",
                    "0",
                    "30.7M",
                    "41.90"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.5",
                    "0.7",
                    "0.9",
                    "21.2M",
                    "41.98"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.5",
                    "0.5",
                    "0.5",
                    "23.0M",
                    "42.26"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.9",
                    "0.7",
                    "0",
                    "21.0M",
                    "42.27"
                ],
                [
                    "[ITALIC] Shared-private",
                    "1",
                    "1",
                    "1",
                    "15.3M",
                    "42.36"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.9",
                    "0.7",
                    "0.5",
                    "18.7M",
                    "42.57"
                ]
            ],
            "title": "Table 4: Performance of models using different sharing coefficients on the validation set of the NIST Chinese-English translation task."
        },
        "insight": "Table 4: Performance of models using different sharing coefficients on the validation set of the NIST ChineseEnglish translation task. [CONTINUE] As shown in Table 4, the decoder WT model can be seen as a kind of shared-private method where zero features are shared between the source and target word embeddings. For the proposed method, \u03bb = (0.5, 0.5, 0.5) and \u03bb = (1, 1, 1) are, respectively, used for sharing half and all features between the embeddings of all categories of words. This allows the model to significantly reduce the number of parameters and also improve the translation quality. For comparison purpose, we also consider sharing a large part of the features among the unrelated words by setting s3 to 0.9, i.e. \u03bb = (0.5, 0.7, 0.9). We argue that it is hard for"
    },
    {
        "id": "283",
        "table": {
            "header": [
                "Tool",
                "Time (minutes) Ubuntu",
                "Time (minutes) macOS"
            ],
            "rows": [
                [
                    "slate",
                    "10",
                    "16"
                ],
                [
                    "YEDDA",
                    "14",
                    "14"
                ],
                [
                    "GATE",
                    "21",
                    "22"
                ],
                [
                    "brat",
                    "-",
                    "-"
                ]
            ],
            "title": "Table 2: Average time for users to set up the tool and identify verbs in a 623 word news article. Only one participant managed to install and use brat, taking 18 minutes on Ubuntu. The differences between GATE and either slate or YEDDA are significant at the 0.01 level according to a t-test."
        },
        "insight": "Table 2 presents the time required to install each tool and complete the first annotation task. [CONTINUE] SLATE and YEDDA are comparable in effort, which fits with their common design as simple tools with minimal dependencies. Participants had great difficulty with brat, with only two managing to install it, one just as their time finished."
    },
    {
        "id": "284",
        "table": {
            "header": [
                "No. of sentences",
                "node2vec",
                "syntree2vec",
                "word2vec"
            ],
            "rows": [
                [
                    "0.01MB : 73",
                    "24.93",
                    "20.11",
                    "22"
                ],
                [
                    "0.03MB : 220",
                    "28.44",
                    "28.24",
                    "28.44"
                ],
                [
                    "0.07MB : 508",
                    "43.59",
                    "43.41",
                    "43.41"
                ],
                [
                    "0.15MB : 1070",
                    "71.20",
                    "71.01",
                    "71.01"
                ]
            ],
            "title": "Table 1: Perplexity Scores"
        },
        "insight": "Since the loss, perplexity of syntree2vec is lower than word2vec, node2vec over most of the data sizes given below we say that the syntree2vec performs slightly better than both of them. [CONTINUE] There is a clear margin of difference between the perplexity scores of node2vec and syntree2vec."
    },
    {
        "id": "285",
        "table": {
            "header": [
                "Label",
                "S \u2208 LA",
                "SA \u2208 S",
                "# Train",
                "# Dev"
            ],
            "rows": [
                [
                    "1",
                    "No",
                    "No",
                    "19,446,120",
                    "870,404"
                ],
                [
                    "2",
                    "No",
                    "Yes",
                    "428,122",
                    "25,814"
                ],
                [
                    "3",
                    "Yes",
                    "No",
                    "442,140",
                    "29,558"
                ],
                [
                    "4",
                    "Yes",
                    "Yes",
                    "61,186",
                    "4,286"
                ]
            ],
            "title": "Table 1: Label description for ASNQ. Here S, LA, SA refer to answer sentence, long answer passage and short answer phrase respectively."
        },
        "insight": "For each question in ASNQ, the positive candidate answers are those sentences that occur in the long answer paragraphs in NQ and contain annotated short answers. The remaining sentences from the document are labeled as negative for the target question. [CONTINUE] while the ASNQ statistics are reported in Table 1. [CONTINUE] ASNQ contains 57,242 distinct questions in the training set and 2,672 distinct questions in the dev. set,"
    },
    {
        "id": "286",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] MAP",
                "[BOLD] MRR"
            ],
            "rows": [
                [
                    "Comp-Agg + LM + LC",
                    "0.764",
                    "0.784"
                ],
                [
                    "Comp-Agg + LM + LC+ TL(QNLI)",
                    "0.834",
                    "0.848"
                ],
                [
                    "BERT-B FT WikiQA",
                    "0.813",
                    "0.828"
                ],
                [
                    "BERT-B FT ASNQ",
                    "0.884",
                    "0.898"
                ],
                [
                    "BERT-B TandA (ASNQ \u2192 WikiQA )",
                    "0.893",
                    "0.903"
                ],
                [
                    "BERT-L FT WikiQA",
                    "0.836",
                    "0.853"
                ],
                [
                    "BERT-L FT ASNQ",
                    "0.892",
                    "0.904"
                ],
                [
                    "BERT-L TandA (ASNQ \u2192 WikiQA)",
                    "0.904",
                    "0.912"
                ],
                [
                    "RoBERTa-B FT ASNQ",
                    "0.882",
                    "0.894"
                ],
                [
                    "RoBERTa-B TandA (ASNQ \u2192 WikiQA)",
                    "0.889",
                    "0.901"
                ],
                [
                    "RoBERTa-L FT ASNQ",
                    "0.910",
                    "0.919"
                ],
                [
                    "RoBERTa-L TandA (ASNQ \u2192 WikiQA )",
                    "[BOLD] 0.920",
                    "[BOLD] 0.933"
                ]
            ],
            "title": "Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. DBLP:journals/corr/abs-1905-12897. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively."
        },
        "insight": "Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. (2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. [CONTINUE] Table 3 reports the MAP and MRR of different pre-trained transformers models for two methods: standard fine-tuning (FT) and TANDA. The latter takes two arguments that we indicate as transfer dataset \u2192 adapt dataset."
    },
    {
        "id": "287",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] MAP",
                "[BOLD] MRR"
            ],
            "rows": [
                [
                    "Comp-Agg + LM + LC",
                    "0.868",
                    "0.928"
                ],
                [
                    "Comp-Agg + LM + LC + TL(QNLI)",
                    "0.875",
                    "0.940"
                ],
                [
                    "BERT-B FT TREC-QA",
                    "0.857",
                    "0.937"
                ],
                [
                    "BERT-B FT ASNQ",
                    "0.823",
                    "0.872"
                ],
                [
                    "BERT-B TandA (ASNQ \u2192 TREC-QA)",
                    "0.912",
                    "0.951"
                ],
                [
                    "BERT-L FT TREC-QA",
                    "0.904",
                    "0.946"
                ],
                [
                    "BERT-L FT ASNQ",
                    "0.824",
                    "0.872"
                ],
                [
                    "BERT-L TandA (ASNQ \u2192 TREC-QA )",
                    "0.912",
                    "0.967"
                ],
                [
                    "RoBERTa-B FT ASNQ",
                    "0.849",
                    "0.907"
                ],
                [
                    "RoBERTa-B TandA (ASNQ \u2192TREC-QA )",
                    "0.914",
                    "0.952"
                ],
                [
                    "RoBERTa-L FT ASNQ",
                    "0.880",
                    "0.928"
                ],
                [
                    "RoBERTa-L TandA (ASNQ \u2192 TREC-QA)",
                    "[BOLD] 0.943",
                    "[BOLD] 0.974"
                ]
            ],
            "title": "Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed in [25]. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively."
        },
        "insight": "Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a CompareAggregate model with Language Modeling and Latent Clustering as proposed in (Yoon et al. 2019). TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively. [CONTINUE] Table 4 reports the results of our experiments with TREC-QA."
    },
    {
        "id": "288",
        "table": {
            "header": [
                "[BOLD] BERT-base",
                "WikiQA MAP",
                "WikiQA % Drop",
                "WikiQA MRR",
                "WikiQA % Drop",
                "TREC-QA MAP",
                "TREC-QA % Drop",
                "TREC-QA MRR",
                "TREC-QA % Drop"
            ],
            "rows": [
                [
                    "No noise Fine-tuning",
                    "0.813",
                    "-",
                    "0.828",
                    "-",
                    "0.857",
                    "-",
                    "0.937",
                    "-"
                ],
                [
                    "10% noise Fine-tuning",
                    "0.775",
                    "4.67%",
                    "0.793",
                    "4.22%",
                    "0.826",
                    "3.62%",
                    "0.902",
                    "3.73%"
                ],
                [
                    "20% noise Fine-tuning",
                    "0.629",
                    "[BOLD] 22.63%",
                    "0.645",
                    "22.10%",
                    "0.738",
                    "[BOLD] 13.88%",
                    "0.843",
                    "10.03%"
                ],
                [
                    "No noise TandA (ASNQ \u2192 *)",
                    "0.893",
                    "-",
                    "0.903",
                    "-",
                    "0.912",
                    "-",
                    "0.951",
                    "-"
                ],
                [
                    "10% noise TandA (ASNQ \u2192 *)",
                    "0.876",
                    "1.90%",
                    "0.889",
                    "1.55%",
                    "0.896",
                    "1.75%",
                    "0.941",
                    "1.05%"
                ],
                [
                    "20% noise TandA (ASNQ \u2192 *)",
                    "0.870",
                    "[BOLD] 2.57%",
                    "0.886",
                    "1.88%",
                    "0.891",
                    "[BOLD] 2.30%",
                    "0.937",
                    "1.47%"
                ]
            ],
            "title": "Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. \u2217 indicates the target dataset for the second step of fine-tuning (adapt step)."
        },
        "insight": "Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. \u2217 indicates the target dataset for the second step of fine-tuning (adapt step). [CONTINUE] Table 5 shows the MAP and MRR of BERTBase using FT and TANDA, also indicating the drop percentage (% ) in accuracy due to the injection of noise."
    },
    {
        "id": "289",
        "table": {
            "header": [
                "Model",
                "WikiQA MAP",
                "WikiQA MRR",
                "TREC-QA MAP",
                "TREC-QA MRR"
            ],
            "rows": [
                [
                    "Neg: 1 Pos: 4",
                    "0.870",
                    "0.880",
                    "0.808",
                    "0.847"
                ],
                [
                    "Neg: 2 Pos: 4",
                    "0.751",
                    "0.763",
                    "0.662",
                    "0.751"
                ],
                [
                    "Neg: 3 Pos: 4",
                    "0.881",
                    "0.895",
                    "0.821",
                    "0.869"
                ],
                [
                    "Neg: 2,3 Pos: 4",
                    "0.883",
                    "0.898",
                    "0.823",
                    "0.871"
                ],
                [
                    "Neg: 1,2,3 Pos: 4",
                    "0.884",
                    "0.898",
                    "0.823",
                    "0.872"
                ]
            ],
            "title": "Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning."
        },
        "insight": "Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning. [CONTINUE] We carried out experiments by fine-tuning BERTBase on ASNQ with specific label categories assigned to the negative class. Table 6 shows the results: Label 3 is the most effective negative type of the three,"
    },
    {
        "id": "290",
        "table": {
            "header": [
                "[BOLD] BERT-Base",
                "[BOLD] WikiQA MAP",
                "[BOLD] WikiQA MRR",
                "[BOLD] TREC-QA MAP",
                "[BOLD] TREC-QA MRR"
            ],
            "rows": [
                [
                    "FT QNLI",
                    "0.760",
                    "0.766",
                    "0.820",
                    "0.890"
                ],
                [
                    "FT ASNQ",
                    "0.884",
                    "0.898",
                    "0.823",
                    "0.872"
                ],
                [
                    "TandA (QNLI \u2192)",
                    "0.832",
                    "0.852",
                    "0.863",
                    "0.906"
                ],
                [
                    "TandA (ASNQ \u2192)",
                    "0.893",
                    "0.903",
                    "0.912",
                    "0.951"
                ]
            ],
            "title": "Table 7: Comparison of TandA with ASNQ and QNLI"
        },
        "insight": "Table 7 shows that both FT and TANDA using ASNQ provide significantly better performance than QNLI on the WikiQA dataset."
    },
    {
        "id": "291",
        "table": {
            "header": [
                "MODEL",
                "MODEL",
                "MODEL",
                "Sample 1 Prec@1",
                "Sample 1 MAP",
                "Sample 1 MRR",
                "Sample 2 Prec@1",
                "Sample 2 MAP",
                "Sample 2 MRR",
                "Sample 3 Prec@1",
                "Sample 3 MAP",
                "Sample 3 MRR"
            ],
            "rows": [
                [
                    "BERT",
                    "Base",
                    "NAD",
                    "49.80",
                    "0.506",
                    "0.638",
                    "52.69",
                    "0.432",
                    "0.629",
                    "41.86",
                    "0.352",
                    "0.543"
                ],
                [
                    "BERT",
                    "Base",
                    "ASNQ",
                    "55.06",
                    "0.557",
                    "0.677",
                    "44.31",
                    "0.395",
                    "0.567",
                    "44.19",
                    "0.369",
                    "0.561"
                ],
                [
                    "BERT",
                    "Base",
                    "TANDA (ASNQ \u2192 NAD)",
                    "58.70",
                    "0.585",
                    "0.703",
                    "58.68",
                    "0.474",
                    "0.683",
                    "49.42",
                    "0.391",
                    "0.613"
                ],
                [
                    "BERT",
                    "Large",
                    "NAD",
                    "53.85",
                    "0.537",
                    "0.671",
                    "53.29",
                    "0.469",
                    "0.629",
                    "43.61",
                    "0.395",
                    "0.558"
                ],
                [
                    "BERT",
                    "Large",
                    "ASNQ",
                    "57.49",
                    "0.552",
                    "0.686",
                    "50.89",
                    "0.440",
                    "0.630",
                    "45.93",
                    "0.399",
                    "0.585"
                ],
                [
                    "BERT",
                    "Large",
                    "TANDA (ASNQ \u2192 NAD)",
                    "61.54",
                    "0.607",
                    "0.725",
                    "63.47",
                    "0.514",
                    "0.727",
                    "51.16",
                    "0.439",
                    "0.616"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "NAD",
                    "59.11",
                    "0.563",
                    "0.699",
                    "56.29",
                    "0.511",
                    "0.670",
                    "48.26",
                    "0.430",
                    "0.612"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "ASNQ",
                    "58.70",
                    "0.587",
                    "0.707",
                    "54.50",
                    "0.473",
                    "0.656",
                    "45.35",
                    "0.437",
                    "0.608"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "TANDA (ASNQ \u2192 NAD)",
                    "65.59",
                    "0.623",
                    "0.757",
                    "62.87",
                    "0.537",
                    "0.714",
                    "56.98",
                    "0.473",
                    "0.679"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "NAD",
                    "70.81",
                    "0.654",
                    "0.796",
                    "63.47",
                    "0.581",
                    "0.734",
                    "52.91",
                    "0.490",
                    "0.651"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "ASNQ",
                    "64.37",
                    "0.627",
                    "0.750",
                    "59.88",
                    "0.526",
                    "0.705",
                    "54.65",
                    "0.478",
                    "0.674"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "TANDA (ASNQ \u2192 NAD)",
                    "71.26",
                    "0.680",
                    "0.805",
                    "74.85",
                    "0.625",
                    "0.821",
                    "58.14",
                    "0.514",
                    "0.699"
                ]
            ],
            "title": "Table 8: Comparison between FT and TandA on real-world datasets derived from Alexa Virtual Assistant traffic"
        },
        "insight": "Table 8: Comparison between FT and TANDA on real-world datasets derived from Alexa Virtual Assistant traffic [CONTINUE] In these experiments, we used, as usual, ASNQ for the transfer step, and NAD as our target dataset for the adapt step. Table 8 reports the comparative results using simple FT on NAD (denoted simply by NAD) and tested on samples 1, 2 and 3."
    },
    {
        "id": "292",
        "table": {
            "header": [
                "IC = 1 Feature",
                "IC = 1 Value",
                "IC = 1 Contribution",
                "IC = 2 Feature",
                "IC = 2 Value",
                "IC = 2 Contribution",
                "IC = 3 Feature",
                "IC = 3 Value",
                "IC = 3 Contribution"
            ],
            "rows": [
                [
                    "has_diff",
                    "0.0",
                    "+0.953",
                    "dif_too",
                    "1.0",
                    "+1.155",
                    "Bias term",
                    "1.0",
                    "+0.872"
                ],
                [
                    "Bias term",
                    "1.0",
                    "+0.419",
                    "Bias term",
                    "1.0",
                    "+0.594",
                    "has_int",
                    "0.0",
                    "+0.450"
                ],
                [
                    "dif_but",
                    "0.0",
                    "+0.114",
                    "dif_consider",
                    "1.0",
                    "+0.491",
                    "dif_may",
                    "1.0",
                    "+0.393"
                ],
                [
                    "dif_because",
                    "0.0",
                    "+0.058",
                    "dif_however",
                    "1.0",
                    "+0.186",
                    "dif_but",
                    "1.0",
                    "+0.306"
                ],
                [
                    "dif_how",
                    "0.0",
                    "+0.033",
                    "dif_how",
                    "1.0",
                    "+0.092",
                    "dif_hope",
                    "1.0",
                    "+0.215"
                ],
                [
                    "dif_yet",
                    "0.0",
                    "+0.033",
                    "dif_hope",
                    "0.0",
                    "+0.027",
                    "dif_while",
                    "0.0",
                    "+0.060"
                ],
                [
                    "int_unity",
                    "0.0",
                    "+0.029",
                    "dif_perhaps",
                    "0.0",
                    "+0.019",
                    "dif_rather",
                    "0.0",
                    "+0.058"
                ],
                [
                    "dif_depend",
                    "0.0",
                    "+0.024",
                    "dif_almost",
                    "0.0",
                    "+0.018",
                    "dif_too",
                    "0.0",
                    "+0.033"
                ],
                [
                    "dif_hope",
                    "0.0",
                    "+0.022",
                    "dif_sometimes",
                    "0.0",
                    "+0.012",
                    "dif_seem",
                    "0.0",
                    "+0.030"
                ],
                [
                    "dif_rather",
                    "0.0",
                    "+0.022",
                    "dif_although",
                    "0.0",
                    "+0.011",
                    "dif_differ",
                    "0.0",
                    "+0.026"
                ],
                [
                    "has_int",
                    "0.0",
                    "-0.009",
                    "dif_while",
                    "0.0",
                    "-0.032",
                    "int_remain",
                    "0.0",
                    "-0.024"
                ],
                [
                    "dif_close_to",
                    "0.0",
                    "-0.009",
                    "dif_rather",
                    "0.0",
                    "-0.033",
                    "dif_separate",
                    "0.0",
                    "-0.026"
                ],
                [
                    "dif_seem",
                    "0.0",
                    "-0.010",
                    "dif_different",
                    "0.0",
                    "-0.050",
                    "int_weigh",
                    "0.0",
                    "-0.028"
                ],
                [
                    "dif_consider",
                    "0.0",
                    "-0.011",
                    "dif_often",
                    "0.0",
                    "-0.050",
                    "dif_possible",
                    "0.0",
                    "-0.029"
                ],
                [
                    "int_account",
                    "0.0",
                    "-0.012",
                    "dif_each",
                    "0.0",
                    "-0.050",
                    "int_unity",
                    "0.0",
                    "-0.031"
                ],
                [
                    "dif_secret",
                    "0.0",
                    "-0.013",
                    "dif_either",
                    "0.0",
                    "-0.069",
                    "dif_however",
                    "0.0",
                    "-0.036"
                ],
                [
                    "dif_differ",
                    "0.0",
                    "-0.018",
                    "dif_about",
                    "0.0",
                    "-0.078",
                    "dif_often",
                    "0.0",
                    "-0.037"
                ],
                [
                    "dif_usually",
                    "0.0",
                    "-0.019",
                    "dif_both",
                    "0.0",
                    "-0.079",
                    "dif_about",
                    "0.0",
                    "-0.068"
                ],
                [
                    "int_remain",
                    "0.0",
                    "-0.024",
                    "dif_because",
                    "0.0",
                    "-0.284",
                    "dif_though",
                    "0.0",
                    "-0.073"
                ],
                [
                    "dif_may",
                    "0.0",
                    "-0.036",
                    "dif_but",
                    "1.0",
                    "-0.575",
                    "dif_because",
                    "0.0",
                    "-0.102"
                ]
            ],
            "title": "Table 5: Top ten and bottom ten features used in successful IC classifications using vocabulary features. Differentiation and integration terms are prefixed with dif and int, while has_dif and has_int are the binary features for whether any differentiation/integration terms are present at all. The bias term is the averaged sum of the value associated with each root node in the ensemble."
        },
        "insight": "The Vocabulary model correctly classified test items in three IC bands only (Table 5)."
    },
    {
        "id": "293",
        "table": {
            "header": [
                "[BOLD] Algorithm",
                "[BOLD] en_ud",
                "[BOLD] en_ptb",
                "[BOLD] sents/s"
            ],
            "rows": [
                [
                    "Eisner (generic)",
                    "96.35",
                    "479.1",
                    "\u223c 80"
                ],
                [
                    "Eisner (ours)",
                    "1.496",
                    "6.31",
                    "\u223c 6009"
                ],
                [
                    "CLE (generic)",
                    "19.12",
                    "93.8",
                    "\u223c 404"
                ],
                [
                    "CLE (ours)",
                    "1.764",
                    "6.98",
                    "\u223c 5436"
                ]
            ],
            "title": "Figure 1, Listing 1 & Table 1: (Right code snippet) Implementation of Kiperwasser and Goldberg (2016)\u2019s neural parser in only a few lines using UniParse. (Right table and left figure) Seconds a decoder takes to decode an entire dataset, given a set of scores. Score matrix entries are generated uniformly on [0, 1]. The random generated data has an impact on CLE since worst-case performance depends on the sorting bottleneck; the figure demonstrates this by the increasingly broad standard deviation band. Experiments are run on an Ubuntu machine with an Intel Xeon E5-2660, 2.60GHz CPU."
        },
        "insight": "Seconds a decoder takes to decode an entire dataset, given a set of scores. Score matrix entries are [CONTINUE] We compare Cython implementations in Table 1 and Figure 1 over randomised score input. Note that our implementations are significantly faster."
    },
    {
        "id": "294",
        "table": {
            "header": [
                "Treebank ar_padt",
                "Baseline form",
                "Baseline 4.19",
                "Our 3.90",
                "Upp. 2.93",
                "Err.red. 23.1"
            ],
            "rows": [
                [
                    "ca_ancora",
                    "form",
                    "4.65",
                    "4.35",
                    "3.32",
                    "22.3"
                ],
                [
                    "cs_cac",
                    "form5",
                    "3.56",
                    "2.25",
                    "1.14",
                    "54.0"
                ],
                [
                    "cs_fictree",
                    "form5",
                    "4.82",
                    "4.08",
                    "2.68",
                    "34.6"
                ],
                [
                    "cs_pdt",
                    "form5",
                    "4.93",
                    "3.41",
                    "1.65",
                    "46.6"
                ],
                [
                    "da_ddt",
                    "form",
                    "2.32",
                    "2.16",
                    "1.55",
                    "21.2"
                ],
                [
                    "en_ewt",
                    "form",
                    "2.29",
                    "2.22",
                    "1.78",
                    "13.8"
                ],
                [
                    "es_ancora",
                    "form",
                    "3.99",
                    "3.38",
                    "2.25",
                    "34.7"
                ],
                [
                    "et_edt",
                    "form5",
                    "4.78",
                    "4.31",
                    "2.54",
                    "20.9"
                ],
                [
                    "fa_seraji",
                    "form",
                    "8.99",
                    "8.76",
                    "7.44",
                    "14.8"
                ],
                [
                    "fr_gsd",
                    "form",
                    "4.12",
                    "3.81",
                    "2.70",
                    "22.0"
                ],
                [
                    "hi_hdtb",
                    "form",
                    "4.18",
                    "3.58",
                    "2.83",
                    "44.3"
                ],
                [
                    "hr_set",
                    "form5",
                    "4.04",
                    "2.87",
                    "1.71",
                    "50.2"
                ],
                [
                    "it_isdt",
                    "form",
                    "4.27",
                    "3.71",
                    "2.78",
                    "37.8"
                ],
                [
                    "it_postwita",
                    "form",
                    "3.60",
                    "4.07",
                    "2.37",
                    "-38.0"
                ],
                [
                    "ja_gsd",
                    "form",
                    "1.64",
                    "1.93",
                    "1.41",
                    "-123.1"
                ],
                [
                    "ko_kaist",
                    "form",
                    "0.14",
                    "2.41",
                    "0.11",
                    "-6392.8"
                ],
                [
                    "la_ittb",
                    "form5",
                    "6.53",
                    "6.97",
                    "3.85",
                    "-16.4"
                ],
                [
                    "la_proiel",
                    "form5",
                    "6.92",
                    "7.42",
                    "4.20",
                    "-18.4"
                ],
                [
                    "lv_lvtb",
                    "form5",
                    "3.90",
                    "3.39",
                    "2.10",
                    "28.0"
                ],
                [
                    "no_bokmaal",
                    "form",
                    "2.79",
                    "2.22",
                    "1.48",
                    "43.6"
                ],
                [
                    "no_nynorsk",
                    "form",
                    "2.73",
                    "2.52",
                    "1.48",
                    "16.7"
                ],
                [
                    "pl_lfg",
                    "form5",
                    "3.68",
                    "3.06",
                    "1.84",
                    "33.6"
                ],
                [
                    "pt_bosque",
                    "form",
                    "3.57",
                    "3.17",
                    "2.55",
                    "39.0"
                ],
                [
                    "ro_nonstd",
                    "form5",
                    "8.13",
                    "7.95",
                    "5.64",
                    "7.2"
                ],
                [
                    "sk_snk",
                    "form5",
                    "2.87",
                    "2.01",
                    "0.63",
                    "38.2"
                ],
                [
                    "uk_iu",
                    "form",
                    "2.66",
                    "1.94",
                    "0.88",
                    "40.7"
                ],
                [
                    "ur_udtb",
                    "form",
                    "3.95",
                    "3.79",
                    "2.65",
                    "12.3"
                ],
                [
                    "Average",
                    "[EMPTY]",
                    "4.08",
                    "3.77",
                    "2.45",
                    "-210.3"
                ],
                [
                    "Median",
                    "[EMPTY]",
                    "3.97",
                    "3.40",
                    "2.31",
                    "22.7"
                ]
            ],
            "title": "Table 2: Results of form clustering, measured in % of 1\u2212vmeasure (expressing the error, i.e.\u00a0lower is better). Baseline (either full form or prefix of form of length 5), our system, and oracle upper bound. Last column is error reduction on the scale from baseline to upper bound, in %."
        },
        "insight": "Our experiments on 23 languages show our approach to be promising, surpassing the baseline on 23 of the 28 evaluation datasets. [CONTINUE] We evaluate our setup on 28 datasets for 23 languages, finding that it outperforms the baseline on 23 of the datasets, [CONTINUE] This results in a set of 28 treebanks for 23 languages [CONTINUE] listed in Table 2. [CONTINUE] The results are listed in Table 2. [CONTINUE] the oracle does not reach 100%; [CONTINUE] We also express the performance of our method as error reduction on the scale from baseline (0%) to upper bound (100%). [CONTINUE] For 23 of the 28 datasets, our method achieves a positive error reduction; the median error reduction is 23%. Because of the extreme result for Korean, the average does not make much sense here. The results are worst for Korean and Japanese, [CONTINUE] the \"form\" baseline very close to the upper bound [CONTINUE] our results are very low here. We also observe deteriorations for a treebank of Italian Tweets and for treebanks of historical Latin, [CONTINUE] On all other datasets, we observe an improvement over the baseline, with an error reduction typically between 10% and 35%. The performance is especially good for Slavic languages (cs, hr, pl, sk, uk), where the error reduction is often around 50%. [CONTINUE] The evaluation showed the approach to be promising, surpassing the baseline on most of the evaluation datasets."
    },
    {
        "id": "295",
        "table": {
            "header": [
                "Distance",
                "Average",
                "Median"
            ],
            "rows": [
                [
                    "[ITALIC] JW",
                    "8.17",
                    "7.92"
                ],
                [
                    "[ITALIC] cos",
                    "4.39",
                    "3.87"
                ],
                [
                    "[ITALIC] JW\u22c5 [ITALIC] cos",
                    "3.77",
                    "3.40"
                ]
            ],
            "title": "Table 3: Comparison of the word form similarities, in % of 1\u2212vmeasure of the clustering. Average and median over the 28 datasets."
        },
        "insight": "In Table 3, we compare the combined distance measure with each of the two components used alone. The results show that combining the edit distance with the embedding similarity is stronger than using any of the measures alone. [CONTINUE] The embedding similarity alone performs much better than the edit distance alone."
    },
    {
        "id": "296",
        "table": {
            "header": [
                "#of Authors",
                "6",
                "6",
                "8",
                "10",
                "12",
                "14"
            ],
            "rows": [
                [
                    "samples/author",
                    "350",
                    "1100",
                    "931",
                    "849",
                    "562",
                    "469"
                ],
                [
                    "Char-CNN",
                    "83",
                    "96",
                    "92",
                    "86",
                    "75",
                    "69"
                ],
                [
                    "W2V(CBOW)",
                    "65.3",
                    "97",
                    "82.8",
                    "83.3",
                    "76.4",
                    "71.8"
                ],
                [
                    "fastText(CBOW)",
                    "65",
                    "73",
                    "58",
                    "35.7",
                    "37.31",
                    "40.3"
                ],
                [
                    "W2V(Skip)",
                    "79",
                    "94",
                    "91.1",
                    "85.4",
                    "[BOLD] 82.2",
                    "78.6"
                ],
                [
                    "fastText(Skip)",
                    "[BOLD] 86",
                    "[BOLD] 98",
                    "[BOLD] 95.2",
                    "[BOLD] 86.35",
                    "80.9",
                    "[BOLD] 81.2"
                ]
            ],
            "title": "TABLE II: Performance comparison of different models with pre-trained embedding"
        },
        "insight": "Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples. [CONTINUE] The classification was carried out with 2 author attribution datasets, one with 6 authors  and our dataset with maximum 14 authors. The larger dataset was trained with 6,8,10,12 and 14 authors to analyze the effects of increasing classes on the proposed model. [CONTINUE] We evaluate the performance of the proposed architecture in terms of accuracy, [CONTINUE] We also try to infer how the character-level model compares with the word level models. All models are compared for the increasing number of authors(classes) on the corpus mentioned to assess the quality of the models. [CONTINUE] both word and character levels are summarized in Table II. [CONTINUE] From the accuracy comparisons shown in Table II we see that Skip-gram implemented by fastText performs well in the given datasets. [CONTINUE] Character level model performs reasonably well in competition with subword level as long as the dataset is big enough. [CONTINUE] With larger datasets, this model will be able to perform significantly better compare character embeddings with word embeddings showing that character embeddings perform almost as good as the best word embedding model."
    },
    {
        "id": "297",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Label Accuracy ( [ITALIC] \u03d5 = 0.76)",
                "[BOLD] Label Accuracy ( [ITALIC] \u03d5 = 0.67)"
            ],
            "rows": [
                [
                    "HexaF - UCL",
                    "80.18",
                    "80.18"
                ],
                [
                    "Our Model (BERT)",
                    "[BOLD] 80.20",
                    "[BOLD] 86.7"
                ]
            ],
            "title": "Table 3: Comparison of the Label accuracy on Development set."
        },
        "insight": "Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of \"SUPPORTS\" label against a supervised approach \u2013 HexaF. [CONTINUE] Results from Table 3 suggests that our approach is comparable to HexaF4 for \u03c6 = 0.76."
    },
    {
        "id": "298",
        "table": {
            "header": [
                "[BOLD] Type of FEVER Set",
                "[BOLD] Total Claims",
                "[BOLD] Claims Converted to Questions",
                "[BOLD] Conversion Accuracy",
                "[BOLD] Total Questions",
                "[BOLD] Questions per claim (Median)"
            ],
            "rows": [
                [
                    "Training Set",
                    "145449",
                    "131969",
                    "90.73",
                    "395717",
                    "3"
                ],
                [
                    "Development Set",
                    "19998",
                    "17749",
                    "88.75",
                    "54422",
                    "3"
                ],
                [
                    "Test Set",
                    "9999",
                    "8863",
                    "88.63",
                    "27359",
                    "3"
                ]
            ],
            "title": "Table 1: Performance of the question generation system on FEVER Dataset."
        },
        "insight": "For the subtask of question generation, the results in Table 1 show that the system is able to generate questions given a claim with considerably good accuracy. The conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims. The results also support our assumption that the claims generally feature information about one or more entities. [CONTINUE] Ta"
    },
    {
        "id": "299",
        "table": {
            "header": [
                "[BOLD] Type of Set",
                "[BOLD] Label Accuracy ( [ITALIC] \u03d5 = 0.76)",
                "[BOLD] Label Accuracy ( [ITALIC] \u03d5 = 0.67)"
            ],
            "rows": [
                [
                    "Training Set",
                    "81.52",
                    "88.05"
                ],
                [
                    "Development Set",
                    "80.20",
                    "86.7"
                ],
                [
                    "Test Set",
                    "80.25",
                    "87.04"
                ]
            ],
            "title": "Table 2: Performance of the question generation system on FEVER Dataset."
        },
        "insight": "Table 2 shows the performance of our Fact Checking system on the \"SUPPORTS\" label, the output of our system. We compare the results against two different classification thresholds. [CONTINUE] Here, \u03c6 = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while \u03c6 = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as \"SUPPORTS\". [CONTINUE] In contrast to the results reported in Table 2, here we consider \u03c6 = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset. [CONTINUE] From the results, we conclude that it is possible to verify the facts with the right kind of factoid questions."
    },
    {
        "id": "300",
        "table": {
            "header": [
                "[EMPTY]",
                "Transductive scenario Gap  [ITALIC] F1",
                "Transductive scenario Gap  [ITALIC] FF1",
                "Transductive scenario Gap  [ITALIC] FM1",
                "Transductive scenario Bias  [ITALIC] FF1 [ITALIC] FM1",
                "Dpr",
                "Wsc",
                "Wnli",
                "[EMPTY]"
            ],
            "rows": [
                [
                    "SOTA",
                    "72.1%",
                    "71.4%",
                    "72.8%",
                    "0.98",
                    "76.4%",
                    "[BOLD] 72.5%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[BOLD] 74.7%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[EMPTY]"
                ],
                [
                    "Bert",
                    "50.0%",
                    "47.2%",
                    "52.7%",
                    "0.90",
                    "59.8%",
                    "61.9%",
                    "65.8%",
                    "no train data"
                ],
                [
                    "Bert_WikiRand",
                    "55.1%",
                    "51.8%",
                    "58.2%",
                    "0.89",
                    "59.2%",
                    "59.3%",
                    "65.8%",
                    "no train data"
                ],
                [
                    "Bert_WikiCREM",
                    "[BOLD] 59.0%",
                    "[BOLD] 57.5%",
                    "[BOLD] 60.5%",
                    "[BOLD] 0.95",
                    "[BOLD] 67.4%",
                    "[BOLD] 63.4%",
                    "[BOLD] 67.1%",
                    "no train data"
                ],
                [
                    "Bert_Gap",
                    "75.2%",
                    "75.1%",
                    "75.3%",
                    "[BOLD] 1.00\u2013\u2013\u2013\u2013\u2013",
                    "66.8%",
                    "63.0%",
                    "68.5%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_Gap",
                    "[BOLD] 77.4%",
                    "[BOLD] 78.4%",
                    "[BOLD] 76.4%",
                    "1.03",
                    "[BOLD] 71.1%",
                    "[BOLD] 64.1%",
                    "[BOLD] 70.5%",
                    "existing train data"
                ],
                [
                    "Bert_Dpr",
                    "60.9%",
                    "61.3%",
                    "60.6%",
                    "1.01",
                    "[BOLD] 83.3%",
                    "67.0%",
                    "71.9%",
                    "existing train data"
                ],
                [
                    "Bert_Gap_Dpr",
                    "[BOLD] 70.0%",
                    "[BOLD] 70.4%",
                    "[BOLD] 69.5%",
                    "1.01",
                    "79.4%",
                    "65.6%",
                    "72.6%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_Dpr",
                    "64.2%",
                    "64.2%",
                    "64.1%",
                    "[BOLD] 1.00\u2013\u2013\u2013\u2013\u2013",
                    "80.0%",
                    "[BOLD] 71.8%",
                    "[BOLD] 74.7%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "existing train data"
                ],
                [
                    "Bert_all",
                    "76.0%",
                    "77.4%",
                    "74.7%",
                    "[BOLD] 1.04",
                    "80.1%",
                    "[BOLD] 70.0%",
                    "74.0%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_all",
                    "[BOLD] 78.0%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[BOLD] 79.4%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[BOLD] 76.7%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[BOLD] 1.04",
                    "[BOLD] 84.8%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "[BOLD] 70.0%",
                    "[BOLD] 74.7%\u2013\u2013\u2013\u2013\u2013\u2013\u2013",
                    "existing train data"
                ]
            ],
            "title": "Table 1: Evaluation of trained models on all test sets. Gap and WinoBias (abbreviated WB) are additionally split into subsets, as introduced in Section\u00a05. Double lines in the table separate results from three different scenarios: when no training data is available, when additional training data exists, and the transductive scenario. The table is further split into sections separated with single horizontal lines. Each section contains a model that has been trained on WikiCREM and models that have not been. The best result in each section is in bold. The best overall result is underlined. Scores on Gap are measured as F1-scores, while the performance on other datasets is given in accuracy. The source of each SOTA is listed in Section\u00a05."
        },
        "insight": "The results of the evaluation of the models on the test sets are shown in Table 1. We notice that additional training on WIKICREM consistently improves the performance of the models in all scenarios and on most tests. Due to the small size of some test sets, some of the results are subject to deviation. This especially applies to PDP (60 test samples) and WNLI (145 test samples). We observe that BERT WIKIRAND generally performs worse than BERT, with GAP and PDP being notable exceptions. This shows that BERT is a strong baseline and that improved performance of BERT WIKICREM is not a consequence of training on shorter sentences or with different loss function. BERT WIKICREM consistently outperforms both baselines on all tests, showing that WIKICREM can be used as a standalone dataset. We observe that training on the data from the target distribution improves the performance the most. Models trained on GAP-train usually show more than a 20% increase in their F1-score on GAP-test. Still, BERT WIKICREM GAP shows [CONTINUE] a consistent improvement over BERT GAP on all subsets of the GAP test set. This confirms that WIKICREM works not just as a standalone dataset, but also as an additional pre-training in the transductive scenario. Similarly, BERT WIKICREM DPR outpertasks, forms BERT DPR on the majority of showing the applicability of WIKICREM to the scenario where additional training data is available. However, good results of BERT GAP DPR show that additional training on a manually constructed dataset, such as GAP, can yield similar results as additional training on WIKICREM. The reason behind this difference is the impact of the data distribution. GAP, DPR, and WIKICREM contain data that follows different distributions which strongly impacts the trained models. This [CONTINUE] can be seen when we fine-tune BERT GAP on DPR to obtain BERT GAP DPR, as the model's performance on GAP-test drops by 8.2%. WIKICREM's data distribution strongly differs from the test sets' as described in Section 3. the achieves However, the best results are achieved when all available data is combined, as shown by the models BERT ALL and BERT WIKICREM ALL. BERT WIKICREM ALL highest performance on GAP, DPR, WNLI, and WINOBIAS among the models, and sets the new state-of-the-art result on GAP, DPR, and WINOBIAS. result on the WINOGENDER dataset is achieved by the BERT WIKICREM DPR model, while BERT WIKICREM ALL and BERT GAP DPR set the new state-of-the-art on the PDP dataset. The new state-of-the-art"
    },
    {
        "id": "301",
        "table": {
            "header": [
                "#of Authors",
                "6",
                "6",
                "8",
                "10",
                "12",
                "14"
            ],
            "rows": [
                [
                    "#of samples/class",
                    "350",
                    "1100",
                    "931",
                    "849",
                    "562",
                    "469"
                ],
                [
                    "Pretrained Embedding",
                    "83",
                    "96",
                    "92",
                    "86",
                    "75",
                    "69"
                ],
                [
                    "Not pretrained",
                    "71",
                    "95",
                    "82",
                    "83",
                    "66",
                    "59.5"
                ]
            ],
            "title": "TABLE III: pretrained vs non-pretrained comparison"
        },
        "insight": "Various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples. [CONTINUE] with and without pre-training character level embedding and comparing the proposed architectures on the held-out dataset. [CONTINUE] To illustrate the need of pre-trained character embeddings, we see from III that using a pre-trained embedding increases the accuracy across datasets and the different number of authors, regardless of the amount of data for each author. [CONTINUE] increase the performance a few degrees. [CONTINUE] we analyzed the importance of pretrained character embedding for author attribution and showed that pre-training can result in better performances."
    },
    {
        "id": "302",
        "table": {
            "header": [
                "Model",
                "bn",
                "jv",
                "lo",
                "mr",
                "pa",
                "te",
                "ti",
                "uk",
                "ug",
                "Avg."
            ],
            "rows": [
                [
                    "[BOLD] Exact",
                    ".00",
                    ".63",
                    ".02",
                    ".00",
                    ".00",
                    ".00",
                    ".02",
                    ".02",
                    ".03",
                    ".08"
                ],
                [
                    "[BOLD] Trans",
                    ".00",
                    ".63",
                    ".02",
                    ".17",
                    ".00",
                    ".00",
                    ".46",
                    ".02",
                    ".03",
                    ".15"
                ],
                [
                    "[BOLD] Encode",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Manual",
                    ".36 (hi)",
                    ".70 (id)",
                    ".07 (th)",
                    ".46 (hi)",
                    ".31 (hi)",
                    ".20 (ta)",
                    ".44 (am)",
                    ".25 (ru)",
                    ".16 (tr)",
                    ".33"
                ],
                [
                    "Best-53",
                    ".38 (ms)",
                    ".70 (id)",
                    ".07 (th)",
                    ".46 (hi)",
                    ".36 (te)",
                    ".36 (pa)",
                    ".44 (am)",
                    ".41 (kk)",
                    ".16 (tr)",
                    ".37"
                ],
                [
                    "[BOLD] Pbel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Manual",
                    ".48 (hi)",
                    ".86 (id)",
                    "[BOLD] .28 (th)",
                    "[BOLD] .62 (hi)",
                    "[BOLD] .49 (hi)",
                    ".33 (ta)",
                    "[BOLD] .69 (am)",
                    ".50 (ru)",
                    ".32 (tr)",
                    ".51"
                ],
                [
                    "Best-53",
                    ".48 (hi)",
                    ".86 (id)",
                    "[BOLD] .28 (th)",
                    "[BOLD] .62 (hi)",
                    "[BOLD] .49 (hi)",
                    "[BOLD] .47 (hi)",
                    "[BOLD] .69 (am)",
                    ".54 (kk)",
                    ".32 (tr)",
                    ".53"
                ],
                [
                    "Multi",
                    "[BOLD] .53",
                    "[BOLD] .87",
                    "[BOLD] .28",
                    "[BOLD] .62",
                    ".48",
                    ".46",
                    "[BOLD] .69",
                    "[BOLD] .56",
                    "[BOLD] .40",
                    "[BOLD] .54"
                ]
            ],
            "title": "Table 1: Accuracy for cross-lingual Wikipedia title linking, with the transfer HRL shown in parentheses. The best accuracy among input representations with graphemes, phonemes or articulatory features for Encode and PBEL is presented here. Complete results for each representation are in the supplementary material."
        },
        "insight": "The entity linking accuracy on the Wikipedia test set are summarized in Table 1. On average, our proposed PBEL method performs significantly better than the baselines, with significant accuracy gains in all nine test languages. [CONTINUE] The EXACT baseline, which is most often used for monolingual EL (Sil et al. 2017), performs reasonably only when the test language is in the same script as English (i.e., Javanese). [CONTINUE] Similarly TRANS, the current state-of-the-art retrieval method in cross-lingual EL (Pan et al. 2017), fails when zero data is available in the test language, unless the HRL is very closely-related to the LRL (as with jv, mr and am). On the other hand, ENCODE presents relatively strong zero-shot transfer results. [CONTINUE] PBEL offers stronger performance than ENCODE [CONTINUE] As seen in the BEST-53 results, the HRL that performs best is closely-related to the respective test LRL [CONTINUE] We also observe that using multiple pivot HRLs leads to better average accuracy, with considerable improvement for some languages. [CONTINUE] In most cases, the MANUAL HRL is also the best performing in BEST-53. However, we see that the Dravidian Telugu (te) seems to obtain higher accuracy with Indo-Aryan HRLs \u2013 Punjabi (pa) or Hindi (hi). [CONTINUE] We also see that the Ukrainian (uk) test set has better performance with another Cyrillic script language, Kazakh (kk), rather than Russian (ru)."
    },
    {
        "id": "303",
        "table": {
            "header": [
                "Lang.",
                "Tigrinya",
                "Oromo"
            ],
            "rows": [
                [
                    "Exact",
                    "0.00",
                    "0.01"
                ],
                [
                    "Trans Supervised",
                    "0.21",
                    "0.05"
                ],
                [
                    "Trans Unsupervised",
                    "0.21",
                    "0.01"
                ],
                [
                    "Encode",
                    "0.16",
                    "0.10"
                ],
                [
                    "Pbel",
                    "[BOLD] 0.33",
                    "[BOLD] 0.11"
                ]
            ],
            "title": "Table 2: Entity linking accuracy on non-Wikipedia data"
        },
        "insight": "Entity linking accuracies on the LORELEI dataset are shown in Table 2. PBEL has considerably higher accuracy than the other methods. However, we see relatively lower improvement in accuracy with Somali-Oromo than Amharic-Tigrinya. [CONTINUE] the supervised TRANS model, which uses Wikipedia parallel data in the LRL itself as a lexicon, does not perform better than the zero-shot PBEL. [CONTINUE] The PBEL model results for our test set with each input representation are presented in Table 3."
    },
    {
        "id": "304",
        "table": {
            "header": [
                "Input",
                "*bn (hi)",
                "jv (id)",
                "*lo (th)",
                "mr (hi)",
                "*pa (hi)",
                "*te (ta)",
                "ti (am)",
                "*uk (ru)",
                "*ug (tr)"
            ],
            "rows": [
                [
                    "Grapheme",
                    ".00",
                    "[BOLD] .86",
                    ".02",
                    "[BOLD] .62",
                    ".00",
                    ".00",
                    ".61",
                    "[BOLD] .50",
                    ".08"
                ],
                [
                    "Phoneme",
                    "[BOLD] .48",
                    ".84",
                    ".20",
                    ".58",
                    ".18",
                    ".10",
                    "[BOLD] .69",
                    ".23",
                    ".21"
                ],
                [
                    "Articulatory",
                    ".45",
                    ".82",
                    "[BOLD] .28",
                    ".56",
                    "[BOLD] .49",
                    "[BOLD] .33",
                    ".63",
                    ".42",
                    "[BOLD] .32"
                ]
            ],
            "title": "Table 3: Entity linking accuracy with PBEL, using Graphemes, Phonemes or Articulatory features as input. The HRL used for training and pivoting is shown in parentheses in the first row. The pairs with the different scripts are marked with a \u201c*\u201d."
        },
        "insight": "see We that using phonological representations (phonemes and articulatory features) offers the ability to map between languages that use different orthographies, explaining the convincing improvement over graphemes for HRL-LRL pairs that are written in different scripts (Table 3). With graphemes, the experiments on these languages achieve \u2248 0 accuracy because the character vocabulary of the HRL encoder simply does not contain the low-resource test language characters. This is the case with Lao-Thai (lo-th), Telugu-Tamil (te-ta) and Bengali-Hindi (bn-hi). In contrast, we observe that the grapheme representation offers strong transfer performance when the LRL and HRL share orthography, notably Javanese-Indonesian (jv-id), Marathi-Hindi (mr-hi) and Ukrainian-Russian (uk-ru)."
    },
    {
        "id": "305",
        "table": {
            "header": [
                "[EMPTY]",
                "Textbooks",
                "Wikipedia"
            ],
            "rows": [
                [
                    "IR",
                    "25.24",
                    "25.14"
                ],
                [
                    "PMI",
                    "26.22",
                    "25.19"
                ],
                [
                    "ESIM w/o scenario",
                    "25.85",
                    "25.41"
                ],
                [
                    "ESIM w/ scenario",
                    "24.34",
                    "24.41"
                ],
                [
                    "DIIN w/o scenario",
                    "24.15",
                    "25.20"
                ],
                [
                    "DIIN w/ scenario",
                    "25.11",
                    "24.89"
                ],
                [
                    "BERT [ITALIC] NLI w/o scenario",
                    "24.29",
                    "24.17"
                ],
                [
                    "BERT [ITALIC] NLI w/ scenario",
                    "24.97",
                    "24.68"
                ],
                [
                    "BiMPM w/o scenario",
                    "24.13",
                    "24.51"
                ],
                [
                    "BiMPM w/ scenario",
                    "24.76",
                    "23.81"
                ],
                [
                    "BERT [ITALIC] RC w/o scenario",
                    "24.81",
                    "24.78"
                ],
                [
                    "BERT [ITALIC] RC w/ scenario",
                    "23.66",
                    "23.01"
                ]
            ],
            "title": "Table 1: Proportions of correctly answered questions."
        },
        "insight": "The results are summarized in Table [CONTINUE] Almost all the methods performed similar to random guess, showing that SQA on our dataset has its unique challenges."
    },
    {
        "id": "306",
        "table": {
            "header": [
                "Embedding",
                "lang.",
                "r@1",
                "r@5",
                "r@10"
            ],
            "rows": [
                [
                    "vse\u00a0",
                    "en",
                    "64.6",
                    "\u2205",
                    "95.7"
                ],
                [
                    "dsve\u00a0",
                    "en",
                    "69.8",
                    "91.9",
                    "96.6"
                ],
                [
                    "dsve w/ w2v",
                    "en",
                    "63.48",
                    "89.48",
                    "95.64"
                ],
                [
                    "dsve w/ FastText",
                    "en",
                    "66.08",
                    "90.7",
                    "96.2"
                ],
                [
                    "Ours w/ bv",
                    "en",
                    "65.58",
                    "90.52",
                    "96.1"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "67.78",
                    "91.58",
                    "96.92"
                ],
                [
                    "Ours w/ muse",
                    "en",
                    "63.1",
                    "89.58",
                    "95.56"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "63.88",
                    "89.2",
                    "95.24"
                ],
                [
                    "[EMPTY]",
                    "en+fr+de",
                    "62.4",
                    "89.18",
                    "95.16"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "63.28",
                    "88.3",
                    "94.6"
                ]
            ],
            "title": "Table 1: Experiment 1: Caption retrieval on the coco dataset. We compare the different reminders of the different methods first in English and then by adding new languages. We also evaluate variations of dsve method with different word embedding."
        },
        "insight": "The table 1 shows the caption retrieval recall on COCO dataset. The first two lines show the state-of-the-art results. The second pair of lines present the results of our model, with W2V and FastText embeddings used as the baseline. [CONTINUE] We can see that our model is close to the Deep SemanticVisual Embedding (DSVE) method  while the W2V method is slightly worst, as the representation power of the word embedding is reduced. [CONTINUE] The BIVEC English-French method is used in English and on both languages simultaneously. If trained only on English, i.e. only on the COCO dataset like the two previous methods, it shows performance similar to the one of the state-of-the-art. This means training using BIVEC does not weaken the English representation. When trained on English and French together, the recall is increased by 3.35 %, going from 65.58 % to 67.78 %. We can also see an improvement for recall@5 and recall@10, with respectively 1.17 % and 0.85 % of increase. This implies that the similarity learning with French captions increases English recognition when using BIVEC. [CONTINUE] First of all, when training with MUSE for English only, we can see a sharp decrease in performance, with a recall going from 66.08 % to 63.10 %. By comparing the model trained with W2V, we obtain similar results. This could come from the fact that both MUSE and W2V embeddings do not have representation for out of vocabulary words like the FastText ones. Moreover, rare words have much more chance to be wrongly projected because of space transformation. When we train the model with additional languages, we can see a slight decrease in performance in English. The maximum decrease is 1.01 % for recall@10, but it is counterbalanced by an increase of 0.29 % for the recall@1. [CONTINUE] We evaluated our method on the COCO dataset for English-only results and shown that using BIVEC embeddings enables the use of another language in order to improve the performance. The obtained improvement is a 3.35 % increase in performance on the COCO dataset, and a 15.15 % increase on the Multi30K dataset."
    },
    {
        "id": "307",
        "table": {
            "header": [
                "Embedding",
                "lang.",
                "r@1",
                "r@5",
                "r@10"
            ],
            "rows": [
                [
                    "vse\u00a0",
                    "en",
                    "52.00",
                    "\u2205",
                    "92.0"
                ],
                [
                    "dsve\u00a0",
                    "en",
                    "55.9",
                    "86.9",
                    "94.0"
                ],
                [
                    "dsve w/ w2v",
                    "en",
                    "51.87",
                    "84.31",
                    "92.48"
                ],
                [
                    "dsve w/ FastText",
                    "en",
                    "54.12",
                    "85.74",
                    "92.93"
                ],
                [
                    "Ours w/ bv",
                    "en",
                    "55.57",
                    "86.92",
                    "93.86"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "56.09",
                    "87.22",
                    "94.03"
                ],
                [
                    "Ours w/ muse",
                    "en",
                    "51.81",
                    "84.70",
                    "92.82"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "52.25",
                    "84.72",
                    "92.74"
                ],
                [
                    "[EMPTY]",
                    "en+fr+de",
                    "51.17",
                    "84.09",
                    "92.22"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "50.44",
                    "83.39",
                    "91.80"
                ]
            ],
            "title": "Table 2: Experiment 2: Image retrieval on the coco dataset. The methods are the same as in table\u00a01."
        },
        "insight": "Given a sentence, in any language, we evaluate the rank of the corresponding image. The evaluation is again made by batches of 1000. The results are presented in table 2. [CONTINUE] The first two lines of the table present the state-of-the-art results, with W2V and FastText embeddings. We can see similar results as in the previous experiment. With BIVEC, we have results close to the FastText embeddings when training only in English. This time, the recall is better with an increase of 2.68 % for recall@1. When trained with English and French, the recall@1 is increased by 3.65 %. This implies, again, that we can improve performance by learning on an additional language. [CONTINUE] Our model is able to use the multi-language representing the effectiveness of MUSE embeddings. We train the model with English, and different combinations of French, German and Czech. On English only, we have similar results to the W2V approach. When adding new languages, we can see a decrease in performance for English. We obtain a maximum decrease of 2.62 % for recall@1 when the models saw English, French, German and Czech. [CONTINUE] For image retrieval from a caption in 4 languages, we obtain a 49.38 % recall@10 on the Multi30K dataset."
    },
    {
        "id": "308",
        "table": {
            "header": [
                "train. lang.",
                "en",
                "fr",
                "de",
                "cs",
                "all"
            ],
            "rows": [
                [
                    "en",
                    "56.60",
                    "46.05",
                    "44.18",
                    "38.75",
                    "46.40"
                ],
                [
                    "en+fr",
                    "50.93",
                    "43.69",
                    "41.61",
                    "34.02",
                    "42.43"
                ],
                [
                    "en+fr+de",
                    "54.63",
                    "46.94",
                    "45.07",
                    "38.26",
                    "46.22"
                ],
                [
                    "all",
                    "55.32",
                    "49.30",
                    "46.84",
                    "46.06",
                    "49.38"
                ]
            ],
            "title": "Table 3: Image Recall@10 on the Multi30k dataset with different languages with muse."
        },
        "insight": "The model is trained with English only, then with English and French (en+fr), with English, French, and German (en+fr+de) and with English, French, German and Czech (all). We can see a decrease in performance when adding French that is not present with other languages. Otherwise, every time we add a new language the recall for this language logically increase. The best performance is achieved with English+French+German+Czech, with an increase of 6.42 % for multilingual retrieval. [CONTINUE] By using MUSE embeddings, we are able to embed more languages in the same model. We showed that adding other languages decrease performance for English, but increase the recall in a multilingual environment."
    },
    {
        "id": "309",
        "table": {
            "header": [
                "train. lang.",
                "en",
                "fr",
                "de",
                "en+fr",
                "en+de"
            ],
            "rows": [
                [
                    "en",
                    "53.35",
                    "26.13",
                    "22.96",
                    "39.74",
                    "34.57"
                ],
                [
                    "en+fr",
                    "59.76",
                    "55.22",
                    "\u2205",
                    "57.50",
                    "\u2205"
                ],
                [
                    "en+de",
                    "61.44",
                    "\u2205",
                    "43.59",
                    "\u2205",
                    "52.51"
                ]
            ],
            "title": "Table 4: Image Recall@10 on Multi30k dataset with different languages with bv Embeddings."
        },
        "insight": "With BIVEC embeddings, we learn two languages at the same time, and test retrieval on one or two of these languages. Results are shown in table 4. Trained in English alone, the model gives worse performance than [CONTINUE] MUSE for languages not seen previously. For example, with English-German BIVEC and a model trained only in English, and test on German, we obtain only 22.96 % recall@10, where MUSE embeddings obtain 44.18 %. But when train on English and French, we obtain 55.22 % recall, an increase of 26.39 % compared to MUSE. With German and English training, we have an increase of 15.16 % on English only recall, with a recall of 61.44 %. Meaning that, once again, learning a new language with BIVEC enables better results in English, as the same kind of results is visible with French as well."
    },
    {
        "id": "310",
        "table": {
            "header": [
                "[BOLD] Resource",
                "[ITALIC] Dev  [ITALIC] EM",
                "[ITALIC] Dev  [ITALIC] F1",
                "[ITALIC] Test  [ITALIC] EM",
                "[ITALIC] Test  [ITALIC] F1"
            ],
            "rows": [
                [
                    "DBiDAF",
                    "63.0",
                    "76.9",
                    "62.6",
                    "78.5"
                ],
                [
                    "DBERT",
                    "59.2",
                    "74.3",
                    "63.9",
                    "76.9"
                ],
                [
                    "DRoBERTa",
                    "58.1",
                    "72.0",
                    "58.7",
                    "73.7"
                ]
            ],
            "title": "Table 1: Non-expert human performance results for a randomly-selected validator per question."
        },
        "insight": "We select a randomly chosen validator's answer to each question and compute Exact Match (EM) and word overlap F1 scores with the original to calculate non-expert human performance; Table 1 shows the result. We observe a clear trend: the stronger the model in the loop used to construct the dataset, the harder the resulting questions become for humans."
    },
    {
        "id": "311",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] F1 R-1",
                "[BOLD] F1 R-2",
                "[BOLD] F1 R-L",
                "[BOLD] Recall R-1",
                "[BOLD] Recall R-2",
                "[BOLD] Recall R-L",
                "[BOLD] Precision R-1",
                "[BOLD] Precision R-2",
                "[BOLD] Precision R-L"
            ],
            "rows": [
                [
                    "PG",
                    "36.82",
                    "15.92",
                    "33.57",
                    "37.36",
                    "16.10",
                    "34.05",
                    "38.72",
                    "16.86",
                    "35.32"
                ],
                [
                    "+M1-latent",
                    "37.76",
                    "16.51",
                    "34.48",
                    "[BOLD] 40.15",
                    "[BOLD] 17.52",
                    "36.65",
                    "37.90",
                    "16.64",
                    "34.61"
                ],
                [
                    "+M1-shallow",
                    "37.45",
                    "16.23",
                    "34.22",
                    "[BOLD] 40.15",
                    "17.38",
                    "[BOLD] 36.68",
                    "37.34",
                    "16.24",
                    "34.13"
                ],
                [
                    "+M2-latent",
                    "[BOLD] 38.04",
                    "[BOLD] 16.73",
                    "[BOLD] 34.83",
                    "38.92",
                    "17.05",
                    "35.62",
                    "[BOLD] 39.54",
                    "[BOLD] 17.51",
                    "[BOLD] 36.23"
                ],
                [
                    "+M2-shallow",
                    "37.15",
                    "16.13",
                    "33.96",
                    "38.52",
                    "16.68",
                    "35.21",
                    "38.19",
                    "16.67",
                    "34.91"
                ],
                [
                    "+M3-latent",
                    "37.04",
                    "16.05",
                    "33.86",
                    "37.52",
                    "16.22",
                    "34.29",
                    "38.95",
                    "16.98",
                    "35.63"
                ],
                [
                    "+M3-shallow",
                    "37.09",
                    "16.15",
                    "33.95",
                    "39.05",
                    "16.97",
                    "35.73",
                    "37.62",
                    "16.46",
                    "34.45"
                ],
                [
                    "PG+Cov",
                    "39.32",
                    "17.22",
                    "36.02",
                    "40.33",
                    "17.61",
                    "36.93",
                    "[BOLD] 40.82",
                    "[BOLD] 17.99",
                    "[BOLD] 37.42"
                ],
                [
                    "+M1-latent",
                    "[BOLD] 40.06",
                    "[BOLD] 17.63",
                    "36.70",
                    "[BOLD] 44.44",
                    "[BOLD] 19.53",
                    "[BOLD] 40.69",
                    "38.60",
                    "17.05",
                    "35.39"
                ],
                [
                    "+M1-shallow",
                    "39.78",
                    "17.50",
                    "36.50",
                    "43.50",
                    "19.08",
                    "39.89",
                    "38.94",
                    "17.22",
                    "35.75"
                ],
                [
                    "+M2-latent",
                    "40.00",
                    "17.62",
                    "[BOLD] 36.72",
                    "43.53",
                    "19.17",
                    "39.94",
                    "39.28",
                    "17.37",
                    "36.09"
                ],
                [
                    "+M2-shallow",
                    "39.58",
                    "17.30",
                    "36.36",
                    "44.00",
                    "19.19",
                    "40.38",
                    "38.40",
                    "16.87",
                    "35.31"
                ],
                [
                    "+M3-latent",
                    "39.23",
                    "17.00",
                    "36.00",
                    "42.95",
                    "18.54",
                    "39.37",
                    "38.29",
                    "16.69",
                    "35.16"
                ],
                [
                    "+M3-shallow",
                    "39.57",
                    "17.31",
                    "36.28",
                    "43.85",
                    "19.14",
                    "40.17",
                    "38.37",
                    "168.6",
                    "35.20"
                ]
            ],
            "title": "Table 1: Abstractive summarization results."
        },
        "insight": "We assess unigram (R-1), bigram (R-2), and longest-commonsubsequence (R-L) overlap, and present F1, recall and precision scores in Table 1. [CONTINUE] For the first baseline (PG), we see that incorporating discourse features consistently improves recall and F1. [CONTINUE] We see similar observations for the second baseline (PG+Cov): recall is generally improved at the expense of precision. [CONTINUE] Observing that our model generally has better recall (Table 1)"
    },
    {
        "id": "312",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] MAE",
                "[BOLD] MAPE"
            ],
            "rows": [
                [
                    "CNN w/ GloVe",
                    "1.16",
                    "14.38"
                ],
                [
                    "+ M1-latent",
                    "1.15",
                    "14.66"
                ],
                [
                    "+ M1-shallow",
                    "[BOLD] 1.12(1) 14.19",
                    "[EMPTY]"
                ],
                [
                    "Bi-LSTM w/ GloVe",
                    "1.14",
                    "14.57"
                ],
                [
                    "+ M1-latent",
                    "1.13",
                    "14.39"
                ],
                [
                    "+ M1-shallow",
                    "1.13",
                    "14.25"
                ],
                [
                    "+ M2-latent",
                    "1.12",
                    "14.02"
                ],
                [
                    "+ M2-shallow",
                    "1.13",
                    "14.20"
                ],
                [
                    "Bi-LSTM w/ latent",
                    "[BOLD] 1.11(2) 13.91",
                    "[EMPTY]"
                ],
                [
                    "Bi-LSTM w/ shallow",
                    "1.15",
                    "14.67"
                ]
            ],
            "title": "Table 2: Average petition regression performance over 3 runs (noting that lower is better for both MAE and MAPE). One-sided t-tests show that both (1) and (2) are significantly better than the baseline (p<0.05 and p<0.005, resp.)."
        },
        "insight": "We present the test results in Table 2. [CONTINUE] We are able to reproduce the performance of the baseline model (\"CNN w/ GloVe\"), and find that once again, adding the shallow discourse features improves results. [CONTINUE] Interestingly, we found that replacing the CNN with an LSTM results in improved MAE, but worse MAPE. Adding discourse features to this model generally has marginal improvement in all cases. [CONTINUE] When we replace the word sequence with EDUs (\"Bi-LSTM w/ latent\" and \"Bi-LSTM w/ shallow\"), we see that the latent features outperform the shallow features."
    },
    {
        "id": "313",
        "table": {
            "header": [
                "Model",
                "[ITALIC] K",
                "IMDb",
                "AG\u2019s",
                "SNLI"
            ],
            "rows": [
                [
                    "Model",
                    "[ITALIC] K",
                    "IMDb",
                    "News",
                    "SNLI"
                ],
                [
                    "BERTSDV",
                    "1",
                    "5.39",
                    "[BOLD] 5.38",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDV",
                    "2",
                    "5.44",
                    "5.39",
                    "91.1"
                ],
                [
                    "BERTSDV",
                    "3",
                    "5.40",
                    "5.50",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDV",
                    "4",
                    "5.47",
                    "5.49",
                    "[BOLD] 91.2"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "[BOLD] 5.35",
                    "5.55",
                    "91.1"
                ],
                [
                    "BERTSDA",
                    "[ITALIC] T\u22121",
                    "5.41",
                    "[BOLD] 5.29",
                    "91.0"
                ],
                [
                    "BERTSDA",
                    "2",
                    "5.46",
                    "5.49",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDA",
                    "3",
                    "5.48",
                    "5.55",
                    "91.1"
                ],
                [
                    "BERTSDA",
                    "4",
                    "5.44",
                    "5.52",
                    "91.1"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "[BOLD] 5.29",
                    "5.41",
                    "91.1"
                ]
            ],
            "title": "Table 2: Results on IMDb dataset over different teacher sizes. BERTSDV(K=1) is same as BERTSDA(K=1). For IMDb and AG\u2019s News, we report test error rate (%). For SNLI, we report accuracy (%). T denotes the total number of iterations."
        },
        "insight": "We choose different teacher size K and evaluate our models in three datasets. Table 2 shows that teacher size is sensitive to datasets. Therefore, we select the best teacher size for each dataset in the following experiment."
    },
    {
        "id": "314",
        "table": {
            "header": [
                "Model",
                "IMDb",
                "AG\u2019s News",
                "DBPedia",
                "Yelp P.",
                "Yelp F.",
                "Avg. \u0394",
                "SNLI",
                "MNLI (m/mm)",
                "Avg. \u0394"
            ],
            "rows": [
                [
                    "Model",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Avg. \u0394",
                    "Accuracy (%)",
                    "Accuracy (%)",
                    "Avg. \u0394"
                ],
                [
                    "ULMFiT ",
                    "4.60",
                    "5.01",
                    "0.80",
                    "2.16",
                    "29.98",
                    "/",
                    "/",
                    "/",
                    "/"
                ],
                [
                    "BERTBASE *",
                    "5.40",
                    "5.25",
                    "0.71",
                    "2.28",
                    "30.06",
                    "/",
                    "/",
                    "/",
                    "/"
                ],
                [
                    "BERTBASE",
                    "5.80",
                    "5.71",
                    "0.71",
                    "2.25",
                    "30.37",
                    "-",
                    "90.7",
                    "84.6/83.3",
                    "-"
                ],
                [
                    "BERTVOTE ( [ITALIC] K=4)",
                    "5.60",
                    "5.41",
                    "0.67",
                    "2.03",
                    "29.44",
                    "5.44%",
                    "91.2",
                    "85.3/84.4",
                    "5.50%"
                ],
                [
                    "BERTAVG ( [ITALIC] K=4)",
                    "5.68",
                    "5.53",
                    "0.68",
                    "2.03",
                    "30.03",
                    "4.07%",
                    "90.8",
                    "85.1/84.2",
                    "3.24%"
                ],
                [
                    "BERTSE (ours)",
                    "5.82",
                    "5.59",
                    "0.65",
                    "2.19",
                    "30.48",
                    "2.50%",
                    "90.8",
                    "84.2/83.3",
                    "-0.51%"
                ],
                [
                    "BERTSDV (ours)",
                    "5.35",
                    "5.38",
                    "[BOLD] 0.68",
                    "2.05",
                    "[BOLD] 29.88",
                    "5.65%",
                    "[BOLD] 91.2",
                    "[BOLD] 85.3/84.3",
                    "[BOLD] 5.30%"
                ],
                [
                    "BERTSDA (ours)",
                    "[BOLD] 5.29",
                    "[BOLD] 5.29",
                    "[BOLD] 0.68",
                    "[BOLD] 2.04",
                    "[BOLD] 29.88",
                    "[BOLD] 6.26%",
                    "91.2",
                    "85.0/84.3",
                    "4.65%"
                ]
            ],
            "title": "Table 3: Effects on fine-tuning the BERT-base model (BERTBASE). \u2018*\u2019 indicates using extra fine-tuning strategies and data preprocessing. \u2018/\u2019 means no available reported result. We implemented a \u201cBERTBASE\u201d without any extra fine-tuning strategy as our baseline. \u201cBERTVOTE\u201d and \u201cBERTAVG\u201d means ensemble BERT (See section 3.1). \u201cBERTSE\u201d means self-ensemble BERT (See section 3.2). \u201cBERTSDV\u201d and \u201cBERTSDA\u201d means self-distillation BERT (See section 3.3). \u2018Avg. \u0394\u2019 means the average of relative change, respectively. We bold the better self-distillation results."
        },
        "insight": "Table 3 shows the results of fine-tuning the BERT-base model on five text classification datasets and two NLI datasets. For ensemble BERT, both the voted BERT (BERTVOTE) and averaged BERT (BERTAVG) outperform the single BERT (BERTBASE). The average improvement of BERTVOTE is 5.44% (for text clas [CONTINUE] sification) and 5.50% (for NLI), while BERTAVG follows closely with 4.07% and 3.24%. BERTVOTE outperforms BERTAVG on all tasks, which adheres to our intuition since BERTVOTE is more complicated. The self-ensemble BERT (BERTSE) has a slight improvement in classification tasks of 2.50%, but it does not work on NLI tasks. This is also a reason why we need self-distillation to improve the base models. Overall, self-distillation model has significant improvement on both classification and NLI tasks. Table 3 shows that BERTSDA and BERTSDV outperform BERTBASE on all datasets. Generally speaking, BERTSDA performs better than BERTSDV on text classification tasks with the improvement of 6.26% vs. 5.65%, but the latter performs better on NLI tasks (BERTSDA vs. BERTSDV is 4.65% vs. 5.30%). Our proposed fine-tuning strategies also outperform the previous method in [Sun et al., 2019] on text classification tasks, which makes extensive efforts to find sophisticated hyperparameters."
    },
    {
        "id": "315",
        "table": {
            "header": [
                "Model",
                "IMDb",
                "AG\u2019s",
                "Avg. \u0394",
                "SNLI",
                "\u0394"
            ],
            "rows": [
                [
                    "Model",
                    "IMDb",
                    "News",
                    "Avg. \u0394",
                    "SNLI",
                    "\u0394"
                ],
                [
                    "MT-DNN ",
                    "/",
                    "/",
                    "/",
                    "91.6",
                    "/"
                ],
                [
                    "BERT-L",
                    "4.98",
                    "5.45",
                    "-",
                    "90.9",
                    "-"
                ],
                [
                    "(our implementation)",
                    "4.98",
                    "5.45",
                    "-",
                    "90.9",
                    "-"
                ],
                [
                    "BERT-LSDA( [ITALIC] K=1)",
                    "4.66",
                    "5.21",
                    "5.62%",
                    "[BOLD] 91.5",
                    "[BOLD] 6.59%"
                ],
                [
                    "BERT-LSDA( [ITALIC] K= [ITALIC] T\u22121)",
                    "[BOLD] 4.58",
                    "[BOLD] 5.15",
                    "[BOLD] 7.02%",
                    "91.4",
                    "5.49%"
                ]
            ],
            "title": "Table 4: Effects on fine-tuning the BERT-large model (BERT-L). For IMDb and AG\u2019s News, we report test error rate (%). For SNLI, we report accuracy (%). MT-DNN fine-tunes BERT with multi-task learning."
        },
        "insight": "We also investigate whether self-distillation has similar findings for the BERTlarge model (BERT-L), which contains 24 Transformer layers. Due to the limitation of our devices, we only conduct an experiment on two text classification datasets and one NLI datasets and evaluate strategy BERTSDA, namely self-distillation with averaged BERT as a teacher. We set two different teacher sizes for comparison. As shown in Table 4, self-distillation also gets a significant gain while fine-tuning the BERT-large model. On two text classification tasks, BERT-LSDA(K = [CONTINUE] \u2212 1) gives better results and the average improvement is 7.02%. For NLI task, BERT-LSDA(K = 1) gives better result and the improvement is 6.59%."
    },
    {
        "id": "316",
        "table": {
            "header": [
                "Model",
                "Overall",
                "High",
                "Medium",
                "Low",
                "OOV"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "47.02",
                    "42.41",
                    "47.25",
                    "48.61",
                    "49.96"
                ],
                [
                    "MemNet",
                    "46.85",
                    "41.93",
                    "47.32",
                    "48.86",
                    "49.52"
                ],
                [
                    "CopyNet",
                    "40.27",
                    "36.26",
                    "40.99",
                    "42.09",
                    "42.24"
                ],
                [
                    "CCM",
                    "39.18",
                    "35.36",
                    "39.64",
                    "40.67",
                    "40.87"
                ],
                [
                    "PostKS",
                    "43.56",
                    "40.65",
                    "44.06",
                    "46.36",
                    "49.32"
                ],
                [
                    "TransDG",
                    "[BOLD] 37.53",
                    "[BOLD] 32.18",
                    "[BOLD] 36.12",
                    "[BOLD] 38.46",
                    "[BOLD] 40.75"
                ]
            ],
            "title": "Table 2: Automatic evaluation with perplexity."
        },
        "insight": "As shown in Table 2, TransDG achieves the lowest perplexity on all the datasets, indicating that the generated responses are more grammatical."
    },
    {
        "id": "317",
        "table": {
            "header": [
                "Model",
                "Overall",
                "High",
                "Medium",
                "Low",
                "OOV"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "0.717",
                    "0.713",
                    "0.740",
                    "0.721",
                    "0.669"
                ],
                [
                    "MemNet",
                    "0.761",
                    "0.764",
                    "0.788",
                    "0.760",
                    "0.706"
                ],
                [
                    "CopyNet",
                    "0.960",
                    "0.910",
                    "0.970",
                    "0.960",
                    "0.960"
                ],
                [
                    "CCM",
                    "1.180",
                    "1.156",
                    "1.191",
                    "1.196",
                    "1.162"
                ],
                [
                    "PostKS",
                    "1.041",
                    "1.007",
                    "1.028",
                    "0.993",
                    "0.978"
                ],
                [
                    "TransDG",
                    "[BOLD] 1.207",
                    "[BOLD] 1.195",
                    "[BOLD] 1.204",
                    "[BOLD] 1.232",
                    "[BOLD] 1.182"
                ]
            ],
            "title": "Table 3: Automatic evaluation with entity score."
        },
        "insight": "Table 3 demonstrates that the models leveraging external knowledge achieve better performance than the standard Seq2Seq model in generating meaningful entity words and diverse responses. In particular, our model outperforms all the baselines significantly with highest entity score."
    },
    {
        "id": "318",
        "table": {
            "header": [
                "Model",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "0.0977",
                    "0.0098",
                    "0.0012",
                    "0.0002"
                ],
                [
                    "MemNet",
                    "0.1652",
                    "0.0174",
                    "0.0028",
                    "0.0004"
                ],
                [
                    "CopyNet",
                    "0.1715",
                    "[BOLD] 0.0181",
                    "0.0029",
                    "0.0005"
                ],
                [
                    "CCM",
                    "0.1625",
                    "0.0175",
                    "0.0030",
                    "0.0005"
                ],
                [
                    "PostKS",
                    "0.1683",
                    "0.0165",
                    "0.0029",
                    "0.0004"
                ],
                [
                    "TransDG",
                    "[BOLD] 0.1807",
                    "0.0178",
                    "[BOLD] 0.0031",
                    "[BOLD] 0.0006"
                ]
            ],
            "title": "Table 4: Automatic evaluation with BLEU."
        },
        "insight": "The BLEU values shown in Table 4 demonstrates the comparison results from word-level overlaps. TransDG tends to generate responses that are more similar to the gold responses than baselines in most cases."
    },
    {
        "id": "319",
        "table": {
            "header": [
                "Model",
                "Fluency",
                "Relevance",
                "Correctness"
            ],
            "rows": [
                [
                    "Seq2Seq",
                    "1.67",
                    "0.68",
                    "0.80"
                ],
                [
                    "MemNet",
                    "1.83",
                    "0.89",
                    "1.32"
                ],
                [
                    "CopyNet",
                    "2.36",
                    "1.13",
                    "1.08"
                ],
                [
                    "CCM",
                    "2.27",
                    "1.35",
                    "1.22"
                ],
                [
                    "PostKS",
                    "2.32",
                    "1.36",
                    "1.31"
                ],
                [
                    "TransDG",
                    "[BOLD] 2.41",
                    "[BOLD] 1.52",
                    "[BOLD] 1.34"
                ]
            ],
            "title": "Table 5: Human evaluation result."
        },
        "insight": "The human evaluation results are reported in Table 5. As shown in Table 5, TransDG tends to generate more appropriate and informative responses in terms of human annotation. Specifically, the responses generated by TransDG have higher knowledge relevance than other models, indicating that TransDG is effective to incorporate appropriate commonsense knowledge."
    },
    {
        "id": "320",
        "table": {
            "header": [
                "Model",
                "Perplexity",
                "Entity",
                "BLEU-1",
                "BLEU-2"
            ],
            "rows": [
                [
                    "TransDG",
                    "37.53",
                    "1.207",
                    "0.1807",
                    "0.0178"
                ],
                [
                    "w/o QRT",
                    "42.17",
                    "1.076",
                    "0.1604",
                    "0.0171"
                ],
                [
                    "w/o KST",
                    "43.05",
                    "0.774",
                    "0.1643",
                    "0.0158"
                ],
                [
                    "w/o QRT+KST",
                    "44.15",
                    "0.772",
                    "0.1612",
                    "0.0170"
                ],
                [
                    "w/o RGA",
                    "38.62",
                    "1.106",
                    "0.1712",
                    "0.0170"
                ],
                [
                    "w/o SSD",
                    "38.18",
                    "1.114",
                    "0.1804",
                    "0.0178"
                ]
            ],
            "title": "Table 7: Ablation results of TransDG on the test set. Here, Entity represents entity score."
        },
        "insight": "The ablation test results are reported in Table 7. From the results, we can observe that the performance of TransDG drops sharply when we discard the question representation module and the knowledge selection module transferred from KBQA. [CONTINUE] Response guiding attention also has noticeable impact on the performance of TransDG, especially on BLEU scores. [CONTINUE] In addition, the second-step decoder can improve the ability of TransDG to generate relevant entities per response."
    },
    {
        "id": "321",
        "table": {
            "header": [
                "[BOLD] Method",
                "Bas.",
                "Blo.",
                "Cal.",
                "Hou.",
                "Pub.",
                "Rec.",
                "Res.",
                "Soc.",
                "[BOLD] Avg."
            ],
            "rows": [
                [
                    "[BOLD] Previous Methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Han ( 2018 )",
                    "88.2",
                    "61.4",
                    "81.5",
                    "74.1",
                    "80.7",
                    "82.9",
                    "80.7",
                    "82.1",
                    "79.0"
                ],
                [
                    "Su and Yan ( 2017 )",
                    "88.2",
                    "62.2",
                    "82.1",
                    "78.8",
                    "80.1",
                    "86.1",
                    "83.7",
                    "83.1",
                    "80.6"
                ],
                [
                    "Herzig and Berant ( 2017 )",
                    "86.2",
                    "62.7",
                    "82.1",
                    "78.3",
                    "80.7",
                    "82.9",
                    "82.2",
                    "81.7",
                    "79.6"
                ],
                [
                    "[BOLD] Our Methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Shaw et al. ( 2019 )",
                    "[BOLD] 89.3",
                    "63.7",
                    "81.5",
                    "82.0",
                    "80.7",
                    "85.6",
                    "89.5",
                    "84.8",
                    "82.1"
                ],
                [
                    "G-R (Beam-10)",
                    "88.7",
                    "66.4",
                    "83.3",
                    "82.5",
                    "78.9",
                    "86.6",
                    "89.8",
                    "83.7",
                    "82.5"
                ],
                [
                    "G-R (Beam-10 & pQ)",
                    "89.0",
                    "65.2",
                    "83.3",
                    "83.6",
                    "78.3",
                    "87.5",
                    "89.5",
                    "85.5",
                    "82.7"
                ],
                [
                    "G-R (Beam-25)",
                    "89.0",
                    "[BOLD] 67.7",
                    "83.3",
                    "[BOLD] 84.1",
                    "[BOLD] 82.6",
                    "87.5",
                    "89.4",
                    "83.9",
                    "83.4"
                ],
                [
                    "G-R (Beam-25 & pQ)",
                    "[BOLD] 89.3",
                    "66.7",
                    "84.5",
                    "83.6",
                    "80.1",
                    "[BOLD] 88.0",
                    "[BOLD] 91.0",
                    "85.2",
                    "83.5"
                ],
                [
                    "G-R (Beam-25 & pQ & TH1)",
                    "89.0",
                    "65.7",
                    "[BOLD] 85.1",
                    "83.6",
                    "81.4",
                    "[BOLD] 88.0",
                    "[BOLD] 91.0",
                    "[BOLD] 86.0",
                    "[BOLD] 83.7"
                ],
                [
                    "G-R (Beam-25 & pQ & TH2)",
                    "88.7",
                    "66.4",
                    "82.7",
                    "83.1",
                    "82.0",
                    "87.0",
                    "89.8",
                    "85.8",
                    "83.2"
                ]
            ],
            "title": "Table 3: Test accuracy for all models on OVERNIGHT dataset, which has eight domains: Basketball, Blocks, Calendar, Housing, Publications, Recipes, Restaurants, and Social. We use the generator-reranker (G-R) architecture with different options. Beam-n: Beam search is applied with size n, pQ: The critic is pre-trained over the Quora dataset, TH1: rerank if there is at least one score above 0.5, TH2: rerank if best score\u2212second best score>0.001. The candidate logical forms are processed with templated expansions method (Section 3.2.3) in this experiment."
        },
        "insight": "We compare our model with the state-of-the-art models in Table 3. [CONTINUE] As Table 3 shows, this model alone (without any reranking) improves the state-of-the-art performance from 79.6% to 82.15% accuracy [CONTINUE] Increasing the beam size improves the performance as expected. [CONTINUE] Using a pre-trained model improves the performance as well. [CONTINUE] Reranking with a threshold rule may be helpful for the overall architecture. We observe that reranking by the critic at all times may not be the best approach. We note that choosing not to rerank when all scores are below 0.5 increases the performance further. On the other hand, reranking if the difference between the best score and second best score is above the threshold we set does not help in this case. [CONTINUE] The overall architecture improves the performance of the generator (82.1% accuracy) to 83.7% accuracy."
    },
    {
        "id": "322",
        "table": {
            "header": [
                "Search Strategy",
                "BLEU"
            ],
            "rows": [
                [
                    "Greedy Search",
                    "86.24"
                ],
                [
                    "2-LA",
                    "86.65"
                ],
                [
                    "3-LA",
                    "86.71"
                ],
                [
                    "4-LA",
                    "86.77"
                ],
                [
                    "5-LA",
                    "[BOLD] 86.79"
                ],
                [
                    "Beam Search (B=10)",
                    "86.28"
                ]
            ],
            "title": "Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method \u2014 noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference"
        },
        "insight": "Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method \u2014 noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference"
    },
    {
        "id": "323",
        "table": {
            "header": [
                "[EMPTY]",
                "DSQuAD",
                "DBiDAF",
                "DBERT",
                "DRoBERTa"
            ],
            "rows": [
                [
                    "Question length",
                    "10.3",
                    "9.8",
                    "9.8",
                    "10.0"
                ],
                [
                    "Answer length",
                    "2.6",
                    "2.9",
                    "3.0",
                    "3.2"
                ],
                [
                    "N-gram overlap",
                    "3.0",
                    "2.2",
                    "2.1",
                    "2.0"
                ]
            ],
            "title": "Table 3: Average number of words per question and answer, and average longest n-gram overlap between passage and question."
        },
        "insight": "The average number of words in questions and answers, as well as the average longest n-gram overlap between passage and question are furthermore given in Table 3. We can again observe two clear trends: from weaker towards stronger models used in the annotation loop, the average length of answers increases, and the largest n-gram overlap drops from 3 to 2 tokens."
    },
    {
        "id": "324",
        "table": {
            "header": [
                "Search Strategy",
                "BLEU",
                "BLEU (Target len\u226525)"
            ],
            "rows": [
                [
                    "Greedy Search",
                    "31.67",
                    "[BOLD] 23.86"
                ],
                [
                    "2-LA",
                    "32.07",
                    "21.50"
                ],
                [
                    "3-LA",
                    "32.20",
                    "22.78"
                ],
                [
                    "4-LA",
                    "[BOLD] 32.42",
                    "22.45"
                ],
                [
                    "5-LA",
                    "32.41",
                    "23.30"
                ],
                [
                    "Beam Search (B=10)",
                    "33.83",
                    "22.45"
                ]
            ],
            "title": "Table 2: The performances of the LSTM model trained on the WMT16 multimodal translation dataset with different LA steps. We show the look-ahead module is able to improve the model on the entire testing set. However, either the LA module or the beam search method harm the models when the length of the target sentences is longer than 25 words."
        },
        "insight": "Table 2: The performances of the LSTM model trained on the WMT16 multimodal translation dataset with different LA steps. We show the look-ahead module is able to improve the model on the entire testing set. However, either the LA module or the beam search method harm the models when the length of the target sentences is longer than 25 words."
    },
    {
        "id": "325",
        "table": {
            "header": [
                "Search Strategy",
                "BLEU"
            ],
            "rows": [
                [
                    "Greedy Search",
                    "27.50"
                ],
                [
                    "2-LA",
                    "[BOLD] 27.71"
                ],
                [
                    "3-LA",
                    "27.62"
                ],
                [
                    "4-LA",
                    "27.56"
                ],
                [
                    "5-LA",
                    "27.35"
                ],
                [
                    "Beam Search (B=10)",
                    "28.21"
                ]
            ],
            "title": "Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem."
        },
        "insight": "Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem."
    },
    {
        "id": "326",
        "table": {
            "header": [
                "Search Strategy",
                "[ITALIC] \u03b3 0.0",
                "[ITALIC] \u03b3 0.25",
                "[ITALIC] \u03b3 0.50",
                "[ITALIC] \u03b3 0.75",
                "[ITALIC] \u03b3 1.0",
                "[ITALIC] \u03b3 1.25"
            ],
            "rows": [
                [
                    "Greedy",
                    "27.50",
                    "27.81",
                    "27.74",
                    "27.75",
                    "[BOLD] 27.90",
                    "27.71"
                ],
                [
                    "2-LA",
                    "27.71",
                    "28.05",
                    "27.95",
                    "27.99",
                    "[BOLD] 28.20",
                    "27.85"
                ],
                [
                    "3-LA",
                    "27.89",
                    "27.82",
                    "27.87",
                    "27.82",
                    "[BOLD] 28.10",
                    "27.68"
                ],
                [
                    "4-LA",
                    "27.56",
                    "27.81",
                    "[BOLD] 27.87",
                    "27.74",
                    "27.84",
                    "27.68"
                ],
                [
                    "5-LA",
                    "27.35",
                    "27.71",
                    "27.74",
                    "27.63",
                    "[BOLD] 27.87",
                    "27.55"
                ]
            ],
            "title": "Table 4: We show the results of integrating auxiliary EOS loss into the training state. \u03b3 is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss."
        },
        "insight": "Table 4: We show the results of integrating auxiliary EOS loss into the training state. \u03b3 is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss."
    },
    {
        "id": "327",
        "table": {
            "header": [
                "Model",
                "WMT\u201914 EN-DE",
                "WMT\u201914 EN-FR",
                "IWSLT\u201914 DE-EN"
            ],
            "rows": [
                [
                    "vaswani2017transformer",
                    "28.4",
                    "41.0",
                    "34.4"
                ],
                [
                    "ahmed2018weighted",
                    "28.9",
                    "41.4",
                    "-"
                ],
                [
                    "chen2018combining",
                    "28.5",
                    "41.0",
                    "-"
                ],
                [
                    "shaw2018relative",
                    "29.2",
                    "41.5",
                    "-"
                ],
                [
                    "ott2018scaling",
                    "29.3",
                    "43.2",
                    "-"
                ],
                [
                    "wu2018dynconv",
                    "[BOLD] 29.7",
                    "43.2",
                    "35.2"
                ],
                [
                    "he2018layerwise",
                    "29.0",
                    "-",
                    "35.1"
                ],
                [
                    "Joint Self-attention",
                    "[BOLD] 29.7",
                    "43.2",
                    "35.3"
                ],
                [
                    "Local Joint Self-attention",
                    "[BOLD] 29.7",
                    "[BOLD] 43.3",
                    "[BOLD] 35.7"
                ]
            ],
            "title": "Table 1: Translation quality evaluation (BLEU scores)."
        },
        "insight": "The entry Joint Self-attention corresponds to the results of our implementation of (He et al., 2018), that significantly improves the original results by 0.7 BLEU point on the WMT14 de-en benchmark, and 0.2 on IWSLT. The same architecture with the proposed locality constraints (Local Joint Self-attention) establishes a new state of the art in IWSLT'14 de-en with 35.7 BLEU, surpassing all previous published results by at least in 0.5 BLEU, and our results with the unconstrained version by 0.4. The [CONTINUE] Table 1 presents a comparison of the translation quality measured via BLEU score between the currently dominant Transformer (Vaswani et al., 2017) and Dynamic Convolutions (Wu et al., 2019) models, as well as the work by He et al. (2018), which also proposes a joint encoderdecoder structure, and also other refinements over the transformer architecture like (Ahmed et al., 2017), (Chen et al., 2018), (Shaw et al., 2018) and (Ott et al., 2018) . [CONTINUE] The Joint Self-attention model obtains the same SoTA BLEU score of (Wu et al., 2019) on WMT'14 en-de, and the same SoTA score of (Ott et al., 2018) and (Wu et al., 2019) on WMT'14 enfr. The local attention constraints do not provide a significant gain on these bigger models, but it improves the BLEU score on WMT'14 en-fr to a new SoTA of 43.3."
    },
    {
        "id": "328",
        "table": {
            "header": [
                "Method",
                "Line iu %",
                "Pixel iu %"
            ],
            "rows": [
                [
                    "wavelength [seuret2017wavelength]",
                    "68.58",
                    "79.13"
                ],
                [
                    "Brigham Young University [simistira2017icdar2017]",
                    "81.50",
                    "83.07"
                ],
                [
                    "CITlab Argus LineDetect [gruuening2017robust]",
                    "96.99",
                    "93.01"
                ],
                [
                    "wavelength* (tight polygons) [simistira2017icdar2017]",
                    "97.86",
                    "97.05"
                ],
                [
                    "proposed method*",
                    "[BOLD] 99.42",
                    "96.11"
                ]
            ],
            "title": "TABLE I: Results of text-line extraction on the DIVA-HisDB dataset (see Section\u00a0III-A measured with the competition tool(see Section\u00a0IV-A. Our proposed method outperforms state-of-the-art results by reducing the error by 80.7% and achieving nearly perfect results. Methods with * notation use semantic segmentation at pixel-level as pre-processing step."
        },
        "insight": "In Table [CONTINUE] there are the results on the DIVA-HisDB dataset (see Section III-A). Our method achieves nearly perfect results (99.42%) and outperforms state-of-the-art (97.86%) resulting in a error reduction of 80.7%. [CONTINUE] Note that the lower end of the heatmap scale compares favourably with state-ofthe-art (see Table I) meaning that regardless of the choice of parameters, our method produces excellent results."
    },
    {
        "id": "329",
        "table": {
            "header": [
                "Method from gt",
                "Line iu %",
                "Pixel iu %"
            ],
            "rows": [
                [
                    "wavelength [seuret2017wavelength]",
                    "66.44",
                    "81.52"
                ],
                [
                    "wavelength (tight polygons) [simistira2017icdar2017]",
                    "99.25",
                    "98.95"
                ],
                [
                    "proposed method",
                    "[BOLD] 100.0",
                    "97.22"
                ]
            ],
            "title": "TABLE II: Results of the experiments shown in Table\u00a0I with the difference that every method listed has received the ground truth of the semantic segmentation at pixel-level as input. Our proposed text-line extraction method is superior to state-of-the-art even if both methods run on the same perfect input. Moreover, in our experience, an algorithm which is not designed to take advantage of this pre-processing step will not benefit from it."
        },
        "insight": "The answer is in Table II where we performed the same task, but this time we swapped our semantic segmentation network [CONTINUE] with the pixel-level ground-truth provided along with the data. This represents the upper-bound performances, as no tool will produce a better segmentation than the ground-truth. In this scenario our method performed at 100% line IU, reinforcing our previous observation that our text-line extraction method has made the mistakes only in the presence of wrong results from the semantic segmentation step."
    },
    {
        "id": "330",
        "table": {
            "header": [
                "[EMPTY]",
                "Task Dataset",
                "Image-Sentence Retrieval Flickr30K\u00a0",
                "Image-Sentence Retrieval MSCOCO\u00a0",
                "Phrase Grounding Flickr30K",
                "Phrase Grounding ReferIt\u00a0",
                "Text-to-Clip DiDeMo\u00a0",
                "Image Captioning MSCOCO\u00a0",
                "Image Captioning MSCOCO\u00a0",
                "VQA VQA\u00a0"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Dataset",
                    "Flickr30K\u00a0",
                    "MSCOCO\u00a0",
                    "Entities\u00a0",
                    "ReferIt\u00a0",
                    "DiDeMo\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "VQA\u00a0"
                ],
                [
                    "[EMPTY]",
                    "Method",
                    "Embedding Network\u00a0",
                    "Embedding Network\u00a0",
                    "CITE\u00a0",
                    "CITE\u00a0",
                    "CITE\u00a0",
                    "ARNet\u00a0",
                    "ARNet\u00a0",
                    "EtEMN\u00a0"
                ],
                [
                    "[EMPTY]",
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "[BOLD] (a)",
                    "[BOLD] Training from scratch",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "44.3",
                    "73.7",
                    "70.46",
                    "51.70",
                    "33.02",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "44.6",
                    "77.6",
                    "70.68",
                    "52.39",
                    "33.48",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "60.0",
                    "77.5",
                    "70.47",
                    "51.57",
                    "32.83",
                    "26.7",
                    "89.7",
                    "60.95"
                ],
                [
                    "[BOLD] (b)",
                    "[BOLD] Word2Vec\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "62.5",
                    "75.0",
                    "70.03",
                    "52.51",
                    "32.95",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "71.5",
                    "78.2",
                    "70.85",
                    "53.29",
                    "32.58",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "63.6",
                    "75.6",
                    "70.19",
                    "52.41",
                    "33.23",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "71.9",
                    "79.9",
                    "70.94",
                    "53.54",
                    "33.26",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "68.5",
                    "72.5",
                    "69.83",
                    "52.86",
                    "33.73",
                    "[BOLD] 28.5",
                    "92.7",
                    "61.40"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.0",
                    "78.2",
                    "70.55",
                    "53.58",
                    "[BOLD] 33.94",
                    "[BOLD] 28.5",
                    "[BOLD] 94.0",
                    "61.35"
                ],
                [
                    "[BOLD] (c)",
                    "[BOLD] FastText\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "69.2",
                    "78.5",
                    "69.75",
                    "51.27",
                    "32.45",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "73.0",
                    "[BOLD] 80.7",
                    "70.62",
                    "53.24",
                    "32.01",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "69.5",
                    "78.6",
                    "69.87",
                    "52.49",
                    "33.31",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "[BOLD] 73.1",
                    "80.6",
                    "[BOLD] 71.23",
                    "53.87",
                    "33.17",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "69.1",
                    "76.9",
                    "69.76",
                    "52.21",
                    "33.06",
                    "[BOLD] 28.5",
                    "92.7",
                    "[BOLD] 61.86"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "68.5",
                    "80.1",
                    "71.09",
                    "[BOLD] 53.95",
                    "32.51",
                    "28.3",
                    "93.2",
                    "61.66"
                ],
                [
                    "[BOLD] (d)",
                    "[BOLD] Sentence-Level",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "InferSent\u00a0",
                    "71.2",
                    "76.4",
                    "57.83",
                    "52.29",
                    "31.87",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "BERT\u00a0",
                    "71.8",
                    "75.4",
                    "69.38",
                    "50.37",
                    "32.46",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ]
            ],
            "title": "Table 1: Word Embedding Comparison Across Vision Language Tasks. (a) contains the results of learning an embedding from scratch random initialization with fine-tuning during training. The remaining sections compare (b) Word2Vec, (c) FastText, and (d) sentence level embeddings InferSent and BERT. All experiments show three model variants: Average Embedding, Self-Attention, and LSTM, with and without fine-tuning during training. Average Embedding and Self-Attention are not used in generation tasks for Image Captioning and VQA as they are known to show worse performance; sentence level embeddings are not applicable for these tasks. See text for discussion."
        },
        "insight": "The datasets and vision-language task models are described in the appendix, but are referenced in Table 1. [CONTINUE] Unsurprisingly, when comparing the first lines of Table 1(a,b), we find that using Word2Vec rather than an embedding trained from scratch tends to improve performance. This is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on DiDeMo and ReferIt, whose embeddings trained from scratch using their smaller vocabulary compare favorably to Word2Vec. [CONTINUE] Word2Vec only falls behind within a point or two across all tasks, and even outperforms or performs equally as well as FastText for certain tasks (e.g. text-to-clip, image captioning). [CONTINUE] Table 1 also contains a comparison of language model variants across the five vision-language tasks we evaluate on. We see that fine-tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model (e.g. 5-10% increase to mean recall on image-sentence retrieval). [CONTINUE] When comparing the architecture choices from Figure 3 we see that for retrieval-based tasks (i.e. where the output is not free-form text) the Average Embedding and SelfAttention models perform better than a simple LSTM-based approach, with Self-Attention being best on average. [CONTINUE] The only apparent exception to this is the text-to-clip task. [CONTINUE] InferSent and BERT reach comparable values to the best Word2Vec models for image-sentence retrieval on Flickr30K, performing more poorly for the MSCOCO dataset. For the remaining retrieval tasks, metrics are below the best performing model and embedding combination within 1-3 points, again noting the unusual exception of InferSent on phrase grounding of Flickr30K Entities, which significantly drops below scratch performance. [CONTINUE] While all language models perform closely on ReferIt phrase grounding, this still suggests that there is no need to use the more complex LSTM language model without additional modification. [CONTINUE] Lastly, sentence level embeddings InferSent and BERT are compared in Table 1(d); results are without fine-tuning. [CONTINUE] The two are comparable to each other with the exception of phrase grounding accuracy on Flickr30K Entities; BERT surprisingly outperforms InferSent by 11.55%. Both InferSent and BERT do not provide the best results across any task, and thus are not a leading option for vision-language tasks."
    },
    {
        "id": "331",
        "table": {
            "header": [
                "[EMPTY]",
                "Task Dataset",
                "Image-Sentence Retrieval Flickr30K",
                "Image-Sentence Retrieval MSCOCO",
                "Phrase Grounding Flickr30K",
                "Phrase Grounding ReferIt",
                "Text-to-Clip DiDeMo",
                "Image Captioning MSCOCO",
                "Image Captioning MSCOCO",
                "VQA"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Dataset",
                    "Flickr30K",
                    "MSCOCO",
                    "Entities",
                    "ReferIt",
                    "DiDeMo",
                    "[EMPTY]",
                    "[EMPTY]",
                    "VQA"
                ],
                [
                    "[EMPTY]",
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "[BOLD] (a)",
                    "[BOLD] Word2Vec + wn\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "72.0",
                    "79.2",
                    "70.51",
                    "53.93",
                    "33.24",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "72.4",
                    "80.0",
                    "70.70",
                    "53.81",
                    "33.65",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.3",
                    "78.9",
                    "70.80",
                    "53.67",
                    "34.16",
                    "28.6",
                    "93.3",
                    "61.06"
                ],
                [
                    "[BOLD] (b)",
                    "[BOLD] GrOVLE",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "72.3",
                    "80.2",
                    "70.77",
                    "[BOLD] 53.99",
                    "33.71",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "72.1",
                    "80.5",
                    "70.95",
                    "53.75",
                    "33.14",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.7",
                    "78.8",
                    "70.18",
                    "[BOLD] 53.99",
                    "34.47",
                    "28.3",
                    "92.5",
                    "61.22"
                ],
                [
                    "[BOLD] (c)",
                    "[BOLD] Visual Word2Vec\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "66.8",
                    "78.7",
                    "70.61",
                    "53.14",
                    "31.73",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "68.8",
                    "79.2",
                    "[BOLD] 71.07",
                    "53.26",
                    "31.15",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "66.7",
                    "74.5",
                    "70.70",
                    "53.19",
                    "32.29",
                    "[BOLD] 28.8",
                    "[BOLD] 94.0",
                    "61.15"
                ],
                [
                    "[BOLD] (d)",
                    "[BOLD] HGLMM (300-D)\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "71.0",
                    "79.8",
                    "70.64",
                    "53.71",
                    "32.62",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "71.8",
                    "80.4",
                    "70.51",
                    "53.83",
                    "33.44",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.5",
                    "77.9",
                    "70.37",
                    "53.10",
                    "33.85",
                    "28.7",
                    "[BOLD] 94.0",
                    "[BOLD] 61.44"
                ],
                [
                    "[BOLD] (e)",
                    "[BOLD] HGLMM (6K-D)\u00a0",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "73.5",
                    "[BOLD] 80.9",
                    "70.83",
                    "53.36",
                    "32.66",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "[BOLD] 75.1",
                    "80.6",
                    "71.02",
                    "53.43",
                    "33.57",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "68.0",
                    "79.4",
                    "70.38",
                    "53.89",
                    "[BOLD] 34.62",
                    "28.0",
                    "92.8",
                    "60.58"
                ]
            ],
            "title": "Table 2: Modifications of Word2Vec. (a) contains Word2Vec retrofitted results using only the WordNet (wn) lexicon from\u00a0[14]. Next, (b) is our baseline embedding which includes the new Visual Genome relational graph. Visual Word2Vec results are provided in (c), and (d), (e) are Fisher vectors on top of Word2Vec. See text for discussion."
        },
        "insight": "We see a small, but consistent improvement across most of the vision-language tasks using GrOVLE as seen in Table 2(b). These changes result in an embedding with comparable performance to the HGLMM 6K-D features, which are reported in Table 2(e). However, our word embedding tends to perform better when embeddings are the same size (i.e. 300-D). For the generation-based tasks (i.e. captioning and VQA), the benefits of using adapted embeddings are less clear. This may simply be an artifact of the challenges in evaluating these tasks (i.e., the captions are improving in a way the metrics don't capture). [CONTINUE] Visual Word2Vec performs comparably amongst results for generation tasks (i.e. image captioning and VQA), but these tasks have little variance in results, with less than a point of difference across the adapted embeddings. [CONTINUE] The small gain provided in generation tasks by Visual Word2Vec does not out-weight the drops in performance across other tasks such as the significant mean recall drop of 6.3 compared to HGLMM's 6K-D Self-Attention result in line two of Table 2(c) and Table 2(e) for image-sentence retrieval of Flickr30K. For comparison, GrOVLE's Self-Attention result in Table 2(b) is only 3 points lower. [CONTINUE] Finally, we report results using HGLMM of different dimension. HGLMM 300-D features are used for a more fair comparison to other embeddings. While the HGLMM 6K-D representation primarily results in the highest performance, it performs more poorly on generation tasks and also results in high variance. For example, column one in Table 2(e) shows a range of 7.1 in mean recall, unlike GrOVLE which has a range of 2.6."
    },
    {
        "id": "332",
        "table": {
            "header": [
                "Task Additional Models",
                "Image-Sentence Retrieval SCAN ",
                "Image-Sentence Retrieval SCAN ",
                "Phrase Grounding QA R-CNN ",
                "Phrase Grounding QA R-CNN ",
                "Text-to-Clip TGN ",
                "Image Captioning BUTD ",
                "Image Captioning BUTD ",
                "VQA BAN"
            ],
            "rows": [
                [
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "Training from scratch",
                    "72.8",
                    "83.2",
                    "68.56",
                    "50.23",
                    "43.91",
                    "35.2",
                    "109.8",
                    "68.98"
                ],
                [
                    "FastText + ft",
                    "72.5",
                    "83.8",
                    "69.27",
                    "53.01",
                    "44.21",
                    "35.2",
                    "110.3",
                    "69.91"
                ],
                [
                    "GrOVLE (w/o multi-task pretraining) + ft",
                    "72.7",
                    "84.1",
                    "70.03",
                    "53.88",
                    "[BOLD] 45.26",
                    "35.1",
                    "110.4",
                    "69.36"
                ],
                [
                    "+ multi-task pretraining w/ target task + ft",
                    "[BOLD] 76.2",
                    "[BOLD] 84.7",
                    "[BOLD] 71.08",
                    "[BOLD] 54.10",
                    "43.61",
                    "[BOLD] 35.7",
                    "[BOLD] 111.6",
                    "[BOLD] 69.97"
                ]
            ],
            "title": "Table 4: We include results with additional models to verify trends. See text for discussion and the appendix for more."
        },
        "insight": "To address this, we fine-tune GrOVLE across the five VL tasks. We provide results for a four and five multi-task trained embedding. The four task experiments are performed with the final task embedding fixed to demonstrate how well the embeddings would generalize to new tasks. We also provide results for pretraining on five tasks with and without finetuning during the last task. [CONTINUE] This multi-task variant is the best performing across all tasks, thus we release this embedding for public use. [CONTINUE] To verify that the multi-task GrOVLE performance improvements generalize across task model architecture, we provide results using additional task models in Table 4. [CONTINUE] Table 4 provides more models per task and demonstrates consistent results: embeddings can significantly affect performance and GrOVLE variants are still the best embedding overall. As we move down the table we find even larger performance improvements made by using the five-task pretrained GrOVLE with fine-tuning than in Table 3."
    },
    {
        "id": "333",
        "table": {
            "header": [
                "Task Metric",
                "Image-Sentence Retrieval Mean Recall",
                "Image-Sentence Retrieval Mean Recall",
                "Phrase Grounding Accuracy",
                "Phrase Grounding Accuracy",
                "Text-to-Clip Average",
                "Image Captioning BLEU-4",
                "Image Captioning CIDEr",
                "VQA Accuracy"
            ],
            "rows": [
                [
                    "GrOVLE w/o multi-task pretraining",
                    "64.7",
                    "75.0",
                    "70.53",
                    "52.15",
                    "34.45",
                    "28.5",
                    "92.7",
                    "61.46"
                ],
                [
                    "+ multi-task pretraining w/o target task",
                    "65.8",
                    "76.4",
                    "70.82",
                    "52.21",
                    "34.57",
                    "[BOLD] 28.8",
                    "[BOLD] 93.3",
                    "61.47"
                ],
                [
                    "+ multi-task pretraining w/ target task",
                    "66.2",
                    "80.2",
                    "70.87",
                    "52.64",
                    "34.82",
                    "28.5",
                    "92.7",
                    "[BOLD] 61.53"
                ],
                [
                    "+ multi-task pretraining w/ target task + ft",
                    "[BOLD] 72.6",
                    "[BOLD] 81.3",
                    "[BOLD] 71.57",
                    "[BOLD] 54.51",
                    "[BOLD] 35.09",
                    "28.7",
                    "93.2",
                    "61.46"
                ]
            ],
            "title": "Table 3: Comparison of training our word embeddings on four tasks and testing on the fifth, as well as training on all five tasks."
        },
        "insight": "Table 3 reports results of the multi-task training procedure described above. We use the best performing language model in our comparisons for each task, i.e. Self-Attention for image-sentence retrieval and phrase grounding, and the LSTM language model for text-to-clip, image captioning, and VQA. The first lines of Table 3 report the results of the original fixed GrOVLE embedding, which should be considered the baseline. The second line of Table 3 reports performance when the four-task pretrained GrOVLE is fixed when used in the target task, i.e. the task currently being run. The third and fourth line of Table 3 report the results of our embedding when they were trained on all five tasks, and kept fixed or fine-tuned for the target task, respectively. [CONTINUE] The results of line three and four demonstrate that our improved embedding tends to transfer better when applied with fine-tuning during the target task. We find similar trends in performance improvements across tasks: larger gains occur for image-sentence retrieval with +7.9 mean recall for the Flickr30K dataset and +6.3 for MSCOCO. All other tasks have performance improvements under one point, showing that while the vision-language tasks appear to transfer well without harming performance, they are leveraged most in image-sentence retrieval, with an exception of phrase grounding accuracy on ReferIt (+2.36%)."
    },
    {
        "id": "334",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Resource",
                "[ITALIC] Seed 1  [ITALIC] EM",
                "[ITALIC] Seed 1  [ITALIC] F1",
                "[ITALIC] Seed 2  [ITALIC] EM",
                "[ITALIC] Seed 2  [ITALIC] F1"
            ],
            "rows": [
                [
                    "BiDAF",
                    "DBiDAF [ITALIC] -dev",
                    "0.0",
                    "5.3",
                    "10.3",
                    "19.4"
                ],
                [
                    "BERT",
                    "DBERT [ITALIC] -dev",
                    "0.0",
                    "4.9",
                    "20.5",
                    "30.3"
                ],
                [
                    "RoBERTa",
                    "DRoBERTa [ITALIC] -dev",
                    "0.0",
                    "6.1",
                    "16.5",
                    "26.4"
                ],
                [
                    "BiDAF",
                    "DBiDAF [ITALIC] -test",
                    "0.0",
                    "5.5",
                    "12.2",
                    "21.7"
                ],
                [
                    "BERT",
                    "DBERT [ITALIC] -test",
                    "0.0",
                    "5.3",
                    "18.6",
                    "29.6"
                ],
                [
                    "RoBERTa",
                    "DRoBERTa [ITALIC] -test",
                    "0.0",
                    "5.9",
                    "16.2",
                    "27.3"
                ]
            ],
            "title": "Table 4: Consistency of the adversarial effect (or lack thereof) for different models in the loop when retraining the model on the same data again, but with a new random seed."
        },
        "insight": "Our annotation pipeline is designed to reject any samples where the model correctly predicts the answer. How reproducible is this when retraining the same model with the same data? To measure this, we evaluate the performance of two models of identical setup for each respective architecture, which differ only in their random initialisation and data order during SGD sampling. We can thus isolate how strongly the resulting dataset depends on the particular random initialisation and order of data points used to train the model. The results of this experiment are shown in Table 4."
    },
    {
        "id": "335",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Perplexity",
                "[BOLD] C Score",
                "[BOLD] BLEU",
                "[BOLD] Distinct-1",
                "[BOLD] Diff Score",
                "\u0394 [BOLD]  Score"
            ],
            "rows": [
                [
                    "Seq2seq",
                    "37.90",
                    "-0.16",
                    "1.29",
                    "0.0018",
                    "0.00",
                    "0.00"
                ],
                [
                    "Seq2seq-F",
                    "[BOLD] 34.42",
                    "-0.13",
                    "1.47",
                    "0.0033",
                    "63.44",
                    "32.73"
                ],
                [
                    "Speaker",
                    "40.17",
                    "-0.14",
                    "1.14",
                    "0.0039",
                    "0.00",
                    "0.00"
                ],
                [
                    "Speaker-F",
                    "36.33",
                    "-0.05",
                    "1.36",
                    "0.0048",
                    "53.36",
                    "27.92"
                ],
                [
                    "PAML",
                    "37.60",
                    "0.07",
                    "1.52",
                    "0.0067",
                    "36.59",
                    "37.36"
                ],
                [
                    "ATAML",
                    "40.27",
                    "[BOLD] 0.12",
                    "0.52",
                    "0.0074",
                    "105.87",
                    "50.35"
                ],
                [
                    "CMAML\u2212",
                    "37.41",
                    "[BOLD] 0.13",
                    "[BOLD] 1.59",
                    "0.0076",
                    "379.72",
                    "193.23"
                ],
                [
                    "CMAML",
                    "35.88",
                    "[BOLD] 0.12",
                    "[BOLD] 1.57",
                    "[BOLD] 0.0089",
                    "[BOLD] 412.77",
                    "[BOLD] 210.07"
                ]
            ],
            "title": "Table 1: Overall performance in terms of personality (C Score), quality (Perplexity, BLEU), diversity (Distinct-1), structure similarity of different users (Diff Score (\u00d710\u221210)), model change after adaptation (\u0394 score (\u00d710\u221210)) ."
        },
        "insight": "We present our results in Table 1. As shown in Table 1, our model CMAML achieves good performance on quality, personality, and diversity."
    },
    {
        "id": "336",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Trained On",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DDROP",
                "[BOLD] Evaluation (Test) Dataset DDROP",
                "[BOLD] Evaluation (Test) Dataset DNQ",
                "[BOLD] Evaluation (Test) Dataset DNQ"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 40.6",
                    "[BOLD] 54.6",
                    "[BOLD] 7.0",
                    "[BOLD] 15.1",
                    "5.3",
                    "12.8",
                    "5.7",
                    "13.2",
                    "4.5",
                    "9.3",
                    "[BOLD] 26.7",
                    "[BOLD] 40.6"
                ],
                [
                    "[ITALIC] BiDAF",
                    "[ITALIC] DBiDAF",
                    "12.1",
                    "22.1",
                    "5.7",
                    "12.9",
                    "6.4",
                    "13.6",
                    "6.0",
                    "13.2",
                    "6.1",
                    "12.0",
                    "14.1",
                    "26.7"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "9.9",
                    "18.8",
                    "6.4",
                    "13.3",
                    "8.5",
                    "15.6",
                    "8.8",
                    "15.7",
                    "8.3",
                    "14.5",
                    "14.9",
                    "27.5"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "10.9",
                    "20.8",
                    "6.6",
                    "13.8",
                    "[BOLD] 10.1",
                    "[BOLD] 18.0",
                    "[BOLD] 9.7",
                    "[BOLD] 16.7",
                    "[BOLD] 14.8",
                    "[BOLD] 23.3",
                    "13.3",
                    "26.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 70.5",
                    "[BOLD] 83.6",
                    "36.4",
                    "50.3",
                    "15.0",
                    "26.5",
                    "10.6",
                    "21.2",
                    "20.0",
                    "31.3",
                    "54.9",
                    "69.5"
                ],
                [
                    "[ITALIC] BERT",
                    "[ITALIC] DBiDAF",
                    "67.9",
                    "81.6",
                    "[BOLD] 46.5",
                    "[BOLD] 62.4",
                    "[BOLD] 37.5",
                    "[BOLD] 49.0",
                    "[BOLD] 32.3",
                    "[BOLD] 44.2",
                    "[BOLD] 41.1",
                    "[BOLD] 51.5",
                    "[BOLD] 55.8",
                    "[BOLD] 71.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "60.9",
                    "75.2",
                    "42.2",
                    "57.8",
                    "36.4",
                    "46.6",
                    "28.3",
                    "39.6",
                    "35.7",
                    "44.4",
                    "50.7",
                    "65.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "57.6",
                    "71.8",
                    "36.8",
                    "50.9",
                    "34.1",
                    "44.9",
                    "31.0",
                    "41.7",
                    "37.6",
                    "45.9",
                    "48.2",
                    "63.8"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 70.0",
                    "[BOLD] 83.7",
                    "39.4",
                    "55.4",
                    "21.5",
                    "33.7",
                    "11.1",
                    "22.1",
                    "20.3",
                    "30.9",
                    "[BOLD] 48.0",
                    "64.8"
                ],
                [
                    "[ITALIC] RoBERTa",
                    "[ITALIC] DBiDAF",
                    "65.0",
                    "80.4",
                    "[BOLD] 46.6",
                    "[BOLD] 62.3",
                    "[BOLD] 38.9",
                    "[BOLD] 50.8",
                    "25.1",
                    "36.0",
                    "[BOLD] 40.0",
                    "[BOLD] 51.3",
                    "46.9",
                    "[BOLD] 65.3"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "58.7",
                    "74.1",
                    "42.5",
                    "58.0",
                    "34.8",
                    "45.6",
                    "24.7",
                    "34.6",
                    "37.8",
                    "48.5",
                    "42.7",
                    "60.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "55.4",
                    "71.4",
                    "37.9",
                    "53.5",
                    "37.5",
                    "48.6",
                    "[BOLD] 28.2",
                    "[BOLD] 38.9",
                    "39.5",
                    "49.0",
                    "38.8",
                    "57.9"
                ]
            ],
            "title": "Table 5: Training models on various datasets, each with 10,000 samples, and measuring their generalisation to different evaluation datasets. Results in bold indicate the best result per model."
        },
        "insight": "We next conduct a series of experiments in which we train on DBiDAF, DBERT, and DRoBERTa, and observe how well models can then learn to generalise on the respective test portions of these datasets. Table 5 shows the results, and there is a multitude of observations. [CONTINUE] First, one clear trend we observe across all training data setups is a clear negative performance progression when evaluated against datasets constructed with a stronger model in the loop. This trend holds true for all but the BiDAF model, in each of the training configurations, and for each of the evaluation datasets. For example, RoBERTa trained on DRoBERTa achieves 71.4, 53.5, 48.6 and 38.9 F1 when evaluated on DSQuAD, DBiDAF, DBERT and DRoBERTa, respectively. Second, we observe that the BiDAF model is not able to generalise well to datasets constructed with a model in the loop, independent of its training setup. In particular it is unable to learn from DBiDAF, thus failing to overcome some of its own blind spots through adversarial training. Both when training only on DBiDAF, as well as when adding DSQuAD to DBiDAF during training (cf. Table 6), BiDAF performs poorly across all the adversarial datasets. [CONTINUE] results in Table 5, where training on DBiDAF in several cases led to better generalisation than training on DRoBERTa. [CONTINUE] we further train each of our three models on either DBiDAF, DBERT, or DRoBERTa and test on DSQuAD, with results in the DSQuAD columns of Table 5. [CONTINUE] First, we observe clear generalisation improvements towards DDROP across all models compared to training on DSQuAD(10K) when using any of the DBiDAF, DBERT, or DRoBERTa datasets for training. That is, including a model in the loop for the training dataset leads to improved transfer towards DDROP. Note that the DROP dataset also makes use of a BiDAF model in the loop during annotation; these results are in line with our prior observations when testing the same setups on DBiDAF, DBERT and DRoBERTa, compared to training on DSQuAD(10K). Second, we observe overall strong transfer results towards DNQ: up to 71.0F1 for a BERT model trained on DBiDAF. Note that this result is similar and even slightly improves over model training with SQuAD data of the same size. That is, relative to training on SQuAD data, training on adversarially collected data DBiDAF does not impede generalisation to the DNQ dataset, which was created without a model in the annotation loop. We then however see a similar negative performance progression as observed before when testing on DSQuAD: the stronger the model in the annotation loop of the training dataset, the lower the test accuracy on test data from a data distribution composed without using a model in the loop."
    },
    {
        "id": "337",
        "table": {
            "header": [
                "\u2193train/eval\u2192",
                "WebSplit 1.0",
                "WikiSplit"
            ],
            "rows": [
                [
                    "Source",
                    "58.0",
                    "73.4"
                ],
                [
                    "SplitHalf",
                    "54.9",
                    "71.7"
                ],
                [
                    "WebSplit",
                    "35.3",
                    "4.2"
                ],
                [
                    "WikiSplit",
                    "59.4",
                    "76.0"
                ],
                [
                    "Both",
                    "[BOLD] 61.4",
                    "[BOLD] 76.1"
                ]
            ],
            "title": "Table 4: Corpus-level BLEU scores on the validation sets for the same model architecture trained on different data."
        },
        "insight": "We compare three training configurations: WEBSPLIT only, WIKISPLIT only, and BOTH, which is simply their concatenation. [CONTINUE] The WEBSPLIT model scores 35.3 BLEU on the WebSplit validation set but fails to generalize beyond its narrow domain, as evidenced by reaching only 4.2 BLEU on the WikiSplit validation set. [CONTINUE] In contrast, the WIKISPLIT model achieves 59.4 BLEU on the WebSplit validation set, [CONTINUE] Reintroducing the downsampled, in-domain training data (BOTH) further improves performance on the WebSplit evaluation."
    },
    {
        "id": "338",
        "table": {
            "header": [
                "[ITALIC] Training",
                "Unsupported",
                "Missing",
                "Repeated",
                "Correct"
            ],
            "rows": [
                [
                    "AG18",
                    "82",
                    "45",
                    "12",
                    "26/119 (22%)"
                ],
                [
                    "WebSplit",
                    "58",
                    "47",
                    "13",
                    "32/100 (32%)"
                ],
                [
                    "WikiSplit",
                    "8",
                    "5",
                    "0",
                    "91/100 (91%)"
                ],
                [
                    "Both",
                    "[BOLD] 4",
                    "[BOLD] 4",
                    "[BOLD] 0",
                    "[BOLD] 95/100 (95%)"
                ]
            ],
            "title": "Table 6: Manual evaluation results, as counts over the simple sentences predicted by each model for a random sample of 50 inputs from WebSplit 1.0 validation set."
        },
        "insight": "As shown in Table 6, the BOTH model produced the most accurate output (95% correct simple sentences), with the lowest incidence of missed or unsupported statements. [CONTINUE] outputs from Aharoni and Goldberg(2018) (AG18), which were 22% accurate."
    },
    {
        "id": "339",
        "table": {
            "header": [
                "[EMPTY]",
                "BLEU",
                "sBLEU",
                "#S/C",
                "#T/S"
            ],
            "rows": [
                [
                    "Reference",
                    "[EMPTY]",
                    "\u2013",
                    "2.5",
                    "10.9"
                ],
                [
                    "Source",
                    "58.7",
                    "56.1",
                    "1.0",
                    "20.5"
                ],
                [
                    "SplitHalf",
                    "55.7",
                    "53.0",
                    "2.0",
                    "10.8"
                ],
                [
                    "AG18",
                    "30.5",
                    "25.5",
                    "2.3",
                    "11.8"
                ],
                [
                    "WebSplit",
                    "34.2",
                    "30.5",
                    "2.0",
                    "8.8"
                ],
                [
                    "WikiSplit",
                    "60.4",
                    "58.0",
                    "2.0",
                    "11.2"
                ],
                [
                    "Both",
                    "[BOLD] 62.4",
                    "[BOLD] 60.1",
                    "2.0",
                    "11.0"
                ]
            ],
            "title": "Table 5: Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed: corpus-level BLEU, sentence-level BLEU (to match past work), simple sentences per complex sentence, and tokens per simple sentence (micro-average). AG18 is the previous best model by aharoni:2018, which used the full WebSplit training set, whereas we downsampled it."
        },
        "insight": "We relate our approach to prior work on Web-Split v1.0 by reporting scores on its test set in Table 5. Our best performance in BLEU is again obtained by combining the proposed WikiSplit dataset with the downsampled WebSplit, yielding a 32 point improvement over the prior best result."
    },
    {
        "id": "340",
        "table": {
            "header": [
                "[EMPTY]",
                "Basic: no walks MA",
                "Basic: no walks MR",
                "Basic: no walks MC",
                "Basic: no walks Avg",
                "EmbDI walks MA",
                "EmbDI walks MR",
                "EmbDI walks MC",
                "EmbDI walks Avg"
            ],
            "rows": [
                [
                    "Movie",
                    "[BOLD] .80",
                    ".54",
                    ".41",
                    ".59",
                    ".55",
                    "[BOLD] .80",
                    "[BOLD] .90",
                    "[BOLD] .75"
                ],
                [
                    "RefS",
                    ".39",
                    ".40",
                    ".24",
                    ".34",
                    "[BOLD] .90",
                    "[BOLD] .59",
                    "[BOLD] .62",
                    "[BOLD] .70"
                ],
                [
                    "RefL",
                    "[BOLD] .63",
                    ".50",
                    ".29",
                    ".47",
                    "[BOLD] .63",
                    "[BOLD] .69",
                    "[BOLD] .78",
                    "[BOLD] .70"
                ]
            ],
            "title": "Table 2: Quality results for local embeddings."
        },
        "insight": "We report the quality results in Table 2, where each number represents the fraction of tests passed. While on average the local embeddings for EmbDI are largely superior to the baseline, our solution is beaten once for MA. By increasing the percentage of row permutations in Basic, results for MR improve but decrease for MA, without significant benefit for MC."
    },
    {
        "id": "341",
        "table": {
            "header": [
                "[EMPTY]",
                "EmbDI P",
                "EmbDI R",
                "EmbDI F",
                "Seep [ITALIC] P P",
                "Seep [ITALIC] P R",
                "Seep [ITALIC] P F",
                "Seep [ITALIC] L P",
                "Seep [ITALIC] L R",
                "Seep [ITALIC] L F"
            ],
            "rows": [
                [
                    "Movie",
                    "[BOLD] .75",
                    "[BOLD] .86",
                    "[BOLD] .80",
                    "[BOLD] .75",
                    ".62",
                    ".68",
                    "[BOLD] .75",
                    ".75",
                    ".75"
                ],
                [
                    "RefS",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    ".75",
                    ".75",
                    ".75",
                    ".88",
                    ".75",
                    ".81"
                ],
                [
                    "RefL",
                    "[BOLD] .71",
                    ".83",
                    "[BOLD] .77",
                    ".50",
                    ".75",
                    ".60",
                    ".63",
                    "[BOLD] .88",
                    ".73"
                ]
            ],
            "title": "Table 3: Quality results for SM."
        },
        "insight": "We test an unsupervised setting using (1) the algorithm proposed in Section 4.4 with EmbDI local embeddings, and (2) an existing matching system  with both pre-trained embeddings (SeepP ) and our local embeddings (SeepL). Pre-trained embeddings for tokens and tuples have been obtained from Glove . [CONTINUE] Table 3 reports the results w.r.t. manually defined attribute matches. The simple unsupervised method with EmbDI local embeddings outperforms the baseline in terms of Fmeasure in all scenarios. Results of RefS are the best because of the high overlap between its datasets. The baseline improves when it is executed with EmbDI local embeddings, showing their superior quality w.r.t. pre-trained ones. The Basic local embeddings lead to 0 attribute matches. [CONTINUE] We also observe that results for SeepPreTrain depend on the quality of the original attribute labels. If we replace the original (expressive and correct) labels with synthetic ones, Seep-PreTrain obtains F-measure values between .30 and .38. Local embeddings from EmbDI do not depend on the presence of the attribute labels. [CONTINUE] Similarly, decreasing the size of the walks to 5 for the SM task raises the F-measure for RefL to .92 (from .77)."
    },
    {
        "id": "342",
        "table": {
            "header": [
                "[EMPTY]",
                "Unsupervised. Basic",
                "Unsupervised. Glove",
                "Unsupervised. EmbDI",
                "Supervised DeepER [ITALIC] P",
                "Supervised DeepER [ITALIC] L"
            ],
            "rows": [
                [
                    "Movie",
                    "0",
                    ".43",
                    "[BOLD] .78",
                    ".82",
                    "[BOLD] .88"
                ],
                [
                    "RefS",
                    "0",
                    ".84",
                    "[BOLD] .95",
                    ".84",
                    "[BOLD] .89"
                ],
                [
                    "RefL",
                    "0",
                    ".73",
                    "[BOLD] .80",
                    ".80",
                    "[BOLD] .87"
                ]
            ],
            "title": "Table 4: F-Measure results for ER."
        },
        "insight": "As baseline, we use our unsupervised algorithm with EmbDI embeddings and pre-trained embeddings. We also test our local embeddings in the supervised setting with a state of the art ER system (DeepERL), comparing its results to the ones obtained with pre-trained embeddings (DeepERP ). [CONTINUE] Results in Table 4 show that EmbDI embeddings obtain better quality results in all scenarios in both settings. As observed in the SM experiments, using local embeddings instead of pre-trained ones increases significantly the quality of an existing system. In this case, supervised DeepER shows an average 6% absolute improvement in F-measure in the tested setting with 5% of the ground truth passed as training data. The improvements decreases to 4% with more training data (10%). Also for ER, the local embeddings obtained with the basic method lead to 0 row matched. [CONTINUE] Execut"
    },
    {
        "id": "343",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Training Dataset",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "[BOLD] 58.7",
                    "[BOLD] 71.9",
                    "0.0",
                    "5.5",
                    "8.9",
                    "17.6",
                    "8.3",
                    "17.0"
                ],
                [
                    "[ITALIC] BiDAF",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "57.3",
                    "70.6",
                    "14.9",
                    "25.8",
                    "16.9",
                    "25.5",
                    "15.3",
                    "24.2"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "57.0",
                    "70.4",
                    "[BOLD] 16.3",
                    "[BOLD] 26.5",
                    "14.5",
                    "24.1",
                    "14.7",
                    "24.1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "55.9",
                    "69.6",
                    "16.2",
                    "25.6",
                    "[BOLD] 17.3",
                    "[BOLD] 26.2",
                    "[BOLD] 15.6",
                    "[BOLD] 25.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "70.7",
                    "84.0",
                    "36.7",
                    "50.2",
                    "0.0",
                    "5.3",
                    "15.2",
                    "25.8"
                ],
                [
                    "[ITALIC] BERT",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "[BOLD] 74.5",
                    "[BOLD] 85.9",
                    "47.2",
                    "[BOLD] 61.1",
                    "33.7",
                    "43.6",
                    "29.1",
                    "39.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "74.3",
                    "85.8",
                    "[BOLD] 48.1",
                    "[BOLD] 61.1",
                    "[BOLD] 37.8",
                    "[BOLD] 47.3",
                    "[BOLD] 31.1",
                    "[BOLD] 41.5"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "73.2",
                    "85.2",
                    "47.3",
                    "60.5",
                    "36.8",
                    "46.2",
                    "30.1",
                    "39.7"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "74.1",
                    "86.8",
                    "50.4",
                    "64.9",
                    "31.9",
                    "44.1",
                    "0.0",
                    "5.9"
                ],
                [
                    "[ITALIC] RoBERTa",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "75.2",
                    "87.6",
                    "56.3",
                    "71.2",
                    "47.8",
                    "58.0",
                    "31.3",
                    "42.8"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "[BOLD] 76.2",
                    "[BOLD] 88.0",
                    "56.3",
                    "70.8",
                    "48.3",
                    "58.2",
                    "33.4",
                    "44.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "75.1",
                    "87.5",
                    "[BOLD] 58.2",
                    "[BOLD] 73.2",
                    "[BOLD] 52.8",
                    "[BOLD] 62.7",
                    "[BOLD] 36.4",
                    "[BOLD] 47.2"
                ]
            ],
            "title": "Table 6: Training models on SQuAD, as well as SQuAD combined with different adversarially created datasets. Results in bold indicate the best result per model."
        },
        "insight": "For example, RoBERTa trained on DRoBERTa reaches 38.9F1 on DRoBERTa, and this number further increases to 47.2F1 when including SQuAD during training (cf. Table 6). [CONTINUE] In Table 6 we show experimental results for the same models and training datasets, but now including SQuAD as additional training data. In this training setup we generally see improved generalisation to DBiDAF, DBERT, and DRoBERTa. Interestingly, the relative differences between DBiDAF, DBERT, and DRoBERTa as training set used in conjunction with SQuAD are now much diminished, and especially DRoBERTa as (part of the) training set now generalises substantially better. RoBERTa achieves the strongest results on any of the DBiDAF, DBERT, and DRoBERTa evaluation sets, in particular when trained on DSQuAD+DRoBERTa. [CONTINUE] we identify a risk of datasets constructed with weaker models in the loop becoming outdated. For example, RoBERTa achieves 58.2EM/73.2F1 on DBiDAF, in contrast to 0.0EM/5.5F1 for BiDAF \u2013 which is not far from non-expert human performance of 62.6EM/78.5F1. [CONTINUE] We furthermore observe a gradual decrease in generalisation to SQuAD when training on DBiDAF towards training on DRoBERTa. This suggests that the stronger the model used in the annotation loop, the more dissimilar the data distribution becomes from the original SQuAD distribution. We will later find further support for this explanation in a qualitative analysis (Section 5). It may however also be due to a limitation of BERT and RoBERTa \u2013 similar to BiDAF \u2013 in learning from a data distribution designed to beat these models; an even stronger model might learn more e.g. from DRoBERTa."
    },
    {
        "id": "344",
        "table": {
            "header": [
                "[EMPTY]",
                "#T/S",
                "#S/C",
                "% SAME",
                "LDSC",
                "SAMSA",
                "SAMSAabl"
            ],
            "rows": [
                [
                    "Complex",
                    "30.75",
                    "1.18",
                    "100",
                    "0.00",
                    "0.36",
                    "0.94"
                ],
                [
                    "MinWikiSplit",
                    "12.12",
                    "3.84",
                    "0.00",
                    "17.73",
                    "0.40",
                    "0.48"
                ]
            ],
            "title": "Table 4: Results of the automatic evaluation procedure on a random sample of 1000 sentences."
        },
        "insight": "The results are provided in Table 4. [CONTINUE] The scores demonstrate that on average our proposed sentence splitting corpus contains four simplified target sentences per complex source sentence, with every target proposition consisting of 12 tokens. Moreover, no input is simply copied to the output, but split into smaller components."
    },
    {
        "id": "345",
        "table": {
            "header": [
                "G",
                "M",
                "S"
            ],
            "rows": [
                [
                    "[BOLD] 4.36",
                    "[BOLD] 4.10",
                    "[BOLD] 3.43"
                ]
            ],
            "title": "Table 6: Averaged human evaluation ratings on a random sample of 300 sentences from MinWikiSplit. Grammaticality (G), meaning preservation (M) and structural simplicity (S) are measured using a 1 (very bad) to 5 (very good) scale."
        },
        "insight": "The results of the human evaluation are displayed in Table 6. [CONTINUE] These scores show that we succeed in producing output sequences that reach a high level of grammatical soundness and almost always perfectly preserve the original meaning of the input. The third dimension under consideration, structural simplicity, which captures the degree of minimality in the simplified sentences, scores high values, too."
    },
    {
        "id": "346",
        "table": {
            "header": [
                "Dataset",
                "# Tweets",
                "Labels",
                "Annotators/Tweet"
            ],
            "rows": [
                [
                    "Chatzakou:2017:MBD:3091478.3091487",
                    "9,484",
                    "aggressive, bullying, spam, normal",
                    "5"
                ],
                [
                    "DBLP:conf/naacl/WaseemH16",
                    "16, 914",
                    "racist, sexist, normal",
                    "1"
                ],
                [
                    "DavidsonWMW17",
                    "24, 802",
                    "hateful, offensive (but not hateful), neither",
                    "3 or more"
                ],
                [
                    "Golbeck2017",
                    "35,000",
                    "the worst, threats, hate speech, direct",
                    "2 to 3"
                ],
                [
                    "Golbeck2017",
                    "35,000",
                    "harassment, potentially offensive, non-harassment",
                    "2 to 3"
                ],
                [
                    "FountaDCLBSVSK18",
                    "80, 000",
                    "offensive, abusive, hateful speech,",
                    "5 to 20"
                ],
                [
                    "FountaDCLBSVSK18",
                    "80, 000",
                    "aggressive, cyberbullying, spam, normal",
                    "5 to 20"
                ],
                [
                    "hatelingo",
                    "28,608",
                    "directed, generalized + target = archaic, class, disability,",
                    "3"
                ],
                [
                    "hatelingo",
                    "28,608",
                    "ethnicity, gender, nationality, religion, sexual orientation",
                    "3"
                ],
                [
                    "Ours",
                    "13,000",
                    "Labels for five different aspects",
                    "5"
                ]
            ],
            "title": "Table 1: Comparative table of some of the available hate speech and abusive language corpora in terms of labels and sizes."
        },
        "insight": "We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets [CONTINUE] Table 1 compares different labelsets that exist in the literature. For instance, Waseem and Hovy (2016) use racist, sexist, and normal as labels; Davidson et al. (2017) label their data as hateful, offensive (but not hateful), and neither, while ElSherief et al. (2018) present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. Founta et al. (2018) label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal."
    },
    {
        "id": "347",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Precision(%)",
                "[BOLD] Recall@5(%)",
                "[BOLD] F1(%)"
            ],
            "rows": [
                [
                    "UNC [unc]",
                    "36.39",
                    "86.79",
                    "51.38"
                ],
                [
                    "UCL [ucl]",
                    "22.74**",
                    "84.54",
                    "35.84"
                ],
                [
                    "UKP-Athene [athene]",
                    "23.67*",
                    "85.81*",
                    "37.11*"
                ],
                [
                    "DREAM-XLNet [xlnetgraph]",
                    "26.60",
                    "87.33",
                    "40.79"
                ],
                [
                    "DREAM-RoBERTa [xlnetgraph]",
                    "26.67",
                    "87.64",
                    "40.90"
                ],
                [
                    "Pointwise",
                    "25.14",
                    "88.25",
                    "39.13"
                ],
                [
                    "Pointwise + Threshold",
                    "[BOLD] 38.18",
                    "88.00",
                    "[BOLD] 53.25"
                ],
                [
                    "Pointwise + HNM",
                    "25.13",
                    "88.29",
                    "39.13"
                ],
                [
                    "Pairwise Ranknet",
                    "24.97",
                    "88.20",
                    "38.93"
                ],
                [
                    "Pairwise Ranknet + HNM",
                    "24.97",
                    "[BOLD] 88.32",
                    "38.93"
                ],
                [
                    "Pairwise Hinge",
                    "24.94",
                    "88.07",
                    "38.88"
                ],
                [
                    "Pairwise Hinge + HNM",
                    "25.01",
                    "88.28",
                    "38.98"
                ]
            ],
            "title": "Table 1: Development set sentence retrieval performance. * We calculated the scores using the official code, and for ** we used the F1 formula to calculate the score."
        },
        "insight": "Table 1 compares the development set performance of different variants of the proposed sentence retrieval method with the state of the art results on the FEVER dataset. The results indicate that both pointwise and pairwise BERT sentence retrieval improve the recall. The UNC and DREAM precision scores are better than our methods without a decision threshold, however, a threshold can regulate the trade-off between the recall and precision, and achieve the best precision and F1 scores."
    },
    {
        "id": "348",
        "table": {
            "header": [
                "Attribute",
                "Model",
                "Macro-F1 EN",
                "Macro-F1 FR",
                "Macro-F1 AR",
                "Macro-F1 Avg",
                "Micro-F1 EN",
                "Micro-F1 FR",
                "Micro-F1 AR",
                "Micro-F1 Avg"
            ],
            "rows": [
                [
                    "Directness",
                    "Majority",
                    "0.50",
                    "0.11",
                    "0.50",
                    "0.47",
                    "0.79",
                    "0.41",
                    "0.54",
                    "0.58"
                ],
                [
                    "Directness",
                    "LR",
                    "0.52",
                    "0.50",
                    "0.53",
                    "0.52",
                    "0.79",
                    "0.50",
                    "0.56",
                    "0.62"
                ],
                [
                    "Directness",
                    "STSL",
                    "[BOLD] 0.94",
                    "[BOLD] 0.80",
                    "[BOLD] 0.84",
                    "[BOLD] 0.86",
                    "[BOLD] 0.89",
                    "[BOLD] 0.69",
                    "[BOLD] 0.72",
                    "[BOLD] 0.76"
                ],
                [
                    "Directness",
                    "MTSL",
                    "[BOLD] 0.94",
                    "0.65",
                    "0.76",
                    "0.78",
                    "[BOLD] 0.89",
                    "0.58",
                    "0.65",
                    "0.70"
                ],
                [
                    "Directness",
                    "STML",
                    "[BOLD] 0.94",
                    "0.79",
                    "0.83",
                    "0.85",
                    "0.88",
                    "0.66",
                    "[BOLD] 0.72",
                    "0.75"
                ],
                [
                    "Directness",
                    "MTML",
                    "[BOLD] 0.94",
                    "0.78",
                    "0.74",
                    "0.82",
                    "0.88",
                    "0.66",
                    "0.65",
                    "0.73"
                ]
            ],
            "title": "Table 3: Full evaluation scores of the only binary classification task where the single task single language model consistently outperforms multilingual multitask models."
        },
        "insight": "single task single language (STSL), single task multilingual (STML), and multitask multilingual models (MTML) on our dataset. [CONTINUE] STSL performs the best among all models on the directness classification, and it is also consistent in both micro and macro-F1 scores. [CONTINUE] due to the fact that the directness has only two labels and multilabeling is not allowed in this task. Tasks involving imbalanced data, multiclass and multilabel annotations harm the performance of the directness in multitask settings. [CONTINUE] Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of microF1 and for most of the macro-F1 scores. This shows the power of the deep learning approach. [CONTINUE] Except for the directness, MTSL usually outperforms STSL or is comparable to it. [CONTINUE] MTML settings do not lead to a big improvement which may be due to the class imbalance, multilabel tasks, and the difference in the nature of the tasks."
    },
    {
        "id": "349",
        "table": {
            "header": [
                "Attribute",
                "Model",
                "Macro-F1 EN",
                "Macro-F1 FR",
                "Macro-F1 AR",
                "Macro-F1 Avg",
                "Micro-F1 EN",
                "Micro-F1 FR",
                "Micro-F1 AR",
                "Micro-F1 Avg"
            ],
            "rows": [
                [
                    "Tweet",
                    "Majority",
                    "0.24",
                    "0.19",
                    "0.20",
                    "0.21",
                    "0.41",
                    "0.27",
                    "0.27",
                    "0.32"
                ],
                [
                    "Tweet",
                    "LR",
                    "0.14",
                    "0.20",
                    "0.25",
                    "0.20",
                    "0.54",
                    "0.56",
                    "[BOLD] 0.48",
                    "0.53"
                ],
                [
                    "Tweet",
                    "STSL",
                    "0.24",
                    "0.12",
                    "0.31",
                    "0.23",
                    "0.49",
                    "0.51",
                    "0.47",
                    "0.49"
                ],
                [
                    "Tweet",
                    "MTSL",
                    "0.09",
                    "0.20",
                    "0.33",
                    "0.21",
                    "[BOLD] 0.55",
                    "[BOLD] 0.59",
                    "0.46",
                    "[BOLD] 0.54"
                ],
                [
                    "Tweet",
                    "STML",
                    "0.04",
                    "0.07",
                    "[BOLD] 0.35",
                    "0.16",
                    "0.54",
                    "0.47",
                    "0.37",
                    "0.46"
                ],
                [
                    "Tweet",
                    "MTML",
                    "[BOLD] 0.30",
                    "[BOLD] 0.28",
                    "[BOLD] 0.35",
                    "[BOLD] 0.31",
                    "0.45",
                    "0.48",
                    "0.44",
                    "0.46"
                ],
                [
                    "Target Attribute",
                    "Majority",
                    "0.15",
                    "0.13",
                    "0.28",
                    "0.19",
                    "0.25",
                    "0.32",
                    "0.40",
                    "0.32"
                ],
                [
                    "Target Attribute",
                    "LR",
                    "0.41",
                    "0.35",
                    "0.47",
                    "0.41",
                    "0.52",
                    "0.55",
                    "0.53",
                    "0.53"
                ],
                [
                    "Target Attribute",
                    "STSL",
                    "0.42",
                    "0.18",
                    "[BOLD] 0.63",
                    "0.41",
                    "[BOLD] 0.68",
                    "0.71",
                    "0.50",
                    "0.63"
                ],
                [
                    "Target Attribute",
                    "MTSL",
                    "0.41",
                    "[BOLD] 0.43",
                    "0.41",
                    "[BOLD] 0.42",
                    "[BOLD] 0.68",
                    "0.67",
                    "[BOLD] 0.56",
                    "[BOLD] 0.64"
                ],
                [
                    "Target Attribute",
                    "STML",
                    "0.39",
                    "0.09",
                    "0.24",
                    "0.24",
                    "0.67",
                    "0.62",
                    "0.53",
                    "0.61"
                ],
                [
                    "Target Attribute",
                    "MTML",
                    "[BOLD] 0.43",
                    "0.24",
                    "0.16",
                    "0.28",
                    "0.66",
                    "[BOLD] 0.72",
                    "0.51",
                    "0.63"
                ],
                [
                    "Target Group",
                    "Majority",
                    "0.07",
                    "0.06",
                    "0.08",
                    "0.07",
                    "0.18",
                    "0.14",
                    "0.35",
                    "0.22"
                ],
                [
                    "Target Group",
                    "LR",
                    "[BOLD] 0.18",
                    "0.33",
                    "[BOLD] 0.40",
                    "[BOLD] 0.30",
                    "0.34",
                    "0.40",
                    "0.62",
                    "0.46"
                ],
                [
                    "Target Group",
                    "STSL",
                    "0.04",
                    "0.21",
                    "0.04",
                    "0.10",
                    "0.48",
                    "[BOLD] 0.59",
                    "0.58",
                    "0.55"
                ],
                [
                    "Target Group",
                    "MTSL",
                    "0.04",
                    "0.27",
                    "0.15",
                    "0.15",
                    "[BOLD] 0.50",
                    "0.54",
                    "0.55",
                    "0.53"
                ],
                [
                    "Target Group",
                    "STML",
                    "0.11",
                    "[BOLD] 0.37",
                    "0.13",
                    "0.20",
                    "0.49",
                    "0.57",
                    "[BOLD] 0.64",
                    "[BOLD] 0.56"
                ],
                [
                    "Target Group",
                    "MTML",
                    "0.06",
                    "0.19",
                    "0.10",
                    "0.11",
                    "[BOLD] 0.50",
                    "0.54",
                    "0.56",
                    "0.53"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "Majority",
                    "0.42",
                    "0.21",
                    "0.17",
                    "0.27",
                    "0.46",
                    "0.31",
                    "0.32",
                    "0.39"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "LR",
                    "0.29",
                    "0.15",
                    "0.14",
                    "0.19",
                    "0.45",
                    "0.30",
                    "0.46",
                    "0.40"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "STSL",
                    "[BOLD] 0.57",
                    "[BOLD] 0.30",
                    "0.12",
                    "[BOLD] 0.33",
                    "0.57",
                    "0.39",
                    "[BOLD] 0.48",
                    "0.48"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "MTSL",
                    "[BOLD] 0.57",
                    "0.17",
                    "0.17",
                    "0.30",
                    "0.57",
                    "[BOLD] 0.50",
                    "0.45",
                    "0.51"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "STML",
                    "0.47",
                    "0.22",
                    "0.13",
                    "0.27",
                    "[BOLD] 0.59",
                    "0.49",
                    "[BOLD] 0.48",
                    "[BOLD] 0.52"
                ],
                [
                    "Annotator\u2019s Sentiment",
                    "MTML",
                    "0.55",
                    "0.20",
                    "[BOLD] 0.21",
                    "0.32",
                    "0.58",
                    "0.45",
                    "0.45",
                    "0.49"
                ]
            ],
            "title": "Table 4: Full evaluation of tasks where multilingual and multitask models outperform on average single task single language model on four different tasks."
        },
        "insight": "When we jointly train each task on the three languages, the performance decreases in most cases, other than the target group classification tasks. [CONTINUE] Yet, multilingual training of the target group classification task improves in all languages. Since the target group classification task involves 16 labels, the amount of data annotated for each label is lower than in other tasks. Hence, when aggregating annotated data in different languages, the size of the training data also increases, due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks."
    },
    {
        "id": "350",
        "table": {
            "header": [
                "[EMPTY]",
                "French F1",
                "French EM",
                "Japanese F1",
                "Japanese EM"
            ],
            "rows": [
                [
                    "Baseline",
                    "61.88",
                    "40.67",
                    "52.19",
                    "37.00"
                ],
                [
                    "Multilingual BERT",
                    "[BOLD] 76.65",
                    "[BOLD] 61.77",
                    "[BOLD] 61.83",
                    "[BOLD] 59.94"
                ]
            ],
            "title": "Table 1: Comparison of Exact Match and F1-score of multilingual BERT and the baseline on French and Japanese SQuAD. F1 and EM are the two official metrics of the SQuAD benchmark. EM measures the percentage of predictions that match exactly the ground-truth location of the answer. F1 measures the average overlap between the prediction and ground truth answer."
        },
        "insight": "A sample of the SQuAD v1.1 test set (only the first paragraph of each of the 48 Wikipedia pages) has been translated by humans in French and Japanese. We here evaluate the performance of the fine-tuned multilingual BERT on them and compare the results to a baseline . [CONTINUE] Table 1 displays the Exact Match (EM) and F1-score of the baseline and multilingual BERT on the selected datasets. We can observe that multilingual BERT is able to significantly outperform the baseline on both the Japanese and the French question answering task. [CONTINUE] was already noted in the public benchmarks and we add here that BERT has a high ability for QA zero-shot transfer. It is even able to significantly outperform the baseline"
    },
    {
        "id": "351",
        "table": {
            "header": [
                "[EMPTY]",
                "Question",
                "En F1",
                "En EM",
                "Fr F1",
                "Fr EM",
                "Jap F1",
                "Jap EM"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "En",
                    "90.57",
                    "[BOLD] 81.96",
                    "78.55",
                    "[BOLD] 67.28",
                    "66.22",
                    "52.91"
                ],
                [
                    "Context",
                    "Fr",
                    "81.10",
                    "65.14",
                    "76.65",
                    "61.77",
                    "60.28",
                    "42.20"
                ],
                [
                    "[EMPTY]",
                    "Jap",
                    "58.95",
                    "57.49",
                    "47.19",
                    "45.26",
                    "61.83",
                    "[BOLD] 59.93"
                ]
            ],
            "title": "Table 2: Exact Match and F1-score of multilingual BERT on each of the cross-lingual SQuAD datasets. The row language is the one of the paragraph and the column language is the one of the question. The figures in bold are the best exact match, for each language, among the datasets where they occur."
        },
        "insight": "To run cross-lingual tests, we build six additional datasets from the existing ones by mixing context in one language with question in another language. The mixed datasets will be made available online [CONTINUE] in a github repository. The performance of BERT on all datasets is displayed in Table 2. [CONTINUE] the performance is the best for the En-En dataset. The performance on Fr-Fr and Jap-Jap is also very good as noted in the first experiment. We additionally note here that results on cross-lingual sets are close to monolingual results: either as good, or slightly worse or slightly better. For instance, the exact match on the En-Fr dataset is higher than the exact match on the Fr-Fr dataset. We also observe that, in general, the exact match and F1-score are close together when the context is in Japanese whereas there is generally a larger gap for the other two languages. [CONTINUE] the performance on Jap-En is lower than on Jap-Jap whereas the performance on En-Fr is higher than on Fr-Fr, [CONTINUE] Results for the Jap-Jap dataset are better than results for the Jap-En dataset"
    },
    {
        "id": "352",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FEVER Score(%)",
                "[BOLD] Label Accuracy(%)"
            ],
            "rows": [
                [
                    "UNC [unc]",
                    "66.14",
                    "69.60"
                ],
                [
                    "UCL [ucl]",
                    "65.41",
                    "69.66"
                ],
                [
                    "UKP-Athene [athene]",
                    "64.74",
                    "-"
                ],
                [
                    "BERT & UKP-Athene",
                    "69.79",
                    "71.70"
                ],
                [
                    "BERT Large & UKP-Athene",
                    "70.64",
                    "72.72"
                ],
                [
                    "BERT & BERT (Pointwise)",
                    "71.38",
                    "73.51"
                ],
                [
                    "BERT & BERT (Pointwise + HNM)",
                    "71.33",
                    "73.54"
                ],
                [
                    "BERT (Large) & BERT (Pointwise)",
                    "[BOLD] 72.42",
                    "74.58"
                ],
                [
                    "BERT (Large) & BERT (Pointwise + HNM)",
                    "[BOLD] 72.42",
                    "[BOLD] 74.59"
                ],
                [
                    "BERT & BERT (Pairwise Ranknet)",
                    "71.02",
                    "73.22"
                ],
                [
                    "BERT & BERT (Pairwise Ranknet + HNM)",
                    "70.99",
                    "73.02"
                ],
                [
                    "BERT & BERT (Pairwise Hinge)",
                    "71.60",
                    "72.74"
                ],
                [
                    "BERT & BERT (Pairwise Hinge + HNM)",
                    "70.70",
                    "72.76"
                ]
            ],
            "title": "Table 2: Development set verification scores."
        },
        "insight": "In Table 2, we compare the development set results of the state of the art methods with the BERT model trained on different retrieved evidence sets. The BERT claim verification system even if it is trained on the UKP-Athene sentence retrieval component (Hanselowski et al., 2018), the state of the art method with the highest recall, improves both label accuracy and FEVER score. Training based on the BERT sentence retrieval predictions significantly enhances the verification results because while it explicitly improves the FEVER score by providing more correct evidence sentences, it provides a better training set for the verification system. The large BERTs are only trained on the best retrieval systems, and as expected significantly improve the performance."
    },
    {
        "id": "353",
        "table": {
            "header": [
                "Track",
                "Target",
                "Constrained",
                "Valid",
                "Test"
            ],
            "rows": [
                [
                    "NLG",
                    "EN",
                    "no",
                    "23.5",
                    "20.5"
                ],
                [
                    "MT",
                    "EN",
                    "yes",
                    "60.2",
                    "58.2"
                ],
                [
                    "MT",
                    "EN",
                    "no",
                    "64.2",
                    "62.2"
                ],
                [
                    "MT+NLG",
                    "EN",
                    "yes",
                    "64.4",
                    "62.2"
                ],
                [
                    "NLG",
                    "DE",
                    "no",
                    "16.9",
                    "16.1"
                ],
                [
                    "MT",
                    "DE",
                    "yes",
                    "49.8",
                    "48.0"
                ],
                [
                    "MT+NLG",
                    "DE",
                    "yes",
                    "49.4",
                    "48.2"
                ]
            ],
            "title": "Table 3: Doc-level BLEU scores on the DGT valid and test sets of our submitted models in all tracks."
        },
        "insight": "For each track, we selected the best models according to their BLEU score on DGT-valid. The scores are shown in Table 3, [CONTINUE] We see that in the same data conditions (unconstrained mode), the MT+NLG models are not better than the pure MT models."
    },
    {
        "id": "354",
        "table": {
            "header": [
                "Model",
                "Rotowire test"
            ],
            "rows": [
                [
                    "Wiseman et\u00a0al. ( 2017 )",
                    "14.5"
                ],
                [
                    "Puduppully et\u00a0al. ( 2019 )",
                    "16.5"
                ],
                [
                    "Ours (4-player)",
                    "22.2"
                ]
            ],
            "title": "Table 6: English NLG comparison against state-of-the-art on Rotowire-test. BLEU of submitted NLG (EN) model, averaged over 3 runs. Because Rotowire tokenization is slightly different, we apply a set of fixes to the model outputs (e.g., 1-of-3 \u2192 1 - of - 3)."
        },
        "insight": "Table 6 shows a 5.7 BLEU improvement on Rotowire-test by our English NLG model compared to the previous state of the art."
    },
    {
        "id": "355",
        "table": {
            "header": [
                "Model",
                "Valid",
                "Test"
            ],
            "rows": [
                [
                    "Baseline (3 players, sorted)",
                    "[BOLD] 22.7",
                    "20.4"
                ],
                [
                    "No player",
                    "20.1",
                    "18.8"
                ],
                [
                    "All players, sorted",
                    "[BOLD] 22.7",
                    "20.9"
                ],
                [
                    "All players, shuffled",
                    "22.0",
                    "20.0"
                ],
                [
                    "(1) No next game",
                    "22.0",
                    "19.9"
                ],
                [
                    "(2) No week day",
                    "22.2",
                    "20.5"
                ],
                [
                    "(3) No player position",
                    "22.6",
                    "20.5"
                ],
                [
                    "(4) No team-level sums",
                    "22.5",
                    "20.5"
                ],
                [
                    "(5) Remove most tags",
                    "22.6",
                    "20.8"
                ],
                [
                    "(1) to (5)",
                    "21.3",
                    "19.7"
                ]
            ],
            "title": "Table 7: English NLG ablation study, starting from a 3 best player baseline (the submitted NLG model has 4 players). BLEU averages over 3 runs. Standard deviation ranges between 0.1 and 0.4."
        },
        "insight": "From Table 7, we see that sorting players helps, but only slightly. Using only team-level information, and no information about players gives worse but still decent BLEU scores. [CONTINUE] Week day, player position or team-level aggregated scores can be removed without hurting BLEU. However, information about next games seems useful. Interestingly, relying on position only and removing most tags (e.g.,  ,  ) seems to be fine. In this case, we also print all-zero stats, for the position of each statistic to be consistent across players and games."
    },
    {
        "id": "356",
        "table": {
            "header": [
                "[EMPTY]",
                "TnT",
                "neural in-lang. plain",
                "neural in-lang. +Poly",
                "neural transfer +Medium src",
                "neural transfer +Large src",
                "neural transfer FineTune"
            ],
            "rows": [
                [
                    "zero-shot",
                    "\u2014",
                    "\u2014",
                    "\u2014",
                    "58.29",
                    "61.18",
                    "\u2014"
                ],
                [
                    "Tiny",
                    "37.48",
                    "36.17",
                    "56.05",
                    "67.14",
                    "67.49",
                    "62.07"
                ],
                [
                    "Small",
                    "44.30",
                    "51.90",
                    "67.18",
                    "[BOLD] 70.82",
                    "70.01",
                    "65.63"
                ]
            ],
            "title": "Table 3: F1 score on the development set for low-resource training setups (none, tiny 5k or small 10k labeled Danish sentences). Transfer via multilingual embeddings from Medium (3.2k sentences, 51k tokens) or Large English source data (14k sentences/203k tokens)."
        },
        "insight": "Cross-lingual transfer is powerful (RQ1). Zero-shot learning reaches an F1 score of 58% in the MEDIUM setup, which outperforms training the neural tagger on very limited gold data (plain). [CONTINUE] Neural NER is better than traditional HMM-based tagging (TnT) (Brants, 2000) and greatly improves by unsupervised word embedding initialization (+Poly). It is noteworthy that zero-shot transfer benefits only to a limiting degree from more source data (F1 increases by almost 3% when training on all English CoNLL data). [CONTINUE] To compare cross-lingual transfer to limited gold data (RQ2), we observe that training the neural system on the small amount of data together with Polyglot embeddings is close to the tiny-shot transfer setup. [CONTINUE] Few-shot learning greatly improves over zero-shot learning. The most beneficial way is to add the target data to the source, in comparison to fine-tuning. [CONTINUE] In both MEDIUM and LARGE setups are further gains obtained by adding TINY or SMALL amounts of Danish gold data. Interestingly, a) finetuning is less effective; b) it is better to transfer from a medium-sized setup than from the entire CoNLL source data."
    },
    {
        "id": "357",
        "table": {
            "header": [
                "Dev",
                "All",
                "PER",
                "LOC",
                "ORG",
                "MISC"
            ],
            "rows": [
                [
                    "Majority",
                    "44.4",
                    "61.8",
                    "0.0",
                    "0.0",
                    "\u2014"
                ],
                [
                    "DKIE",
                    "58.9",
                    "68.9",
                    "63.6",
                    "23.3",
                    "\u2014"
                ],
                [
                    "Polyglot",
                    "64.5",
                    "73.7",
                    "[BOLD] 73.4",
                    "36.8",
                    "\u2014"
                ],
                [
                    "Bilstm",
                    "[BOLD] 70.8",
                    "[BOLD] 83.3",
                    "71.8",
                    "[BOLD] 60.0",
                    "23.9"
                ],
                [
                    "Test",
                    "All",
                    "PER",
                    "LOC",
                    "ORG",
                    "MISC"
                ],
                [
                    "Polyglot",
                    "61.6",
                    "78.4",
                    "[BOLD] 69.7",
                    "24.7",
                    "\u2014"
                ],
                [
                    "Bilstm",
                    "[BOLD] 66.0",
                    "[BOLD] 86.6",
                    "63.6",
                    "[BOLD] 42.5",
                    "24.8"
                ]
            ],
            "title": "Table 4: F1 score for Danish NER."
        },
        "insight": "Existing systems (RQ3) are evaluated and results Polyglot (Al-Rfou et al., 2013) overall performs better than DKIE (Derczynski et al., 2014).2 The best system is our cross-lingual transfer NER from MEDIUM source data paired with SMALL amounts of gold data. [CONTINUE] Per-entity evaluation shows that the neural Bilstm tagger outperforms Polyglot except for Location, which is consistent across evaluation sets."
    },
    {
        "id": "358",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Inspec  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Inspec  [ITALIC] F1@5",
                "[BOLD] Krapivin  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Krapivin  [ITALIC] F1@5",
                "[BOLD] NUS  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] NUS  [ITALIC] F1@5",
                "[BOLD] SemEval  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] SemEval  [ITALIC] F1@5",
                "[BOLD] KP20k  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] KP20k  [ITALIC] F1@5"
            ],
            "rows": [
                [
                    "catSeq",
                    "0.262",
                    "0.225",
                    "0.354",
                    "0.269",
                    "0.397",
                    "0.323",
                    "0.283",
                    "0.242",
                    "0.367",
                    "0.291"
                ],
                [
                    "catSeqD",
                    "0.263",
                    "0.219",
                    "0.349",
                    "0.264",
                    "0.394",
                    "0.321",
                    "0.274",
                    "0.233",
                    "0.363",
                    "0.285"
                ],
                [
                    "catSeqCorr",
                    "0.269",
                    "0.227",
                    "0.349",
                    "0.265",
                    "0.390",
                    "0.319",
                    "0.290",
                    "0.246",
                    "0.365",
                    "0.289"
                ],
                [
                    "catSeqTG",
                    "0.270",
                    "0.229",
                    "0.366",
                    "0.282",
                    "0.393",
                    "0.325",
                    "0.290",
                    "0.246",
                    "0.366",
                    "0.292"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.300",
                    "0.250",
                    "0.362",
                    "0.287",
                    "0.426",
                    "0.364",
                    "0.327",
                    "0.285",
                    "0.383",
                    "0.310"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "0.292",
                    "0.242",
                    "0.360",
                    "0.282",
                    "0.419",
                    "0.353",
                    "0.316",
                    "0.272",
                    "0.379",
                    "0.305"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "0.291",
                    "0.240",
                    "[BOLD] 0.369",
                    "0.286",
                    "0.414",
                    "0.349",
                    "0.322",
                    "0.278",
                    "0.382",
                    "0.308"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "[BOLD] 0.301",
                    "[BOLD] 0.253",
                    "[BOLD] 0.369",
                    "[BOLD] 0.300",
                    "[BOLD] 0.433",
                    "[BOLD] 0.375",
                    "[BOLD] 0.329",
                    "[BOLD] 0.287",
                    "[BOLD] 0.386",
                    "[BOLD] 0.321"
                ]
            ],
            "title": "Table 2: Results of present keyphrase prediction on five datasets. Suffix \u201c-2RF1\u201d denotes that a model is trained by our reinforcement learning approach."
        },
        "insight": "The evaluation results of different models on predicting present keyphrases are shown in Table 2. We observe that our reinforcement learning algorithm catSeqTG-2 [ITALIC] RF1 consistently improves the keyphrase extraction ability of all baseline generative models by a large margin."
    },
    {
        "id": "359",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Present MAE",
                "[BOLD] Present Avg. #",
                "[BOLD] Absent MAE",
                "[BOLD] Absent Avg. #"
            ],
            "rows": [
                [
                    "oracle",
                    "0.000",
                    "2.837",
                    "0.000",
                    "2.432"
                ],
                [
                    "catSeq",
                    "2.271",
                    "3.781",
                    "1.943",
                    "0.659"
                ],
                [
                    "catSeqD",
                    "2.225",
                    "3.694",
                    "1.961",
                    "0.629"
                ],
                [
                    "catSeqCorr",
                    "2.292",
                    "3.790",
                    "1.914",
                    "0.703"
                ],
                [
                    "catSeqTG",
                    "2.276",
                    "3.780",
                    "1.956",
                    "0.638"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "2.118",
                    "3.733",
                    "1.494",
                    "1.574"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "[BOLD] 2.087",
                    "[BOLD] 3.666",
                    "1.541",
                    "1.455"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "2.107",
                    "3.696",
                    "1.557",
                    "1.409"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "2.204",
                    "3.865",
                    "[BOLD] 1.439",
                    "[BOLD] 1.749"
                ]
            ],
            "title": "Table 4: The abilities of predicting the correct number of keyphrases on the KP20k dataset. MAE denotes the mean absolute error (the lower the better), Avg. # denotes the average number of generated keyphrases per document."
        },
        "insight": "We also report the average number of generated keyphrases per document, denoted as [CONTINUE] \"Avg. #\". The results are shown in Table 4, where oracle is a model that always generates the ground-truth keyphrases. The resultant MAEs demonstrate that our deep reinforced models notably outperform the baselines on predicting the number of absent keyphrases and slightly outperform the baselines on predicting the number of present keyphrases. Moreover, our deep reinforced models generate significantly more absent keyphrases than the baselines [CONTINUE] Besides, the baseline models and our reinforced models generate similar numbers of present keyphrases, while our reinforced models achieve notably higher F -measures, implying that our methods generate present keyphrases more accurately than the baselines."
    },
    {
        "id": "360",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Present  [ITALIC] F1@5",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@5"
            ],
            "rows": [
                [
                    "catSeq",
                    "0.367",
                    "0.291",
                    "0.032",
                    "0.015"
                ],
                [
                    "catSeq- [ITALIC] RF1",
                    "0.380",
                    "0.336",
                    "0.006",
                    "0.003"
                ],
                [
                    "catSeq-2 [ITALIC] F1",
                    "0.378",
                    "0.278",
                    "0.042",
                    "0.020"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.383",
                    "0.310",
                    "0.047",
                    "0.024"
                ]
            ],
            "title": "Table 5: Ablation study on the KP20k dataset. Suffix \u201c-2RF1\u201d denotes our full RL approach. Suffix \u201c-2F1\u201d denotes that we replace our adaptive RF1 reward function in the full approach by an F1 reward function. Suffix \u201c-RF1\u201d denotes that we replace the two separate RF1 reward signals in our full approach with only one RF1 reward signal for all the generated keyphrases."
        },
        "insight": "We conduct an ablation study to further analyze our reinforcement learning algorithm. The results are reported in Table 5. [CONTINUE] As seen in Table 5, although the performance of catSeq-RF1 is competitive to catSeq-2RF1 on predicting present keyphrases, it yields an extremely poor performance on absent keyphrase prediction. [CONTINUE] By comparing the last two rows in Table 5, we observe that our RF1 reward function slightly outperforms the F1 reward function."
    },
    {
        "id": "361",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M"
            ],
            "rows": [
                [
                    "[BOLD] Model",
                    "old",
                    "new",
                    "old",
                    "new"
                ],
                [
                    "catSeq",
                    "0.367",
                    "0.376",
                    "0.032",
                    "0.034"
                ],
                [
                    "catSeqD",
                    "0.363",
                    "0.372",
                    "0.031",
                    "0.033"
                ],
                [
                    "catSeqCorr",
                    "0.365",
                    "0.375",
                    "0.032",
                    "0.034"
                ],
                [
                    "catSeqTG",
                    "0.366",
                    "0.374",
                    "0.032",
                    "0.033"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.383",
                    "0.396",
                    "0.047",
                    "0.054"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "0.379",
                    "0.390",
                    "0.046",
                    "0.052"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "0.382",
                    "0.393",
                    "0.045",
                    "0.051"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "0.386",
                    "0.398",
                    "0.050",
                    "0.056"
                ]
            ],
            "title": "Table 6: Keyphrase prediction results on the KP20k dataset with our new evaluation method."
        },
        "insight": "Table 6 shows that for all generative models, the evaluation scores computed by our method are higher than those computed by prior method."
    },
    {
        "id": "362",
        "table": {
            "header": [
                "Model",
                "BLEU",
                "distinct-1",
                "distinct-2",
                "Avg.length",
                "Stopword",
                "adv succ"
            ],
            "rows": [
                [
                    "Human",
                    "-",
                    "16.8%",
                    "58.1%",
                    "14.2",
                    "69.8%",
                    "[EMPTY]"
                ],
                [
                    "AR",
                    "1.64",
                    "3.7%",
                    "9.5%",
                    "6.4",
                    "82.3%",
                    "2.7%"
                ],
                [
                    "AR+MMI",
                    "2.10",
                    "10.6%",
                    "20.5%",
                    "7.2",
                    "76.4%",
                    "6.3%"
                ],
                [
                    "AR+MMI+diverse",
                    "2.16",
                    "16.0%",
                    "27.3%",
                    "7.5",
                    "72.1%",
                    "6.4%"
                ],
                [
                    "AR+MMI+RL",
                    "2.34",
                    "13.7%",
                    "25.2%",
                    "7.3",
                    "73.0%",
                    "8.0%"
                ],
                [
                    "NonAR",
                    "1.54",
                    "8.9%",
                    "14.6%",
                    "7.1",
                    "77.9%",
                    "2.4%"
                ],
                [
                    "NonAR+MMI",
                    "2.68",
                    "15.9%",
                    "27.0%",
                    "7.4",
                    "71.9%",
                    "9.2%"
                ]
            ],
            "title": "Table 1: Automatic Metrics Evaluation for Different Models."
        },
        "insight": "Results are shown in Table 1. [CONTINUE] When comparing AR with AR+MMI, AR+MMI significantly outperforms AR across all metrics, [CONTINUE] lookahead strategy to estimate For the variants of AR+MMI, AR+MMI+diverse generates a more diverse N-best list for reranking, and thus outperforms AR+MMI; AR+MMI+RL uses future and thus outperforms backward probability, AR+MMI as well. [CONTINUE] It's hard to tell which model performs better, AR or non-AR: AR performs better than non-AR for BLEU and adversarial success, but worse for the other metrics. This means comparing with AR model, non-AR model tends to generate more diverse responses, but might be less coherent. [CONTINUE] When comparing non-AR with AR+MMI+diverse, non-AR has relatively lower distinct score, but significantly higher scores BLEU and adversarial success."
    },
    {
        "id": "363",
        "table": {
            "header": [
                "Model",
                "disagr (%)",
                "un(%)",
                "agr(%)"
            ],
            "rows": [
                [
                    "Coherence",
                    "Coherence",
                    "Coherence",
                    "Coherence"
                ],
                [
                    "Human",
                    "17.4",
                    "20.8",
                    "61.8"
                ],
                [
                    "AR",
                    "28.6",
                    "29.5",
                    "41.9"
                ],
                [
                    "AR+MMI",
                    "25.3",
                    "27.9",
                    "46.8"
                ],
                [
                    "AR+MMI+diverse",
                    "24.8",
                    "27.8",
                    "47.4"
                ],
                [
                    "AR+MMI+RL",
                    "24.1",
                    "26.5",
                    "49.4"
                ],
                [
                    "nonAR",
                    "29.9",
                    "28.7",
                    "41.4"
                ],
                [
                    "nonAR+MMI",
                    "23.1",
                    "24.0",
                    "52.9"
                ],
                [
                    "Content Richness",
                    "Content Richness",
                    "Content Richness",
                    "Content Richness"
                ],
                [
                    "Human",
                    "14.0",
                    "16.6",
                    "69.4"
                ],
                [
                    "AR",
                    "38.2",
                    "30.4",
                    "31.4"
                ],
                [
                    "AR+MMI",
                    "30.6",
                    "26.2",
                    "43.2"
                ],
                [
                    "AR+MMI+diverse",
                    "23.9",
                    "21.3",
                    "54.8"
                ],
                [
                    "AR+MMI+RL",
                    "26.4",
                    "24.9",
                    "48.7"
                ],
                [
                    "NonAR",
                    "31.4",
                    "25.0",
                    "44.6"
                ],
                [
                    "NonAR+MMI",
                    "24.2",
                    "20.5",
                    "55.3"
                ]
            ],
            "title": "Table 3: Human judgments for Coherence and Content Richeness of the different models."
        },
        "insight": "For dialogue coherence trend is 3. that NonAR+MMI than AR+MMI, followed by AR and Non-AR. AR is slightly better than Non-AR. For Content Richness, the is significantly better proposed NonAR+MMI than AR+MMI, and the gap is greater than dialogue coherence. [CONTINUE] The output from the AR+MMI model is thus by far less diverse than nonAR+MMI, which obtains the MMI score for each generated token."
    },
    {
        "id": "364",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] WMT14 En\u2192De",
                "[BOLD] WMT14 De\u2192En",
                "[BOLD] WMT16 Ro\u2192En"
            ],
            "rows": [
                [
                    "NAT (Gu et al.,  2018 )",
                    "17.69",
                    "20.62",
                    "29.79"
                ],
                [
                    "iNAT (Lee et al.,  2018 )",
                    "21.54",
                    "25.43",
                    "29.32"
                ],
                [
                    "FlowSeq-large (raw data) (Ma et al.,  2019 )",
                    "20.85",
                    "25.40",
                    "29.86"
                ],
                [
                    "NAT (our implementation)",
                    "22.32",
                    "24.83",
                    "29.93"
                ],
                [
                    "NAT +MMI",
                    "23.80",
                    "26.05",
                    "30.50"
                ],
                [
                    "[EMPTY]",
                    "(+1.48)",
                    "(+1.22)",
                    "(+0.57)"
                ]
            ],
            "title": "Table 4: The performances of NonAR+MMI methods on WMT14 En\u2194De and WMT16 Ro\u2192En. Results from Gu et al. (2018); Lee et al. (2018); Ma et al. (2019) are copied from original papers for reference purposes."
        },
        "insight": "Results are shown in Table 4. As can be seen, the incorporation of MMI model significantly improves MT performances."
    },
    {
        "id": "365",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] W param",
                "[BOLD] Cos-d",
                "[BOLD] Q1",
                "[BOLD] Q2",
                "[BOLD] Q3",
                "\u2264 [BOLD] 5"
            ],
            "rows": [
                [
                    "TransWeight-feat",
                    "[ITALIC] n+ [ITALIC] n",
                    "0.344",
                    "2",
                    "5",
                    "28",
                    "50.82%"
                ],
                [
                    "TransWeight-trans",
                    "[ITALIC] t+ [ITALIC] n",
                    "0.338",
                    "2",
                    "5",
                    "24",
                    "52.90%"
                ],
                [
                    "TransWeight-mat",
                    "[ITALIC] tn+ [ITALIC] n",
                    "0.338",
                    "2",
                    "5",
                    "25",
                    "53.24%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[ITALIC] tn2+ [ITALIC] n",
                    "[BOLD] 0.310",
                    "[BOLD] 1",
                    "[BOLD] 3",
                    "[BOLD] 11",
                    "[BOLD] 65.21%"
                ]
            ],
            "title": "Table 3: Different weighting variations evaluated on the compounds dataset (32,246 nominal compounds). All variations use t=100 transformations, word representations with n=200 dimensions and the dropout rate that was observed to work best on the dev dataset (see Appendix\u00a0B for details). Results on the 6442 compounds in the test set of the German compounds dataset."
        },
        "insight": "Table 3 compares the performance of the four weighting variants introduced in Section 3.2. TransWeight-feat, which sums the transformed representations and then weights each component of the summed representation, has the weakest performance, with only 50.82% of the test compounds receiving a rank that is lower than 5. A better performance \u2013 52.90% \u2013 is obtained by applying the same weighting for each column of the transformations matrix H. The results of TransWeight-trans are interesting in two respects: first, it outperforms the feature variation, TransWeight-feat, despite training a smaller number of parameters (300 vs. 400 in our setup). Second, it performs on par with the TransWeight-mat variation, although the latter has a larger number of parameters (20,200 in our setup). This suggests that an effective combination method needs to take into account full transformations, i.e. entire rows of H and combine them in a systematic way. TransWeight builds on this insight by making each element of the final composed representation p dependent on each component of the transformed representation H. The result is a noteworthy increase in the quality of the predictions, with \u223c12% more of the test representations having a rank \u22645. Although this weighting does use significantly more parameters than the previous weightings (4,000,200 parameters), the number of parameters is relative to the number of transformations t and does not grow with the size of the vocabulary. As the results in the next subsection show, a relatively small number of transformations is sufficient even for larger training vocabularies."
    },
    {
        "id": "366",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Nominal Compounds  [BOLD] Cos-d",
                "[BOLD] Nominal Compounds  [BOLD] Q1",
                "[BOLD] Nominal Compounds  [BOLD] Q2",
                "[BOLD] Nominal Compounds  [BOLD] Q3",
                "[BOLD] Nominal Compounds \u2264 [BOLD] 5",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Cos-d",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q1",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q2",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q3",
                "[BOLD] Adjective-Noun Phrases \u2264 [BOLD] 5",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Cos-d",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q1",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q2",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q3",
                "[BOLD] Adverb-Adjective Phrases \u2264 [BOLD] 5"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English"
                ],
                [
                    "Addition",
                    "0.408",
                    "2",
                    "7",
                    "38",
                    "46.14%",
                    "0.431",
                    "2",
                    "7",
                    "32",
                    "44.25%",
                    "0.447",
                    "2",
                    "5",
                    "15",
                    "53.01%"
                ],
                [
                    "SAddition",
                    "0.408",
                    "2",
                    "7",
                    "38",
                    "46.14%",
                    "0.421",
                    "2",
                    "5",
                    "26",
                    "50.95%",
                    "0.420",
                    "1",
                    "3",
                    "8",
                    "67.76%"
                ],
                [
                    "VAddition",
                    "0.403",
                    "2",
                    "6",
                    "33",
                    "47.95%",
                    "0.415",
                    "2",
                    "5",
                    "22",
                    "53.30%",
                    "0.410",
                    "1",
                    "2",
                    "6",
                    "71.94%"
                ],
                [
                    "Matrix",
                    "0.354",
                    "1",
                    "2",
                    "9",
                    "67.37%",
                    "0.365",
                    "1",
                    "2",
                    "6",
                    "74.38%",
                    "0.343",
                    "1",
                    "1",
                    "2",
                    "91.17%"
                ],
                [
                    "WMask+",
                    "0.344",
                    "1",
                    "2",
                    "7",
                    "71.53%",
                    "0.342",
                    "1",
                    "1",
                    "3",
                    "82.67%",
                    "0.335",
                    "1",
                    "1",
                    "2",
                    "93.27%"
                ],
                [
                    "BiLinear",
                    "0.335",
                    "1",
                    "2",
                    "6",
                    "73.63%",
                    "0.332",
                    "1",
                    "1",
                    "3",
                    "85.32%",
                    "0.331",
                    "1",
                    "1",
                    "1",
                    "93.59%"
                ],
                [
                    "FullLex+",
                    "0.338",
                    "1",
                    "2",
                    "7",
                    "72.82%",
                    "0.309",
                    "1",
                    "1",
                    "2",
                    "90.74%",
                    "0.327",
                    "1",
                    "1",
                    "1",
                    "94.28%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.323",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 4.5",
                    "[BOLD] 77.31%",
                    "[BOLD] 0.307",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 91.39%",
                    "[BOLD] 0.311",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 95.78%"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German"
                ],
                [
                    "Addition",
                    "0.439",
                    "9",
                    "48",
                    "363",
                    "17.49%",
                    "0.428",
                    "4",
                    "13",
                    "71",
                    "32.95%",
                    "0.500",
                    "4",
                    "19",
                    "215.5",
                    "29.87%"
                ],
                [
                    "SAddition",
                    "0.438",
                    "9",
                    "46",
                    "347",
                    "18.02%",
                    "0.414",
                    "2",
                    "8",
                    "53",
                    "42.80%",
                    "0.473",
                    "2",
                    "7",
                    "99.5",
                    "45.44%"
                ],
                [
                    "VAddition",
                    "0.430",
                    "8",
                    "39",
                    "273",
                    "19.02%",
                    "0.408",
                    "2",
                    "7",
                    "43",
                    "45.14%",
                    "0.461",
                    "2",
                    "5",
                    "52",
                    "51.12%"
                ],
                [
                    "Matrix",
                    "0.363",
                    "3",
                    "8",
                    "45",
                    "41.88%",
                    "0.355",
                    "1",
                    "2",
                    "8",
                    "68.67%",
                    "0.398",
                    "1",
                    "1",
                    "5",
                    "76.41%"
                ],
                [
                    "WMask+",
                    "0.340",
                    "2",
                    "5",
                    "25",
                    "52.05%",
                    "0.332",
                    "1",
                    "2",
                    "5",
                    "77.68%",
                    "0.387",
                    "1",
                    "1",
                    "3",
                    "80.94%"
                ],
                [
                    "BiLinear",
                    "0.339",
                    "2",
                    "5",
                    "26",
                    "53.46%",
                    "0.322",
                    "1",
                    "1",
                    "3",
                    "81.84%",
                    "0.383",
                    "1",
                    "1",
                    "3",
                    "83.02%"
                ],
                [
                    "FullLex+",
                    "0.329",
                    "2",
                    "4",
                    "20",
                    "56.83%",
                    "0.306",
                    "1",
                    "1",
                    "2",
                    "86.29%",
                    "0.383",
                    "1",
                    "1",
                    "3",
                    "83.13%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.310",
                    "[BOLD] 1",
                    "[BOLD] 3",
                    "[BOLD] 11",
                    "[BOLD] 65.21%",
                    "[BOLD] 0.297",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 89.28%",
                    "[BOLD] 0.367",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 87.17%"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch"
                ],
                [
                    "Addition",
                    "0.477",
                    "5",
                    "27",
                    "223.5",
                    "27.74%",
                    "0.476",
                    "3",
                    "13",
                    "87",
                    "35.63%",
                    "0.532",
                    "3",
                    "9",
                    "75",
                    "38.04%"
                ],
                [
                    "SAddition",
                    "0.477",
                    "5",
                    "27",
                    "221",
                    "27.71%",
                    "0.462",
                    "2",
                    "7",
                    "65",
                    "44.95%",
                    "0.503",
                    "2",
                    "4",
                    "34",
                    "55.57%"
                ],
                [
                    "VAddition",
                    "0.470",
                    "4",
                    "22",
                    "177",
                    "29.09%",
                    "0.454",
                    "2",
                    "6",
                    "47",
                    "48.13%",
                    "0.486",
                    "1",
                    "3",
                    "14",
                    "63.18%"
                ],
                [
                    "Matrix",
                    "0.411",
                    "2",
                    "5",
                    "26",
                    "52.19%",
                    "0.394",
                    "1",
                    "2",
                    "6",
                    "74.92%",
                    "0.445",
                    "1",
                    "1",
                    "4",
                    "78.39%"
                ],
                [
                    "WMask+",
                    "0.378",
                    "1",
                    "3",
                    "15",
                    "60.14%",
                    "0.378",
                    "1",
                    "1",
                    "4",
                    "80.78%",
                    "0.429",
                    "1",
                    "1",
                    "2",
                    "83.02%"
                ],
                [
                    "BiLinear",
                    "0.375",
                    "1",
                    "3",
                    "19",
                    "59.23%",
                    "0.375",
                    "1",
                    "1",
                    "3",
                    "81.50%",
                    "0.426",
                    "1",
                    "1",
                    "2",
                    "83.57%"
                ],
                [
                    "FullLex+",
                    "0.388",
                    "1",
                    "3",
                    "14",
                    "60.84%",
                    "0.362",
                    "1",
                    "1",
                    "2",
                    "85.24%",
                    "0.433",
                    "1",
                    "1",
                    "3",
                    "82.36%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.376",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 11",
                    "[BOLD] 66.61%",
                    "[BOLD] 0.349",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 88.55%",
                    "[BOLD] 0.423",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 84.01%"
                ]
            ],
            "title": "Table 4: Results for English, German and Dutch on the composition of nominal compounds, adjective-noun phrases and adverb-adjective phrases."
        },
        "insight": "The results using the corrected rank eval [CONTINUE] TransWeight, the composition model proposed in this paper, delivers consistent results, being the best performing model across all languages and phrase types. The difference in performance to the runner-up model, FullLex+, translates into more of the test phrases being close to the original repachieving a rank \u2264 5. This resentations, i.e. difference ranges from 8% of the test phrases in the German compounds dataset to less than 1% for English adjective-noun phrases. However, it is important to note the substantial difference in the number of parameters used by the two models: all TransWeight models use 100 transformations and have, therefore, a constant number of 12,020,200 parameters. In contrast the number of parameters used by FullLex+ increases with the size of the training vocabulary, reaching 739,320,200 parameters in the case of the English adjective-noun dataset. The most difficult task for all the composition models in any of the three languages is compound composition. We believe this difficulty can be mainly attributed to the complexity introduced by the position. For example in adjective-noun composition, the adjective always takes the first position, and the noun the second. However, in compounds the same noun can occur in both positions throughout different training examples. Consider for example the compounds boat house and house boat. In boat house \u2013 a house to store boats \u2013 the meaning of house is shifted towards shelter for an inanimate object, whereas house boat selects from house aspects related to human beings and their daily lives happening on the boat. These positionrelated differences can make it more challenging to create composed representations. that makes adverbadjective easier is the high dataset frequency of some of the adverbs/adjectives. For example, in the English and adjective-noun datasets Another aspect the [CONTINUE] adjective-noun dataset a small subset of 52 adjectives like new, good, small, public, etc. are extremely frequent, occurring more than 500 times in the training portion of the adjective-noun sample dataset. Because the adjective is always the first element of the composition, the phrases that include these frequent adjectives amount to around 24.8% of the test dataset. Frequent constituents are more likely to be modeled correctly by composition \u2013 thus leading to better results. [CONTINUE] The additive models (Addition, SAddition, VAddition) are the least competitive models in our evaluation, on all datasets. The results strongly argue for the point that additive models are too limited for composition. An adequate composed representation cannot be obtained simply as an (weighted) average of the input components. [CONTINUE] The Matrix model clearly outperforms the additive models. However, its results are modest in comparison to models like WMask+, BiLinear, FullLex+ and TransWeight. This is to be expected: having a single affine transformation limits the model's capacity to adapt to all the possi [CONTINUE] ble input vectors u and v. Because of its small number of parameters, the Matrix model can only capture the general trends in the data. [CONTINUE] More interaction between u and v is promoted by the BiLinear model through the d bilinear forms in the tensor [CONTINUE] \u2208 Rn\u00d7d\u00d7n. This capacity to absorb more information from the training data translates into better results \u2014 the BiLinear model outperforms the Matrix model on all datasets. [CONTINUE] In evaluating FullLex we tried to mitigate its treatment of unknown words. Instead of using unknown matrices to model composition of phrases not in the training data, we take a nearest neighbor approach to composition. Take for example the phrase sky-blue dress, where sky-blue does not occur in train. Our implementation, FullLex+, looks for the nearest neighbor of sky-blue that appears in train, blue and uses the matrix associated with it for building the composed representation. The same approach is also used for the WMask model, which is referred to as WMask+. [CONTINUE] On this dataset WMask+ fares only slightly worse than FullLex+ (0.70%), an indication that FullLex+ suffers from data sparsity in such scenarios and cannot produce good results without an adequate amount of training data. By contrast, the gap between the two models increases considerably on datasets with more phrases per word \u2014 e.g. FullLex+ outperforms WMask+ with 8.07% on the English adjective-noun phrase dataset, which has 11.6 phrases per word."
    },
    {
        "id": "367",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "52.7",
                    "43.1",
                    "47.4"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "75.8",
                    "60.7",
                    "67.3"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "66.5",
                    "70.6",
                    "68.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "59.8",
                    "61.5",
                    "60.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "67.6",
                    "65.1",
                    "66.3"
                ],
                [
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "74.0",
                    "69.4",
                    "71.6"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "74.8",
                    "71.7",
                    "73.1"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "71.5",
                    "73.4",
                    "72.4"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "72.8",
                    "69.4",
                    "71.1"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "71.6",
                    "76.4",
                    "[BOLD] 73.9"
                ],
                [
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "69.6",
                    "72.3",
                    "70.9"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "69.4",
                    "74.9",
                    "72.0"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "71.0",
                    "69.7",
                    "71.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "72.2",
                    "69.5",
                    "70.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "71.0",
                    "74.3",
                    "72.6"
                ],
                [
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "69.3",
                    "71.4",
                    "70.4"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "72.2",
                    "71.9",
                    "72.0"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "69.7",
                    "73.9",
                    "71.7"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "70.1",
                    "71.1",
                    "70.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "72.7",
                    "72.9",
                    "72.8"
                ]
            ],
            "title": "Table 1: Results on DDI 2013"
        },
        "insight": "1. Comparing ENT-SENT, ENT-DYM and ENT-ONLY, we see that the pooling methods over the whole sentence (i.e., ENT-SENT and ENT-DYM) are significantly better than ENT-ONLY that only focuses on the two entity mentions of interest in the DDI-2013 dataset. This is true across different deep learning models in this work. Comparing ENT-SENT and ENT-DYM, their performance are comparable in DDI-2013 (except for CNN where ENT-DYM is better). Comparing the syntax-based pooling methods and the non-syntax pooling methods, the pooling based on dependency paths (i.e., ENT-DEP0) is worse than the non-syntax pooling methods (i.e., ENT-SENT and ENT-DYM) and perform comparably with ENT-ONLY in the DDI-2013 dataset over all the models (except for the CNN model where ENT-ONLY is much worse)."
    },
    {
        "id": "368",
        "table": {
            "header": [
                "[EMPTY]",
                "TrecQA MRR",
                "TrecQA MAP",
                "WikiQA MRR",
                "WikiQA MAP",
                "YahooQA MRR",
                "YahooQA MAP",
                "SemEvalcQA-16 MRR",
                "SemEvalcQA-16 MAP",
                "SemEvalcQA-17 MRR",
                "SemEvalcQA-17 MAP"
            ],
            "rows": [
                [
                    "epoch=3",
                    "0.927",
                    "0.877",
                    "0.770",
                    "0.753",
                    "0.942",
                    "0.942",
                    "0.872",
                    "0.810",
                    "0.951",
                    "0.909"
                ],
                [
                    "epoch=5",
                    "0.944",
                    "0.883",
                    "0.784",
                    "0.769",
                    "0.942",
                    "0.942",
                    "0.890",
                    "0.816",
                    "0.953",
                    "0.908"
                ],
                [
                    "SOTA",
                    "0.865",
                    "0.904",
                    "0.758",
                    "0.746",
                    "-",
                    "0.801",
                    "0.872",
                    "0.801",
                    "0.926",
                    "0.887"
                ]
            ],
            "title": "Table 2: Results of BERTbase in test set of five datasets with different epochs. The SOTA results are from\u00a0Madabushi et\u00a0al. (2018) (TrecQA), Sha et\u00a0al. (2018) (WikiQA, SemEvalcQA-16), Tay et\u00a0al. (2018b) (YahooQA), Nakov et\u00a0al. (2017) (SemEvalcQA-17)."
        },
        "insight": "We show the main result in Table 2 and 3. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on all datasets"
    },
    {
        "id": "369",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "rows": [
                [
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "54.2",
                    "65.7",
                    "59.1"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "55.0",
                    "62.5",
                    "59.1"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "54.6",
                    "53.3",
                    "53.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.9",
                    "65.8",
                    "60.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "55.7",
                    "67.7",
                    "61.1"
                ],
                [
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "58.9",
                    "59.6",
                    "59.2"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "60.7",
                    "59.2",
                    "59.9"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "50.2",
                    "66.0",
                    "56.9"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "51.6",
                    "78.0",
                    "61.9"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.7",
                    "72.6",
                    "62.4"
                ],
                [
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "56.4",
                    "66.2",
                    "60.8"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "53.6",
                    "69.2",
                    "60.5"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "47.1",
                    "78.0",
                    "58.7"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.9",
                    "71.4",
                    "[BOLD] 62.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.1",
                    "74.7",
                    "62.4"
                ],
                [
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "62.7",
                    "56.1",
                    "58.9"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "58.4",
                    "58.7",
                    "58.5"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "56.8",
                    "58.4",
                    "56.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.6",
                    "67.4",
                    "60.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.4",
                    "71.1",
                    "61.5"
                ]
            ],
            "title": "Table 2: Results on BioNLP BB3"
        },
        "insight": "However, this comparison is reversed for the BB3 dataset where ENT-ONLY is in general better or comparable to ENT-SENT and ENT-DYM over different deep learning models. in the BB3 dataset, ENT-SENT singificantly outperforms ENT-DYM over all the models. When we switch to the BB3 dataset, it turns out thatENT-DEP0 is significantly better than all the non-syntax pooling methods (i.e., ENT-ONLY, ENT-SENT and ENT-DYM) for all the comparing models."
    },
    {
        "id": "370",
        "table": {
            "header": [
                "Total triples (millions)",
                "OPIEC 341.0",
                "OPIEC",
                "OPIEC-Clean 104.0",
                "OPIEC-Clean",
                "OPIEC-Linked 5.8",
                "OPIEC-Linked"
            ],
            "rows": [
                [
                    "Triples with semantic annotations",
                    "166.3",
                    "(49%)",
                    "51.46",
                    "(49%)",
                    "3.37",
                    "(58%)"
                ],
                [
                    "negative polarity",
                    "5.3",
                    "(2%)",
                    "1.33",
                    "(1%)",
                    "0.01",
                    "(0%)"
                ],
                [
                    "possibility modality",
                    "13.9",
                    "(4%)",
                    "3.27",
                    "(3%)",
                    "0.04",
                    "(1%)"
                ],
                [
                    "quantities",
                    "59.4",
                    "(17%)",
                    "15.91",
                    "(15%)",
                    "0.45",
                    "(8%)"
                ],
                [
                    "attribution",
                    "6.4",
                    "(2%)",
                    "1.44",
                    "(1%)",
                    "0.01",
                    "(0%)"
                ],
                [
                    "time",
                    "65.3",
                    "(19%)",
                    "19.66",
                    "(19%)",
                    "0.58",
                    "(1%)"
                ],
                [
                    "space",
                    "61.5",
                    "(18%)",
                    "22.11",
                    "(21%)",
                    "2.64",
                    "(45%)"
                ],
                [
                    "space OR time",
                    "111.3",
                    "(33%)",
                    "37.22",
                    "(36%)",
                    "3.01",
                    "(52%)"
                ],
                [
                    "space AND time",
                    "15.4",
                    "(5%)",
                    "4.54",
                    "(4%)",
                    "0.20",
                    "(4%)"
                ],
                [
                    "Triple length in tokens ( [ITALIC] \u03bc\u00b1 [ITALIC] \u03c3)",
                    "7.66\u00b14.25",
                    "7.66\u00b14.25",
                    "6.06\u00b12.82",
                    "6.06\u00b12.82",
                    "6.45\u00b12.65",
                    "6.45\u00b12.65"
                ],
                [
                    "subject ( [ITALIC] \u03bc\u00b1 [ITALIC] \u03c3)",
                    "2.12\u00b12.12",
                    "2.12\u00b12.12",
                    "1.48\u00b10.79",
                    "1.48\u00b10.79",
                    "1.92\u00b10.94",
                    "1.92\u00b10.94"
                ],
                [
                    "relation ( [ITALIC] \u03bc\u00b1 [ITALIC] \u03c3)",
                    "3.01\u00b12.47",
                    "3.01\u00b12.47",
                    "3.10\u00b12.56",
                    "3.10\u00b12.56",
                    "2.77\u00b12.14",
                    "2.77\u00b12.14"
                ],
                [
                    "object ( [ITALIC] \u03bc\u00b1 [ITALIC] \u03c3)",
                    "2.52\u00b12.69",
                    "2.52\u00b12.69",
                    "1.48\u00b10.79",
                    "1.48\u00b10.79",
                    "1.76\u00b10.94",
                    "1.76\u00b10.94"
                ],
                [
                    "Confidence score ( [ITALIC] \u03bc\u00b1 [ITALIC] \u03c3)",
                    "0.53\u00b10.23",
                    "0.53\u00b10.23",
                    "0.59\u00b10.23",
                    "0.59\u00b10.23",
                    "0.61\u00b10.26",
                    "0.61\u00b10.26"
                ]
            ],
            "title": "Table 2: Statistics for different OPIEC corpora. All frequencies are in millions. We count triples with annotations (not annotations directly). Percentages refer to the respective subcorpus."
        },
        "insight": "Basic statistics such as corpus sizes, frequency of various semantic annotations, and information about the length of the extracted triples of OPIEC and its subcorpora are shown in Tab. 2. [CONTINUE] Roughly 30% of the triples (104M) in OPIEC are clean according to the above constraints. Table 2 shows that clean triples are generally shorter on average and tend to have a higher confidence score than the full set of triples in OPIEC. [CONTINUE] About 49% of all triples in OPIEC contain some sort of semantic annotation (cf. Tab. 2); in OPIEC-Linked, the fraction increases to 58%. Most of the semantic annotations referred to quantities, space or time; these annotations provide important context for the extractions. There is a significantly smaller amount of negative polarity and possibility modality annotations. One reason for the lack of such annotations may be in the nature of the Wikipedia articles, which aim to contain encyclopedic, factual statements and are thus more rarely negated or hedged."
    },
    {
        "id": "371",
        "table": {
            "header": [
                "location  [ITALIC] \u201cbe in\u201d",
                "location (43,842)",
                "associatedMusicalArtist  [ITALIC] \u201cbe\u201d",
                "associatedMusicalArtist (6,273)",
                "spouse  [ITALIC] \u201cbe wife of\u201d",
                "spouse (1,965)"
            ],
            "rows": [
                [
                    "[ITALIC] \u201chave\u201d",
                    "(3,175)",
                    "[ITALIC] \u201chave\u201d",
                    "(3,600)",
                    "[ITALIC] \u201cbe\u201d",
                    "(1,308)"
                ],
                [
                    "[ITALIC] \u201cbe\u201d",
                    "(1,901)",
                    "[ITALIC] \u201cbe member of\u201d",
                    "(740)",
                    "[ITALIC] \u201cmarry\u201d",
                    "(702)"
                ],
                [
                    "[ITALIC] \u201cbe at\u201d",
                    "(1,109)",
                    "[ITALIC] \u201cbe guitarist of\u201d",
                    "(703)",
                    "[ITALIC] \u201cbe widow of\u201d",
                    "(479)"
                ],
                [
                    "[ITALIC] \u201cbe of\u201d",
                    "(706)",
                    "[ITALIC] \u201cbe drummer of\u201d",
                    "(458)",
                    "[ITALIC] \u201chave\u201d",
                    "(298)"
                ],
                [
                    "[ITALIC] \u201cbe historic home",
                    "(491)",
                    "[ITALIC] \u201cbe feature\u201d",
                    "(416)",
                    "[ITALIC] \u201cbe husband of\u201d",
                    "(284)"
                ],
                [
                    "[ITALIC] located at\u201d",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 4: The most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse in OPEIC-Linked"
        },
        "insight": "Tab. 4 shows the most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse. The frequencies correspond to the number of OIE triples that (1) have the specified open relation (e.g., \"be wife of \") and (2) have a KB hit with the specified KB relation (e.g., spouse). There is clearly no 1:1 correspondence between open relations and KB relations. On the one hand, open relations can be highly ambiguous (e.g., \"be\" has hits to location and associatedMusicalArtits). On the other hand, open relations can also be more specific than KB relations (e.g., \"be guitarist of \" is more specific than associatedMusicalArtist) or semantically different (e.g., \"be widow of \" and spouse) than the KB relations they align to."
    },
    {
        "id": "372",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FrameNet 1.5  [BOLD] All",
                "[BOLD] FrameNet 1.5  [BOLD] Ambiguous",
                "[BOLD] FrameNet 1.7  [BOLD] All",
                "[BOLD] FrameNet 1.7  [BOLD] Ambiguous"
            ],
            "rows": [
                [
                    "[ITALIC] Das\u2019s Test Set ",
                    "[ITALIC] Das\u2019s Test Set ",
                    "[ITALIC] Das\u2019s Test Set ",
                    "[ITALIC] Das\u2019s Test Set ",
                    "[ITALIC] Das\u2019s Test Set "
                ],
                [
                    "SEMAFOR ",
                    "83.60",
                    "69.19",
                    "-",
                    "-"
                ],
                [
                    "Hermann et al. ",
                    "88.73",
                    "73.67",
                    "-",
                    "-"
                ],
                [
                    "Yang and Mitchell ",
                    "88.20",
                    "75.70",
                    "-",
                    "-"
                ],
                [
                    "Hartmann et al. ",
                    "87.63",
                    "73.80",
                    "-",
                    "-"
                ],
                [
                    "Botschen et al. ",
                    "88.82",
                    "75.28",
                    "-",
                    "-"
                ],
                [
                    "Peng et al. ",
                    "90.00",
                    "78.00",
                    "89.10",
                    "77.50"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by LUs",
                    "[BOLD] 92.22",
                    "[BOLD] 82.90",
                    "[BOLD] 91.44",
                    "[BOLD] 82.55"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by Targets",
                    "[BOLD] 91.39",
                    "[BOLD] 82.80",
                    "[BOLD] 90.15",
                    "[BOLD] 81.92"
                ],
                [
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set "
                ],
                [
                    "SEMAFOR (Reported by )",
                    "60.01",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Hartmann et al. ",
                    "62.51",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by LUs",
                    "[BOLD] 75.06",
                    "[BOLD] 69.07",
                    "[BOLD] -",
                    "[BOLD] -"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by Targets",
                    "[BOLD] 75.01",
                    "[BOLD] 69.07",
                    "[BOLD] -",
                    "[BOLD] -"
                ]
            ],
            "title": "Table 2: Results with frame filtering"
        },
        "insight": "Table 2 and 3 compare the accuracies of our models to the results reported by several representative studies in frame-semantic parsing. [CONTINUE] As can be seen from the tables, PAFIBERT outperformed other models on both the in-domain Das's test set and the out-of-domain YAGS test set, yielding new state-of-the-art results for frame identification. As expected, the results obtained with LU-based frame filtering were slightly better than those produced using target-based frame filtering, suggesting that a realistic setup is crucial in measuring model performance more precisely. [CONTINUE] On Das's test set, for example, PAFIBERT achieved absolute improvements of 4-5% and 9% with frame filtering (Table 2) and without frame filtering (Table 3) respectively. As reported by , frame identification models usually suffer a drastic drop in performance when tested on out-of-domain data. The same trend was observed in our experiments for the YAGS test set. Nevertheless, PAFIBERT still outperformed existing methods by a large margin on this out-of-domain test set. Besides, in the more challenging setup that involved no frame filtering, the results obtained by PAFIBERT were on par with the prior state-of-the-art accuracies achieved with frame filtering."
    },
    {
        "id": "373",
        "table": {
            "header": [
                "[EMPTY]",
                "TrecQA base",
                "TrecQA large",
                "WikiQA base",
                "WikiQA large",
                "YahooQA base",
                "YahooQA large",
                "SemEvalcQA-16 base",
                "SemEvalcQA-16 large",
                "SemEvalcQA-17 base",
                "SemEvalcQA-17 large"
            ],
            "rows": [
                [
                    "MRR",
                    "0.927",
                    "[BOLD] 0.961",
                    "0.770",
                    "[BOLD] 0.875",
                    "[BOLD] 0.942",
                    "0.938",
                    "0.872",
                    "[BOLD] 0.911",
                    "0.951",
                    "[BOLD] 0.958"
                ],
                [
                    "MAP",
                    "0.877",
                    "[BOLD] 0.904",
                    "0.753",
                    "[BOLD] 0.860",
                    "[BOLD] 0.942",
                    "0.938",
                    "0.810",
                    "[BOLD] 0.844",
                    "[BOLD] 0.909",
                    "0.907"
                ]
            ],
            "title": "Table 3: Results of BERTbase and BERTlarge in test set of five datasets. The number of training epochs is 3."
        },
        "insight": "We show the main result in Table 2 and 3. Despite training on a fraction of the data available, the proposed BERT-based models surpass the previous state-of-the-art models by a large margin on all datasets"
    },
    {
        "id": "374",
        "table": {
            "header": [
                "[width=13em]Hyperparam TuningDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "rows": [
                [
                    "Default",
                    "81.55",
                    "62.55",
                    "80.29",
                    "55.15",
                    "81.98"
                ],
                [
                    "Default",
                    "80.85 (1.31)",
                    "81.62 (1.35)",
                    "87.76 (1.03)",
                    "67.28 (1.83)",
                    "86.57 (0.58)"
                ],
                [
                    "Manual Search",
                    "-",
                    "[BOLD] 65.53",
                    "[BOLD] 81.74",
                    "[BOLD] 59.75",
                    "[BOLD] 83.17"
                ],
                [
                    "Manual Search",
                    "[EMPTY]",
                    "82.23 (0.32)\u2022",
                    "88.40 (0.48)\u2022",
                    "70.10 (0.85)",
                    "86.45 (0.58)\u2022"
                ],
                [
                    "Random Search",
                    "[BOLD] 82.2",
                    "62.29",
                    "79.04",
                    "55.0",
                    "80.77"
                ],
                [
                    "Random Search",
                    "81.10 (1.26)\u2022",
                    "75.43 (1.48)",
                    "83.54 (0.60)",
                    "60.66 (1.43)",
                    "82.73 (0.49)"
                ]
            ],
            "title": "Table 6: Hyperparameter tuning methods with original pre-processing and fixed CRCNN model. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Default with p<0.05 except those marked with a \u2022. Note that hyperparameter tuning can involve much higher performance variation depending on the distribution of the data. Therefore, even though there is no statistical significance in the manual search case for the held out fold in the ddi dataset, there was statistical significance for the dev fold which drove those set of hyperparameters. For both ddi and i2b2 datasets, manual search is better than random search with p<0.05."
        },
        "insight": "table 6, where we compare hyperparameter tuning methodologies. [CONTINUE] Perturbations on the hyperparameter search are listed in table 6 and compare performance with different hyperparameter values found using different tuning strategies. [CONTINUE] We tested how manual tuning, requiring less expert knowledge than Bayesian optimization, would compare to the random search strategy in table 6. For both i2b2 and ddi corpora, manual search outperformed random search."
    },
    {
        "id": "375",
        "table": {
            "header": [
                "[width=10em]PreprocessDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "rows": [
                [
                    "Original",
                    "[BOLD] 81.55",
                    "65.53",
                    "81.74",
                    "59.75",
                    "83.17"
                ],
                [
                    "Original",
                    "80.85 (1.31)",
                    "82.23 (0.32)",
                    "88.40 (0.48)",
                    "70.10 (0.85)",
                    "86.45 (0.58)"
                ],
                [
                    "Entity Blinding",
                    "72.73",
                    "[BOLD] 67.02",
                    "[BOLD] 82.37",
                    "[BOLD] 68.76",
                    "[BOLD] 84.37"
                ],
                [
                    "Entity Blinding",
                    "71.31 (1.14)",
                    "83.56 (2.05)\u2022",
                    "89.45 (1.05)\u2022",
                    "76.59 (1.07)",
                    "88.41 (0.37)"
                ],
                [
                    "Punct and Digit",
                    "81.23",
                    "63.41",
                    "80.49",
                    "58.85",
                    "81.96"
                ],
                [
                    "Punct and Digit",
                    "80.95 (1.21)\u2022",
                    "80.44 (1.77)",
                    "87.52 (0.98)",
                    "69.37 (1.43)\u2022",
                    "85.82 (0.43)"
                ],
                [
                    "Punct, Digit and Stop",
                    "72.92",
                    "55.87",
                    "76.57",
                    "56.19",
                    "80.47"
                ],
                [
                    "Punct, Digit and Stop",
                    "71.61 (1.25)",
                    "78.52 (1.99)",
                    "85.65 (1.21)",
                    "68.14 (2.05)\u2022",
                    "84.84 (0.77)"
                ],
                [
                    "NER Blinding",
                    "81.63",
                    "57.22",
                    "79.03",
                    "50.41",
                    "81.61"
                ],
                [
                    "NER Blinding",
                    "80.85 (1.07)\u2022",
                    "78.06 (1.45)",
                    "86.79 (0.65)",
                    "66.26 (2.44)",
                    "86.72 (0.57)\u2022"
                ]
            ],
            "title": "Table 4: Pre-processing techniques with CRCNN model. Row labels Original = simple tokenization and lower casing of words, Punct = punctuation removal, Digit = digit removal and Stop = stop word removal. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Original pre-processing (p<0.05) using a paired t-test except those marked with a \u2022"
        },
        "insight": "importance of pre-processing in performance improvements. Experiments in table 4 reveal that they can cause larger variations in performance than modeling. [CONTINUE] Punctuation and digits hold more importance for the ddi dataset, which is a biomedical dataset, compared to the other two datasets. [CONTINUE] We found that stop words seem to be important for relation extraction for all three datasets that we looked at, to a smaller degree for i2b2 compared to the other two datasets. [CONTINUE] Entity blinding causes almost 9% improvement in classification performance and 1% improvement in detection performance. [CONTINUE] While entity blinding hurts performance for semeval, possibly due to the coarse-grained nature of the replacement, NER blinding does not hurt performance. [CONTINUE] entity blinding seems to help test set performance for ddi in table 4, but shows no statistical significance."
    },
    {
        "id": "376",
        "table": {
            "header": [
                "[width=10em]ModelingDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "rows": [
                [
                    "CRCNN",
                    "81.55",
                    "65.53",
                    "81.74",
                    "59.75",
                    "83.17"
                ],
                [
                    "CRCNN",
                    "80.85 (1.31)",
                    "82.23 (0.32)",
                    "88.40 (0.48)",
                    "70.10 (0.85)",
                    "86.45 (0.58)"
                ],
                [
                    "Piecewise pool",
                    "81.59",
                    "63.01",
                    "80.62",
                    "60.85",
                    "83.69"
                ],
                [
                    "Piecewise pool",
                    "80.55 (0.99)\u2022",
                    "81.99 (0.38)\u2022",
                    "88.47 (0.48)\u2022",
                    "73.79 (0.97)",
                    "89.29 (0.61)"
                ],
                [
                    "BERT-tokens",
                    "85.67",
                    "[BOLD] 71.97",
                    "[BOLD] 86.53",
                    "63.11",
                    "[BOLD] 84.91"
                ],
                [
                    "BERT-tokens",
                    "85.63 (0.83)",
                    "85.35 (0.53)",
                    "90.70 (0.46)",
                    "72.06 (1.36)",
                    "87.57 (0.75)"
                ],
                [
                    "BERT-CLS",
                    "82.42",
                    "61.3",
                    "79.63",
                    "56.79",
                    "81.91"
                ],
                [
                    "BERT-CLS",
                    "80.83 (1.18)\u2022",
                    "82.71 (0.68)\u2022",
                    "88.35 (0.77)\u2022",
                    "67.37 (1.08)",
                    "85.43 (0.36)"
                ],
                [
                    "ELMo",
                    "[BOLD] 85.89",
                    "66.63",
                    "83.05",
                    "[BOLD] 63.18",
                    "84.54"
                ],
                [
                    "ELMo",
                    "84.79 (1.08)",
                    "84.53 (0.96)",
                    "90.11 (0.56)",
                    "72.53 (0.80)",
                    "87.81 (0.34)"
                ]
            ],
            "title": "Table 5: Modeling techniques with original pre-processing. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to CRCNN model (p<0.05) using a paired t-test except those marked with a \u2022. In terms of statistical significance, comparing contextualized embeddings with each other reveals that BERT-tokens is equivalent to ELMo for i2b2, but for semeval BERT-tokens is better than ELMo and for ddi BERT-tokens is better than ELMo only for detection."
        },
        "insight": "No statistical significance is seen even when the test set result worsens in performance for BERTCLS and Piecewise Pool in table 5 where it hurts test set performance on ddi but is not statistically significant when cross validation is performed. [CONTINUE] In table 5, we tested the generalizability of the commonly used piecewise pooling technique [CONTINUE] While piecewise pooling helps i2b2 by 1%, it hurts test set performance on ddi and doesn't affect performance on semeval. [CONTINUE] We found ELMo and BER tokens to boost performance significantly for all datasets, but that BERT-CLS hurt performance for the medical datasets. While BERT-CLS boosted test set performance for semeval, this was not found to be a statistically significant difference for cross validation."
    },
    {
        "id": "377",
        "table": {
            "header": [
                "activation",
                "de\u2192en",
                "ja\u2192en",
                "ro\u2192en",
                "en\u2192de"
            ],
            "rows": [
                [
                    "softmax",
                    "29.79",
                    "21.57",
                    "32.70",
                    "26.02"
                ],
                [
                    "1.5-entmax",
                    "29.83",
                    "[BOLD] 22.13",
                    "[BOLD] 33.10",
                    "25.89"
                ],
                [
                    "[ITALIC] \u03b1-entmax",
                    "[BOLD] 29.90",
                    "21.74",
                    "32.89",
                    "[BOLD] 26.93"
                ]
            ],
            "title": "Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 de\u2192en, KFTT ja\u2192en, WMT 2016 ro\u2192en and WMT 2014 en\u2192de, respectively."
        },
        "insight": "We report test set tokenized BLEU (Papineni et al., 2002) results in Table 1. We can see that replacing softmax by entmax does not hurt performance in any of the datasets; indeed, sparse attention Transformers tend to have slightly higher BLEU, but their sparsity leads to a better potential for analysis. In the next section, we make use of this potential by exploring the learned internal mechanics of the self-attention heads."
    },
    {
        "id": "378",
        "table": {
            "header": [
                "[EMPTY]",
                "subtask 1.1 macro F1",
                "subtask 1.1 micro F1",
                "subtask 1.2 macro F1",
                "subtask 1.2 micro F1"
            ],
            "rows": [
                [
                    "CC",
                    "54.42",
                    "67.61",
                    "[BOLD] 74.42",
                    "[BOLD] 78.87"
                ],
                [
                    "arXiv",
                    "[BOLD] 67.49",
                    "[BOLD] 70.96",
                    "67.02",
                    "74.37"
                ]
            ],
            "title": "Table 6: Results for C-LSTM models trained with CC and arXiv embeddings on both subtasks."
        },
        "insight": "Table 6 shows that the C-LSTM model performs well on the scientific embeddings, but consistently worse than the SVM model using handcrafted features and achieves a macro-F1 score of 67.49 and 67.02 for subtask 1.1 and subtask 1.2 respectively."
    },
    {
        "id": "379",
        "table": {
            "header": [
                "[EMPTY]",
                "val",
                "UNC testA",
                "testB",
                "val",
                "UNC+ testA",
                "testB",
                "G-Ref val",
                "ReferIt test"
            ],
            "rows": [
                [
                    "LSTM-CNN\u00a0",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "28.14",
                    "48.03"
                ],
                [
                    "RMI\u00a0",
                    "45.18",
                    "45.69",
                    "45.57",
                    "29.86",
                    "30.48",
                    "29.50",
                    "34.52",
                    "58.73"
                ],
                [
                    "DMN\u00a0",
                    "49.78",
                    "54.83",
                    "45.13",
                    "38.88",
                    "44.22",
                    "32.29",
                    "36.76",
                    "52.81"
                ],
                [
                    "KWA\u00a0",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "36.92",
                    "59.09"
                ],
                [
                    "RRN\u00a0",
                    "55.33",
                    "57.26",
                    "53.93",
                    "39.75",
                    "42.15",
                    "36.11",
                    "36.45",
                    "63.63"
                ],
                [
                    "Ours",
                    "[BOLD] 58.32",
                    "[BOLD] 60.61",
                    "[BOLD] 55.09",
                    "[BOLD] 43.76",
                    "[BOLD] 47.60",
                    "[BOLD] 37.89",
                    "[BOLD] 39.98",
                    "[BOLD] 63.80"
                ]
            ],
            "title": "Table 1: Comparison of segmentation performance with the state-of-the-art methods on four evaluation datasets in terms of IoU."
        },
        "insight": "Table 1 presents comparisons of our method with existing state-of-the-art approaches. [CONTINUE] Our proposed method consistently outperforms all other methods on all four datasets. The improvement is particularly significant on the more challenging datasets, such as UNC+ which has no location words and G-Ref which contains longer and richer query expressions."
    },
    {
        "id": "380",
        "table": {
            "header": [
                "Method",
                "IoU"
            ],
            "rows": [
                [
                    "No attention",
                    "45.63"
                ],
                [
                    "Word attention",
                    "47.01"
                ],
                [
                    "Pixel attention",
                    "47.84"
                ],
                [
                    "Word-pixel pair attention",
                    "47.57"
                ],
                [
                    "Cross-modal self-attention",
                    "[BOLD] 50.12"
                ]
            ],
            "title": "Table 2: Ablation study of different attention methods for multimodal features on the UNC val set."
        },
        "insight": "As shown in Table 2, the proposed cross-modal self-attention outperforms all other attention methods significantly."
    },
    {
        "id": "381",
        "table": {
            "header": [
                "Method",
                "prec@0.5",
                "prec@0.6",
                "prec@0.7",
                "prec@0.8",
                "prec@0.9",
                "IoU"
            ],
            "rows": [
                [
                    "RMI-LSTM\u00a0",
                    "42.99",
                    "33.24",
                    "22.75",
                    "12.11",
                    "2.23",
                    "45.18"
                ],
                [
                    "RRN-CNN\u00a0\u2217",
                    "47.59",
                    "38.76",
                    "26.53",
                    "14.79",
                    "3.17",
                    "46.95"
                ],
                [
                    "CMSA-S",
                    "51.19",
                    "41.31",
                    "29.57",
                    "14.99",
                    "2.61",
                    "48.53"
                ],
                [
                    "CMSA-W",
                    "[BOLD] 51.95",
                    "[BOLD] 43.11",
                    "[BOLD] 32.74",
                    "[BOLD] 19.28",
                    "[BOLD] 4.11",
                    "[BOLD] 50.12"
                ],
                [
                    "CMSA+PPM",
                    "58.25",
                    "49.82",
                    "39.09",
                    "24.76",
                    "5.73",
                    "53.54"
                ],
                [
                    "CMSA+Deconv",
                    "58.29",
                    "49.94",
                    "39.16",
                    "25.42",
                    "6.75",
                    "54.18"
                ],
                [
                    "CMSA+ConvLSTM",
                    "64.73",
                    "56.03",
                    "45.23",
                    "29.15",
                    "7.86",
                    "56.56"
                ],
                [
                    "CMSA+Gated",
                    "65.17",
                    "57.25",
                    "47.37",
                    "33.31",
                    "9.66",
                    "57.08"
                ],
                [
                    "CMSA+GF(Ours)",
                    "[BOLD] 66.44",
                    "[BOLD] 59.70",
                    "[BOLD] 50.77",
                    "[BOLD] 35.52",
                    "[BOLD] 10.96",
                    "[BOLD] 58.32"
                ]
            ],
            "title": "Table 3: Ablation study on the UNC val set. The top four methods compare results of different methods for multimodal feature representations. The bottom five results show comparisons of multi-level feature fusion methods. CMSA and GF denote the proposed cross-modal self-attention and gated multi-level fusion modules. All methods use the same base model (DeepLab-101) and DenseCRF for postprocessing. \u2217The numbers for\u00a0[15] are slightly higher than original numbers reported in their paper which did not use DenseCRF postprocessing."
        },
        "insight": "As shown in Table 3 (top 4 rows), the proposed crossmodal self-attentive feature based approaches achieve significantly better performance than other baselines. [CONTINUE] the word based method CMSA-W outperforms sentence based method CMSA-S for multimodal feature representation. [CONTINUE] As presented in the bottom 5 rows in Table 3, the proposed gated multi-level fusion outperforms these other multi-scale feature fusion methods."
    },
    {
        "id": "382",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC]  [BOLD] Model 1",
                "[ITALIC]  [BOLD] Model 2",
                "[ITALIC]  [BOLD] Model 3"
            ],
            "rows": [
                [
                    "First metric",
                    "First metric",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Disp.",
                    "[BOLD] 0.94",
                    "0.72",
                    "0"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Gen. perf.",
                    "0.68",
                    "0.63",
                    "1"
                ],
                [
                    "Second metric",
                    "Second metric",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Disp.",
                    "0.93",
                    "[BOLD] 0.96",
                    "0"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Gen. perf.",
                    "0.68",
                    "0.68",
                    "1"
                ]
            ],
            "title": "Table 2: Experimental results of the evaluation metrics."
        },
        "insight": "We report metric results when grouping crowd workers into 5 bins using the full range of ADR of the dataset. The ADR-based bias measures (Tab. 2) follow the expected trend (model 1 appears more biased than models 2 and 3) contrary to the protected attributes-based measures. Model 1 trained with MV labels exhibits similar performance across demographic groups [CONTINUE] Model 2's performance is different across groups"
    },
    {
        "id": "383",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] SwDA",
                "[BOLD] MRDA"
            ],
            "rows": [
                [
                    "TF-IDF GloVe",
                    "66.5",
                    "78.7"
                ],
                [
                    "Kalchbrenner and Blunsom ( 2013 )",
                    "73.9",
                    "-"
                ],
                [
                    "Lee and Dernoncourt ( 2016 )",
                    "73.9",
                    "84.6"
                ],
                [
                    "Khanpour et\u00a0al. ( 2016 )",
                    "75.8",
                    "86.8"
                ],
                [
                    "Ji et\u00a0al. ( 2016 )",
                    "77.0",
                    "-"
                ],
                [
                    "Shen and Lee ( 2016 )",
                    "72.6",
                    "-"
                ],
                [
                    "Li and Wu ( 2016 )",
                    "79.4",
                    "-"
                ],
                [
                    "Ortega and Vu ( 2017 )",
                    "73.8",
                    "84.3"
                ],
                [
                    "Tran et\u00a0al. ( 2017 )",
                    "74.5",
                    "-"
                ],
                [
                    "Kumar et\u00a0al. ( 2018 )",
                    "79.2",
                    "90.9"
                ],
                [
                    "Chen et\u00a0al. ( 2018 )",
                    "81.3",
                    "[BOLD] 91.7"
                ],
                [
                    "[BOLD] Our Method",
                    "[BOLD] 82.9",
                    "91.1"
                ],
                [
                    "[BOLD] Human Agreement",
                    "84.0",
                    "-"
                ]
            ],
            "title": "Table 3: DA Classification Accuracy"
        },
        "insight": "We compare the classification accuracy of our model against several other recent methods (Table 3). [CONTINUE] Our model outperforms state-of-the-art methods by 1.6% on SwDA, the primary dataset for this task, and comes within 0.6% on MRDA. It also beats a TF-IDF GloVe baseline (described in Section 5.2) by 16.4% and 12.2%, respectively. [CONTINUE] The improvements that the model is able to make over the other methods are significant, however, the gains on MRDA still fall short of the state-of-the-art by 0.6%."
    },
    {
        "id": "384",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Inspec\u00a0(Avg=7.8)  [BOLD] F1@5",
                "[BOLD] Inspec\u00a0(Avg=7.8)  [BOLD] F1@10",
                "[BOLD] Krapivin\u00a0(Avg=3.4)  [BOLD] F1@5",
                "[BOLD] Krapivin\u00a0(Avg=3.4)  [BOLD] F1@10",
                "[BOLD] NUS\u00a0(Avg=6.1)  [BOLD] F1@5",
                "[BOLD] NUS\u00a0(Avg=6.1)  [BOLD] F1@10",
                "[BOLD] SemEval\u00a0(Avg=6.7)  [BOLD] F1@5",
                "[BOLD] SemEval\u00a0(Avg=6.7)  [BOLD] F1@10",
                "[BOLD] Average  [BOLD] F1@5",
                "[BOLD] Average  [BOLD] F1@10"
            ],
            "rows": [
                [
                    "[BOLD] One2One",
                    "0.244",
                    "0.289",
                    "0.305",
                    "[BOLD] 0.266",
                    "[BOLD] 0.376",
                    "[BOLD] 0.352",
                    "0.318",
                    "[BOLD] 0.318",
                    "0.311",
                    "[BOLD] 0.306"
                ],
                [
                    "[BOLD] Random",
                    "0.283",
                    "0.206",
                    "0.288",
                    "0.183",
                    "0.344",
                    "0.238",
                    "0.304",
                    "0.218",
                    "0.305",
                    "0.211"
                ],
                [
                    "[BOLD] Length",
                    "0.298",
                    "0.224",
                    "0.321",
                    "0.206",
                    "0.364",
                    "0.259",
                    "0.311",
                    "0.222",
                    "0.324",
                    "0.228"
                ],
                [
                    "[BOLD] No-Sort",
                    "0.323",
                    "0.253",
                    "0.317",
                    "0.209",
                    "[BOLD] 0.376",
                    "0.264",
                    "0.318",
                    "0.244",
                    "0.333",
                    "0.243"
                ],
                [
                    "[BOLD] Alpha",
                    "0.319",
                    "0.283",
                    "[BOLD] 0.329",
                    "0.238",
                    "[BOLD] 0.376",
                    "0.289",
                    "[BOLD] 0.343",
                    "0.266",
                    "[BOLD] 0.342",
                    "0.269"
                ],
                [
                    "[BOLD] Appear-Pre",
                    "0.320",
                    "0.307",
                    "0.322",
                    "0.245",
                    "0.369",
                    "0.302",
                    "0.327",
                    "0.291",
                    "0.334",
                    "0.286"
                ],
                [
                    "[BOLD] Appear-Ap",
                    "[BOLD] 0.344",
                    "[BOLD] 0.333",
                    "0.320",
                    "0.236",
                    "0.367",
                    "0.295",
                    "0.324",
                    "0.286",
                    "0.339",
                    "0.287"
                ]
            ],
            "title": "Table 1: Present keyphrase prediction performance (F1-score) on four benchmark datasets. Dataset names are listed in the header followed by the average number of target phrases per document. The baseline One2One\u00a0model is on the first row, followed by different One2Seq\u00a0variants. Bold indicates best score in each column."
        },
        "insight": "We report experimental results on four common benchmark datasets (totalling 1241 testing data points) in Table 1. [CONTINUE] As shown in Table 1, the average performance increases from RANDOM to APPEAR-AP: a trend that becomes particularly obvious for F1@10."
    },
    {
        "id": "385",
        "table": {
            "header": [
                "[BOLD] Model  [BOLD] #(Param)",
                "[BOLD] BaseRNN 13M",
                "[BOLD] BaseRNN 13M",
                "[BOLD] BigRNN 37M",
                "[BOLD] BigRNN 37M",
                "[BOLD] Transformer 80M",
                "[BOLD] Transformer 80M"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[BOLD] F@5",
                    "[BOLD] F@10",
                    "[BOLD] F@5",
                    "[BOLD] F@10",
                    "[BOLD] F@5",
                    "[BOLD] F@10"
                ],
                [
                    "[BOLD] Random",
                    "0.358",
                    "0.304",
                    "0.356",
                    "0.305",
                    "0.359",
                    "0.289"
                ],
                [
                    "[BOLD] Length",
                    "0.351",
                    "0.319",
                    "0.349",
                    "0.321",
                    "[BOLD] 0.361",
                    "0.318"
                ],
                [
                    "[BOLD] No-Sort",
                    "[BOLD] 0.364",
                    "0.325",
                    "[BOLD] 0.361",
                    "0.329",
                    "0.358",
                    "0.329"
                ],
                [
                    "[BOLD] Alpha",
                    "0.354",
                    "0.341",
                    "0.358",
                    "0.341",
                    "0.353",
                    "0.336"
                ],
                [
                    "[BOLD] Appear-Pre",
                    "0.337",
                    "0.345",
                    "0.339",
                    "0.341",
                    "0.352",
                    "0.343"
                ],
                [
                    "[BOLD] Appear-Ap",
                    "0.339",
                    "[BOLD] 0.347",
                    "0.344",
                    "[BOLD] 0.346",
                    "0.357",
                    "[BOLD] 0.345"
                ]
            ],
            "title": "Table 4: F1 scores on present keyphrase generation of One2Seq\u00a0models with different model complexities (BeamWidth=50)."
        },
        "insight": "As shown in Table 4, BigRNN and Transformer are not able to outperform BaseRNN."
    },
    {
        "id": "386",
        "table": {
            "header": [
                "[EMPTY]",
                "Diagnosis",
                "Prescription",
                "Penn Adverse Drug",
                "Chemical\u2013Disease",
                "Drug\u2013Disease"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Detection",
                    "Reasons",
                    "Reactions (ADR)",
                    "Relations (CDR)",
                    "Relations"
                ],
                [
                    "rand-LSTM-CRF",
                    "77.3 \u00b1 0.05",
                    "69.6 \u00b1 0.25",
                    "53.8 \u00b1 0.88",
                    "85.1 \u00b1 0.10",
                    "48.2 \u00b1 1.12"
                ],
                [
                    "HB-CRF",
                    "82.0 \u00b1 0.05",
                    "78.5 \u00b1 0.01",
                    "58.8 \u00b1 0.12",
                    "86.2 \u00b1 0.02",
                    "42.3 \u00b1 0.30"
                ],
                [
                    "ELMo-LSTM-CRF",
                    "83.9 \u00b1 0.35",
                    "81.0 \u00b1 0.20",
                    "65.7 \u00b1 0.35",
                    "88.2 \u00b1 0.34",
                    "50.6 \u00b1 0.64"
                ],
                [
                    "ELMo-LSTM-CRF-HB",
                    "[BOLD] 85.3 \u00b1 0.24\u2217\u2217\u2217",
                    "[BOLD] 82.0 \u00b1 0.03\u2217\u2217\u2217",
                    "[BOLD] 68.5 \u00b1 1.67\u2217",
                    "[BOLD] 89.9 \u00b1 0.12\u2217\u2217\u2217",
                    "[BOLD] 51.9 \u00b1 0.52\u2217\u2217"
                ]
            ],
            "title": "Table 2: Per-token macro-F1 scores. For ADR, the F1 scores are for chunks via approximate matching (Nikfarjam et\u00a0al., 2015; Tsai et\u00a0al., 2006). \u2018rand-LSTM\u2019 is an LSTM with randomly initialized word vectors. \u2018ELMo-LSTM\u2019 is an LSTM initialized with pretrained ELMo embeddings. \u2018HB\u2019 signals sparse, high-dimensional feature representations based on hand-built feature functions. The mean values and standard deviations are calculated using F1 scores of three runs of repeated experiments, as discussed in section\u00a03. Statistical significance notation for the last two rows (two top-performing models) is \u2217: p<0.05; \u2217\u2217: p<0.01; \u2217\u2217\u2217: p<0.001."
        },
        "insight": "The power these features bring to the model, beyond what is captured in the ELMo-LSTM representations, is evident in table 2, column 1. [CONTINUE] The results in table 2, column 2, clearly favor the combined model ELMo-LSTM-CRF-HB that uses both these features and the ELMo-LSTM. [CONTINUE] Our test-set results, given in table 2, column 3, show the power of our combined model ELMo-LSTM-CRF-HB. [CONTINUE] We report results for the standard test set. The power of the combined model ELMo-LSTM-CRF-HB is again evident in the results in table 2, column 4. [CONTINUE] Our results for this experiment are given in table 2, column 5, and point to the superiority of our combined model ELMo-LSTM-CRF-HB. [CONTINUE] Our discussion seeks to show that the combined model ELMo-LSTM-CRF-HB, which shows superior performance in all tasks (table 2), is making meaningful use of both kinds of features (hand-built and ELMo) and both of the major model components (LSTM and CRF). [CONTINUE] We note also that, where the performance of the two base models is very similar (table 2), the potential scores in the combined model are also more similar."
    },
    {
        "id": "387",
        "table": {
            "header": [
                "Diagnosis Detection Label",
                "Diagnosis Detection Support",
                "Diagnosis Detection F1 score",
                "Diagnosis Detection Improvement",
                "Prescription Reasons Label",
                "Prescription Reasons Support",
                "Prescription Reasons F1 score",
                "Prescription Reasons Improvement"
            ],
            "rows": [
                [
                    "Other",
                    "74888",
                    "95.3",
                    "1.4%",
                    "Other",
                    "83618",
                    "95.8",
                    "0.9%"
                ],
                [
                    "Positive",
                    "24489",
                    "86.1",
                    "4.4%",
                    "Reason",
                    "9114",
                    "64.7",
                    "8.6%"
                ],
                [
                    "Ruled-out",
                    "2797",
                    "86.4",
                    "3.6%",
                    "Prescribed",
                    "5967",
                    "84.7",
                    "4.4%"
                ],
                [
                    "Concern",
                    "2780",
                    "72.1",
                    "5.6%",
                    "Discontinued",
                    "2754",
                    "82.7",
                    "5.6%"
                ],
                [
                    "Chemical\u2013Disease Relations (CDR)",
                    "Chemical\u2013Disease Relations (CDR)",
                    "Chemical\u2013Disease Relations (CDR)",
                    "Chemical\u2013Disease Relations (CDR)",
                    "Drug\u2013Disease Relations",
                    "Drug\u2013Disease Relations",
                    "Drug\u2013Disease Relations",
                    "Drug\u2013Disease Relations"
                ],
                [
                    "Label",
                    "Support",
                    "F1 score",
                    "Improvement",
                    "Label",
                    "Support",
                    "F1 score",
                    "Improvement"
                ],
                [
                    "Other",
                    "104530",
                    "98.3",
                    "0.5%",
                    "Other",
                    "10634",
                    "90.8",
                    "2.3%"
                ],
                [
                    "Disease",
                    "6887",
                    "84.2",
                    "6.3%",
                    "Treats",
                    "3671",
                    "76.0",
                    "5.7%"
                ],
                [
                    "Chemical",
                    "6270",
                    "87.0",
                    "6.7%",
                    "Unrelated",
                    "1145",
                    "53.8",
                    "71.3%"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Prevents",
                    "320",
                    "41.1",
                    "103.5%"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Contraindicated-for",
                    "69",
                    "0",
                    "\u2013"
                ]
            ],
            "title": "Table 3: Relative F1 score improvements of different labels. For each label, we give the number of supporting examples (Support), the F1 score of our combined model, and the relative improvements over the HB-CRF model. The F1 scores of minor labels suffer from insufficient training data, and thus have lower values. However, the combined model shows the largest relative improvements in these categories. ADR results are shown in table\u00a0A4."
        },
        "insight": "Table 3 suggests that the combined model does make progress here, in that the largest gains, across all relevant datasets, tend to be for the smallest categories."
    },
    {
        "id": "388",
        "table": {
            "header": [
                "[BOLD] Dataset Friends",
                "[BOLD] Dataset Training",
                "#Dialogues / #Utterances 4,000 / 58,012",
                "#Avg. utterances per dialogue 14.50",
                "#Avg. length of dialogues 160.92",
                "Neutral 45.0%",
                "Joy 11.8%",
                "Sadness 3.4%",
                "Anger 5.2%",
                "Out-Of-Domain 34.6%"
            ],
            "rows": [
                [
                    "Friends",
                    "Test",
                    "240 / 3296",
                    "13.73",
                    "156.38",
                    "31.4%",
                    "15.3%",
                    "3.7%",
                    "4.3%",
                    "45.3%"
                ],
                [
                    "EmotionPush",
                    "Training",
                    "4,000 / 58,968",
                    "14.74",
                    "114.96",
                    "66.8%",
                    "14.2%",
                    "3.5%",
                    "0.9%",
                    "14.6%"
                ],
                [
                    "EmotionPush",
                    "Test",
                    "240 / 3536",
                    "14.73",
                    "92.43",
                    "60.7%",
                    "17.0%",
                    "3.1%",
                    "0.8%",
                    "18.4%"
                ]
            ],
            "title": "Table 2: Corpus statistics and label distributions of Friends and EmotionPush datasets."
        },
        "insight": "we describe data and label distribution of Friends and EmotionPush datasets. In terms of label distribution for both datasets, Neutral are the most common class, followed by Joy, Sadness, and Anger. Both datasets have imbalanced class distribution, and especially the ratio of Sadness and Anger is very small. For instance, they account for only 3.4% and 5.2%, respectively in the Friends dataset. In the case of EmotionPush, Anger label accounts for less than 1% of the training set."
    },
    {
        "id": "389",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Dataset",
                "[BOLD] # of extracted pairs",
                "[BOLD] # of valid pairs",
                "[BOLD] Precision"
            ],
            "rows": [
                [
                    "Essentia",
                    "Snips",
                    "173",
                    "84",
                    "48.55%"
                ],
                [
                    "Essentia",
                    "HotelQA",
                    "2221",
                    "642",
                    "28.91%"
                ],
                [
                    "FSA",
                    "Snips",
                    "18",
                    "15",
                    "83.33%"
                ],
                [
                    "FSA",
                    "HotelQA",
                    "342",
                    "185",
                    "54.09 %"
                ]
            ],
            "title": "Table 1: Comparison between Essentia and FSA baseline on paraphrase extraction"
        },
        "insight": "Table 1 compares the performance of ESSENTIA with the FSA baseline for paraphrase mining. Specifically, we show the number of phrase pairs extracted by ESSENTIA and FSA from both datasets (\"# of extracted pairs\" column), number of valid paraphrases within these pairs (\"# of valid pairs\" column), and precision (\"Precision\" column). Although FSA has higher precision due to conservative sentence alignment, ESSENTIA extracts significantly more paraphrases, improving the recall by 460% (Snips) and 247% (HotelQA) over the baseline."
    },
    {
        "id": "390",
        "table": {
            "header": [
                "[BOLD] BR",
                "[BOLD] US",
                "[BOLD] BR + US"
            ],
            "rows": [
                [
                    "85%",
                    "72%",
                    "70%"
                ]
            ],
            "title": "Table 5: Classification test scores for classifying R vs U in the BR, US, and combined BR + US dataset. The baseline score is 50%."
        },
        "insight": "The classification of BR and US datasets used the 60 and 49 most relevant features, respectively. The test accuracy for the BR and US datasets were 85% and 72%, respectively, with a baseline score of 50%. The combined dataset used a reduced set of features consisting of an intersection of the most relevant features observed in both BR and US, achieving a test score of 70% using only 18 features. [CONTINUE] classes. We have shown that these features may be used to classify news articles in a language other than English."
    },
    {
        "id": "391",
        "table": {
            "header": [
                "2 Data type",
                "geth",
                "ethanos",
                "Diff"
            ],
            "rows": [
                [
                    "Headers",
                    "316.68",
                    "348.16",
                    "+31.48"
                ],
                [
                    "Bodies",
                    "11,110.00",
                    "12,750.00",
                    "+1,640.00"
                ],
                [
                    "Receipts",
                    "3,578.97",
                    "3,605.66",
                    "+26.69"
                ],
                [
                    "Difficulties",
                    "15.57",
                    "15.33",
                    "-0.24"
                ],
                [
                    "Block number -> hash",
                    "39.66",
                    "39.35",
                    "-0.31"
                ],
                [
                    "Block hash -> number",
                    "39.10",
                    "39.10",
                    "0"
                ],
                [
                    "Transaction Index",
                    "3,650.00",
                    "3,650.00",
                    "0"
                ],
                [
                    "Bloombit index",
                    "20.49",
                    "20.49",
                    "0"
                ],
                [
                    "Trie nodes",
                    "183,100.00",
                    "165,140.00",
                    "-17,960.00"
                ],
                [
                    "Trie preimages",
                    "857.00",
                    "888.11",
                    "+31.11"
                ],
                [
                    "total",
                    "202,727.47",
                    "186,496.20",
                    "-16,231.27"
                ],
                [
                    "2",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 1: Storage size(MB) comparison of full archive sync between geth and ethanos from 7M+1 to 8M block."
        },
        "insight": "Table 1 gives full archive sync size of each data type for geth and ethanos at the 8M block. Ethanos reduces the total blockchain size about 16GB compared to geth. Especially, it reduces the size of Trie nodes about 18GB, because ethanos only maintains the state trie only with active accounts during a month which are only 10% of the total accounts. However, hash field of bloom filter in block header slightly increases Headers, and restore transactions increment Bodies and Receipts about 1.6GB. As a result, ethanos reduced full archive node by about 8% compared to ethereum."
    },
    {
        "id": "392",
        "table": {
            "header": [
                "2 Data type",
                "fast sync geth",
                "fast sync ethanos",
                "compact sync geth",
                "compact sync ethanos"
            ],
            "rows": [
                [
                    "Headers",
                    "276.66",
                    "303.83",
                    "276.66",
                    "303.83"
                ],
                [
                    "Bodies",
                    "9,260.00",
                    "10,130.00",
                    "8.12",
                    "13.02"
                ],
                [
                    "Receipts",
                    "2,973.66",
                    "2,979.81",
                    "10.43",
                    "10.44"
                ],
                [
                    "Difficulties",
                    "11.49",
                    "11.48",
                    "11.49",
                    "11.48"
                ],
                [
                    "Block number -> hash",
                    "31.86",
                    "31.88",
                    "31.86",
                    "31.88"
                ],
                [
                    "Block hash -> number",
                    "33.79",
                    "33.79",
                    "33.79",
                    "33.79"
                ],
                [
                    "Transaction Index",
                    "3,020.00",
                    "3,020.00",
                    "0.19",
                    "0.19"
                ],
                [
                    "Bloombit index",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Trie nodes",
                    "2,220.00",
                    "804.79",
                    "2,220.00",
                    "802.33"
                ],
                [
                    "Trie preimages",
                    "0",
                    "0.28",
                    "0",
                    "0"
                ],
                [
                    "total",
                    "17,827.46",
                    "17,315.86",
                    "2,592.54",
                    "1,206.97"
                ],
                [
                    "2",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 2: Storage size(MB) comparison of fast sync and compact sync between geth and ethanos from 7M+1 to 7M+864K (5th checkpoint) block."
        },
        "insight": "Table 2 shows storage size of geth and ethanos for each bootstrapping mode. Ethanos reduces Trie nodes size of geth from 2,220MB to 804.79MB; however, it increases the transaction size (Bodies, Receipts) with block headers (Headers). Therefore, ethanos did not reduce the size of fast sync much, but it significantly reduced compact sync from 2,592.54MB of geth to 1,206.97MB, which is less than 50%. Our result shows that the trie node size of each checkpoint is about 800MB,"
    },
    {
        "id": "393",
        "table": {
            "header": [
                "[EMPTY]",
                "Avg MT02 \u2013 08 BLEU",
                "Avg MT02 \u2013 08 HE",
                "Avg MT02 \u2013 08 chrF1",
                "Avg MT02 \u2013 08 RD"
            ],
            "rows": [
                [
                    "Transformer-word",
                    "39.75",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "32.42",
                    "3.22",
                    "0.82",
                    "0.184"
                ],
                [
                    "Search (0.3)",
                    "28.83",
                    "2.45",
                    "0.77",
                    "0.275"
                ],
                [
                    "Ours",
                    "[BOLD] 33.28",
                    "[BOLD] 3.69",
                    "[BOLD] 0.80",
                    "[BOLD] 0.163"
                ],
                [
                    "Transformer-BPE",
                    "43.38",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "34.27",
                    "3.87",
                    "0.89",
                    "0.210"
                ],
                [
                    "Search (0.4)",
                    "27.27",
                    "2.91",
                    "0.80",
                    "0.371"
                ],
                [
                    "Ours",
                    "[BOLD] 31.35",
                    "[BOLD] 3.66",
                    "[BOLD] 0.80",
                    "[BOLD] 0.277"
                ],
                [
                    "RNN-search-BPE",
                    "39.38",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "31.83",
                    "3.83",
                    "0.89",
                    "0.192"
                ],
                [
                    "Search (0.4)",
                    "26.13",
                    "2.82",
                    "0.79",
                    "0.336"
                ],
                [
                    "Ours",
                    "[BOLD] 31.18",
                    "[BOLD] 3.60",
                    "[BOLD] 0.83",
                    "[BOLD] 0.208"
                ]
            ],
            "title": "Table 3: Experiment results. Note that sequence length for word level system is shorter, thus we search by ratio 0.3 which shares similar chrF1 with search on subword level system with ratio 0.4. An ideal adversarial sample generation must achieve degradation with respect to higher semantic similarity with origin inputs (HE)."
        },
        "insight": "our model can stably generate adversarial samples without significant change in semantics with the same training setting for different models,"
    },
    {
        "id": "394",
        "table": {
            "header": [
                "Model",
                "Task SLC P",
                "Task SLC R",
                "Task SLC F1",
                "Task FLC P",
                "Task FLC R",
                "Task FLC F1"
            ],
            "rows": [
                [
                    "All-Propaganda",
                    "23.92",
                    "100.0",
                    "38.61",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "[BOLD] 63.20",
                    "53.16",
                    "57.74",
                    "21.48",
                    "[BOLD] 21.39",
                    "21.39"
                ],
                [
                    "Joint",
                    "62.84",
                    "55.46",
                    "58.91",
                    "20.11",
                    "19.74",
                    "19.92"
                ],
                [
                    "Granu",
                    "62.80",
                    "55.24",
                    "58.76",
                    "23.85",
                    "20.14",
                    "21.80"
                ],
                [
                    "Multi-Granularity",
                    "Multi-Granularity",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "ReLU",
                    "60.41",
                    "[BOLD] 61.58",
                    "[BOLD] 60.98",
                    "23.98",
                    "20.33",
                    "21.82"
                ],
                [
                    "Sigmoid",
                    "62.27",
                    "59.56",
                    "60.71",
                    "[BOLD] 24.42",
                    "21.05",
                    "[BOLD] 22.58"
                ]
            ],
            "title": "Table 1: Sentence-level (left) and fragment-level experiments (right). All-propaganda is a baseline that always output the propaganda class."
        },
        "insight": "The left side of Table 1 shows the performance for the three baselines and for our multi-granularity network on the FLC task. [CONTINUE] Table 1 (right) shows that using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification. [CONTINUE] The right side of Table 1 shows the results for the SLC task. We apply our multi-granularity network model to the sentence-level classification task to see its effect on low granularity when we train the model with a high granularity task. Interestingly, it yields huge performance improvements on the sentence-level classification result. Compared to the BERT baseline, it increases the recall by 8.42%, resulting in a 3.24% increase of the F1 score."
    },
    {
        "id": "395",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "rows": [
                [
                    "BERT (baseline)",
                    "63.8"
                ],
                [
                    "CoS-E-open-ended",
                    "65.5"
                ],
                [
                    "CAGE-reasoning",
                    "[BOLD] 72.6"
                ]
            ],
            "title": "Table 2: Results on CQA dev-random-split with CoS-E used during training."
        },
        "insight": "Table 2 shows results that compare a BERT baseline that uses only the CQA inputs and the same architecture but trained using inputs that contain explanations from CoS-E during training. The BERT baseline model reaches 64% accuracy and adding open-ended human explanations (CoS-E-open-ended) alongside the questions during training results in a 2% boost in accuracy. [CONTINUE] In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%."
    },
    {
        "id": "396",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "rows": [
                [
                    "RC (Talmor et\u00a0al.,  2019 )",
                    "47.7"
                ],
                [
                    "GPT (Talmor et\u00a0al.,  2019 )",
                    "54.8"
                ],
                [
                    "CoS-E-open-ended",
                    "60.2"
                ],
                [
                    "CAGE-reasoning",
                    "[BOLD] 64.7"
                ],
                [
                    "Human (Talmor et\u00a0al.,  2019 )",
                    "95.3"
                ]
            ],
            "title": "Table 3: Test accuracy on CQA v1.0. The addition of CoS-E-open-ended during training dramatically improves performance. Replacing CoS-E during training with CAGE reasoning during both training and inference leads to an absolute gain of 10% over the previous state-of-the-art."
        },
        "insight": "Table 3 shows the results obtained on the CQA test split. We report our two best models that represent using human explanations (CoS-E-openended) for training only and using language model explanations (CAGE-reasoning) during both train and test."
    },
    {
        "id": "397",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "rows": [
                [
                    "CoS-E-selected w/o ques",
                    "53.0"
                ],
                [
                    "CoS-E-limited-open-ended",
                    "67.6"
                ],
                [
                    "CoS-E-selected",
                    "70.0"
                ],
                [
                    "CoS-E-open-ended w/o ques",
                    "84.5"
                ],
                [
                    "CoS-E-open-ended*",
                    "[BOLD] 89.8"
                ]
            ],
            "title": "Table 4: Oracle results on CQA dev-random-split using different variants of CoS-E for both training and validation. * indicates CoS-E-open-ended used during both training and validation to contrast with CoS-E-open-ended used only during training in Table\u00a02."
        },
        "insight": "Table 4 also contains results that use only the explanation and exclude the original question from CQA denoted by 'w/o question'. These variants also use explanation during both train and validation. [CONTINUE] We observe that even using these limited kind of explanations improves over the BERT baseline in Table 4, which suggests that the explanations are providing useful information beyond just mentioning the correct or incorrect answers. [CONTINUE] In Table 2, using CAGE-reasoning at both train and validation resulted in an accuracy of 72%, but Table 4 shows that if CAGE-reasoning truly captured all information provided in CoS-E-openended, performance would be 90%."
    },
    {
        "id": "398",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] SWAG",
                "[BOLD] Story Cloze"
            ],
            "rows": [
                [
                    "BERT",
                    "84.2",
                    "89.8"
                ],
                [
                    "+ expl transfer",
                    "83.6",
                    "89.5"
                ]
            ],
            "title": "Table 6: Results for explanation transfer from CQA to out-of-domain SWAG and Sotry Cloze tasks."
        },
        "insight": "Table 6 shows the results obtained by the BERT baseline without explanations and using our transferred explanations from CQA to SWAG and Story Cloze. We observed that adding explanations led to a very small decrease (< 0.6%) in the performance compared to the baseline for both tasks."
    },
    {
        "id": "399",
        "table": {
            "header": [
                "Method",
                "P",
                "R",
                "F1",
                "eP",
                "eR",
                "eF1"
            ],
            "rows": [
                [
                    "Redi et\u00a0al.  2019",
                    "0.753",
                    "0.709",
                    "0.730 [0.76]*",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "0.788 \u00b1 0.013",
                    "0.837 \u00b1 0.045",
                    "0.810 \u00b1 0.015",
                    "0.790",
                    "0.853",
                    "0.820"
                ],
                [
                    "BERT + PU",
                    "0.788 \u00b1 0.009",
                    "0.843 \u00b1 0.030",
                    "0.814 \u00b1 0.010",
                    "0.790",
                    "0.856",
                    "0.822"
                ],
                [
                    "BERT +  [ITALIC] PUC",
                    "0.784 \u00b1 0.009",
                    "[BOLD] 0.856 \u00b1 0.032",
                    "[BOLD] 0.818 \u00b1 0.010",
                    "0.786",
                    "[BOLD] 0.871",
                    "[BOLD] 0.826"
                ]
            ],
            "title": "Table 1: F1 and ensembled F1 score for citation needed detection training on the FA split and testing on the LQN split of Redi et\u00a0al. (2019). The FA split contains statements with citations from featured articles and the LQN split consists of statements which were flagged as not having a citation but needing one. Listed are the mean, standard deviation, and ensembled results across 15 seeds (eP, eR, and eF1). Bold indicates best performance, underline indicates second best. *The reported value is from rerunning their released model on the test dataset. The value in brackets is the value reported in the original paper."
        },
        "insight": "Our results for citation needed detection are given in Table 1. The vanilla BERT model already significantly outperforms the state of the art model from Redi et al. (2019) (a GRU network with global attention) by 6 F1 points. We saw further gains in performance with PU learning, as well as when using PUC. Additionally, the models using PU learning had lower variance, indicating more consistent performance across runs. The best performing model we saw was the one trained using PUC with an F1 score of 0.826."
    },
    {
        "id": "400",
        "table": {
            "header": [
                "Top-synset",
                "Size",
                "Max.",
                "#basic",
                "[ITALIC] \u03ba"
            ],
            "rows": [
                [
                    "of domain",
                    "#syns.",
                    "depth",
                    "level",
                    "[EMPTY]"
                ],
                [
                    "Hand tool",
                    "157",
                    "6",
                    "30",
                    "0.73"
                ],
                [
                    "Edible fruit",
                    "197",
                    "5",
                    "77",
                    "0.78"
                ],
                [
                    "Musical Instr.",
                    "164",
                    "7",
                    "54",
                    "0.64"
                ],
                [
                    "All",
                    "518",
                    "7",
                    "161",
                    "0.73"
                ]
            ],
            "title": "Table 1: Properties of the training and test set: the number of synsets, the maximum depth of the hierarchy counted from (and including) the top synset of the domain, the number of basic level concepts, and the inter-rater agreement (\u03ba)."
        },
        "insight": "Table 1 lists the properties of the training and test set including the inter-rater agreement (\u03ba). The agreement is substantial, with some variation between the domains: \u03ba is higher for tools and fruit than for the musical domain."
    },
    {
        "id": "401",
        "table": {
            "header": [
                "Feature",
                "All",
                "Tool",
                "Fruit",
                "Music"
            ],
            "rows": [
                [
                    "depth_in_hierarchy",
                    "1",
                    "1",
                    "1",
                    "3"
                ],
                [
                    "G.Ngrams_2008_max",
                    "2",
                    "2",
                    "5",
                    "2"
                ],
                [
                    "gloss_length",
                    "3",
                    "4",
                    "7",
                    "1"
                ],
                [
                    "polysemy_max",
                    "4",
                    "3",
                    "4",
                    "6"
                ],
                [
                    "word_length_min",
                    "5",
                    "5",
                    "3",
                    "4"
                ],
                [
                    "nr_of_partOfs",
                    "6",
                    "8",
                    "2",
                    "8"
                ],
                [
                    "nr_of_hyponyms",
                    "7",
                    "6",
                    "6",
                    "5"
                ],
                [
                    "nr_of_synonyms",
                    "8",
                    "7",
                    "8",
                    "7"
                ],
                [
                    "nr_of_direct_hypernyms",
                    "9",
                    "9",
                    "9",
                    "9"
                ]
            ],
            "title": "Table 2: Features ranked in order of importance."
        },
        "insight": "Table 2 lists the importance of each variable in the global model and in the three single domain models, where the variable with the highest weight is ranked 1. The lists are relatively stable, with some marked differences, such as the importance of the gloss length and the number of partOf relations for music and fruit, respectively."
    },
    {
        "id": "402",
        "table": {
            "header": [
                "Normalized features: New",
                "Normalized features: Trained",
                "None Bal.",
                "None  [ITALIC] \u03ba",
                "Structural Bal.",
                "Structural  [ITALIC] \u03ba",
                "Lexical Bal.",
                "Lexical  [ITALIC] \u03ba",
                "Frequency Bal.",
                "Frequency  [ITALIC] \u03ba"
            ],
            "rows": [
                [
                    "domain",
                    "on",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]"
                ],
                [
                    "Tools",
                    "Fruit+Music",
                    "0.69",
                    "0.40",
                    "0.84",
                    "0.74",
                    "0.83",
                    "0.68",
                    "0.65",
                    "0.30"
                ],
                [
                    "Fruit",
                    "Tools+Music",
                    "0.66",
                    "0.30",
                    "0.82",
                    "0.62",
                    "0.73",
                    "0.43",
                    "0.66",
                    "0.31"
                ],
                [
                    "Music",
                    "Tools+Fruit",
                    "0.62",
                    "0.27",
                    "0.68",
                    "0.37",
                    "0.73",
                    "0.41",
                    "0.55",
                    "0.12"
                ]
            ],
            "title": "Table 3: Balanced accuracy and \u03ba of predictions made in a new domain, with or without normalization."
        },
        "insight": "We find that normalization of both structural and lexical features leads to a significant performance gain. Normalization of the frequency feature seems to hurt performance (Table 3)"
    },
    {
        "id": "403",
        "table": {
            "header": [
                "POS",
                "Training",
                "Dev.",
                "Test"
            ],
            "rows": [
                [
                    "Noun",
                    "6,710",
                    "111",
                    "1,499"
                ],
                [
                    "Verb",
                    "11,269",
                    "193",
                    "2,426"
                ],
                [
                    "Adjective",
                    "610",
                    "9",
                    "118"
                ],
                [
                    "Preposition",
                    "146",
                    "1",
                    "25"
                ],
                [
                    "Overall Event Tokens",
                    "18,735",
                    "314",
                    "4,068"
                ]
            ],
            "title": "Table 1: Distribution of the event mentions per POS per token in all datasets of the EVENTI corpus."
        },
        "insight": "Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. [CONTINUE] verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases."
    },
    {
        "id": "404",
        "table": {
            "header": [
                "Class",
                "Training",
                "Dev.",
                "Test"
            ],
            "rows": [
                [
                    "OCCURRENCE",
                    "9,041",
                    "162",
                    "1,949"
                ],
                [
                    "ASPECTUAL",
                    "446",
                    "14",
                    "107"
                ],
                [
                    "I_STATE",
                    "1,599",
                    "29",
                    "355"
                ],
                [
                    "I_ACTION",
                    "1,476",
                    "25",
                    "357"
                ],
                [
                    "PERCEPTION",
                    "162",
                    "2",
                    "37"
                ],
                [
                    "REPORTING",
                    "714",
                    "8",
                    "149"
                ],
                [
                    "STATE",
                    "4,090",
                    "61",
                    "843"
                ],
                [
                    "Overall Events",
                    "17,528",
                    "301",
                    "3,798"
                ]
            ],
            "title": "Table 2: Distribution of the event mentions per class in all datasets of the EVENTI corpus."
        },
        "insight": "Tables 1 and 2 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. [CONTINUE] As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I STATE and [CONTINUE] ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION)."
    },
    {
        "id": "405",
        "table": {
            "header": [
                "Embedding Parameter",
                "Strict Evaluation R",
                "Strict Evaluation P",
                "Strict Evaluation F1",
                "Strict Evaluation F1-class",
                "Relaxed Evaluation R",
                "Relaxed Evaluation P",
                "Relaxed Evaluation F1",
                "Relaxed Evaluation F1-class"
            ],
            "rows": [
                [
                    "Berardi2015_w2v",
                    "0.868",
                    "0.868",
                    "0.868",
                    "0.705",
                    "0.892",
                    "0.892",
                    "0.892",
                    "0.725"
                ],
                [
                    "Berardi2015_Glove",
                    "0.848",
                    "0.872",
                    "0.860",
                    "0.697",
                    "0.870",
                    "0.895",
                    "0.882",
                    "0.714"
                ],
                [
                    "Fastext-It",
                    "[BOLD] 0.897",
                    "0.863",
                    "[BOLD] 0.880",
                    "[BOLD] 0.736",
                    "[BOLD] 0.921",
                    "0.887",
                    "[BOLD] 0.903",
                    "[BOLD] 0.756"
                ],
                [
                    "ILC-ItWack",
                    "0.831",
                    "[BOLD] 0.884",
                    "0.856",
                    "0.702",
                    "0.860",
                    "[BOLD] 0.914",
                    "0.886",
                    "0.725"
                ],
                [
                    "DH-FBK_100",
                    "0.855",
                    "0.859",
                    "0.857",
                    "0.685",
                    "0.881",
                    "0.885",
                    "0.883",
                    "0.705"
                ],
                [
                    "FBK-HLT@EVENTI 2014",
                    "0.850",
                    "[ITALIC] 0.884",
                    "0.867",
                    "0.671",
                    "0.868",
                    "0.902",
                    "0.884",
                    "0.685"
                ]
            ],
            "title": "Table 3: Results for Bubtask B Main Task - Event detection and classification."
        },
        "insight": "Results for the experiments are illustrated in Table 3. [CONTINUE] The network obtains the best F1 score, both for detection (F1 of 0.880 for strict evaluation and 0.903 for relaxed evaluation with Fastext-It embeddings) and for classification (F1-class of 0.756 for strict evaluation, and 0.751 for relaxed evaluation with Fastext-It embeddings). [CONTINUE] Although FBKHLT suffers in the classification subtask, it qualifies as a highly competitive system for the detection subtask. [CONTINUE] By observing the strict F1 scores, FBK-HLT beats three configurations (DH-FBK100, ILC-ItWack, Berardi2015 Glove) 11, almost equals one (Berardi2015 w2v) 12, and it is outperformed only by one (Fastext-It) 13. In the relaxed evaluation setting, DH-FBK-100 is the only configuration that does not beat FBK-HLT (although the difference is only 0.001 point). Nevertheless, it is remarkable to observe that FBK-HLT has a very high Precision (0.902, relaxed evaluation mode), that is overcome by only one embedding configuration, ILC-ItWack. The results also indicates that word embeddings have a major contribution on Recall, [CONTINUE] these results are obtained using a single step approach,"
    },
    {
        "id": "406",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] BLEU R",
                "[BOLD] BLEU P",
                "[BOLD] BLEU F1",
                "[BOLD] BOW Embedding A",
                "[BOLD] BOW Embedding E",
                "[BOLD] BOW Embedding G",
                "[BOLD] intra-dist dist-1",
                "[BOLD] intra-dist dist-2",
                "[BOLD] inter-dist dist-1",
                "[BOLD] inter-dist dist-2",
                "[BOLD] L"
            ],
            "rows": [
                [
                    "HRED",
                    "0.262",
                    "0.262",
                    "0.262",
                    "0.820",
                    "0.537",
                    "0.832",
                    "0.813",
                    "0.452",
                    "0.081",
                    "0.045",
                    "12.1"
                ],
                [
                    "SeqGAN",
                    "0.282",
                    "[BOLD] 0.282",
                    "0.282",
                    "0.817",
                    "0.515",
                    "0.748",
                    "0.705",
                    "0.521",
                    "0.070",
                    "0.052",
                    "[BOLD] 17.2"
                ],
                [
                    "CVAE",
                    "0.295",
                    "0.258",
                    "0.275",
                    "0.836",
                    "0.572",
                    "0.846",
                    "0.803",
                    "0.415",
                    "0.112",
                    "0.102",
                    "12.4"
                ],
                [
                    "CVAE-BOW",
                    "0.298",
                    "0.272",
                    "0.284",
                    "0.828",
                    "0.555",
                    "0.840",
                    "0.819",
                    "0.493",
                    "0.107",
                    "0.099",
                    "12.5"
                ],
                [
                    "CVAE-CO",
                    "0.299",
                    "0.269",
                    "0.283",
                    "0.839",
                    "0.557",
                    "0.855",
                    "0.863",
                    "0.581",
                    "0.111",
                    "0.110",
                    "10.3"
                ],
                [
                    "VHRED",
                    "0.253",
                    "0.231",
                    "0.242",
                    "0.810",
                    "0.531",
                    "0.844",
                    "[BOLD] 0.881",
                    "0.522",
                    "0.110",
                    "0.092",
                    "8.74"
                ],
                [
                    "VHCR",
                    "0.276",
                    "0.234",
                    "0.254",
                    "0.826",
                    "0.546",
                    "0.851",
                    "0.877",
                    "0.536",
                    "0.130",
                    "0.131",
                    "9.29"
                ],
                [
                    "DialogWAE",
                    "0.394",
                    "0.254",
                    "0.309",
                    "0.897",
                    "0.627",
                    "0.887",
                    "0.713",
                    "0.651",
                    "0.245",
                    "0.413",
                    "15.5"
                ],
                [
                    "DialogWAE-GMP",
                    "[BOLD] 0.420",
                    "0.258",
                    "[BOLD] \u20040.319",
                    "[BOLD] 0.925",
                    "[BOLD] \u20040.661",
                    "[BOLD] \u20040.894",
                    "0.713",
                    "[BOLD] \u20040.671",
                    "[BOLD] 0.333",
                    "[BOLD] \u20040.555",
                    "15.2"
                ]
            ],
            "title": "Table 1: Performance comparison on the SwitchBoard dataset (P: n-gram precision, R: n-gram recall, A: Average, E: Extrema, G: Greedy, L: average length)"
        },
        "insight": "Tables 1 and 2 show the performance of DialogWAE and baselines on the two datasets. DialogWAE outperforms the baselines in the majority of the experiments. In terms of BLEU scores, DialogWAE (with a Gaussian mixture prior network) generates more relevant responses, with the average recall of 42.0% and 37.2% on both of the datasets. These are significantly higher than those of the CVAE baselines (29.9% and 26.5%). We observe a similar trend to the BOW embedding metrics."
    },
    {
        "id": "407",
        "table": {
            "header": [
                "Model",
                "Coherence",
                "Diversity",
                "Informative"
            ],
            "rows": [
                [
                    "CVAE-CO",
                    "14.4%",
                    "19.2%",
                    "24.8%"
                ],
                [
                    "VHCR",
                    "26.8%",
                    "22.4%",
                    "20.4%"
                ],
                [
                    "DialogWAE",
                    "27.6%",
                    "[BOLD] 29.2%",
                    "25.6%"
                ],
                [
                    "DialogWAE-GMP",
                    "[BOLD] 31.6%",
                    "[BOLD] 29.2%",
                    "[BOLD] 29.6%"
                ]
            ],
            "title": "Table 5: Human judgments for models trained on the Dailydialog dataset"
        },
        "insight": "To validate the previous results, we further conduct a human evaluation with Amazon Mechanical Turk. We randomly selected 50 dialogues from the test set of DailyDialog. For each dialogue context, we generated 10 responses from each of the four models. Responses for each context were inspected by 5 participants who were asked to choose the model which performs the best in regarding to coherence, diversity and informative while being blind to the underlying algorithms. The average percentages that each model was selected as the best to a specific criterion are shown in Table 5."
    },
    {
        "id": "408",
        "table": {
            "header": [
                "[EMPTY]",
                "BLEU",
                "Empathy",
                "Relevance",
                "Fluency"
            ],
            "rows": [
                [
                    "Gold",
                    "-",
                    "3.651",
                    "3.752",
                    "3.718"
                ],
                [
                    "Seq2Seq",
                    "2.29",
                    "3.013",
                    "3.173",
                    "2.967"
                ],
                [
                    "MultiSeq",
                    "[BOLD] 2.45",
                    "2.979",
                    "3.25",
                    "2.952"
                ],
                [
                    "RL Current",
                    "2.31",
                    "3.18",
                    "3.187",
                    "3.14"
                ],
                [
                    "RL Look-ahead",
                    "2.32",
                    "[BOLD] 3.327",
                    "[BOLD] 3.593",
                    "[BOLD] 3.327"
                ]
            ],
            "title": "Table 2: Comparison between our proposed methods and baselines. RL Look-ahead model achieves the highest score for all three aspects: Empathy, Relevance, and Fluency. MultiSeq model achieves highest BLEU score, but not significant."
        },
        "insight": "From the BLEU scores in Table 2, we can see that although MultiSeq has the highest BLEU score, all models have similar BLEU scores and the difference is not that significant. [CONTINUE] We can clearly notice from human evaluations shown in Table 2 that our model, RL Look-ahead, outperforms all of the others in all three evaluated categories."
    },
    {
        "id": "409",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] SParC Ques.Match",
                "[BOLD] SParC Int.Match",
                "[BOLD] CoSQL Ques.Match",
                "[BOLD] CoSQL Int.Match"
            ],
            "rows": [
                [
                    "SyntaxSQL-con",
                    "18.5",
                    "4.3",
                    "15.1",
                    "2.7"
                ],
                [
                    "CD-Seq2Seq",
                    "21.9",
                    "8.1",
                    "13.8",
                    "2.1"
                ],
                [
                    "EditSQL",
                    "33.0",
                    "16.4",
                    "22.2",
                    "5.8"
                ],
                [
                    "Ours",
                    "[BOLD] 41.8",
                    "[BOLD] 20.6",
                    "[BOLD] 33.5",
                    "[BOLD] 9.6"
                ],
                [
                    "EditSQL + BERT",
                    "47.2",
                    "29.5",
                    "40.0",
                    "11.0"
                ],
                [
                    "Ours + BERT",
                    "[BOLD] 52.6",
                    "[BOLD] 29.9",
                    "[BOLD] 41.0",
                    "[BOLD] 14.0"
                ]
            ],
            "title": "Table 1: We report the best performance observed in 5 runs on the development sets of both SParC and CoSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the bold results show the improvements of our model are significant with p < 0.005."
        },
        "insight": "Taking CONCAT as a representative, we compare the performance of our model with other models, as shown in Table 1. As illustrated, our model outperforms baselines by a large margin with or without BERT, achieving new SOTA performances on both datasets. Compared with the previous SOTA without BERT on SPARC, our model improves Ques.Match and Int.Match by 10.6 and 5.4 points, respectively."
    },
    {
        "id": "410",
        "table": {
            "header": [
                "[EMPTY]",
                "Detection (Average Precision) BLESS",
                "Detection (Average Precision) EVAL",
                "Detection (Average Precision) LEDS",
                "Detection (Average Precision) SHWARTZ",
                "Detection (Average Precision) WBLESS",
                "Direction (Average Accuracy) BLESS",
                "Direction (Average Accuracy) WBLESS",
                "Direction (Average Accuracy) BIBLESS"
            ],
            "rows": [
                [
                    "Count based p(x,y)",
                    ".49",
                    ".38",
                    ".71",
                    ".29",
                    ".74",
                    ".46",
                    ".69",
                    ".62"
                ],
                [
                    "ppmi(x,y)",
                    ".45",
                    ".36",
                    ".70",
                    ".28",
                    ".72",
                    ".46",
                    ".68",
                    ".61"
                ],
                [
                    "SVD ppmi(x,y)",
                    ".76",
                    ".48",
                    ".84",
                    ".44",
                    ".96",
                    ".96",
                    ".87",
                    ".85"
                ],
                [
                    "HyperbolicCones",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    ".89",
                    "[BOLD] .50",
                    "[BOLD] .98",
                    ".94",
                    ".90",
                    ". [BOLD] 87"
                ],
                [
                    "Proposed SPON",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    "[BOLD] .91",
                    "[BOLD] .50",
                    "[BOLD] .98",
                    "[BOLD] .97",
                    "[BOLD] .91",
                    "[BOLD] .87"
                ]
            ],
            "title": "Table 3: Results on the unsupervised hypernym detection and direction prediction tasks. The first three rows of results are from Roller, Kiela, and Nickel (2018). The HyperbolicCones results were reported by Le et al. (2019). The improvements in LEDS and BLESS benchmark are statistically significant with two-tailed p values being 0.019 and \u2264 0.001 respectively."
        },
        "insight": "Table 3 shows the results on the unsupervised tasks of hypernym detection and direction predictions, reporting average precision and average accuracy, respectively. The first row titled Count based (in Table 3) depicts the performance of a Hearst-like Pattern system baseline, that uses a frequency based threshold to classify candidate hyponym-hypernym pairs as positive (i.e. exhibiting hypernymy) or negative (i.e. not exhibiting hypernymy). The ppmi approach in Table 3 builds upon the Count based approach [CONTINUE] by using Pointwise Mutual Information values for classification. SVD ppmi approach, the main contribution from Roller, Kiela, and Nickel (2018) builds low-rank embeddings of the PPMI matrix, which allows to make predictions for unseen pairs as well. HyperbolicCones is the SOTA (Le et al. 2019) in both these tasks. The final row reports the application of SPON (on the input provided by SVD ppmi) which is an original contribution of our work."
    },
    {
        "id": "411",
        "table": {
            "header": [
                "[EMPTY]",
                "BLESS",
                "EVAL",
                "LEDS",
                "WBLESS"
            ],
            "rows": [
                [
                    "[ITALIC] RELU+ [ITALIC] Residual",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    "[BOLD] .91",
                    "[BOLD] .98"
                ],
                [
                    "[ITALIC] RELU Only",
                    ".73",
                    ".49",
                    ".82",
                    ".96"
                ],
                [
                    "[ITALIC] Tanh+ [ITALIC] Residual",
                    ".79",
                    ".49",
                    ".90",
                    "[BOLD] .98"
                ]
            ],
            "title": "Table 4: Ablation tests reporting Average Precision values on the unsupervised hypernym detection task, signifying the choice of layers utilized in our proposed SPON model. The first row represents SPON i.e.\u00a0a RELU layer followed by a Residual connection. The second row removes the Residual connection, whereas the third row substitutes the non-negative activation layer RELU with Tanh that can take negative values."
        },
        "insight": "The analysis in Section 4 which shows that our choice of function f satisfies asymmetry and transitive properties, holds true because f satisfies f ((cid:126)x) \u2265 (cid:126)x component-wise. [CONTINUE] Table 4 shows the results for each of these ablation experiments, when evaluated on the unsupervised hypernym detection task across four datasets chosen randomly. Removing the Residual layer and using RELU activation function only, violates the aforementioned component-wise inequality f ((cid:126)x) \u2265 (cid:126)x, and has the worst results out of the three. On the other hand, using Residual connections with Tanh activations may not violate the aforementioned inequality, since, it depends upon the sign of the activation outputs. This argument is supported by the results in Table 4, wherein using Tanh activations instead of RELU almost provides identical results, except for the BLESS dataset. Nevertheless, the results in Table 4 show that encouraging asymmetry and transitive properties for this task, in fact improves the results as opposed to not doing the same."
    },
    {
        "id": "412",
        "table": {
            "header": [
                "Method",
                "Average Precision"
            ],
            "rows": [
                [
                    "OE ",
                    "0.761"
                ],
                [
                    "Smoothed Box ",
                    "0.795"
                ],
                [
                    "SPON (Our Approach)",
                    "[BOLD] 0.811"
                ]
            ],
            "title": "Table 5: Results on the unsupervised hypernym detection task for BLESS dataset. With 13,089 test instances, the improvement in Average Precision values obtained by SPON as compared against Smoothed Box model is statistically significant with two-tailed p value equals 0.00116."
        },
        "insight": "Furthermore, Table 5 illustrates the results on the unsupervised hypernym detection task for BLESS dataset, wherein we compare our proposed SPON model to other supervised SOTA approaches for hypernym prediction task, namely Order Embeddings (OE) approach as introduced by (Vendrov et al. 2016), and Smoothed Box model as introduced by (Li et al. 2019)."
    },
    {
        "id": "413",
        "table": {
            "header": [
                "Model",
                "R-1",
                "R-2"
            ],
            "rows": [
                [
                    "First sentences",
                    "28.6",
                    "17.3"
                ],
                [
                    "First  [ITALIC] k words",
                    "35.7",
                    "21.6"
                ],
                [
                    "Full\u00a0",
                    "42.2",
                    "24.9"
                ],
                [
                    "ML+RL+intra-attn\u00a0",
                    "42.94",
                    "26.02"
                ],
                [
                    "Two-Stage + RL\u00a0(Ours)",
                    "[BOLD] 45.33",
                    "[BOLD] 26.53"
                ]
            ],
            "title": "Table 2: Limited length ROUGE recall results on the NYT50 test set."
        },
        "insight": "Also we compare performance of our model with two recent models, we see 2.39 ROUGE-1 improvements compared to the ML+RL with intra-attn approach(previous SOTA) carries over to this dataset, which is a large margin. On ROUGE-2, our model also get an im- provement of 0.51. The experiment proves that our approach can outperform competitive methods on different data distributions."
    },
    {
        "id": "414",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Music MAP",
                "[BOLD] Music MRR",
                "[BOLD] Music P@5"
            ],
            "rows": [
                [
                    "CRIM",
                    "40.97",
                    "60.93",
                    "41.31"
                ],
                [
                    "SPON",
                    "[BOLD] 54.70",
                    "[BOLD] 71.20",
                    "[BOLD] 56.30"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Medical",
                    "[BOLD] Medical",
                    "[BOLD] Medical"
                ],
                [
                    "[EMPTY]",
                    "MAP",
                    "MRR",
                    "P@5"
                ],
                [
                    "CRIM",
                    "[BOLD] 34.05",
                    "[BOLD] 54.64",
                    "[BOLD] 36.77"
                ],
                [
                    "SPON",
                    "33.50",
                    "50.60",
                    "35.10"
                ]
            ],
            "title": "Table 7: Results on SemEval 2018 Domain-specific hypernym discovery task. CRIM is the best system on the domain specific datasets."
        },
        "insight": "Similarly, Table 7 shows the results on the two domain-specific tasks of music and medical domain corpora. SPON outperforms the SOTA systems in all tasks except for the medical domain in which it achieves comparable results."
    },
    {
        "id": "415",
        "table": {
            "header": [
                "[BOLD] Length",
                "1-3",
                "4-6",
                "7-10"
            ],
            "rows": [
                [
                    "Real Input",
                    "0.439",
                    "0.518",
                    "0.566"
                ],
                [
                    "Pre-trained Greedy",
                    "0.446",
                    "0.529",
                    "0.559"
                ],
                [
                    "RL Greedy",
                    "0.486",
                    "0.560",
                    "0.588"
                ],
                [
                    "RL BeamSearch(50)",
                    "0.599",
                    "0.678",
                    "0.709"
                ],
                [
                    "RL BeamSearch(200)",
                    "0.621",
                    "0.694",
                    "0.726"
                ]
            ],
            "title": "Table 1: Average embedding similarity scores between the output and the target output in terms of Real target output list."
        },
        "insight": "We calculate these similarity scores for each Real targeted output for the real inputs and the inputs found by the proposed model and report the average value in Table 1. According to the table, we observe that even inputting the real inputs, the similarity scores between the outputs and the target outputs are not high. Besides, with the crafted inputs from the proposed framework, these similarity scores are significantly improved. For example, for RL BeamSearch(200), the similarity is improved by 41.5%, 34.0% and 28.3% for the target outputs with length 1-3, 4-6 and 7-10, respectively."
    },
    {
        "id": "416",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] ROC",
                "[BOLD] SST",
                "[BOLD] Amazon"
            ],
            "rows": [
                [
                    "[BOLD] Hard",
                    "62.2 (4K)",
                    "75.5 (6K)",
                    "88.5 (67K)"
                ],
                [
                    "[BOLD] DAN",
                    "64.3 (91K)",
                    "83.1 (91K)",
                    "85.4 (91K)"
                ],
                [
                    "[BOLD] BiLSTM",
                    "65.2 (844K)",
                    "84.8 (1.5M)",
                    "[BOLD] 90.8 (844K)"
                ],
                [
                    "[BOLD] CNN",
                    "64.3 (155K)",
                    "82.2 (62K)",
                    "90.2 (305K)"
                ],
                [
                    "[BOLD] SoPa",
                    "[BOLD] 66.5 (255K)",
                    "[BOLD] 85.6 (255K)",
                    "90.5 (256K)"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1",
                    "64.4",
                    "84.8",
                    "90.0"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1\u2216{ [ITALIC] sl}",
                    "63.2",
                    "84.6",
                    "89.8"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1\u2216{ [ITALIC] \u03f5}",
                    "64.3",
                    "83.6",
                    "89.7"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1\u2216{ [ITALIC] sl, [ITALIC] \u03f5}",
                    "64.0",
                    "85.0",
                    "89.5"
                ]
            ],
            "title": "Table 1: Test classification accuracy (and the number of parameters used). The bottom part shows our ablation results: SoPa: our full model. SoPams1: running with max-sum semiring (rather than max-product), with the identity function as our encoder E (see Equation\u00a03). sl: self-loops, \u03f5: \u03f5 transitions. The final row is equivalent to a one-layer CNN."
        },
        "insight": "Table 1 shows our main experimental results. In two of the cases (SST and ROC), SoPa outperforms all models. On Amazon, SoPa performs within 0.3 points of CNN and BiLSTM, and outperforms the other two baselines. The table also shows the number of parameters used by each model for each task. [CONTINUE] SoPa performs better or roughly the same as a BiLSTM, which has 3\u20136 times as many parameters. [CONTINUE] Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and (cid:15)-transitions. The last line is equivalent to a CNN with [CONTINUE] multiple window sizes. Interestingly, the most notable difference between SoPa and CNN is the semiring and encoder function, while (cid:15) transitions and self-loops have little effect on performance.1"
    },
    {
        "id": "417",
        "table": {
            "header": [
                "[BOLD] Annotators",
                "[ITALIC] Ao",
                "[ITALIC] \u03ba"
            ],
            "rows": [
                [
                    "Questions",
                    "0.73",
                    "0.63"
                ],
                [
                    "Features",
                    "0.90",
                    "0.67"
                ],
                [
                    "Answers",
                    "0.59",
                    "0.49"
                ]
            ],
            "title": "Table 4: Cohen\u2019s Kappa score (\u03ba) and observed agreement (Ao) for gold standard dialogue"
        },
        "insight": "To evaluate the annotations, inter-annotator agreement was calculated based on a subset of the gold standard corpus.4 Table 4 illustrates the values of observed agreement (Ao) and Cohen's \u03ba (Cohen, 1960) obtained for question, feature and answer annotation. The agreement values obtained for question types were over 0.6 (for all annotators combined)."
    },
    {
        "id": "418",
        "table": {
            "header": [
                "Wikipedia",
                "Wikipedia",
                "Majority 16.7%",
                "Benchmark 46.7\u00b10.34%",
                "Doc2Vec 23.2\u00b11.41%",
                "Inceptionfixed 43.7\u00b10.51",
                "biLSTM 54.1\u00b10.47%",
                "Inception 57.0\u00b10.63%",
                "Joint  [BOLD] 59.4\u00b10.47%\u2020"
            ],
            "rows": [
                [
                    "arXiv",
                    "cs.ai",
                    "92.2%",
                    "92.6%",
                    "73.3\u00b19.81%",
                    "92.3\u00b10.29",
                    "91.5\u00b11.03%",
                    "92.8\u00b10.79%",
                    "[BOLD] 93.4\u00b11.07%\u2020"
                ],
                [
                    "arXiv",
                    "cs.cl",
                    "68.9%",
                    "75.7%",
                    "66.2\u00b18.38%",
                    "75.0\u00b11.95",
                    "76.2\u00b11.30%",
                    "76.2\u00b12.92%",
                    "[BOLD] 77.1\u00b13.10%"
                ],
                [
                    "arXiv",
                    "cs.lg",
                    "67.9%",
                    "70.7%",
                    "64.7\u00b19.08%",
                    "73.9\u00b11.23",
                    "[BOLD] 81.1\u00b10.83%",
                    "79.3\u00b12.94%",
                    "79.9\u00b12.54%"
                ]
            ],
            "title": "Table 3: Experimental results. The best result for each dataset is indicated in bold, and marked with \u201c\u2020\u201d if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p<0.05). The results of Benchmark on the arXiv dataset are from the original paper, where the standard deviation values were not reported. All neural models except for Inceptionfixed have larger standard deviation values on arXiv than Wikipedia, which can be explained by the small size of the arXiv test set."
        },
        "insight": "Experimental Results Table 3 shows the performance of the different models over our two datasets, in the form of the average accuracy on the test set (along with the standard deviation) over 10 runs, with different random initializations. [CONTINUE] On Wikipedia, we observe that the performance of BILSTM, INCEPTION, and JOINT is much better than that of [CONTINUE] all four baselines. INCEPTION achieves 2.9% higher accuracy than BILSTM. The performance of JOINT achieves an accuracy of 59.4%, which is 5.3% higher than using textual features alone (BILSTM) and 2.4% higher than using visual features alone (INCEPTION). Based on a one-tailed Wilcoxon signed-rank test, the performance of Jis sta [CONTINUE] of-the-art results in combination. For arXiv, baseline methods MAJORITY, BENCHMARK, and INCEPTIONFIXED outperform BILSTM over cs.ai, in large part because of the class imbalance in this dataset (90% of papers are rejected). Surprisingly, INCEPTIONFIXED is better than MAJORITY and BENCHMARK over the arXiv cs.lg subset, which verifies the usefulness of visual features, even when only the last layer is fine-tuned. Table 3 also shows that INCEPTION and BILSTM achieve similar performance on arXiv, showing that textual and visual representations are equally discriminative: INCEPTION and BILSTM are indistinguishable over cs.cl; BILSTM achieves 1.8% higher accuracy over cs.lg, while INCEPTION achieves 1.3% higher accuracy over cs.ai. Once again, the JOINT model achieves the highest accuracy on cs.ai and cs.cl by combining textual and visual representations (at a level of statistical significance for cs.ai). This, again, confirms that textual and visual [CONTINUE] state-of-the-art results. On arXiv cs.lg, JOINT achieves a 0.6% higher accuracy than INCEPTION by combining visual features and textual features, but BILSTM achieves the highest accuracy. One characteristic of cs.lg documents is"
    },
    {
        "id": "419",
        "table": {
            "header": [
                "Quality",
                "FA",
                "GA",
                "B",
                "C",
                "Start",
                "Stub"
            ],
            "rows": [
                [
                    "FA",
                    "397",
                    "83",
                    "20",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "GA",
                    "112",
                    "299",
                    "65",
                    "22",
                    "2",
                    "0"
                ],
                [
                    "B",
                    "23",
                    "53",
                    "253",
                    "75",
                    "44",
                    "7"
                ],
                [
                    "C",
                    "5",
                    "33",
                    "193",
                    "124",
                    "100",
                    "12"
                ],
                [
                    "Start",
                    "1",
                    "6",
                    "36",
                    "85",
                    "239",
                    "84"
                ],
                [
                    "Stub",
                    "0",
                    "0",
                    "6",
                    "7",
                    "63",
                    "345"
                ]
            ],
            "title": "Table 4: Confusion matrix of the Joint model on Wikipedia. Rows are the actual quality classes and columns are the predicted quality classes. The diagonal (gray cells) indicates correct predictions."
        },
        "insight": "Table 4 shows the confusion matrix of JOINT on Wikipedia. We can see that more than 50% of documents for each quality class are correctly classified, except for the [CONTINUE] class where more documents are misclassified into B."
    },
    {
        "id": "420",
        "table": {
            "header": [
                "Data set",
                "Train",
                "Test",
                "classes",
                "Task"
            ],
            "rows": [
                [
                    "AG news",
                    "120k",
                    "7.6k",
                    "4",
                    "English news categorization"
                ],
                [
                    "AG news",
                    "5k",
                    "7.6k",
                    "4",
                    "English news categorization"
                ],
                [
                    "Sogou news",
                    "450k",
                    "60k",
                    "5",
                    "Chinese news categorization"
                ],
                [
                    "Sogou news",
                    "10k",
                    "60k",
                    "5",
                    "Chinese news categorization"
                ]
            ],
            "title": "Table 1: Large-scale text classification data sets"
        },
        "insight": "We present our results on two freely available large scale datasets introduced by Zhang et al. . We also shrink the scale of training samples by randomly picking up the same amount of data in each category (see Table 1). [CONTINUE] AG News The AG News corpus consists of news articles from the AG's corpus of news articles on the web pertaining to the 4 largest classes, which are Work, Sports, Business, Sci/Tech. The data set contains 30,000 training samples for each class, 1,900 samples for each class for testing. In this data set, there are three columns which are label, title, description, we treat title as abstract input and description as contents input. Sogou News A Chinese news data set. This data set is a combination of the SogouCA and SogouCS news corpora pertaining to 5 categories, which are Sports, Finance, Entertainment, Automobile and Technology. It contains 450,000 training samples and 60,000 samples for testing in total. Sogou New also has three columns in data set files, label, title, description, similarly, we treat title as abstract input and description as contents input."
    },
    {
        "id": "421",
        "table": {
            "header": [
                "Data set",
                "AG",
                "Sogou",
                "AG(5k)",
                "Sogou(10k)"
            ],
            "rows": [
                [
                    "[ITALIC] - our model",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SeMemNN-ct",
                    "9.29",
                    "4.73",
                    "16.72",
                    "10.82"
                ],
                [
                    "SeMemNN-abs",
                    "9.04",
                    "4.62",
                    "15.32",
                    "9.80"
                ],
                [
                    "B-SeMemNN-ct",
                    "9.01",
                    "4.52",
                    "15.37",
                    "9.37"
                ],
                [
                    "B-SeMemNN-abs",
                    "8.68",
                    "4.19",
                    "14.35",
                    "8.76"
                ],
                [
                    "SAB-SeMemNN-ct",
                    "8.88",
                    "4.33",
                    "14.07",
                    "7.95"
                ],
                [
                    "SAB-SeMemNN-abs",
                    "8.37",
                    "3.67",
                    "13.79",
                    "7.89"
                ],
                [
                    "[ITALIC] - related studies",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Bow ",
                    "11.19",
                    "7.15",
                    "-",
                    "-"
                ],
                [
                    "Bow TFIDF ",
                    "10.36",
                    "6.55",
                    "-",
                    "-"
                ],
                [
                    "ngrams TFIDF ",
                    "7.64",
                    "2.81",
                    "-",
                    "-"
                ],
                [
                    "Bag-of-means ",
                    "16.91",
                    "10.79",
                    "-",
                    "-"
                ],
                [
                    "LSTM ",
                    "13.94",
                    "4.82",
                    "-",
                    "-"
                ],
                [
                    "char-CNN ",
                    "9.51",
                    "4.39",
                    "-",
                    "-"
                ],
                [
                    "VDCNN ",
                    "[BOLD] 8.67",
                    "[BOLD] 3.18",
                    "-",
                    "-"
                ],
                [
                    "VDCNN",
                    "10.64",
                    "6.53",
                    "19.25",
                    "unable"
                ],
                [
                    "XLNet ",
                    "4.49",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "ULMFiT ",
                    "5.01",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "CNN ",
                    "6.57",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "DPCNN ",
                    "6.87",
                    "1.84",
                    "-",
                    "-"
                ]
            ],
            "title": "Table 2: Testing error of our model and related studies"
        },
        "insight": "The experiment results are presented in Table 2. The best performances of our configurations are highlighted in red, which are 8.37, 3.67, 13.79 and 7.89 for the error rates of our proposed model on AG, Sogou, AG(5k), Sogou(10k) respectively. The bold numbers are the officially reported accuracy of VDCNN, to which our proposed model is close. The numbers in blue are the results coming from our comparison experiment by using VDCNN where we set the sequence length to 256 under word level. From these results we can see that our model outperforms VDCNN on AG News for the official error rate, and is very close to VDCNN's performance on Sogou News. If we set VCDNN with the same input sequence length (256) in word-level, the performance of our proposed model is obviously better. [CONTINUE] Most of the contributions come from external matrix From the results of Table 2, in the case of training with a large scale dataset, no matter we use simple LSTM or more complex bi-directional LSTM with self-attention, the testing error rates of different configurations are basically similar to each other. It can be said that such near state-of-the-art performance mainly attributes to the contribution from external memory. [CONTINUE] Using abstract to build the external memory is better than contents [CONTINUE] From Table 2 we can see that the results of using the description to construct the semantics matrix are better than using the abstract. [CONTINUE] SeMemNN can still work on a few-shot learning Table 2 shows that although we have greatly shrank the scale of the training set, our proposed method can still outperform VDCNN. After shrinking the scale of the data, the performance of VDCNN has been greatly decreased, especially for Sogou news, VDCNN has been unable to learn from the training samples."
    },
    {
        "id": "422",
        "table": {
            "header": [
                "[BOLD] Description",
                "[BOLD] Average"
            ],
            "rows": [
                [
                    "All models",
                    "91.51"
                ],
                [
                    "All prefix models",
                    "[BOLD] 94.43"
                ],
                [
                    "All postfix models",
                    "92.37"
                ],
                [
                    "All infix models",
                    "87.72"
                ],
                [
                    "All pre-trained models",
                    "91.06"
                ],
                [
                    "All non-pre-trained models",
                    "91.96"
                ],
                [
                    "All medium (type 1) models",
                    "90.49"
                ],
                [
                    "All small (type 2) models",
                    "92.09"
                ],
                [
                    "All minimal (type 3) models",
                    "91.94"
                ]
            ],
            "title": "TABLE II: Summary of BLEU scores from Table I."
        },
        "insight": "Table III provides detailed results of Experiment 2. The numbers are absolute accuracies, i.e., they correspond to cases where the arithmetic expression generated is 100% correct, leading to the correct numeric answer. Results by , , ,  are sparse but indicate the scale of success compared to recent past approaches. Prefix, postfix, and infix representations in Table III show that network capabilities are changed by how teachable the target data is. [CONTINUE] While our networks fell short of  AI2 testing accuracy, we present state-of-the-art results for the remaining three datasets. [CONTINUE] The type 2 postfix Transformer received the highest testing average of 87.2%. [CONTINUE] Our attempt at language pre-training fell short of our expectations in all but one tested dataset."
    },
    {
        "id": "423",
        "table": {
            "header": [
                "[BOLD] (Type) Model",
                "[BOLD] AI2",
                "[BOLD] CC",
                "[BOLD] IL",
                "[BOLD] MAWPS",
                "[BOLD] Average"
            ],
            "rows": [
                [
                    " Hosseini, et.al.",
                    "77.7",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u221777.7"
                ],
                [
                    " Kushman, et.al.",
                    "64.0",
                    "73.7",
                    "2.3",
                    "\u2013",
                    "\u221746.7"
                ],
                [
                    " Roy, et.al.",
                    "\u2013",
                    "\u2013",
                    "52.7",
                    "\u2013",
                    "\u221752.7"
                ],
                [
                    " Robaidek, et.al.",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "62.8",
                    "\u221762.8"
                ],
                [
                    " Wang, et.al.",
                    "[BOLD] 78.5",
                    "75.5",
                    "73.3",
                    "\u2013",
                    "\u221775.4"
                ],
                [
                    "[ITALIC] Pre-trained",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "(1) Prefix-Transformer",
                    "70.2",
                    "91.1",
                    "95.2",
                    "82.4",
                    "84.7"
                ],
                [
                    "(1) Postfix-Transformer",
                    "68.4",
                    "90.0",
                    "92.9",
                    "82.7",
                    "83.5"
                ],
                [
                    "(1) Infix-Transformer",
                    "75.4",
                    "74.4",
                    "64.3",
                    "56.4",
                    "67.6"
                ],
                [
                    "(2) Prefix-Transformer",
                    "66.7",
                    "91.1",
                    "[BOLD] 96.4",
                    "82.1",
                    "84.1"
                ],
                [
                    "(2) Postfix-Transformer",
                    "73.7",
                    "93.3",
                    "94.1",
                    "82.4",
                    "85.9"
                ],
                [
                    "(2) Infix-Transformer",
                    "75.4",
                    "75.6",
                    "66.7",
                    "59.0",
                    "69.2"
                ],
                [
                    "(3) Prefix-Transformer",
                    "70.2",
                    "91.1",
                    "95.2",
                    "82.4",
                    "84.7"
                ],
                [
                    "(3) Postfix-Transformer",
                    "73.7",
                    "92.2",
                    "94.1",
                    "82.1",
                    "85.5"
                ],
                [
                    "(3) Infix-Transformer",
                    "75.4",
                    "75.6",
                    "64.3",
                    "58.7",
                    "68.5"
                ],
                [
                    "[ITALIC] Non-pre-trained",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "(1) Prefix-Transformer",
                    "71.9",
                    "[BOLD] 94.4",
                    "95.2",
                    "83.4",
                    "86.3"
                ],
                [
                    "(1) Postfix-Transformer",
                    "73.7",
                    "81.1",
                    "92.9",
                    "75.7",
                    "80.8"
                ],
                [
                    "(1) Infix-Transformer",
                    "77.2",
                    "73.3",
                    "61.9",
                    "56.8",
                    "67.3"
                ],
                [
                    "(2) Prefix-Transformer",
                    "71.9",
                    "[BOLD] 94.4",
                    "94.1",
                    "[BOLD] 84.7",
                    "86.3"
                ],
                [
                    "(2) Postfix-Transformer",
                    "77.2",
                    "[BOLD] 94.4",
                    "94.1",
                    "83.1",
                    "[BOLD] 87.2"
                ],
                [
                    "(2) Infix-Transformer",
                    "77.2",
                    "76.7",
                    "66.7",
                    "61.5",
                    "70.5"
                ],
                [
                    "(3) Prefix-Transformer",
                    "71.9",
                    "93.3",
                    "95.2",
                    "84.1",
                    "86.2"
                ],
                [
                    "(3) Postfix-Transformer",
                    "77.2",
                    "94.4",
                    "94.1",
                    "82.4",
                    "87.0"
                ],
                [
                    "(3) Infix-Transformer",
                    "77.2",
                    "76.7",
                    "66.7",
                    "62.4",
                    "70.7"
                ]
            ],
            "title": "TABLE III: Test results for Experiment 2 (* denotes averages on present values only)."
        },
        "insight": "The values in the last column of Table III are summarized in Table IV. How the models compare with respect to accuracy closely resembles the comparison of BLEU scores, presented earlier. Thus, BLEU scores seem to correlate well with accuracy values in our case."
    },
    {
        "id": "424",
        "table": {
            "header": [
                "Model",
                "Joint Accuracy"
            ],
            "rows": [
                [
                    "NBT-DNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                    "0.844"
                ],
                [
                    "BT-CNN Ramadan et\u00a0al. ( 2018 )",
                    "0.855"
                ],
                [
                    "GLAD Zhong et\u00a0al. ( 2018 )",
                    "0.881"
                ],
                [
                    "GCE Nouri and Hosseini-Asl ( 2018 )",
                    "0.885"
                ],
                [
                    "StateNetPSI Ren et\u00a0al. ( 2018 )",
                    "0.889"
                ],
                [
                    "BERT+RNN (baseline 1)",
                    "0.892\u00a0(\u00b10.011)"
                ],
                [
                    "BERT+RNN+Ontology (baseline 2)",
                    "0.893\u00a0(\u00b10.013)"
                ],
                [
                    "Slot-dependent SUMBT (baseline 3)",
                    "0.891\u00a0(\u00b10.010)"
                ],
                [
                    "Slot-independent SUMBT (proposed)",
                    "[BOLD] 0.910\u00a0(\u00b10.010)"
                ]
            ],
            "title": "Table 1: Joint goal accuracy on the evaluation dataset of WOZ 2.0 corpus."
        },
        "insight": "The experimental results on WOZ 2.0 corpus are presented in Table 1. [CONTINUE] the three baseline models, BERT+RNN, BERT+RNN+Ontology, and the slot-dependent [CONTINUE] SUMBT, showed no significant performance differences. On the other hand, the slot-independent SUMBT which learned the shared information from all across domains and slots significantly outperformed those baselines, resulting in 91.0% joint accuracy. [CONTINUE] The proposed model achieved state-of-theart performance in both WOZ 2.0 and MultiWOZ datasets."
    },
    {
        "id": "425",
        "table": {
            "header": [
                "Model",
                "Joint Accuracy"
            ],
            "rows": [
                [
                    "Benchmark baseline",
                    "0.2583"
                ],
                [
                    "GLAD Zhong et\u00a0al. ( 2018 )",
                    "0.3557"
                ],
                [
                    "GCE Nouri and Hosseini-Asl ( 2018 )",
                    "0.3558"
                ],
                [
                    "SUMBT",
                    "[BOLD] 0.4240\u00a0(\u00b10.0187)"
                ]
            ],
            "title": "Table 2: Joint goal accuracy on the evaluation dataset of MultiWOZ corpus."
        },
        "insight": "Table 2 shows the experimental results of the slot-independent SUMBT model on MultiWOZ corpus. [CONTINUE] The SUMBT greatly surpassed the performances of previous approaches by yielding 42.4% joint accuracy."
    },
    {
        "id": "426",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Target Corpus",
                "[BOLD] Transfer BERT",
                "[BOLD] Transfer Filler",
                "[BOLD] Transfer Role",
                "[BOLD] Baseline Acc. (%)",
                "[BOLD] Fine-tuned Acc. (%)",
                "[BOLD] Gain (%)"
            ],
            "rows": [
                [
                    "BERT",
                    "QNLI",
                    "True",
                    "\u2013",
                    "\u2013",
                    "91.60",
                    "91.27",
                    "\u2013 0.33"
                ],
                [
                    "BERT",
                    "QQP",
                    "True",
                    "\u2013",
                    "\u2013",
                    "91.45",
                    "91.12",
                    "\u2013 0.33"
                ],
                [
                    "BERT",
                    "RTE",
                    "True",
                    "\u2013",
                    "\u2013",
                    "71.12",
                    "73.65",
                    "+ 2.53"
                ],
                [
                    "BERT",
                    "SNLI",
                    "True",
                    "\u2013",
                    "\u2013",
                    "90.45",
                    "90.69",
                    "+ 0.24"
                ],
                [
                    "BERT",
                    "SST",
                    "True",
                    "\u2013",
                    "\u2013",
                    "93.23",
                    "92.78",
                    "\u2013 0.45"
                ],
                [
                    "HUBERT (Transformer)",
                    "QNLI",
                    "True",
                    "True",
                    "False",
                    "90.56",
                    "91.16",
                    "[BOLD] + 0.60"
                ],
                [
                    "HUBERT (Transformer)",
                    "QQP",
                    "False",
                    "False",
                    "True",
                    "90.81",
                    "91.42",
                    "[BOLD] + 0.61"
                ],
                [
                    "HUBERT (Transformer)",
                    "RTE",
                    "True",
                    "True",
                    "True",
                    "61.73",
                    "74.01",
                    "[BOLD] + 12.28"
                ],
                [
                    "HUBERT (Transformer)",
                    "SNLI",
                    "True",
                    "False",
                    "True",
                    "90.66",
                    "91.36",
                    "[BOLD] + 0.70"
                ],
                [
                    "HUBERT (Transformer)",
                    "SST",
                    "True",
                    "False",
                    "True",
                    "91.28",
                    "92.43",
                    "[BOLD] + 1.15"
                ]
            ],
            "title": "Table 3: Transfer learning results for GLUE tasks. The source corpus is MNLI. Baseline accuracy is when Transfer BERT, Filler, and Role are all False, equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options."
        },
        "insight": "Table 3 summarizes the results for these transfer learning experiments when the source task is MNLI. Gain shows the difference between Fine-tuned model's accuracy and Baseline's accuracy. For HUBERT (Transformer), we observe substantial gain across all 5 target corpora after transfer. However, for BERT we have a drop for QNLI, QQP, and SST."
    },
    {
        "id": "427",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Target Corpus",
                "[BOLD] Transfer BERT",
                "[BOLD] Transfer Filler",
                "[BOLD] Transfer Role",
                "[BOLD] Baseline Acc. (%)",
                "[BOLD] Fine-tuned Acc. (%)",
                "[BOLD] Gain (%)"
            ],
            "rows": [
                [
                    "BERT",
                    "QNLI",
                    "True",
                    "\u2013",
                    "\u2013",
                    "91.60",
                    "90.96",
                    "\u2013 0.64"
                ],
                [
                    "BERT",
                    "MNLI",
                    "True",
                    "\u2013",
                    "\u2013",
                    "84.15",
                    "84.41",
                    "+ 0.26"
                ],
                [
                    "BERT",
                    "RTE",
                    "True",
                    "\u2013",
                    "\u2013",
                    "71.12",
                    "62.45",
                    "\u2013 8.67"
                ],
                [
                    "BERT",
                    "SNLI",
                    "True",
                    "\u2013",
                    "\u2013",
                    "90.45",
                    "90.88",
                    "+ 0.43"
                ],
                [
                    "BERT",
                    "SST",
                    "True",
                    "\u2013",
                    "\u2013",
                    "93.23",
                    "92.09",
                    "\u2013 1.14"
                ],
                [
                    "HUBERT (Transformer)",
                    "QNLI",
                    "False",
                    "True",
                    "True",
                    "88.32",
                    "90.55",
                    "[BOLD] + 2.23"
                ],
                [
                    "HUBERT (Transformer)",
                    "MNLI",
                    "False",
                    "True",
                    "True",
                    "84.30",
                    "85.24",
                    "[BOLD] + 0.94"
                ],
                [
                    "HUBERT (Transformer)",
                    "RTE",
                    "False",
                    "True",
                    "False",
                    "61.73",
                    "65.70",
                    "[BOLD] + 3.97"
                ],
                [
                    "HUBERT (Transformer)",
                    "SNLI",
                    "False",
                    "False",
                    "True",
                    "90.63",
                    "91.20",
                    "[BOLD] + 0.57"
                ],
                [
                    "HUBERT (Transformer)",
                    "SST",
                    "True",
                    "True",
                    "True",
                    "86.12",
                    "91.06",
                    "[BOLD] + 4.94"
                ]
            ],
            "title": "Table 4: Transfer learning results for GLUE tasks. The source corpus is QQP. Baseline accuracy is for when Transfer BERT, Filler, and Role are all False, which is equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options."
        },
        "insight": "Again transferring roles gives positive results except for RTE. Filler vectors learned from QQP are more transferable compared to MNLI and gives a boost to all tasks except for SNLI. Surprisingly, transferring BERT parameters is hurting the results now even when TPR is present."
    },
    {
        "id": "428",
        "table": {
            "header": [
                "[BOLD] Source Corpus",
                "[BOLD] Target Corpus",
                "[BOLD] HUBERT Transfer BERT",
                "[BOLD] HUBERT Transfer Filler",
                "[BOLD] HUBERT Transfer Role",
                "[BOLD] BERT Acc. (%)",
                "[BOLD] HUBERT Acc. (%)"
            ],
            "rows": [
                [
                    "MNLI",
                    "QNLI",
                    "True",
                    "True",
                    "False",
                    "[BOLD] 90.50",
                    "[BOLD] 90.50"
                ],
                [
                    "MNLI",
                    "QQP",
                    "False",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 89.30"
                ],
                [
                    "MNLI",
                    "RTE",
                    "True",
                    "True",
                    "True",
                    "66.40",
                    "[BOLD] 69.30"
                ],
                [
                    "MNLI",
                    "SNLI",
                    "True",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 90.35"
                ],
                [
                    "MNLI",
                    "SST",
                    "True",
                    "False",
                    "True",
                    "[BOLD] 93.50",
                    "92.60"
                ],
                [
                    "QQP",
                    "QNLI",
                    "False",
                    "True",
                    "True",
                    "90.50",
                    "[BOLD] 90.70"
                ],
                [
                    "QQP",
                    "MNLI",
                    "False",
                    "True",
                    "True",
                    "84.60",
                    "[BOLD] 84.70"
                ],
                [
                    "QQP",
                    "RTE",
                    "False",
                    "True",
                    "False",
                    "[BOLD] 66.40",
                    "63.20"
                ],
                [
                    "QQP",
                    "SNLI",
                    "False",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 90.36"
                ],
                [
                    "QQP",
                    "SST",
                    "True",
                    "True",
                    "True",
                    "[BOLD] 93.50",
                    "91.00"
                ]
            ],
            "title": "Table 5: Test set results for HUBERT (Transformer) and BERT. BERT accuracy indicates test results on target corpus (without transfer) for bert-base-uncased which are directly taken from the GLUE leaderboard. Fine-tuned accuracy are the test results for best performing HUBERT (Transformer) model on target dev set after transfer (see Tables 3 and 4)."
        },
        "insight": "We also verified that our TPR layer is not hurting the performance by comparing the test set results for HUBERT (Transformer) and BERT. The results are obtained by submitting models to the GLUE evaluation server. The results are presented in Table 5."
    },
    {
        "id": "429",
        "table": {
            "header": [
                "Model",
                "Model",
                "Mortality AUPRC",
                "Mortality AUROC",
                "Primary CCS Top-1 Recall",
                "Primary CCS Top-5 Recall",
                "All ICD-9 AUPRC",
                "All ICD-9 AUROC, weighted"
            ],
            "rows": [
                [
                    "No notes",
                    "-",
                    "0.449 (0.006)",
                    "0.869 (0.001)",
                    "0.526 (0.006)",
                    "0.796 (0.003)",
                    "0.305 (0.001)",
                    "0.873 (<0.001)"
                ],
                [
                    "Bag-of-words",
                    "Unigrams (notes only)",
                    "0.383 (0.004)",
                    "0.832 (0.003)",
                    "0.591 (0.004)",
                    "0.849 (0.002)",
                    "0.328 (0.002)",
                    "0.880 (0.001)"
                ],
                [
                    "[EMPTY]",
                    "Unigrams (all features)",
                    "[BOLD] 0.479 (0.008)",
                    "0.880 (0.001)",
                    "0.592 (0.003)",
                    "0.842 (0.003)",
                    "0.331 (0.001)",
                    "0.883 (0.001)"
                ],
                [
                    "[EMPTY]",
                    "Unigrams and bigrams (all features)",
                    "0.460 (0.005)",
                    "0.872 (0.002)",
                    "0.587 (0.008)",
                    "0.829 (0.005)",
                    "0.325 (0.002)",
                    "0.881 (<0.001)"
                ],
                [
                    "Hierarchical (without pretraining)",
                    "Notes only",
                    "0.351 (0.003)",
                    "0.825 (0.003)",
                    "0.606 (0.003)",
                    "0.850 (0.001)",
                    "0.345 (0.005)",
                    "0.887 (0.002)"
                ],
                [
                    "[EMPTY]",
                    "All features",
                    "0.471 (0.006)",
                    "0.876 (0.003)",
                    "0.591 (0.008)",
                    "0.833 (0.006)",
                    "0.301 (0.004)",
                    "0.868 (0.001)"
                ],
                [
                    "SHiP",
                    "Notes only",
                    "0.353 (0.005)",
                    "0.825 (0.004)",
                    "0.667 (0.006)",
                    "[BOLD] 0.897*\u2020 (0.003)",
                    "[BOLD] 0.352\u2020 (0.001)",
                    "[BOLD] 0.891\u2020 (0.001)"
                ],
                [
                    "[EMPTY]",
                    "All features",
                    "[BOLD] 0.479 (0.007)",
                    "[BOLD] 0.882 (0.001)",
                    "[BOLD] 0.671*\u2020 (0.004)",
                    "0.890 (0.001)",
                    "0.345 (0.005)",
                    "0.889 (0.002)"
                ]
            ],
            "title": "Table 1: Model performance results on the tasks of interest. Best values for each metric are bolded."
        },
        "insight": "Table 1 compares the performance of all model variants. The SHiP models significantly improved over the BOW baselines on the two diagnosis tasks (p < 0.001 under Welch's t-test): for CCS prediction, the best SHiP models improved top-1 recall by 7.9 percentage points and top-5 recall by 4.8 percentage points, respectively, over the best BOW models; for ICD-9 prediction, area under the precision-recall curve (AUPRC) increased by 2.1 percentage points and weighted area under the ROC curve (AUROC) increased by 0.8 percentage points. For mortality prediction, we saw negligible benefit from the SHiP architecture. The SHiP models also improved over the corresponding hierarchical models without pretraining. For mortality, pretraining the all-features model increased AUPRC by 0.8 percentage points (p = 0.06) and AUROC by 0.6 percentage points (p = 0.004); for primary CCS, pretraining the all-feature model increased top-1 recall by 8.0 percentage points (p < 0.001), while pretraining the notes-only model increased top-5 recall by 4.7 percentage points (p < 0.001); for all ICD-9, pretraining the notes-only model increased AUPRC by 0.7 percentage points (p = 0.03) and weighted AUROC by 0.4 percentage points (p = 0.01)."
    },
    {
        "id": "430",
        "table": {
            "header": [
                "Method",
                "# Params",
                "Inference",
                "SST-2",
                "MRPC",
                "QQP",
                "MNLI",
                "QNLI",
                "RTE",
                "Average"
            ],
            "rows": [
                [
                    "Method",
                    "# Params",
                    "Speedup",
                    "SST-2",
                    "MRPC",
                    "QQP",
                    "MNLI",
                    "QNLI",
                    "RTE",
                    "Average"
                ],
                [
                    "BERT12",
                    "109M",
                    "1x",
                    "93.5",
                    "88.9",
                    "71.2",
                    "84.6",
                    "90.5",
                    "66.4",
                    "82.5"
                ],
                [
                    "BERT12-T",
                    "109M",
                    "1x",
                    "93.3",
                    "88.7",
                    "71.1",
                    "84.8",
                    "90.4",
                    "66.1",
                    "82.4"
                ],
                [
                    "BERT6-PKD",
                    "67.0M",
                    "1.9x",
                    "92.0",
                    "85.0",
                    "70.7",
                    "81.5",
                    "[BOLD] 89.0",
                    "[BOLD] 65.5",
                    "[BOLD] 80.6"
                ],
                [
                    "BERT3-PKD",
                    "45.7M",
                    "3.7x",
                    "87.5",
                    "80.7",
                    "68.1",
                    "76.7",
                    "84.7",
                    "58.2",
                    "76.0"
                ],
                [
                    "DistilBERT4",
                    "52.2M",
                    "3.0x",
                    "91.4",
                    "82.4",
                    "68.5",
                    "78.9",
                    "85.2",
                    "54.1",
                    "76.8"
                ],
                [
                    "TinyBert4",
                    "14.5M",
                    "9.4x",
                    "[BOLD] 92.6",
                    "[BOLD] 86.4",
                    "[BOLD] 71.3",
                    "[BOLD] 82.5",
                    "87.7",
                    "62.9",
                    "[BOLD] 80.6"
                ],
                [
                    "BiLSTM [ITALIC] SOFT",
                    "10.1M",
                    "7.6x",
                    "90.7",
                    "-",
                    "68.2",
                    "73.0",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "AdaBERT",
                    "[BOLD] 6.4M \u223c 9.5M",
                    "[BOLD] 12.7x \u223c 29.3x",
                    "91.8",
                    "85.1",
                    "70.7",
                    "81.6",
                    "86.8",
                    "64.4",
                    "80.1"
                ]
            ],
            "title": "Table 1: The compression results including model efficiency and accuracy from the GLUE test server, and the MNLI result is evaluated for matched-accuracy (MNLI-m). BERT12 indicates the results of the fine-tuned BERT-base from Devlin et al. (2019) and BERT12-T indicates the results of the fine-tuned BERT-base in our implementation. The results of BERT-PKD are from Sun et al. (2019), the results of DistilBERT4 and TinyBERT4 are from Jiao et al. (2019), and the results of BiLSTMSOFT is from Tang et al. (2019). The number of model parameters includes the embedding size, and the inference time is tested with a batch size of 128 over 50,000 samples. The bold numbers and underlined numbers indicate the best and the second-best performance respectively."
        },
        "insight": "The compression results on the six adopted datasets, including parameter size, inference speedup and classification accuracy, are summarized in Table 1. [CONTINUE] Overall speaking, on all the evaluated datasets, the proposed AdaBERT method achieves significant efficiency improvement while maintaining comparable performance. Compared to the BERT12-T, the compressed models are 11.5x to 17.0x smaller in parameter size and 12.7x to 29.3x faster in inference speed with an average performance degradation of 2.79%. [CONTINUE] Comparing structureheterogeneous method, BiLSTMSOF [CONTINUE] , AdaBERT searches CNN-based models and achieves much another with better improvements, especially on the MNLI dataset. [CONTINUE] Comparing with different Transformers-based compression baselines, the proposed AdaBERT method is 1.35x to 3.12x faster than the fastest baseline, TinyBERT4, and achieves comparable performance with the two baselines that have the best averaged accuracy, BERT6-PKD and TinyBERT4."
    },
    {
        "id": "431",
        "table": {
            "header": [
                "StructureTask",
                "SST-2",
                "MRPC",
                "QQP",
                "MNLI",
                "QNLI",
                "RTE"
            ],
            "rows": [
                [
                    "AdaBERT-SST-2",
                    "[BOLD] 91.9",
                    "78.1",
                    "58.6",
                    "64.0",
                    "74.1",
                    "53.8"
                ],
                [
                    "AdaBERT-MRPC",
                    "81.5",
                    "[BOLD] 84.7",
                    "68.9",
                    "75.9",
                    "82.2",
                    "60.3"
                ],
                [
                    "AdaBERT-QQP",
                    "81.9",
                    "84.1",
                    "[BOLD] 70.5",
                    "76.3",
                    "82.5",
                    "60.5"
                ],
                [
                    "AdaBERT-MNLI",
                    "82.1",
                    "81.5",
                    "66.8",
                    "[BOLD] 81.3",
                    "86.1",
                    "63.2"
                ],
                [
                    "AdaBERT-QNLI",
                    "81.6",
                    "82.3",
                    "67.7",
                    "79.2",
                    "[BOLD] 87.2",
                    "62.9"
                ],
                [
                    "AdaBERT-RTE",
                    "82.9",
                    "81.1",
                    "66.5",
                    "79.8",
                    "86.0",
                    "[BOLD] 64.1"
                ],
                [
                    "Random",
                    "80.4 \u00b1 4.3",
                    "79.2 \u00b1 2.8",
                    "61.8 \u00b1 4.9",
                    "69.7 \u00b1 6.7",
                    "78.2 \u00b1 5.5",
                    "55.3 \u00b1 4.1"
                ]
            ],
            "title": "Table 3: Accuracy comparison on the dev sets with the searched compression structures applying to different tasks. For Random, 5-times averaging results with standard deviations are reported."
        },
        "insight": "The results of cross-task validation is summarized in Table 3. From Table 3, we can observe that: The searched structures achieve the best performance on their original target tasks compared with other tasks, in other words, the performance numbers along the diagonal line of this table are the best. Further, the performance degradation is quite significant across different kinds of tasks (for example, applying the searched structures of sentiment classification tasks to entailment recognition task, or vice verse), while the performance degradations within the same kind [CONTINUE] of tasks (for example, MRPC and QQP for semantic equivalence classification) are relatively small, since they have the same input format (i.e., a pair of sentences) and similar targets. [CONTINUE] From the last row of Table 3, we can see that the randomly sampled structures perform worse than the searched structures and their performances are not stable."
    },
    {
        "id": "432",
        "table": {
            "header": [
                "[EMPTY]",
                "SST-2",
                "MRPC",
                "QNLI",
                "RTE"
            ],
            "rows": [
                [
                    "[ITALIC] \u03b2 = 0",
                    "91.8",
                    "84.5",
                    "87.1",
                    "63.9"
                ],
                [
                    "[ITALIC] \u03b2 = 0",
                    "(7.5M)",
                    "(7.8M)",
                    "(8.3M)",
                    "(9.1M)"
                ],
                [
                    "[ITALIC] \u03b2 = 4",
                    "[BOLD] 91.9",
                    "[BOLD] 84.7",
                    "[BOLD] 87.2",
                    "[BOLD] 64.1"
                ],
                [
                    "[ITALIC] \u03b2 = 4",
                    "(6.4M)",
                    "(7.5M)",
                    "(7.9M)",
                    "(8.6M)"
                ],
                [
                    "[ITALIC] \u03b2 = 8",
                    "91.3",
                    "84.2",
                    "86.4",
                    "63.3"
                ],
                [
                    "[ITALIC] \u03b2 = 8",
                    "(5.3M)",
                    "(6.4M)",
                    "(7.1M)",
                    "(7.8M)"
                ]
            ],
            "title": "Table 5: The effect of efficiency loss term."
        },
        "insight": "The model performance and corresponding model size are reported in Table 5. On the one hand, removing the efficiency-aware loss (\u03b2 = 0) leads to the increase in model parameter size, on the other hand, a more aggressive efficiency preference (\u03b2 = 8) results in the small model size but degraded performance, since a large \u03b2 encourages the compressed model to adopt more lightweight operations such as zero and skip which hurt the performance. A moderate efficiency constraint (\u03b2 = 4) provides a regularization, guiding the AdaBERT method to achieve a trade-off between the small parameter size and the good performance."
    },
    {
        "id": "433",
        "table": {
            "header": [
                "[EMPTY]",
                "SST-2",
                "MRPC",
                "QNLI",
                "RTE"
            ],
            "rows": [
                [
                    "Base-KD",
                    "86.6",
                    "77.2",
                    "82.0",
                    "56.7"
                ],
                [
                    "+ Probe",
                    "88.4",
                    "78.7",
                    "83.3",
                    "58.1"
                ],
                [
                    "+ DA",
                    "91.4",
                    "83.9",
                    "86.5",
                    "63.2"
                ],
                [
                    "+ L [ITALIC] CE (All)",
                    "91.9",
                    "84.7",
                    "87.2",
                    "64.1"
                ]
            ],
            "title": "Table 4: The effect of knowledge loss terms."
        },
        "insight": "The Base-KD is a naive knowledge distillation version in which only the logits of the last layer are distilled without considering hidden layer knowledge and supervised label knowledge. By incorporating the probe models, the performance (line 2 in Table 4) is consistently improved, indicating the benefits from hierarchically decomposed taskoriented knowledge. We then leverage Data Augmentation (DA) to enrich task-oriented knowledge and this technique also improves performance for all tasks, especially for tasks that have a limited scale of data (i.e., MRPC and RTE). DA is also adopted in existing KD-based compression studies (Tang et al., 2019; Jiao et al., 2019). When taking the supervised label knowledge (LCE) into consideration, the performance is further boosted, showing that this term is also important for AdaBERT by providing focused search hints."
    },
    {
        "id": "434",
        "table": {
            "header": [
                "Task Metric",
                "BA\u2191",
                "F1\u2191",
                "MAE\u2193",
                "Corr\u2191"
            ],
            "rows": [
                [
                    "SOTA3",
                    "77.1",
                    "77.0",
                    "0.968",
                    "0.625"
                ],
                [
                    "SOTA2",
                    "77.4",
                    "77.3",
                    "0.965",
                    "0.632"
                ],
                [
                    "SOTA1",
                    "78.4",
                    "78.0",
                    "0.922",
                    "0.681"
                ],
                [
                    "BERT",
                    "83.36",
                    "85.53",
                    "0.736",
                    "0.777"
                ],
                [
                    "M-BERT",
                    "[BOLD] 84.38",
                    "[BOLD] 86.34",
                    "[BOLD] 0.732",
                    "[BOLD] 0.790"
                ],
                [
                    "\u0394 [ITALIC] SOTA",
                    "\u2191  [BOLD] 5.98",
                    "\u2191  [BOLD] 8.34",
                    "\u2193  [BOLD] 0.19",
                    "\u2191 [BOLD] 0.11"
                ]
            ],
            "title": "Table 1: Sentiment prediction results on CMU-MOSI. SOTA1, SOTA2 and SOTA3 refer to the previous best, second best and third best state of the art models respectively. Best results are highlighted in bold and \u0394SOTA represents the change in performance of M-BERT model over SOTA1. Our model significantly outperforms the current SOTA across all evaluation metrics."
        },
        "insight": "Our proposed approach sets a new state of the art of 84.38% binary accuracy on CMU-MOSI dataset of multimodal sentiment analysis; a significant leap from previous state of [CONTINUE] We compare the performance of M-BERT with the following models on the multimodal sentiment analysis task: RMFN (SOTA1)1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage (Liang et al., 2018). MFN (SOTA2) synchronizes states of three separate LSTMs with a multi-view gated memory (Zadeh et al., 2018a). MARN (SOTA3) models view-specific interactions using hybrid LSTM memories and cross-modal interactions using a Multi-Attention Block(MAB) (Zadeh et al., 2018c). [CONTINUE] We perform two different evaluation tasks on CMU-MOSI datset: i) Binary Classification, and ii) Regression. We formulate it as a regression problem and report Mean-absolute Error (MAE) and the correlation of model predictions with true labels. Besides, we convert the regression outputs into categorical values to obtain binary classification accuracy (BA) and F1 score. [CONTINUE] The performances of M-BERT and BERT are described in Table 1. [CONTINUE] M-BERT model outperforms all the baseline models (described in Sec.4.4) on every evaluation metrics with large margin. It sets new state-of-the-art performance for this task and achieves 84.38% accuracy, a 5.98% increase with respect to the SOTA1 and 1.02% increase with respect to BERT (text-only). [CONTINUE] Even BERT (text-only) model achieves 83.36% accuracy, an increase of 4.96% from the SOTA1 78.4%, using text information only. It achieves higher performance in all evaluation metrics compare to SOTA1; reinforcing the expressiveness and utility of BERT contextual representation."
    },
    {
        "id": "435",
        "table": {
            "header": [
                "[BOLD] Length of sentence",
                "[BOLD] Attention",
                "[BOLD] Gaussian Mask only",
                "[BOLD] Gaussian Mask + RL model"
            ],
            "rows": [
                [
                    "4-7",
                    "1.46",
                    "1.13",
                    "1.33"
                ],
                [
                    "8-11",
                    "2.32",
                    "1.96",
                    "2.12"
                ],
                [
                    "12-15",
                    "2.86",
                    "2.43",
                    "2.61"
                ]
            ],
            "title": "Table 1: Inference times in ms"
        },
        "insight": "We find the inference times are lower in the former case with negligible difference in BLEU scores. [CONTINUE] The inference times are of the various models are mentioned in Table 1."
    },
    {
        "id": "436",
        "table": {
            "header": [
                "[BOLD] Score",
                "Omniglot",
                "Omniglot Classifier",
                "Imagenet",
                "Imagenet Classifier"
            ],
            "rows": [
                [
                    "0.2",
                    "24,304",
                    "2,144",
                    "1,280",
                    "2,048"
                ],
                [
                    "0.1",
                    "99,248",
                    "22,912",
                    "8,448",
                    "3,072"
                ],
                [
                    "0.05",
                    "160,608",
                    "43,328",
                    "111,872",
                    "8,960"
                ]
            ],
            "title": "Table 3: Number of images after which the ME Score falls below threshold."
        },
        "insight": "The results are summarized in Figure 6 and Table 3. The probability that a new image belongs to an unseen class P (N |t) is higher than the ME score of the classifier through most of the learning phase. Comparing the statistics of the datasets to the inductive biases in the classifiers, the ME score for the classifiers is substantially lower than the baseline ME measure in the dataset, P (N |t) (Table 3). For instance, the ImageNet classifier drops its ME score below 0.05 after about 8,960 images, while the approximate ME measure for the dataset shows that new classes are encountered at above this rate until at least 111,000 images."
    },
    {
        "id": "437",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Method",
                "[BOLD] en-de  [BOLD] Success Rate",
                "[BOLD] en-de  [BOLD] NOR",
                "[BOLD] en-fr  [BOLD] Success Rate",
                "[BOLD] en-fr  [BOLD] NOR"
            ],
            "rows": [
                [
                    "BLSTM",
                    "random + HotFlip",
                    "25.4%",
                    "0.23, 0.21",
                    "28.2%",
                    "0.21, 0.18"
                ],
                [
                    "BLSTM",
                    "Min-Grad + HotFlip",
                    "31.8%",
                    "0.22, 0.19",
                    "40.2%",
                    "0.19, 0.17"
                ],
                [
                    "BLSTM",
                    "random + Soft-Att",
                    "61.2%",
                    "0.58, 0.62",
                    "64.6%",
                    "0.62, 0.67"
                ],
                [
                    "BLSTM",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 67.8%",
                    "0.58, 0.61",
                    "[BOLD] 70.8%",
                    "0.61, 0.66"
                ],
                [
                    "Transformer",
                    "random + HotFlip",
                    "35.0%",
                    "0.26, 0.24",
                    "40.6%",
                    "0.24, 0.21"
                ],
                [
                    "Transformer",
                    "Min-Grad + HotFlip",
                    "45.0%",
                    "0.26, 0.24",
                    "44.0%",
                    "0.23, 0.21"
                ],
                [
                    "Transformer",
                    "random + Soft-Att",
                    "50.2%",
                    "0.40, 0.39",
                    "59.0%",
                    "0.37, 0.35"
                ],
                [
                    "Transformer",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 61.6%",
                    "0.41, 0.42",
                    "[BOLD] 64.8%",
                    "0.36, 0.34"
                ]
            ],
            "title": "TABLE IV: Success rate (in %) and number of replacements for different methods. NOR represents the mean/median of the normalized Number Of Replacements across all the sentences. The highest Success rate is marked in bold."
        },
        "insight": "Table IV shows the success rate and the mean, median of the number of replacements (normalized by the length of original sentence) for different methods. [CONTINUE] As we can see from Table IV, for both Hotflip and Soft-Att, Min-Grad method gives significant improvement in success rate in comparison with random baseline across all the NMT models. The number of replacement for Min-Grad is comparable with random. [CONTINUE] Table IV shows that Transformer is more robust to our proposed method than BLSTM. This is because our proposed method has less number of replacements and lower success rate in case of Transformer than BLSTM for both the language pairs. Interestingly, HotFlip has higher success rate and similar number of replacement in case for Transformer than BLSTM. Overall, as is evident from Table IV, our proposed method (Min-Grad + Soft-Att) achieves the highest success rate across the NMT models. [CONTINUE] From Table IV, across all the NMT models, we can see that Soft-Att significantly outperforms HotFlip both in terms of success rate and number of replacements."
    },
    {
        "id": "438",
        "table": {
            "header": [
                "[BOLD] Transformer",
                "[BOLD] Method",
                "src",
                "[ITALIC] l1",
                "[ITALIC] l2",
                "[ITALIC] lblstm1",
                "[ITALIC] lblstm2"
            ],
            "rows": [
                [
                    "[BOLD] en-de",
                    "random + HotFlip",
                    "51.04",
                    "80.49",
                    "47.53",
                    "36.42",
                    "43.66"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + HotFlip",
                    "53.23",
                    "83.13",
                    "49.15",
                    "36.51",
                    "44.76"
                ],
                [
                    "[BOLD] en-de",
                    "random + Soft-Att",
                    "32.01",
                    "84.79",
                    "[BOLD] 29.72",
                    "[BOLD] 20.62",
                    "27.85"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 31.17",
                    "[BOLD] 88.55",
                    "31.09",
                    "20.63",
                    "[BOLD] 27.43"
                ],
                [
                    "[BOLD] en-fr",
                    "random + HotFlip",
                    "55.51",
                    "85.18",
                    "40.35",
                    "52.00",
                    "36.18"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + HotFlip",
                    "57.92",
                    "88.40",
                    "41.98",
                    "54.39",
                    "37.68"
                ],
                [
                    "[BOLD] en-fr",
                    "random + Soft-Att",
                    "[BOLD] 33.61",
                    "89.77",
                    "[BOLD] 21.59",
                    "[BOLD] 32.37",
                    "[BOLD] 19.09"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + Soft-Att",
                    "35.40",
                    "[BOLD] 91.99",
                    "23.28",
                    "34.32",
                    "20.29"
                ]
            ],
            "title": "TABLE V: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other Transformer model. lblstm1,lblstm2 are the BLSTM counterparts of l1 and l2."
        },
        "insight": "1 2 , lblstm Table V shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table V, l1 denotes the Transformer model under attack (e.g. en-de), l2 denotes the other Transformer model (e.g. en-fr), and lblstm are the BLSTM counterparts of l1 and l2."
    },
    {
        "id": "439",
        "table": {
            "header": [
                "[BOLD] BLSTM",
                "[BOLD] Method",
                "src",
                "[ITALIC] l1",
                "[ITALIC] l2",
                "[ITALIC] ltrans1",
                "[ITALIC] ltrans2"
            ],
            "rows": [
                [
                    "[BOLD] en-de",
                    "random + HotFlip",
                    "57.09",
                    "71.35",
                    "48.84",
                    "43.90",
                    "49.42"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + HotFlip",
                    "59.28",
                    "75.55",
                    "50.38",
                    "45.96",
                    "52.26"
                ],
                [
                    "[BOLD] en-de",
                    "random + Soft-Att",
                    "[BOLD] 13.77",
                    "87.14",
                    "[BOLD] 19.20",
                    "[BOLD] 18.36",
                    "[BOLD] 21.62"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + Soft-Att",
                    "14.49",
                    "[BOLD] 89.86",
                    "19.74",
                    "18.51",
                    "21.98"
                ],
                [
                    "[BOLD] en-fr",
                    "random + HotFlip",
                    "60.87",
                    "79.62",
                    "39.28",
                    "58.60",
                    "41.73"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + HotFlip",
                    "63.97",
                    "84.87",
                    "41.16",
                    "61.80",
                    "44.94"
                ],
                [
                    "[BOLD] en-fr",
                    "random + Soft-Att",
                    "12.99",
                    "92.44",
                    "10.62",
                    "28.34",
                    "12.12"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 12.66",
                    "[BOLD] 93.87",
                    "[BOLD] 9.95",
                    "[BOLD] 27.21",
                    "[BOLD] 11.92"
                ]
            ],
            "title": "TABLE VI: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other BLSTM model. ltrans1,ltrans2 are the Transformer counterparts of l1 and l2."
        },
        "insight": "Table VI shows the BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. In Table VI, l1 denotes the BLSTM model under attack, l2 denotes the other BLSTM model, and ltrans are the Transformer counterparts of l1 and l2. , ltrans 1 2"
    },
    {
        "id": "440",
        "table": {
            "header": [
                "% noised",
                "SacreBleu Newstest \u201912",
                "SacreBleu Newstest \u201917"
            ],
            "rows": [
                [
                    "0%",
                    "22.4",
                    "28.1"
                ],
                [
                    "20%",
                    "22.4",
                    "27.9"
                ],
                [
                    "80%",
                    "21.5",
                    "27.0"
                ],
                [
                    "100%",
                    "21.2",
                    "25.6"
                ]
            ],
            "title": "Table 2: SacreBleu degradation as a function of the proportion of bitext data that is noised."
        },
        "insight": "We first show that noising EnDe bitext sources does not seriously impact the translation quality of the transformer-base baseline. [CONTINUE] Table 2 shows results for various values of p. Specifically, it presents the somewhat unexpected finding that even when noising 100% of the source bitext (so the model has never seen wellformed English), BLEU on well-formed test data only drops by 2.5."
    },
    {
        "id": "441",
        "table": {
            "header": [
                "a. Forward models (EnRo) Model",
                "a. Forward models (EnRo) dev",
                "a. Forward models (EnRo) test"
            ],
            "rows": [
                [
                    "Gehring et\u00a0al. ( 2017 )",
                    "[EMPTY]",
                    "[ITALIC] 29.9"
                ],
                [
                    "Sennrich 2016 (BT)",
                    "[ITALIC] 29.3",
                    "[ITALIC] 28.1"
                ],
                [
                    "bitext",
                    "26.5",
                    "28.3"
                ],
                [
                    "BT",
                    "31.6",
                    "32.6"
                ],
                [
                    "NoisedBT",
                    "29.9",
                    "32.0"
                ],
                [
                    "TaggedBT",
                    "30.5",
                    "33.0"
                ],
                [
                    "It.-3 BT",
                    "31.3",
                    "32.8"
                ],
                [
                    "It.-3 NoisedBT",
                    "31.2",
                    "32.6"
                ],
                [
                    "It.-3 TaggedBT",
                    "31.4",
                    "[BOLD] 33.4"
                ],
                [
                    "b. Reverse models (RoEn)",
                    "b. Reverse models (RoEn)",
                    "b. Reverse models (RoEn)"
                ],
                [
                    "Model",
                    "dev",
                    "test"
                ],
                [
                    "bitext",
                    "32.9",
                    "31.9"
                ],
                [
                    "It.-2 BT",
                    "39.5",
                    "[BOLD] 37.3"
                ]
            ],
            "title": "Table 4: Comparing SacreBleu scores for different flavors of BT for WMT16 EnRo. Previous works\u2019 scores are reported in italics as they use detok.multi-bleu instead of SacreBleu, so are not guaranteed to be comparable. In this case, however, we do see identical Bleu on our systems when we score them with detok.multi-bleu, so we believe it to be a fair comparison."
        },
        "insight": "We repeat these experiments for WMT EnRo (Table 4). [CONTINUE] In this case, NoisedBT is actually harmful, lagging standard BT by -0.6 BLEU. TaggedBT closes this gap and passes standard BT by +0.4 BLEU, for a total gain of +1.0 BLEU over NoisedBT. [CONTINUE] We further investigate the effects of TaggedBT by performing one round of iterative backtranslation (Cotterell and Kreutzer, 2018; Vu Cong Duy Hoang and Cohn, 2018; Niu et al., 2018), and find another difference between the different varieties of BT: NoisedBT and TaggedBT allow the model to bootstrap improvements from an improved reverse model, whereas standard BT does not. [CONTINUE] SacreBLEU scores for all these models are displayed in Table 4. [CONTINUE] We find that the iteration-3 BT models improve over their Iteration-1 counterparts only for NoisedBT (+1.0 BLEU, dev+test avg) and TaggedBT (+0.7 BLEU, dev+test avg), whereas the Iteration-3 BT model shows no improvement over its Iteration-1 counterpart (-0.1 BLEU, dev+test avg). [CONTINUE] iteration-3 TaggedBT (Table 4), [CONTINUE] Our best BLEU score of 33.4 BLEU, obtained using Iterative TaggedBT, shows a gain of +3.5 BLEU over the highest previously published result on this test-set that we are aware of. [CONTINUE] We furthermore match or out-perform the highest published results we are aware of on WMT EnDe that use only back-translation, with higher or equal BLEU on five of seven test sets."
    },
    {
        "id": "442",
        "table": {
            "header": [
                "Model",
                "Avg",
                "2008",
                "2009",
                "2010",
                "2011",
                "2012",
                "2013",
                "2014",
                "2015"
            ],
            "rows": [
                [
                    "Bitext",
                    "32.8",
                    "26.3",
                    "28.8",
                    "32.0",
                    "32.9",
                    "30.1",
                    "33.5",
                    "40.6",
                    "38.4"
                ],
                [
                    "BT",
                    "29.2",
                    "22.2",
                    "27.3",
                    "28.8",
                    "29.3",
                    "27.9",
                    "30.7",
                    "32.6",
                    "34.8"
                ],
                [
                    "NoisedBT",
                    "33.8",
                    "26.8",
                    "29.9",
                    "33.4",
                    "[BOLD] 33.9",
                    "[BOLD] 31.3",
                    "34.3",
                    "42.3",
                    "38.8"
                ],
                [
                    "TaggedBT",
                    "[BOLD] 34.1",
                    "[BOLD] 27.0",
                    "[BOLD] 30.0",
                    "[BOLD] 33.6",
                    "[BOLD] 33.9",
                    "31.2",
                    "[BOLD] 34.4",
                    "[BOLD] 42.7",
                    "[BOLD] 39.8"
                ]
            ],
            "title": "Table 5: Results on WMT15 EnFr, with bitext, BT, NoisedBT, and TaggedBT."
        },
        "insight": "We performed a minimal set of experiments on WMT EnFr, which are summarized in Table 5. [CONTINUE] In this case, we see that BT alone hurts performance compared to the strong bitext baseline, but NoisedBT indeed surpasses the bitext model. [CONTINUE] It is worth noting that our numbers are lower than those reported by Edunov et al. (2018) on the years they report (36.1, 43.8, and 40.9 on 2013, 2014, and 2015 respectively). [CONTINUE] On WMT16 EnRo, TaggedBT improves on vanilla BT by 0.4 BLEU."
    },
    {
        "id": "443",
        "table": {
            "header": [
                "Model",
                "ASR0",
                "ASR| [ITALIC] x|",
                "~H"
            ],
            "rows": [
                [
                    "Bitext baseline",
                    "0.31",
                    "10.21",
                    "0.504"
                ],
                [
                    "BT",
                    "0.28",
                    "10.98",
                    "[BOLD] 0.455"
                ],
                [
                    "P3BT",
                    "0.37",
                    "7.66",
                    "0.558"
                ],
                [
                    "NoisedBT",
                    "1.01",
                    "3.96",
                    "0.619"
                ],
                [
                    "TaggedBT",
                    "[BOLD] 5.31",
                    "5.31",
                    "0.597"
                ],
                [
                    "TaggedNoisedBT",
                    "[BOLD] 7.33",
                    "7.33",
                    "0.491"
                ]
            ],
            "title": "Table 6: Attention sink ratio on the first and last token and entropy (at decoder layer 5) for the models in Table\u00a03.a, averaged over all sentences in newstest14. For ASR, data is treated as if it were BT (noised and/or tagged, resp.), whereas for entropy the natural text is used. Outliers discussed in the text are bolded."
        },
        "insight": "To understand how the model treats the tag and what biases it learns from the data, we investigate the entropy of the attention probability distribution, as well as the attention captured by the tag. [CONTINUE] For the tagged variants, there is heavy attention on the tag when it is present (Table 6), indicating that the model relies on the information signalled by the tag. [CONTINUE] Table 6 reports the average length-normalized Shannon entropy: [CONTINUE] The entropy of the attention probabilities from the model trained on BT data is the clear outlier."
    },
    {
        "id": "444",
        "table": {
            "header": [
                "Model",
                "Decode type",
                "AVG 13-17",
                "2010",
                "2011",
                "2012",
                "2013",
                "2014",
                "2015",
                "2016",
                "2017"
            ],
            "rows": [
                [
                    "TaggedBT",
                    "standard",
                    "[BOLD] 33.24",
                    "26.5",
                    "[BOLD] 24.2",
                    "[BOLD] 25.2",
                    "[BOLD] 28.7",
                    "[BOLD] 32.8",
                    "[BOLD] 34.5",
                    "[BOLD] 38.1",
                    "[BOLD] 32.4"
                ],
                [
                    "[EMPTY]",
                    "as BT (tagged)",
                    "30.30",
                    "24.3",
                    "22.2",
                    "23.4",
                    "26.6",
                    "30.0",
                    "30.5",
                    "34.2",
                    "30.2"
                ],
                [
                    "NoisedBT",
                    "standard",
                    "33.06",
                    "[BOLD] 26.7",
                    "24.0",
                    "[BOLD] 25.2",
                    "28.6",
                    "32.6",
                    "33.9",
                    "38.0",
                    "32.2"
                ],
                [
                    "[EMPTY]",
                    "as BT (noised)",
                    "10.66",
                    "8.1",
                    "6.5",
                    "7.5",
                    "8.2",
                    "11.1",
                    "10.0",
                    "12.7",
                    "11.3"
                ]
            ],
            "title": "Table 7: Comparing standard decoding with decoding as if the input were back-translated data, meaning that it is tagged (for the TaggedBT model) or noised (for the NoisedBT model) ."
        },
        "insight": "The BLEU scores of each decoding method are presented in Table 7. [CONTINUE] The noised decode \u2013 decoding newstest sentences with the NoisedBT model after noising the source \u2013 yields poor performance. [CONTINUE] The tagged decode, however, yields only somewhat lower performance than the standard decode on the same model (-2.9BLEU on average)."
    },
    {
        "id": "445",
        "table": {
            "header": [
                "Data",
                "src-tgt unigram overlap"
            ],
            "rows": [
                [
                    "TaggedBT (standard decode)",
                    "8.9%"
                ],
                [
                    "TaggedBT (tagged decode)",
                    "10.7%"
                ],
                [
                    "Bitext",
                    "5.9%"
                ],
                [
                    "BT Data",
                    "11.4 %"
                ]
            ],
            "title": "Table 9: Source-target overlap for both back-translated data with decoding newstest as if it were bitext or BT data. Model decodes are averaged over newstest2010-newstest2017."
        },
        "insight": "We quantify the copy rate with the unigram overlap between source and target as a percentage of tokens in the target side, and compare those statistics to the bitext and the back-translated data (Table 9)."
    },
    {
        "id": "446",
        "table": {
            "header": [
                "Class",
                "Number of samples"
            ],
            "rows": [
                [
                    "acq",
                    "2292"
                ],
                [
                    "crude",
                    "374"
                ],
                [
                    "earn",
                    "3923"
                ],
                [
                    "grain",
                    "51"
                ],
                [
                    "interest",
                    "271"
                ],
                [
                    "money-fx",
                    "293"
                ],
                [
                    "ship",
                    "144"
                ],
                [
                    "trade",
                    "326"
                ]
            ],
            "title": "TABLE I: Document distribution over the classes of the Reuters-8"
        },
        "insight": "We used the Reuters-8 dataset without stop words from  aiming at single-label classification, which is a preprocessed format of the Reuters-215782. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table I."
    },
    {
        "id": "447",
        "table": {
            "header": [
                "Method",
                "Feature",
                "Accuracy (%)",
                "Std. Deviation"
            ],
            "rows": [
                [
                    "SA",
                    "w2v",
                    "78.73",
                    "1.56"
                ],
                [
                    "MSM",
                    "w2v",
                    "[BOLD] 90.62",
                    "0.42"
                ],
                [
                    "TF-MSM",
                    "w2v",
                    "[BOLD] 92.01",
                    "0.30"
                ],
                [
                    "MVB",
                    "binBOW",
                    "62.70",
                    "0.69"
                ],
                [
                    "MNB",
                    "tfBOW",
                    "[BOLD] 91.47",
                    "0.37"
                ],
                [
                    "LSA",
                    "w2v",
                    "34.58",
                    "0.40"
                ],
                [
                    "LSA",
                    "binBOW",
                    "86.92",
                    "0.74"
                ],
                [
                    "LSA",
                    "tfBOW",
                    "86.23",
                    "0.96"
                ],
                [
                    "LSA",
                    "tfidfBOW",
                    "86.35",
                    "1.03"
                ],
                [
                    "SVM",
                    "w2v",
                    "26.61",
                    "0.30"
                ],
                [
                    "SVM",
                    "binBOW",
                    "89.23",
                    "0.24"
                ],
                [
                    "SVM",
                    "tfBOW",
                    "89.10",
                    "0.29"
                ],
                [
                    "SVM",
                    "tfidfBOW",
                    "88.78",
                    "0.40"
                ]
            ],
            "title": "TABLE II: Results for the text classification experiment on Reuters-8 database"
        },
        "insight": "In this experiment, we performed text classification among the classes in the Reuters-8 database. [CONTINUE] The results can be seen in Table II. The simplest baseline, SA with w2v, achieved an accuracy rate of 78.73%. This [CONTINUE] LSA with BOW features was almost 10% more accurate than SA, where the best results with binary weights were achieved with an approximation with 130 dimensions, with TF weights were achieved with 50 dimensions, and with TFIDF weights were achieved with 30 dimensions. SVM with BOW features was about 3% more accurate than LSA, with binary weights leading to a higher accuracy rate. [CONTINUE] Among the baselines, the best method was MNB with tfBOW features, with an accuracy of 91.47%, [CONTINUE] MSM with w2v had an accuracy rate of 90.62%, with the best results achieved with word subspace dimensions for the training classes ranging from 150 to 181, and for the query ranging from 3 to 217. Incorporating the frequency information in the subspace modeling resulted in higher accuracy, with TFMSM achieving 92.01%, with dimensions of word subspaces"
    },
    {
        "id": "448",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] System",
                "[BOLD] ERRANT  [BOLD] P",
                "[BOLD] ERRANT  [BOLD] R",
                "[BOLD] ERRANT  [BOLD] F0.5",
                "[BOLD] M2  [BOLD] P",
                "[BOLD] M2  [BOLD] R",
                "[BOLD] M2  [BOLD] F0.5"
            ],
            "rows": [
                [
                    "[BOLD] CoNLL-2014",
                    "Felice et\u00a0al. ( 2014 ) \u2020",
                    "-",
                    "-",
                    "-",
                    "39.71",
                    "30.10",
                    "37.33"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Yannakoudakis et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "58.79",
                    "30.63",
                    "49.66"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Chollampatt and Ng ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "62.74",
                    "32.96",
                    "53.14"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Chollampatt and Ng ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "65.49",
                    "33.14",
                    "54.79"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Ge et\u00a0al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 74.12",
                    "[BOLD] 36.30",
                    "[BOLD] 61.34"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Bryant and Briscoe ( 2018 )",
                    "36.62",
                    "19.93",
                    "31.37",
                    "40.56",
                    "20.81",
                    "34.09"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "BERT",
                    "33.27",
                    "[BOLD] 27.14",
                    "31.83",
                    "35.69",
                    "[BOLD] 27.99",
                    "33.83"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "GPT-1",
                    "49.58",
                    "27.06",
                    "42.5",
                    "51.08",
                    "27.45",
                    "43.57"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "GPT-2",
                    "[BOLD] 57.73",
                    "24.75",
                    "[BOLD] 45.58",
                    "[BOLD] 58.51",
                    "24.9",
                    "[BOLD] 46.08"
                ],
                [
                    "[BOLD] FCE",
                    "Yannakoudakis et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 65.03",
                    "32.45",
                    "[BOLD] 54.15"
                ],
                [
                    "[BOLD] FCE",
                    "Bryant and Briscoe ( 2018 )",
                    "41.92",
                    "13.62",
                    "29.61",
                    "44.78",
                    "14.12",
                    "31.22"
                ],
                [
                    "[BOLD] FCE",
                    "BERT",
                    "29.56",
                    "[BOLD] 34.67",
                    "30.46",
                    "31.97",
                    "[BOLD] 35.01",
                    "32.53"
                ],
                [
                    "[BOLD] FCE",
                    "GPT-1",
                    "[BOLD] 62.75",
                    "32.19",
                    "52.74",
                    "[BOLD] 64.01",
                    "32.33",
                    "[BOLD] 53.52"
                ],
                [
                    "[BOLD] FCE",
                    "GPT-2",
                    "61.91",
                    "33.47",
                    "[BOLD] 52.92",
                    "62.64",
                    "33.74",
                    "53.48"
                ]
            ],
            "title": "Table 2: Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction. For each of the datasets, we use the corresponding test set, and we do not train our models on the corpora. As BERT, we report the best performing BERT model (12 layers, retaining uppercase characters). In the top part of each dataset, we report the scores of supervised methods and in the bottom the unsupervised ones. \u2020 denotes this system won the shared task competition."
        },
        "insight": "Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by Bryant and Briscoe (2018). [CONTINUE] A key result of Table 2 is that Transformer Language Models prove to be more than just a competitive baseline to legitimate Grammatical Error Correction systems on their own. Across the board, Transformer Models are able to outperform the simple n-gram model and even approach the performance of supervised GEC systems. [CONTINUE] we see that their performance is nearly identical with GPT-2 leading by a small margin in the CoNLL14 dataset. [CONTINUE] BERT surpasses the n-gram baseline overall, it achieves worse performance than the rest in terms of precision and F0.5 score."
    },
    {
        "id": "449",
        "table": {
            "header": [
                "[BOLD] Reply Type",
                "[BOLD] Description",
                "[BOLD] Antichat",
                "[BOLD] Hackforums"
            ],
            "rows": [
                [
                    "Buy",
                    "Someone wants to buy or bought a product.",
                    "8%",
                    "12%"
                ],
                [
                    "Sell",
                    "Someone making a sale offer to the original poster of a thread.",
                    "8%",
                    "2%"
                ],
                [
                    "Other",
                    "Anything that didn\u2019t fall into the previous categories.",
                    "84%",
                    "86%"
                ],
                [
                    "TOTAL",
                    "[EMPTY]",
                    "9,992",
                    "5,898"
                ]
            ],
            "title": "Table III: Reply classification labels and distribution per source"
        },
        "insight": "We label each reply into three categories: buy, sell and other. The distribution of reply types is highly dependant on the structure and rules of the forum as shown in Table III."
    },
    {
        "id": "450",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Reply"
            ],
            "rows": [
                [
                    "[BOLD] Model",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1"
                ],
                [
                    "[BOLD] FastText",
                    "0.824",
                    "0.734",
                    "0.764",
                    "0.823",
                    "0.318",
                    "0.450",
                    "0.722",
                    "0.582",
                    "0.627",
                    "0.800",
                    "0.427",
                    "0.539"
                ],
                [
                    "[BOLD] Logistic Regression",
                    "0.831",
                    "0.718",
                    "0.753",
                    "[BOLD] 0.874",
                    "0.245",
                    "0.381",
                    "0.617",
                    "0.564",
                    "0.617",
                    "[BOLD] 0.852",
                    "0.363",
                    "0.492"
                ],
                [
                    "[BOLD] SVM",
                    "0.817",
                    "0.748",
                    "0.767",
                    "0.654",
                    "0.213",
                    "0.301",
                    "0.716",
                    "0.578",
                    "0.614",
                    "0.812",
                    "0.332",
                    "0.440"
                ],
                [
                    "[BOLD] XGBoost",
                    "[BOLD] 0.824",
                    "0.677",
                    "0.729",
                    "0.713",
                    "0.227",
                    "0.328",
                    "[BOLD] 0.734",
                    "0.577",
                    "0.627",
                    "0.819",
                    "0.352",
                    "0.465"
                ]
            ],
            "title": "Table IV: Weighted precision, recall and F1 scores of classifiers across datasets and tasks, with stratified k-fold cross-validation"
        },
        "insight": "Similarly a single classifier outperforms the rest in reply classification, as seen in Table IV. By our weighted nonother precision metric, Logistic Regression performed the best across both datasets, providing 0.874 precision on Antichat and 0.852 precision on Hack Forums."
    },
    {
        "id": "451",
        "table": {
            "header": [
                "[BOLD] Task",
                "[BOLD] [CLS]",
                "[BOLD] Mean",
                "[BOLD] Max",
                "[BOLD] [SEP]"
            ],
            "rows": [
                [
                    "Semantic Similarity",
                    "34.1",
                    "[BOLD] 84.5",
                    "80.7",
                    "13.0"
                ],
                [
                    "Text Classification",
                    "90.7",
                    "[BOLD] 95.4",
                    "89.7",
                    "88.9"
                ],
                [
                    "Entailment",
                    "72.4",
                    "[BOLD] 89.3",
                    "87.1",
                    "66.1"
                ],
                [
                    "Surface Information",
                    "45.6",
                    "[BOLD] 78.9",
                    "47.3",
                    "42.8"
                ],
                [
                    "Syntactic Information",
                    "78.2",
                    "[BOLD] 86.0",
                    "75.7",
                    "72.2"
                ],
                [
                    "Semantic Information",
                    "90.3",
                    "[BOLD] 93.7",
                    "89.5",
                    "86.7"
                ],
                [
                    "Average Score",
                    "68.6",
                    "[BOLD] 88.0",
                    "78.3",
                    "61.6"
                ]
            ],
            "title": "Table 1: Comparison of pooling methods"
        },
        "insight": "are summarized in Table 1, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods."
    },
    {
        "id": "452",
        "table": {
            "header": [
                "[BOLD] Dataset / Model  [BOLD] WikiPassageQA",
                "[BOLD] Metrics  [BOLD] MAP",
                "[BOLD] Metrics  [BOLD] P@5",
                "[BOLD] Metrics  [BOLD] P@10"
            ],
            "rows": [
                [
                    "BM25",
                    "53.7",
                    "19.5",
                    "11.5"
                ],
                [
                    "Memory-CNN-LSTM Cohen2018",
                    "56.1",
                    "20.8",
                    "12.3"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "55.0",
                    "21.6",
                    "13.7"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "52.5",
                    "20.6",
                    "12.8"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 74.9",
                    "[BOLD] 27.2",
                    "[BOLD] 15.2"
                ],
                [
                    "[BOLD] InsuranceQA",
                    "[BOLD] P@1",
                    "[BOLD] P@5",
                    "[BOLD] P@10"
                ],
                [
                    "BM25",
                    "60.2",
                    "19.5",
                    "10.9"
                ],
                [
                    "SUBMULT+NN Wang2016g",
                    "75.6",
                    "-",
                    "-"
                ],
                [
                    "DSSM Huang2013a",
                    "30.3",
                    "-",
                    "-"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "44.9",
                    "17.6",
                    "10.6"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "48.0",
                    "18.5",
                    "11.0"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 78.3",
                    "[BOLD] 25.4",
                    "[BOLD] 13.7"
                ],
                [
                    "[BOLD] Quasar-t",
                    "[BOLD] R@1",
                    "[BOLD] R@5",
                    "[BOLD] R@10"
                ],
                [
                    "BM25",
                    "38.7",
                    "59.2",
                    "66.0"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "48.6",
                    "66.6",
                    "71.7"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "49.3",
                    "66.1",
                    "71.0"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 59.5",
                    "[BOLD] 70.9",
                    "[BOLD] 74.6"
                ],
                [
                    "[BOLD] SearchQA",
                    "[BOLD] R@1",
                    "[BOLD] R@5",
                    "[BOLD] R@10"
                ],
                [
                    "BM25",
                    "50.5",
                    "83.3",
                    "90.9"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "66.2",
                    "89.7",
                    "95.0"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "66.8",
                    "90.0",
                    "95.1"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 76.3",
                    "[BOLD] 93.0",
                    "[BOLD] 96.7"
                ]
            ],
            "title": "Table 3: Results of BERT passage-level embeddings on question-answering datasets"
        },
        "insight": "The comparison between BERT embeddings and other models is presented in Table 3. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA (33% improvement in MAP) and InsuranceQA (version 1.0) (3.6% improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and (u, v, u \u2217 v, |u \u2212 v|) shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM (Huang et al., 2013), but still far behind the state-of-the-art interaction-based model SUBMULT+NN (Wang and Jiang, 2016) and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly."
    },
    {
        "id": "453",
        "table": {
            "header": [
                "[EMPTY]",
                "Arman word",
                "Arman phrase",
                "Peyma word",
                "Peyma phrase"
            ],
            "rows": [
                [
                    "Bokaei and Mahmoudi Bokaei and Mahmoudi ( 2018 )",
                    "81.50",
                    "76.79",
                    "-",
                    "-"
                ],
                [
                    "Shahshahani et al.Shahshahani et al. ( 2018 )",
                    "-",
                    "-",
                    "80.0",
                    "-"
                ],
                [
                    "Beheshti-NER (Our Model)",
                    "[BOLD] 84.03",
                    "[BOLD] 79.93",
                    "[BOLD] 90.59",
                    "[BOLD] 87.62"
                ]
            ],
            "title": "Table 3: comparing results of our trained model with others"
        },
        "insight": "these models are evaluated on two common datasets for NER: PEYMA and ARMAN. Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) and Shahshahani et al. (Shahshahani et al., 2018) had reported the best results which you can see in Table 3 [CONTINUE] As you see in Table 3 in both word and phrase levels, our model outperform other NER approaches for the Persian language. [CONTINUE] dataset.Table 3 shows that our results are 10 percent better than Shahshahani and colleagues on the same platform. On the other hand Bokaei and Mahmoudi (Bokaei and Mahmoudi, 2018) reported their results on Arman dataset Which is lower than ours in both word and phrase levels according to Table 3."
    },
    {
        "id": "454",
        "table": {
            "header": [
                "Team",
                "Team",
                "Test Data 1 In Domain",
                "Test Data 1 In Domain",
                "Test Data 1 In Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Total",
                "Test Data 1 Total",
                "Test Data 1 Total"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "P",
                    "R",
                    "F1",
                    "P",
                    "R",
                    "F1",
                    "P",
                    "R",
                    "F1"
                ],
                [
                    "1",
                    "MorphoBERT",
                    "88.7",
                    "85.5",
                    "87.1",
                    "86.3",
                    "83.8",
                    "85",
                    "87.3",
                    "84.5",
                    "85.9"
                ],
                [
                    "2",
                    "Beheshti-NER-1",
                    "85.3",
                    "84.4",
                    "84.8",
                    "84.4",
                    "82.6",
                    "83.5",
                    "84.8",
                    "83.3",
                    "84"
                ],
                [
                    "3",
                    "Team-3",
                    "87.4",
                    "77.2",
                    "82",
                    "87.4",
                    "73.4",
                    "79.8",
                    "87.4",
                    "75",
                    "80.7"
                ],
                [
                    "4",
                    "ICTRC-NLPGroup",
                    "87.5",
                    "76",
                    "81.3",
                    "86.2",
                    "69.6",
                    "77",
                    "86.8",
                    "72.3",
                    "78.9"
                ],
                [
                    "5",
                    "UT-NLP-IR",
                    "75.3",
                    "68.9",
                    "72",
                    "72.3",
                    "60.7",
                    "66",
                    "73.6",
                    "64.1",
                    "68.5"
                ],
                [
                    "6",
                    "SpeechTrans",
                    "41.5",
                    "39.5",
                    "40.5",
                    "43.1",
                    "38.7",
                    "40.8",
                    "42.4",
                    "39",
                    "40.6"
                ],
                [
                    "7",
                    "Baseline",
                    "32.2",
                    "45.8",
                    "37.8",
                    "32.8",
                    "39.1",
                    "35.7",
                    "32.5",
                    "41.9",
                    "36.6"
                ]
            ],
            "title": "Table 4: Phrase-level evaluation for subtask A: 3-classes"
        },
        "insight": "Tables 4, 5, 6, 7 and 8 show the results of evaluation reported by competition for all teams which participated in the challenge. Our method is mentioned as Beheshti-NER-1 [CONTINUE] Table 4 and 5 show the results for subtask A. according to the tables, we reached to 84.0% and 87.9% F1 score respectively for phrase and word level evaluations."
    },
    {
        "id": "455",
        "table": {
            "header": [
                "Medical Device Term",
                "Medical Device Score",
                "Medical Robot Term",
                "Medical Robot Score",
                "Sports Rehab Machine Term",
                "Sports Rehab Machine Score"
            ],
            "rows": [
                [
                    "root",
                    "0.8802",
                    "stroke",
                    "0.919",
                    "kingdom",
                    "0.907"
                ],
                [
                    "mouse",
                    "0.8633",
                    "kingdom",
                    "0.893",
                    "stroke",
                    "0.8495"
                ],
                [
                    "kingdom",
                    "0.8383",
                    "vessel",
                    "0.8651",
                    "progressive",
                    "0.8414"
                ],
                [
                    "iron",
                    "0.8381",
                    "thread",
                    "0.8385",
                    "net",
                    "0.8334"
                ],
                [
                    "internal",
                    "0.8043",
                    "floating",
                    "0.8045",
                    "suspension",
                    "0.8322"
                ],
                [
                    "progressive",
                    "0.7957",
                    "strain",
                    "0.8018",
                    "induction",
                    "0.8244"
                ],
                [
                    "agent",
                    "0.7875",
                    "mouse",
                    "0.7997",
                    "thread",
                    "0.8236"
                ],
                [
                    "express",
                    "0.7733",
                    "progressive",
                    "0.7983",
                    "root",
                    "0.8093"
                ],
                [
                    "plasma",
                    "0.7685",
                    "die",
                    "0.787",
                    "transmission",
                    "0.7871"
                ],
                [
                    "net",
                    "0.7678",
                    "secondary",
                    "0.786",
                    "die",
                    "0.7821"
                ],
                [
                    "\u22ee",
                    "\u22ee",
                    "\u22ee",
                    "\u22ee",
                    "\u22ee",
                    "\u22ee"
                ],
                [
                    "argued",
                    "0.1631",
                    "corresponding",
                    "0.1695",
                    "corresponding",
                    "0.196"
                ],
                [
                    "richard",
                    "0.1608",
                    "corresponds",
                    "0.1693",
                    "told",
                    "0.1958"
                ],
                [
                    "authority",
                    "0.1606",
                    "feel",
                    "0.167",
                    "joseph",
                    "0.1956"
                ],
                [
                    "michael",
                    "0.1569",
                    "coating",
                    "0.1666",
                    "understanding",
                    "0.1953"
                ],
                [
                    "required",
                    "0.154",
                    "wife",
                    "0.1603",
                    "love",
                    "0.1902"
                ],
                [
                    "peter",
                    "0.1388",
                    "michael",
                    "0.159",
                    "economic",
                    "0.1902"
                ],
                [
                    "robert",
                    "0.1381",
                    "authority",
                    "0.156",
                    "coating",
                    "0.1892"
                ],
                [
                    "david",
                    "0.1201",
                    "peter",
                    "0.1437",
                    "pay",
                    "0.1856"
                ],
                [
                    "james",
                    "0.1188",
                    "david",
                    "0.1412",
                    "authority",
                    "0.1838"
                ],
                [
                    "charles",
                    "0.1157",
                    "required",
                    "0.1382",
                    "causing",
                    "0.1822"
                ]
            ],
            "title": "TABLE II: Ranked List of Target Words based on their Ambiguity Scores"
        },
        "insight": "In order to study the cases of disagreement between the approaches proposed by this paper and Ferrari et al. (2018), the top-5 words with the largest absolute differences between the assigned ranks have been reported for each scenario by Table III. The number of target words for each project scenario have also been mentioned in parenthesis. It can be observed that most of the cases of disagreement have a higher rank, i.e. relatively lower ambiguity score assigned by the linear transformation approach proposed by this paper. Most of such cases are proper names such as robert, peter, and daniel."
    },
    {
        "id": "456",
        "table": {
            "header": [
                "Target \u2192 System \u2193 <italic>neural network models</italic>",
                "<bold>MT</bold> <italic>neural network models</italic>",
                "<bold>MT</bold> <italic>neural network models</italic>",
                "<bold>OC</bold> <italic>neural network models</italic>",
                "<bold>OC</bold> <italic>neural network models</italic>",
                "<bold>PE</bold> <italic>neural network models</italic>",
                "<bold>PE</bold> <italic>neural network models</italic>",
                "<bold>VG</bold> <italic>neural network models</italic>",
                "<bold>VG</bold> <italic>neural network models</italic>",
                "<bold>WD</bold> <italic>neural network models</italic>",
                "<bold>WD</bold> <italic>neural network models</italic>",
                "<bold>WTP</bold> <italic>neural network models</italic>",
                "<bold>WTP</bold> <italic>neural network models</italic>",
                "<bold>Average</bold> <italic>neural network models</italic>",
                "<bold>Average</bold> <italic>neural network models</italic>"
            ],
            "rows": [
                [
                    "BiLSTM",
                    "68.8",
                    "41.8",
                    "58.0",
                    "22.4",
                    "73.0",
                    "62.0",
                    "60.9",
                    "37.7",
                    "60.0",
                    "24.5",
                    "57.9",
                    "28.5",
                    "63.1",
                    "36.1"
                ],
                [
                    "CNN:rand",
                    "78.6",
                    "67.3",
                    "<bold>60.5</bold>",
                    "<bold>25.6</bold>",
                    "<bold>73.6</bold>",
                    "61.1",
                    "<bold>65.9</bold>",
                    "<bold>45.0</bold>",
                    "61.1",
                    "25.8",
                    "58.6",
                    "28.9",
                    "66.4",
                    "42.3"
                ],
                [
                    "CNN:w2vec",
                    "73.7",
                    "60.9",
                    "58.2",
                    "23.7",
                    "74.0",
                    "61.7",
                    "63.8",
                    "33.5",
                    "62.6",
                    "<bold>28.9</bold>",
                    "57.3",
                    "24.3",
                    "64.9",
                    "38.8"
                ],
                [
                    "LSTM",
                    "65.2",
                    "48.3",
                    "58.5",
                    "22.3",
                    "71.8",
                    "60.7",
                    "61.3",
                    "40.1",
                    "61.6",
                    "25.9",
                    "58.0",
                    "28.4",
                    "62.7",
                    "37.6"
                ],
                [
                    "LR",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "-Discourse",
                    "73.0",
                    "60.8",
                    "59.9",
                    "22.9",
                    "70.6",
                    "60.6",
                    "62.5",
                    "42.6",
                    "63.7",
                    "23.2",
                    "59.7",
                    "30.2",
                    "64.9",
                    "40.0"
                ],
                [
                    "-Embeddings",
                    "74.6",
                    "62.9",
                    "59.6",
                    "22.6",
                    "70.4",
                    "60.4",
                    "62.9",
                    "43.1",
                    "63.9",
                    "23.5",
                    "59.4",
                    "29.9",
                    "65.1",
                    "40.4"
                ],
                [
                    "-Lexical",
                    "72.1",
                    "59.5",
                    "59.6",
                    "22.5",
                    "65.9",
                    "55.1",
                    "60.8",
                    "40.5",
                    "60.1",
                    "18.5",
                    "57.7",
                    "27.8",
                    "62.7",
                    "37.3"
                ],
                [
                    "-Structure",
                    "74.4",
                    "62.6",
                    "60.0",
                    "23.0",
                    "70.4",
                    "60.4",
                    "62.0",
                    "41.8",
                    "64.2",
                    "23.4",
                    "59.5",
                    "30.0",
                    "65.1",
                    "40.2"
                ],
                [
                    "-Syntax",
                    "<bold>79.8</bold>",
                    "<bold>70.3</bold>",
                    "59.8",
                    "22.9",
                    "72.1",
                    "<bold>62.5</bold>",
                    "63.4",
                    "43.8",
                    "<bold>65.1</bold>",
                    "25.5",
                    "<bold>60.1</bold>",
                    "<bold>30.5</bold>",
                    "<bold>66.7</bold>",
                    "<bold>42.6</bold>"
                ],
                [
                    "All Features",
                    "74.4",
                    "62.7",
                    "59.9",
                    "22.9",
                    "70.6",
                    "60.6",
                    "62.5",
                    "42.6",
                    "63.8",
                    "23.3",
                    "59.7",
                    "30.2",
                    "65.1",
                    "40.4"
                ],
                [
                    "LR",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "+Discourse",
                    "70.0",
                    "56.7",
                    "49.4",
                    "13.8",
                    "50.1",
                    "41.7",
                    "49.6",
                    "30.6",
                    "57.6",
                    "14.9",
                    "49.5",
                    "18.4",
                    "54.4",
                    "29.3"
                ],
                [
                    "+Embeddings",
                    "72.4",
                    "59.8",
                    "58.8",
                    "20.8",
                    "68.2",
                    "57.7",
                    "59.7",
                    "39.3",
                    "64.2",
                    "23.8",
                    "59.0",
                    "28.9",
                    "63.7",
                    "38.4"
                ],
                [
                    "+Lexical",
                    "75.9",
                    "64.7",
                    "59.5",
                    "21.4",
                    "71.8",
                    "62.1",
                    "61.1",
                    "40.5",
                    "64.0",
                    "22.2",
                    "59.0",
                    "27.7",
                    "65.2",
                    "39.8"
                ],
                [
                    "+Structure",
                    "57.1",
                    "42.0",
                    "56.5",
                    "20.0",
                    "54.2",
                    "39.5",
                    "55.4",
                    "33.3",
                    "48.4",
                    "9.0",
                    "55.4",
                    "25.2",
                    "54.5",
                    "28.2"
                ],
                [
                    "+Syntax",
                    "66.7",
                    "52.5",
                    "58.1",
                    "21.0",
                    "64.1",
                    "52.9",
                    "60.7",
                    "40.4",
                    "57.6",
                    "15.5",
                    "57.0",
                    "27.0",
                    "60.7",
                    "34.9"
                ],
                [
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "50.7",
                    "33.2",
                    "49.9",
                    "13.5",
                    "50.8",
                    "38.0",
                    "50.4",
                    "28.8",
                    "51.6",
                    "10.8",
                    "48.9",
                    "18.8",
                    "50.4",
                    "23.9"
                ]
            ],
            "title": "Table 2: In-domain experiments, best values per column are highlighted. For each dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column)."
        },
        "insight": "The average performances of LR\u2212syntax and CNN:rand are virtually identical, both for Macro6Described as FscoreM in Sokolova and Lapalme (2009). F1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p \u2264 0.05). Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p \u2264 0.05). [CONTINUE] The performance of the learners is quite divergent across datasets, with Macro-F1 scores6 ranging from 60% (WTP) to 80% (MT), average 67% (see Table 2). [CONTINUE] On all datasets, our best systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information."
    },
    {
        "id": "457",
        "table": {
            "header": [
                "Target \u2192 Source/Sys. \u2193",
                "<bold>MT</bold> <italic>CNN:rand</italic>",
                "<bold>MT</bold> <italic>CNN:rand</italic>",
                "<bold>OC</bold> <italic>CNN:rand</italic>",
                "<bold>OC</bold> <italic>CNN:rand</italic>",
                "<bold>PE</bold> <italic>CNN:rand</italic>",
                "<bold>PE</bold> <italic>CNN:rand</italic>",
                "<bold>VG</bold> <italic>CNN:rand</italic>",
                "<bold>VG</bold> <italic>CNN:rand</italic>",
                "<bold>WD</bold> <italic>CNN:rand</italic>",
                "<bold>WD</bold> <italic>CNN:rand</italic>",
                "<bold>WTP</bold> <italic>CNN:rand</italic>",
                "<bold>WTP</bold> <italic>CNN:rand</italic>",
                "<bold>Average</bold>",
                "<bold>Average</bold>"
            ],
            "rows": [
                [
                    "MT",
                    "<italic>78.6</italic>",
                    "<italic>67.3</italic>",
                    "51.0",
                    "7.4",
                    "56.9",
                    "22.1",
                    "57.2",
                    "15.7",
                    "52.4",
                    "9.4",
                    "49.4",
                    "10.9",
                    "53.4",
                    "13.1"
                ],
                [
                    "OC",
                    "57.1",
                    "39.7",
                    "<italic>60.5</italic>",
                    "<italic>25.6</italic>",
                    "56.4",
                    "42.8",
                    "58.9",
                    "37.3",
                    "54.6",
                    "13.2",
                    "<bold>58.4</bold>",
                    "<bold>28.9</bold>",
                    "57.1",
                    "32.4"
                ],
                [
                    "PE",
                    "59.8",
                    "18.0",
                    "54.2",
                    "9.5",
                    "<italic>73.6</italic>",
                    "<italic>61.1</italic>",
                    "57.5",
                    "18.7",
                    "<bold>55.5</bold>",
                    "<bold>15.9</bold>",
                    "54.7",
                    "16.0",
                    "56.3",
                    "15.6"
                ],
                [
                    "VG",
                    "<bold>68.7</bold>",
                    "<bold>51.5</bold>",
                    "55.8",
                    "19.2",
                    "<bold>57.0</bold>",
                    "32.0",
                    "<italic>65.9</italic>",
                    "<italic>45.0</italic>",
                    "51.7",
                    "10.5",
                    "54.7",
                    "22.0",
                    "57.6",
                    "27.0"
                ],
                [
                    "WD",
                    "64.4",
                    "3.5",
                    "51.3",
                    "1.3",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "<italic>61.1</italic>",
                    "<italic>25.8</italic>",
                    "46.7",
                    "0.0",
                    "49.6",
                    "1.0"
                ],
                [
                    "WTP",
                    "58.5",
                    "26.6",
                    "56.8",
                    "15.4",
                    "56.0",
                    "18.5",
                    "55.3",
                    "19.4",
                    "52.9",
                    "11.6",
                    "<italic>58.6</italic>",
                    "<italic>28.9</italic>",
                    "55.9",
                    "18.3"
                ],
                [
                    "<italic>Average</italic>",
                    "61.7",
                    "27.9",
                    "53.8",
                    "10.6",
                    "53.5",
                    "23.1",
                    "54.7",
                    "18.2",
                    "53.4",
                    "12.1",
                    "52.8",
                    "15.6",
                    "55.0",
                    "17.9"
                ],
                [
                    "[EMPTY]",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "MT",
                    "<italic>74.4</italic>",
                    "<italic>62.7</italic>",
                    "53.9",
                    "17.0",
                    "51.9",
                    "29.5",
                    "56.1",
                    "34.2",
                    "55.1",
                    "14.5",
                    "52.5",
                    "21.2",
                    "53.9",
                    "23.3"
                ],
                [
                    "OC",
                    "60.0",
                    "45.1",
                    "<italic>59.9</italic>",
                    "<italic>22.9</italic>",
                    "56.7",
                    "<bold>47.0</bold>",
                    "58.6",
                    "<bold>38.0</bold>",
                    "54.1",
                    "12.2",
                    "57.7",
                    "27.5",
                    "57.4",
                    "<bold>34.0</bold>"
                ],
                [
                    "PE",
                    "58.1",
                    "36.3",
                    "54.6",
                    "17.3",
                    "<italic>70.6</italic>",
                    "<italic>60.6</italic>",
                    "54.1",
                    "21.4",
                    "54.0",
                    "13.5",
                    "54.4",
                    "20.4",
                    "55.0",
                    "21.8"
                ],
                [
                    "VG",
                    "65.8",
                    "51.4",
                    "<bold>57.3</bold>",
                    "<bold>21.7</bold>",
                    "<bold>57.0</bold>",
                    "45.1",
                    "<italic>62.5</italic>",
                    "<italic>42.6</italic>",
                    "54.5",
                    "13.1",
                    "55.1",
                    "24.8",
                    "<bold>57.9</bold>",
                    "31.2"
                ],
                [
                    "WD",
                    "62.6",
                    "38.5",
                    "55.4",
                    "19.0",
                    "56.0",
                    "30.1",
                    "55.1",
                    "23.3",
                    "<italic>63.8</italic>",
                    "<italic>23.3</italic>",
                    "53.6",
                    "20.9",
                    "56.5",
                    "26.3"
                ],
                [
                    "WTP",
                    "58.0",
                    "41.7",
                    "56.1",
                    "20.3",
                    "56.8",
                    "42.6",
                    "<bold>59.1</bold>",
                    "<bold>38.0</bold>",
                    "52.2",
                    "11.2",
                    "<italic>59.7</italic>",
                    "<italic>30.2</italic>",
                    "56.5",
                    "30.8"
                ],
                [
                    "<italic>Average</italic>",
                    "60.9",
                    "42.6",
                    "55.5",
                    "19.1",
                    "55.7",
                    "38.9",
                    "56.6",
                    "31.0",
                    "54.0",
                    "12.9",
                    "54.7",
                    "23.0",
                    "56.2",
                    "27.9"
                ],
                [
                    "LR",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "+Discourse",
                    "40.2",
                    "15.0",
                    "31.7",
                    "5.8",
                    "30.3",
                    "27.4",
                    "27.7",
                    "19.9",
                    "40.9",
                    "4.5",
                    "25.3",
                    "13.3",
                    "32.7",
                    "14.3"
                ],
                [
                    "+Embeddings",
                    "56.6",
                    "35.2",
                    "51.4",
                    "12.8",
                    "53.6",
                    "30.7",
                    "53.3",
                    "24.3",
                    "54.2",
                    "13.2",
                    "52.9",
                    "19.0",
                    "53.7",
                    "22.5"
                ],
                [
                    "+Lexical",
                    "61.0",
                    "42.2",
                    "55.2",
                    "18.3",
                    "56.2",
                    "38.6",
                    "54.7",
                    "29.1",
                    "53.1",
                    "11.9",
                    "54.9",
                    "23.4",
                    "55.9",
                    "27.2"
                ],
                [
                    "+Structure",
                    "44.2",
                    "22.9",
                    "53.6",
                    "18.5",
                    "52.5",
                    "38.4",
                    "53.6",
                    "32.1",
                    "49.1",
                    "9.0",
                    "53.4",
                    "23.3",
                    "51.1",
                    "24.0"
                ],
                [
                    "+Syntax",
                    "54.8",
                    "37.0",
                    "54.2",
                    "17.5",
                    "54.3",
                    "40.6",
                    "55.7",
                    "32.0",
                    "53.0",
                    "11.8",
                    "53.8",
                    "22.5",
                    "54.3",
                    "26.9"
                ],
                [
                    "[EMPTY]",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "47.5",
                    "30.6",
                    "50.5",
                    "14.0",
                    "51.0",
                    "38.4",
                    "51.0",
                    "29.3",
                    "49.3",
                    "9.3",
                    "50.3",
                    "20.2",
                    "49.9",
                    "23.6"
                ]
            ],
            "title": "Table 3: Cross-domain experiments, best values per column are highlighted, in-domain results (for comparison) in italics; results only for selected systems. For each source/target combination we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column)."
        },
        "insight": "Table 3 lists the results of the best feature-based (LR All features) and deep learning (CNN:rand) systems, as well as single feature groups (averages over all source domains, [CONTINUE] For the lowest scoring datasets, OC and WTP, the differences are only marginal when trained on a suitable dataset [CONTINUE] The best feature-based approach outperforms the best deep learning approach in most scenarios. [CONTINUE] In particular, as opposed to the in-domain experiments, the difference of the Claim-F1 measure between the feature-based approaches and the deep learning approaches is striking. In the feature-based approaches, on average, a combination of all features yields the best results for both Macro-F1 and Claim-F1. When comparing single features, lexical ones do the best job."
    },
    {
        "id": "458",
        "table": {
            "header": [
                "Target \u2192 System \u2193 CNN:rand",
                "<bold>MT</bold> 62.8",
                "<bold>MT</bold> 41.4",
                "<bold>OC</bold> <bold>57.8</bold>",
                "<bold>OC</bold> <bold>22.4</bold>",
                "<bold>PE</bold> <bold>59.7</bold>",
                "<bold>PE</bold> 36.2",
                "<bold>VG</bold> <bold>58.6</bold>",
                "<bold>VG</bold> 28.1",
                "<bold>WD</bold> <bold>54.2</bold>",
                "<bold>WD</bold> <bold>14.1</bold>",
                "<bold>WTP</bold> <bold>56.8</bold>",
                "<bold>WTP</bold> 25.6",
                "<bold>Avg</bold> <bold>58.3</bold>",
                "<bold>Avg</bold> 28.0"
            ],
            "rows": [
                [
                    "All features",
                    "<bold>64.7</bold>",
                    "<bold>49.5</bold>",
                    "56.4",
                    "20.6",
                    "57.8",
                    "<bold>45.8</bold>",
                    "58.2",
                    "<bold>36.4</bold>",
                    "52.3",
                    "11.3",
                    "56.0",
                    "<bold>26.0</bold>",
                    "57.6",
                    "<bold>31.6</bold>"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "47.5",
                    "30.6",
                    "50.5",
                    "14.0",
                    "51.0",
                    "38.4",
                    "51.0",
                    "29.3",
                    "49.3",
                    "9.3",
                    "50.3",
                    "20.2",
                    "49.9",
                    "23.6"
                ]
            ],
            "title": "Table 4: Leave-one-domain-out experiments, best values per column are highlighted. For each test dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column)."
        },
        "insight": "We also performed experiments with mixed sources, the results are shown in Table 4. [CONTINUE] In this scenario, the neural network systems seem to benefit from the increased amount of training data and thus gave the best results."
    },
    {
        "id": "459",
        "table": {
            "header": [
                "<bold>Model</bold>",
                "<bold>Intent Detection</bold>",
                "<bold>Slot Filling</bold>"
            ],
            "rows": [
                [
                    "Random baseline",
                    "02.67",
                    "07.32"
                ],
                [
                    "Majority baseline",
                    "06.34",
                    "64.96"
                ],
                [
                    "HMM",
                    "-",
                    "87.20"
                ],
                [
                    "SVM",
                    "94.98",
                    "-"
                ],
                [
                    "GolVe-based",
                    "92.22",
                    "98.45"
                ]
            ],
            "title": "Table 2: The accuracy(%) of the ML models for NLU."
        },
        "insight": "First, we represent utterances by their TF-IDF representations as feature-vectors. We then supply the vector representation of each utterance to a Support Vector Machine (SVM) with a linear kernel for domain and intent identification, and to a Hidden Markov Model (HMM) for slot filling. Second, we encode words in a dialogue utterance by GLoVe, as benchmark pre-trained word embeddings, to include the semantic relationships among words. We compute the average of word embeddings in an utterance to represent the utterance by a vector. Table 2 shows the performance of the described models."
    },
    {
        "id": "460",
        "table": {
            "header": [
                "[EMPTY]",
                "1 (strongly agree)",
                "2 (agree)",
                "3 (disagree)",
                "4 (strongly disagree)"
            ],
            "rows": [
                [
                    "was able to \u201cunderstand\u201d my questions",
                    "16.7%",
                    "<bold>50.0%</bold>",
                    "33.3%",
                    "00.0%"
                ],
                [
                    "was able to provide answers to my questions",
                    "00.0%",
                    "<bold>50.0%</bold>",
                    "<bold>50.0%</bold>",
                    "00.0%"
                ],
                [
                    "I was satisfied with the informativeness of the answers provided by",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "00.0%",
                    "<bold>33.3%</bold>"
                ],
                [
                    "I was satisfied with the fluency of the answers provided by",
                    "16.7%",
                    "<bold>50.0%</bold>",
                    "16.7%",
                    "16.7%"
                ],
                [
                    "could respond in a reasonable time",
                    "<bold>50.0%</bold>",
                    "33.3%",
                    "16.7%",
                    "00.0%"
                ],
                [
                    "The GUI of was suitable for reading the provided answers",
                    "<bold>50.0%</bold>",
                    "00.0%",
                    "33.3%",
                    "16.7%"
                ],
                [
                    "reduces my need to google a specific information",
                    "16.7%",
                    "<bold>66.7%</bold>",
                    "00.0%",
                    "16.7%"
                ],
                [
                    "would help me save some time in my work",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "00.0%"
                ],
                [
                    "I would like to use in the future on a daily basis",
                    "00.0%",
                    "<bold>66.7%</bold>",
                    "00.0%",
                    "33.3%"
                ],
                [
                    "I will use to plan for my next conference",
                    "16.7%",
                    "16.7%",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>"
                ]
            ],
            "title": "Table 4: The output of the general survey."
        },
        "insight": "The results of the survey shows a general sat, confirmisfaction feeling of interactions with ing our motivation that the existence of such an agent helps researchers (See Table 4). 83% of participants agree that reduces their needs to search through the web (e.g. using search engines) to obtain information related to their research; and in the future. However, 66% of hu66% use man judges disagreed on using for planning their schedule for a conference. This observation could be because the current version of mainly retrieves information for users but planning for a conference needs some inferences on such information as well."
    },
    {
        "id": "461",
        "table": {
            "header": [
                "<bold>Model</bold>",
                "\u2211",
                "<bold>InsuranceQA</bold>",
                "<bold>Travel</bold>",
                "<bold>Cooking</bold>",
                "<bold>Academia</bold>",
                "<bold>Apple</bold>",
                "<bold>Aviation</bold>",
                "<bold>WikiPassageQA</bold>"
            ],
            "rows": [
                [
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BM25",
                    "30.3",
                    "24.9",
                    "38.1",
                    "30.9",
                    "29.2",
                    "21.8",
                    "37.0",
                    "53.00 / 61.71"
                ],
                [
                    "TF*IDF",
                    "32.4",
                    "18.7",
                    "39.9",
                    "35.1",
                    "32.2",
                    "26.7",
                    "41.9",
                    "39.92 / 46.38"
                ],
                [
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "InferSent",
                    "23.0",
                    "14.8",
                    "27.0",
                    "21.3",
                    "22.5",
                    "22.8",
                    "29.3",
                    "43.62 / 50.53"
                ],
                [
                    "p-mean\u2004Embeddings",
                    "25.7",
                    "17.0",
                    "32.1",
                    "29.3",
                    "24.3",
                    "19.6",
                    "31.7",
                    "42.82 / 50.44"
                ],
                [
                    "CNN",
                    "25.9",
                    "24.4",
                    "36.9",
                    "25.9",
                    "22.5",
                    "20.2",
                    "25.3",
                    "27.33 / 31.48"
                ],
                [
                    "BiLSTM",
                    "34.8",
                    "32.4",
                    "45.3",
                    "35.2",
                    "31.5",
                    "27.2",
                    "37.3",
                    "46.16 / 52.89"
                ],
                [
                    "Att.-BiLSTM",
                    "34.5",
                    "37.9",
                    "43.0",
                    "36.2",
                    "31.2",
                    "24.7",
                    "33.9",
                    "47.04 / 54.36"
                ],
                [
                    "AP-BiLSTM",
                    "31.3",
                    "31.9",
                    "38.8",
                    "32.2",
                    "27.3",
                    "22.9",
                    "34.5",
                    "46.98 / 55.20"
                ],
                [
                    "LW-BiLSTM",
                    "34.1",
                    "36.9",
                    "43.2",
                    "32.3",
                    "30.2",
                    "23.4",
                    "38.5",
                    "47.56 / 54.33"
                ],
                [
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Bigrams",
                    "18.3",
                    "19.4",
                    "19.3",
                    "16.7",
                    "19.8",
                    "13.0",
                    "21.5",
                    "39.84 / 47.55"
                ],
                [
                    "CA-Wang",
                    "39.1",
                    "37.0",
                    "46.5",
                    "39.4",
                    "36.1",
                    "29.2",
                    "46.5",
                    "48.71 / 56.11"
                ],
                [
                    "COALA",
                    "43.6",
                    "38.0",
                    "53.8",
                    "47.3",
                    "42.2",
                    "32.0",
                    "48.4",
                    "<bold>60.58</bold> / <bold>69.40</bold>"
                ],
                [
                    "COALA\u2004p-means",
                    "<bold>45.2</bold>",
                    "<bold>39.9</bold>",
                    "53.4",
                    "46.5",
                    "<bold>44.2</bold>",
                    "<bold>34.5</bold>",
                    "<bold>52.9</bold>",
                    "59.29 / 68.48"
                ],
                [
                    "COALA\u2004syntax-aware",
                    "44.3",
                    "39.5",
                    "<bold>54.1</bold>",
                    "<bold>47.8</bold>",
                    "43.5",
                    "32.7",
                    "48.3",
                    "60.48 / 68.75"
                ]
            ],
            "title": "Table 2: Accuracies of the different models on the cQA datasets and MAP/MRR on WikiPassageQA. \u03a3 denotes the average accuracy over all cQA datasets."
        },
        "insight": "We compare all approaches across the InsuranceQA and WikiPassageQA benchmarks as well as the five StackExchange datasets in Table 2. For the cQA answer selection datasets we measure the accuracy, which is the ratio of correctly selected answers, and for the passage retrieval in WikiPassageQA we report MAP/MRR. The results show that COALA substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets. For instance, on the cQA datasets COALA improves by 4.5pp over CA-Wang and by 8.8pp over the best semantic similarity method on average. [CONTINUE] Our extended approach COALA p-means improves the performance of COALA on these datasets by an additional 1.6pp. The proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases. [CONTINUE] The results in Table 2 show that our proposed syntax-aware extension COALA syntax-aware, which incorporates syntactic roles of word sequences to learn syntax-aware aspect representations, improves the results in five out of seven cases. [CONTINUE] It thereby achieves an an average improvement of 0.7pp over COALA in our cQA datasets."
    },
    {
        "id": "462",
        "table": {
            "header": [
                "[EMPTY]",
                "synt",
                "glob",
                "pair",
                "loc",
                "RND",
                "UB"
            ],
            "rows": [
                [
                    "EN",
                    "SE1",
                    ".869",
                    ".887",
                    ".867",
                    ".509",
                    ".927"
                ],
                [
                    "EN",
                    "SE2",
                    "<bold>.930</bold>",
                    "<bold>.929</bold>",
                    "<bold>.913</bold>",
                    ".500",
                    "<bold>.932</bold>"
                ],
                [
                    "DE",
                    "SD1",
                    ".655",
                    ".726",
                    ".637",
                    ".471",
                    ".818"
                ],
                [
                    "DE",
                    "SD2",
                    "<bold>.790</bold>",
                    "<bold>.820</bold>",
                    "<bold>.820</bold>",
                    ".456",
                    "<bold>.920</bold>"
                ]
            ],
            "title": "Table 3: Thematic ranker evaluation, incl. random ranker (RND) and upper bound (UB); bold - best result over syntactic rankers, underlined - best result over thematic rankers"
        },
        "insight": "we first compare the performance of thematic rankers with respect to syntactic rankers and available datasets. The results of this comparison are summarized in Table 3 and show that syntactic rankers positioning the object second in the hierarchy (SE2 and SD2) lead to better alignment on both datasets and have a higher upper bound. [CONTINUE] For English the global hierarchy-based ranker approaches the upper bound, closely followed by the pairwise ranker. The accuracy on German data is lower and the pairwise and local rankers outperform the global hierarchy-based ranker."
    },
    {
        "id": "463",
        "table": {
            "header": [
                "EN",
                "Agent \u227a Cause/Instrument/Experiencer \u227a Pivot \u227a Theme \u227a Patient \u227a Material/Source/Asset \u227a Product \u227a Recipient/Beneficiary/Destination/Location \u227a Value/Stimulus/Topic/Result/Predicate/Goal/InitialLocation/Attribute/Extent"
            ],
            "rows": [
                [
                    "DE",
                    "Agent \u227a Experiencer \u227a Stimulus/Pivot \u227a Cause \u227a Theme \u227a Patient \u227a Topic \u227a Instrument \u227a Beneficiary/InitialLocation \u227a Result \u227a Product/Goal \u227a Destination/Attribute \u227a Recipient \u227a Value/Time/CoAgent/Locus/Manner/Source/Trajectory/Location/Duration/Path/Extent"
                ]
            ],
            "title": "Table 4: Induced hierarchies"
        },
        "insight": "Table 4 shows full rankings extracted for English and German data. While some correspondence to the hierarchies proposed for English Agent in literature is evident (e.g. \u227a Instrument \u227a Theme, similar to (Fillmore, 1968)), a direct comparison is impossible due to the differences in role definitions and underlying syntactic formalisms. Notice the high number of ties: some roles never co-occur (either by chance or by design) or occur on the same syntactic rank (e.g. oblique) so there is no evidence for preference even if we enforce transitivity."
    },
    {
        "id": "464",
        "table": {
            "header": [
                "[EMPTY]",
                "EN-test",
                "DE-test"
            ],
            "rows": [
                [
                    "UB",
                    ".932",
                    ".920"
                ],
                [
                    "EN-train",
                    ".930",
                    ".787"
                ],
                [
                    "DE-train",
                    ".852",
                    ".790"
                ],
                [
                    "RND",
                    ".500",
                    ".456"
                ]
            ],
            "title": "Table 5: Cross-lingual evaluation, global ranker"
        },
        "insight": "Table 5 contrasts the performance of THs induced from English and German training data, and evaluated on German and English test data respectively. While the cross-lingual performance is expectedly lower than the monolingual performance, it outperforms the random baseline by a large margin, suggesting the potential for crosslingual hierarchy induction."
    },
    {
        "id": "465",
        "table": {
            "header": [
                "<bold>Test</bold>",
                "<bold>Metric</bold>",
                "<bold>tf-idf</bold>",
                "<bold>bm25</bold>",
                "<bold>emb</bold>",
                "<bold>feat</bold>",
                "<bold>auto/ cosine</bold>",
                "<bold>auto- rank</bold>",
                "<bold>auto- rank + bm25</bold>",
                "<bold>auto- rank + feat</bold>"
            ],
            "rows": [
                [
                    "EGFR",
                    "MAP",
                    "0.289",
                    "0.632",
                    "0.310",
                    "0.575",
                    "0.054",
                    "0.545",
                    "0.588",
                    "<bold>0.699</bold>"
                ],
                [
                    "EGFR",
                    "nDCG",
                    "0.424",
                    "0.728",
                    "0.460",
                    "0.695",
                    "0.129",
                    "0.653",
                    "0.716",
                    "<bold>0.810</bold>"
                ],
                [
                    "KRAS",
                    "MAP",
                    "0.327",
                    "0.610",
                    "0.466",
                    "0.609",
                    "0.058",
                    "0.575",
                    "0.774",
                    "<bold>0.820</bold>"
                ],
                [
                    "KRAS",
                    "nDCG",
                    "0.456",
                    "0.723",
                    "0.592",
                    "0.712",
                    "0.145",
                    "0.688",
                    "0.867",
                    "<bold>0.914</bold>"
                ],
                [
                    "BRAF",
                    "MAP",
                    "0.342",
                    "0.656",
                    "0.427",
                    "0.704",
                    "0.063",
                    "0.563",
                    "0.702",
                    "<bold>0.812</bold>"
                ],
                [
                    "BRAF",
                    "nDCG",
                    "0.480",
                    "0.751",
                    "0.572",
                    "0.802",
                    "0.163.",
                    "0.671",
                    "0.820",
                    "<bold>0.901</bold>"
                ],
                [
                    "PIK3CA",
                    "MAP",
                    "0.341",
                    "0.633",
                    "0.486",
                    "0.625",
                    "0.079",
                    "0.541",
                    "0.779",
                    "<bold>0.810</bold>"
                ],
                [
                    "PIK3CA",
                    "nDCG",
                    "0.473",
                    "0.729",
                    "0.617",
                    "0.718",
                    "0.171",
                    "0.656",
                    "0.859",
                    "<bold>0.895</bold>"
                ]
            ],
            "title": "Table 3: Test Scores Document Ranking"
        },
        "insight": "In Table 3 we have listed the average MAP and nDCG scores of the test sets. The tf-idf model is outperformed by most of the other models. However, bm25, which additionally takes the length of a document into account, performs very tf-idf and bm25 have the major benefit of well. fast computation. The feat model slightly outperforms the auto [CONTINUE] auto-rank + feat model is slightly better than the auto-rank + bm25 model, both of which have the overall best performance. This shows, that the auto-encoder learns something orthogonal to term frequency and document length. The best model with respect to document ranking is the auto-rank + feat model. In Figure 3 we show the correlation between the different models. Interestingly, the bm25 and the feat strongly correlate. However, the scores of bm25 do not correlate with the scores of the combination of auto-rank and bm25. This indicates, that the model does not primarily learn to use the bm25 score but also focuses on the the auto-encoded representation. This underlines the hypothesis that the auto-encoder is able to represent latent features of the relationship of the query terms in the document. rank model. The distance features are a strong indicator for the semantic dependency between entities. These relationships need to be learned in the auto-rank model. The cosine similarity of a query and a document (auto/cos) does not yield a good result. This shows that the auto-encoder has learned many features, most of which do not correlate with our task. We also find that emb does not yield an equal performance to auto-rank. The combination of the"
    },
    {
        "id": "466",
        "table": {
            "header": [
                "<bold>Med</bold>",
                "BiLSTM",
                "EG 71.60",
                "EE 80.20",
                "HG 69.28",
                "DC 65.32"
            ],
            "rows": [
                [
                    "<bold>Med</bold>",
                    "UB",
                    "85.61",
                    "90.25",
                    "86.37",
                    "85.58"
                ],
                [
                    "<bold>TEd</bold>",
                    "BiLSTM",
                    "78.53",
                    "78.87",
                    "57.16",
                    "61.77"
                ],
                [
                    "<bold>TEd</bold>",
                    "UB",
                    "93.29",
                    "90.71",
                    "81.77",
                    "82.11"
                ]
            ],
            "title": "Table 1: Individual macro-F1 scores following Schulz et al. (2019a) for each of the epistemic activities. The BiLSTM uses FastText embeddings Bojanowski et al. (2017). This architecture is equal to Flair when only using FastText embeddings. UB reports the human upper bound (inter-annotator agreement) indicating room for improvement."
        },
        "insight": "In Table 1, we report the performance of the BiLSTM implementation for predicting epistemic activities in the Med and TEd data. As we can see, the difficulty of predicting the classes varies between different activities. Despite some room for improvement with respect to the human upper bound (UB) based on inter-rater agreement, the interactive nature of FAMULUS helps in succeeding in this attempt by continually improving the model when new data is available. We conduct similar experiments for the prediction of fine-grained diagnostic entities, but omit a comprehensive discussion due to space limitations."
    },
    {
        "id": "467",
        "table": {
            "header": [
                "<bold>Features</bold>",
                "<bold>Model</bold>",
                "<bold>Development</bold> P",
                "<bold>Development</bold> R",
                "<bold>Development</bold> F"
            ],
            "rows": [
                [
                    "text",
                    "BERT",
                    "0.69",
                    "0.55",
                    "0.61"
                ],
                [
                    "<bold>text</bold>",
                    "<bold>BERT*</bold>",
                    "<bold>0.57</bold>",
                    "<bold>0.79</bold>",
                    "<bold>0.66</bold>"
                ],
                [
                    "context",
                    "BERT",
                    "0.70",
                    "0.53",
                    "0.60"
                ],
                [
                    "context",
                    "BERT*",
                    "0.63",
                    "0.67",
                    "0.65"
                ],
                [
                    "BERT logits + handcrafted**",
                    "LR",
                    "0.70",
                    "0.56",
                    "0.61"
                ],
                [
                    "BERT logits + handcrafted**",
                    "LR*",
                    "0.60",
                    "0.71",
                    "0.65"
                ],
                [
                    "BERT logits + tagged spans",
                    "LR",
                    "0.70",
                    "0.53",
                    "0.60"
                ],
                [
                    "BERT logits + tagged spans",
                    "LR*",
                    "0.61",
                    "0.71",
                    "0.66"
                ],
                [
                    "BERT logits + all",
                    "LR",
                    "0.71",
                    "0.52",
                    "0.60"
                ],
                [
                    "BERT logits + all",
                    "LR*",
                    "0.61",
                    "0.71",
                    "0.66"
                ]
            ],
            "title": "Table 2: SLC experiments on different feature sets"
        },
        "insight": "The final model used the finetuned BERT model mentioned above with a condition to predict non-propaganda only if the prediction probability is above 0.70 for the nonpropaganda class. Otherwise the prediction of the sentence will be propaganda even if the majority of the prediction probability mass was for the non-propaganda class. This was a way to handle the unbalance in the training data without having to discard part of the data. The 0.70 threshold was chosen after elaborate experiments on both the local and the shared-task's development sets. This condition consistently provided an improvement of around 5 points in F1 score of the propaganda class on all experiments using different sets of features as shown in Table 2. [CONTINUE] In SLC, we ran multiple experiments using BERT with and without additional features as shown in Table 2. The features include using the text passed as is to BERT without any preprocessing. Also, we experimented with adding the context which includes the two sentences that come before and after the target sentence. Context sentences were concatenated and passed as the second BERT input, while the target sentence was passed as the first BERT input. In addition, we experimented with using BERT logits (i.e., the probability predictions per class) as features in a Logistic Regression (LR) classifier concatenated with handcrafted features (e.g., LIWC, quotes, questions), and with predictions of our FLC classifier (tagged spans: whether the sentence has a propaganda fragment or not). However, none of these features added any statistically significant improvements. Therefore, we used BERT predictions for our final model with a condition to predict the majority class non-propaganda only if its prediction probability is more than 0.70 as shown in Table 3. This is a modified threshold as opposed to 0.80 in the experiments shown in Table 2 to avoid overfitting on a one dataset. The final threshold of 0.70 was chosen after experiments on both the local and shared task development sets, which also represents the ratio of the non-propaganda class in the training set."
    },
    {
        "id": "468",
        "table": {
            "header": [
                "<bold>Propaganda</bold> <bold>Technique</bold>",
                "<bold>Development</bold> <bold>P</bold>",
                "<bold>Development</bold> <bold>R</bold>",
                "<bold>Development</bold> <bold>F</bold>",
                "<bold>Test</bold> <bold>F</bold>"
            ],
            "rows": [
                [
                    "Appeal to Authority",
                    "0",
                    "0",
                    "0",
                    "0.212"
                ],
                [
                    "Appeal to Fear/Prejudice",
                    "0.285",
                    "0.006",
                    "0.011",
                    "0"
                ],
                [
                    "Bandwagon",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Black-and-White Fallacy",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Causal Oversimplification",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Doubt",
                    "0.007",
                    "0.001",
                    "0.002",
                    "0"
                ],
                [
                    "Exaggeration,Minimisation",
                    "0.833",
                    "0.085",
                    "0.154",
                    "0"
                ],
                [
                    "Flag-Waving",
                    "0.534",
                    "0.102",
                    "0.171",
                    "0.195"
                ],
                [
                    "Loaded Language",
                    "0.471",
                    "0.160",
                    "0.237",
                    "0.130"
                ],
                [
                    "Name Calling,Labeling",
                    "0.270",
                    "0.112",
                    "0.158",
                    "0.150"
                ],
                [
                    "O,IV,C",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Red Herring",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Reductio ad hitlerum",
                    "0.318",
                    "0.069",
                    "0.113",
                    "0"
                ],
                [
                    "Repetition",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Slogans",
                    "0.221",
                    "0.034",
                    "0.059",
                    "0.003"
                ],
                [
                    "Straw Men",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Thought-terminating Cliches",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Whataboutism",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "<bold>Overall</bold>",
                    "0.365",
                    "0.073",
                    "0.122",
                    "0.131\u2217"
                ]
            ],
            "title": "Table 5: Precision, recall and F1 scores of the FLC task on the development and test sets of the shared task."
        },
        "insight": "We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on lower 2https://www.urbandictionary.com/ 3https://data.world/jaredfern/urban-dictionary embedding [CONTINUE] In FLC, we only show the results of our best model in Table 5 to focus more on the differences between propaganda techniques. A more elaborate study of performance of different models should follow in future work. The best model is a BiLSTM-CRF with flair and urban glove embed [CONTINUE] As we can see in Table 5, we can divide the propaganda techniques into three groups according to the model's performance on the development and test sets. The first group includes techniques with non-zero F1 scores on both datasets: Flag-Waving, Loaded Language, Name Calling,Labeling and Slogans. This group has techniques that appear frequently in the data and/or techniques with strong lexical signals (e.g. \"American People\" in Flag-Waving) or punctuation signals (e.g. quotes in Slogans). The second group has the techniques with a nonzero F1 score on only one of the datasets but not the other, such as: Appeal to Authority, Appeal to Fear, Doubt, Reduction, and Exaggeration,Minimisation. Two out of these five techniques (Appeal to Fear and Doubt) have very small non-zero F1 on the development set which indicates that they are generally challenging on our model and were only tagged due to minor differences between the two datasets. However, the remaining three types show significant drops from development to test sets or vice-versa. This requires further analysis to understand why the model was able to do well on one dataset but get zero on the other dataset, which we leave for future work. The third group has the remaining nine techniques were our sequence tagger fails to correctly tag any text span on either dataset. This group has the most infrequent types as well as types beyond the ability for our tagger to spot by looking at the sentence only such as Repetition. [CONTINUE] Overall, our model has the highest precision among all teams on both datasets, which could be due to adding the UBY [CONTINUE] one-hot encoded features that highlighted some strong signals for some propaganda types. This also could be the reason for our model to have the lowest recall among the top 7 teams on both datasets as having explicit handcrafted signals suffers from the usual sparseness that accompanies these kinds of representations which could have made the model more conservative in tagging text spans."
    },
    {
        "id": "469",
        "table": {
            "header": [
                "<bold>Metrics</bold>",
                "<bold>rocchio</bold>",
                "<bold>relevance model</bold>",
                "<bold>auto-ref</bold>",
                "<bold>rocchio + relevance</bold>",
                "<bold>auto-ref + rocchio + relevance</bold>"
            ],
            "rows": [
                [
                    "nDCG@10",
                    "0.232",
                    "0.274",
                    "0.195",
                    "0.341",
                    "<bold>0.464</bold>"
                ],
                [
                    "nDCG@100",
                    "0.360",
                    "0.397",
                    "0.329",
                    "0.439",
                    "<bold>0.536</bold>"
                ],
                [
                    "MAP",
                    "0.182",
                    "0.223",
                    "0.156",
                    "0.270",
                    "<bold>0.386</bold>"
                ]
            ],
            "title": "Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing"
        },
        "insight": "To evaluate the ranking of entity terms we have computed nDCG@10, nDCG@100 and MAP, see Table 4 for the results. We also compute Recall@k of relevant documents for automatically refined queries using the 1st, 2nd and 3rd ranked entities. The scores can be found in Table 5. Tables 4 and 5 show that the Relevance Model outperforms the Rocchio algorithm in every aspect. Both models outperform the auto-encoder approach (auto-ref ). We suspect that summing over the encodings distorts the individual features too much for a correct extraction of relevant entities to be possible. The combination of all three models (auto-ref + rocchio + relevance) outperforms the other models in most cases. Especially the performance for ranking of entity terms is increased using the autoencoded features. However, it is interesting to see that the rocchio + relevance model outperforms the recall for second and third best terms. This indicates that for user-evaluated term suggestions, the inclusion of the auto-encoded features is advisable. For automatic query refinement however, in average, this is not the case."
    },
    {
        "id": "470",
        "table": {
            "header": [
                "[EMPTY]",
                "#Lang",
                "#Task-Type",
                "Web",
                "Offline",
                "Static",
                "Models",
                "Layers",
                "Epochs"
            ],
            "rows": [
                [
                    "Faruqui and Dyer (<ref id='bib-bib7'>2014a</ref>)",
                    "4",
                    "10-WST",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Nayak et\u00a0al. (<ref id='bib-bib14'>2016</ref>)",
                    "1",
                    "7-DT",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "K\u00f6hn (<ref id='bib-bib10'>2015</ref>)",
                    "7",
                    "7-PT",
                    "[EMPTY]",
                    "\u00d7",
                    "\u00d7",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Ours",
                    "28",
                    "16-PT",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7",
                    "\u00d7"
                ]
            ],
            "title": "Table 1: Features of previous evaluation applications compared to Ours (Linspector Web). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similarity tasks, DT: Downstream Tasks, PT: Probing Tasks. Static: Static word embeddings and Models: Pretrained downstream models."
        },
        "insight": "A now retired evaluation suite for word embeddings was wordvectors.org (Faruqui and Dyer, 2014a). [CONTINUE] VecEval (Nayak et al., 2016) is another web based suite for static English word embeddings that perform evaluation on a set of downstream tasks which may take several hours. [CONTINUE] K\u00a8ohn (2015) introduced an offline, multilingual probing suite for static embeddings limited in terms of the languages and the probing tasks. [CONTINUE] A comparison of the system features of previous studies is given in Table 1."
    },
    {
        "id": "471",
        "table": {
            "header": [
                "<bold>Variants</bold>",
                "<bold>Diseases</bold>",
                "<bold>Drugs</bold>"
            ],
            "rows": [
                [
                    "H1047R",
                    "Color. Neop.",
                    "Lapatinib"
                ],
                [
                    "V600E",
                    "Liposarcoma",
                    "Mitomycin"
                ],
                [
                    "T790M",
                    "Adenocarcin.",
                    "Linsitinib"
                ],
                [
                    "E545K",
                    "Glioblastoma",
                    "Dactolisib"
                ],
                [
                    "E542K",
                    "Stomach Neop.",
                    "Pictrelisib"
                ]
            ],
            "title": "Table 6: Refinement Terms for Query {PIK3CA}"
        },
        "insight": "In Table 6 we show the top ranked entities of type Variants, Diseases and Drugs for the query {PIK3CA}. While the diseases and the drugs are all relevant, V600E and T790M are in fact not variants of the gene PIK3CA. However, when refining the query {PIK3CA, V600E, BRAF, H1047R, Dabrafenib}, the top ranked diseases are [Melanoma, Neoplasms, Carcinoma Non Small Cell Lung (CNSCL), Thyroid Neoplasms, Colorectal Neoplasms]. Using Melanoma for refinement, retrieves the top ranked paper (Falchook et al., 2013) which perfectly includes all these entities in a biomarker relationship."
    },
    {
        "id": "472",
        "table": {
            "header": [
                "Model",
                "<bold>Original data</bold> <italic>\u03c1</italic>",
                "<bold>Original data</bold> rmse",
                "<bold>Original data</bold> qw<italic>\u03ba</italic>",
                "<bold>New data</bold> <italic>\u03c1</italic>",
                "<bold>New data</bold> rmse",
                "<bold>New data</bold> qw<italic>\u03ba</italic>"
            ],
            "rows": [
                [
                    "SVM (original)",
                    ".50",
                    ".23",
                    ".44",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "SVM (reproduced)",
                    ".49",
                    ".24",
                    ".47",
                    ".50",
                    ".21",
                    ".39"
                ],
                [
                    "MLP",
                    ".42",
                    ".25",
                    ".31",
                    ".41",
                    ".22",
                    ".25"
                ],
                [
                    "BiLSTM",
                    ".49",
                    ".24",
                    ".35",
                    ".39",
                    ".24",
                    ".27"
                ]
            ],
            "title": "Table 1: Results of the difficulty prediction approaches. SVM (original) has been taken from Beinborn (2016)"
        },
        "insight": "The right-hand side of table 1 shows the performance of our SVM and the two neural methods. The results indicate that the SVM setup is well suited for the difficulty prediction task and that it successfully generalizes to new data."
    },
    {
        "id": "473",
        "table": {
            "header": [
                "Strategy",
                "Brown",
                "Reuters",
                "Gutenberg"
            ],
            "rows": [
                [
                    "SEL",
                    ".11",
                    ".12",
                    ".10"
                ],
                [
                    "SIZE",
                    ".13",
                    ".15",
                    ".12"
                ]
            ],
            "title": "Table 2: rmse for both strategies on each corpora with randomly sampled target difficulties \u03c4"
        },
        "insight": "Table 2 shows the results for our three corpora. Throughout all three corpora, both manipulation strategies perform well. SEL consistently outperforms SIZE, which matches our observations from the previous experiment."
    },
    {
        "id": "474",
        "table": {
            "header": [
                "[EMPTY]",
                "easy (dec) SEL",
                "easy (dec) SIZE",
                "default DEF",
                "hard (inc) SEL",
                "hard (inc) SIZE"
            ],
            "rows": [
                [
                    "<italic>T</italic>1",
                    "\u2013",
                    "\u2013",
                    ".30",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "<italic>T</italic>2",
                    ".17\u2217",
                    ".11\u2217",
                    ".34",
                    ".66\u2217",
                    ".44\u2217"
                ],
                [
                    "<italic>T</italic>3",
                    ".16\u2217",
                    ".10\u2217",
                    ".27",
                    ".52\u2217",
                    ".43\u2217"
                ],
                [
                    "<italic>T</italic>4",
                    ".28",
                    ".09\u2217",
                    ".30",
                    ".43\u2217",
                    ".45\u2217"
                ],
                [
                    "Average",
                    ".20\u2217",
                    ".10\u2217",
                    ".30",
                    ".53\u2217",
                    ".44\u2217"
                ]
            ],
            "title": "Table 3: Mean error rates e(T) per text and strategy. Results marked with \u2217 deviate significantly from DEF"
        },
        "insight": "Table 3 shows the error rates per C-test and strategy. Both SEL and SIZE are overall able to significantly (p < 0.025) increase and decrease the test's difficulty over DEF, and with the exception of [CONTINUE] SEL,dec , the effect is also statistically significant 4 for all individual text and strategy pairs."
    },
    {
        "id": "475",
        "table": {
            "header": [
                "[EMPTY]",
                "<bold>model</bold>",
                "<bold>with lexicon</bold> <bold>acc</bold>",
                "<bold>with lexicon</bold> <bold>acc_amb</bold>",
                "<bold>with lexicon</bold> <bold>F1-m</bold>",
                "<bold>with lexicon</bold> <bold>F1-m_amb</bold> 28.452756pt",
                "<bold>without lexicon</bold> <bold>acc</bold>",
                "<bold>without lexicon</bold> <bold>acc_amb</bold>",
                "<bold>without lexicon</bold> <bold>F1-m</bold>",
                "<bold>without lexicon</bold> <bold>F1-m_amb</bold>"
            ],
            "rows": [
                [
                    "FrameNet",
                    "Data Baseline",
                    "79.06",
                    "69.73",
                    "33.00",
                    "37.42",
                    "79.06",
                    "69.73",
                    "33.00",
                    "37.42"
                ],
                [
                    "FrameNet",
                    "Lexicon Baseline",
                    "79.89",
                    "55.52",
                    "65.61",
                    "30.95",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "FrameNet",
                    "<bold>Data-Lexicon Baseline</bold>",
                    "86.32",
                    "69.73",
                    "64.54",
                    "37.42",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "FrameNet",
                    "hermann2014semantic",
                    "88.41",
                    "73.10",
                    "\u2013",
                    "\u2013 28.452756pt",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "FrameNet",
                    "Hartmann2017OOD",
                    "87.63",
                    "73.80",
                    "\u2013",
                    "\u2013 28.452756pt",
                    "77.49",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "FrameNet",
                    "<bold>our</bold>_uni",
                    "88.66",
                    "74.92",
                    "76.65",
                    "53.86",
                    "79.96",
                    "71.70",
                    "57.07",
                    "47.40"
                ],
                [
                    "FrameNet",
                    "<bold>our</bold>_mm (im, synsV)",
                    "88.82",
                    "75.28",
                    "76.77",
                    "54.80",
                    "81.21",
                    "72.51",
                    "57.81",
                    "49.38"
                ],
                [
                    "SALSA",
                    "Data Baseline",
                    "77.00",
                    "70.51",
                    "37.40",
                    "28.87",
                    "77.00",
                    "70.51",
                    "37.40",
                    "28.87"
                ],
                [
                    "SALSA",
                    "Lexicon Baseline",
                    "61.57",
                    "52.5",
                    "19.36",
                    "15.68",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "SALSA",
                    "<bold>Data-Lexicon Baseline</bold>",
                    "77.16",
                    "70.51",
                    "38.48",
                    "28.87",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "SALSA",
                    "<bold>our</bold>_uni",
                    "80.76",
                    "75.59",
                    "48.42",
                    "41.38",
                    "80.59",
                    "75.52",
                    "47.64",
                    "41.17"
                ],
                [
                    "SALSA",
                    "<bold>our</bold>_mm (im)",
                    "80.71",
                    "75.58",
                    "48.29",
                    "41.19",
                    "80.51",
                    "75.51",
                    "47.36",
                    "40.93"
                ]
            ],
            "title": "Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni \u2013 our-mm \u2013 with Imagined embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run\u2019s results for English were: our_uni: acc: 89.35 ; acc_amb: 76.45 ; F1-m: 76.95 ; F1-m_amb: 54.02 (with lexicon) our_mm (im, synsV): acc: 89.09 ; acc_amb: 75.86 ; F1-m: 78.17 ; F1-m_amb: 57.48 (with lexicon)"
        },
        "insight": "First, we report our results on English data (see Table 3, top) [CONTINUE] we compare against German data (see Table 3, bottom). [CONTINUE] Our new strong Data-Lexicon Baseline reaches a considerable accuracy of 86.32 %, which is hard to beat by trained models. Even the most recent state of the art only beats it by about two points: 88.41 % (Hermann et al., 2014). However, the accuracy of the baseline drops for ambiguous predicates (69.73 %) and the F1-macro score reveals its weakness toward minority classes (drop from 64.54 % to 37.42 %). [CONTINUE] Our unimodal system trained and evaluated on English data slightly exceeds the accuracy of the previous state of the art (88.66 % on average versus 88.41 % for Hermann et al., 2014); our best run's accuracy is 89.35 %. [CONTINUE] Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni \u2013 our-mm \u2013 with IMAGINED embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run's results for English were: our uni: acc: 89.35 ; acc amb: 76.45 ; F1-m: 76.95 ; F1-m amb: 54.02 (with lexicon) our mm (im, synsV): acc: 89.09 ; acc amb: 75.86 ; F1-m: 78.17 ; F1-m amb: 57.48 (with lexicon) [CONTINUE] Our system evaluated on German data sets a new state of the art on this corpus with 80.76 % accuracy, outperforming the baselines (77.16 %; no other system evaluated on this dataset). [CONTINUE] We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017). On English data, our systems outperforms Hartmann et al. (2017) by more than two points in accuracy and we achieve a large improvement over the Data Baseline. Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data. For German data, the increase of F1-macro with lexicon versus without is small (one point)."
    },
    {
        "id": "476",
        "table": {
            "header": [
                "Method",
                "<bold>BLEU-1</bold>",
                "<bold>BLEU-2</bold>",
                "<bold>BLEU-3</bold>",
                "<bold>BLEU-4</bold>",
                "<bold>METEOR</bold>",
                "<bold>ROUGE</bold>",
                "<bold>CIDEr</bold>"
            ],
            "rows": [
                [
                    "Soft-Attention (xu2015show)",
                    "70.7",
                    "49.2",
                    "34.4",
                    "24.3",
                    "23.9",
                    "-",
                    "-"
                ],
                [
                    "CNN+Att (aneja2018convolutional)",
                    "71.1",
                    "53.8",
                    "39.4",
                    "28.7",
                    "24.4",
                    "52.2",
                    "91.2"
                ],
                [
                    "SCST (rennie2017self)",
                    "77.4",
                    "60.9",
                    "46.0",
                    "34.1",
                    "26.7",
                    "55.7",
                    "114.0"
                ],
                [
                    "Adaptive (lu2017knowing)",
                    "74.2",
                    "58.0",
                    "43.9",
                    "33.2",
                    "26.6",
                    "54.9",
                    "108.5"
                ],
                [
                    "GroupCap (chen2018groupcap)",
                    "74.4",
                    "58.1",
                    "44.3",
                    "33.8",
                    "26.2",
                    "-",
                    "-"
                ],
                [
                    "NBT (lu2018neural)",
                    "75.5",
                    "-",
                    "-",
                    "34.7",
                    "27.1",
                    "-",
                    "107.2"
                ],
                [
                    "StackCap (gu2018stack)",
                    "78.4",
                    "62.5",
                    "47.9",
                    "36.1",
                    "27.4",
                    "56.9",
                    "120.4"
                ],
                [
                    "Up-Down (anderson2018bottom)",
                    "79.8",
                    "63.4",
                    "48.4",
                    "36.3",
                    "27.7",
                    "56.9",
                    "120.1"
                ],
                [
                    "UGRIC",
                    "81.3",
                    "65.2",
                    "50.3",
                    "38.2",
                    "28.4",
                    "58.6",
                    "123.5"
                ],
                [
                    "UGRIC w/o copying",
                    "80.8",
                    "64.6",
                    "49.6",
                    "37.7",
                    "28.1",
                    "58.2",
                    "122.7"
                ],
                [
                    "UGRIC w/o discriminator",
                    "80.5",
                    "64.2",
                    "49.1",
                    "37.3",
                    "27.9",
                    "57.9",
                    "122.3"
                ]
            ],
            "title": "Table 1. The performances of various models on MSCOCO Karpathy split"
        },
        "insight": "The automatic evaluation results on the test set of MSCOCO are shown in Table 1. Our model outperforms all the compared approaches on all automatic evaluation metrics. In particular, by benefiting from the retrieved captions, our model gets the highest CIDEr score of 123.5, suggesting that the captions generated by our model are informative."
    },
    {
        "id": "477",
        "table": {
            "header": [
                "<bold>Model</bold>",
                "<bold>Incorrect</bold>",
                "<bold>ROUGE-1</bold>",
                "<bold>ROUGE-2</bold>",
                "<bold>ROUGE-L</bold>",
                "<bold>Length</bold>"
            ],
            "rows": [
                [
                    "PGC See17",
                    "8%",
                    "39.49%",
                    "17.24%",
                    "36.35%",
                    "59.7"
                ],
                [
                    "FAS Chen18",
                    "26%",
                    "40.88%",
                    "17.80%",
                    "38.53%",
                    "72.1"
                ],
                [
                    "BUS Gehrmann18",
                    "25%",
                    "41.52%",
                    "18.76%",
                    "38.60%",
                    "54.4"
                ]
            ],
            "title": "Table 1: Fraction of incorrect summaries produced by recent summarization systems on the CNN-DM test set, evaluated on a subset of 100 summaries. ROUGE scores (on full test set) and average summary length for reference."
        },
        "insight": "Table 1 shows the evaluation results. In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS compared to PGC. The length of the generated summaries appears to be unrelated to the number of errors. Instead, the higher abstractiveness of summaries produced by FAS and BUS, as analyzed in their respective papers, seems to also increase the chance of introducing errors. In addition, we also observe that among the three systems correctness and ROUGE scores do not correlate, emphasizing one more time that a ROUGE based evaluation alone is far too limited to account for the full scope of the summarization task."
    },
    {
        "id": "478",
        "table": {
            "header": [
                "<bold>Split</bold>",
                "<bold>NLI Model</bold>",
                "<bold>Incor.</bold>",
                "\u0394",
                "\u2191",
                "\u2193"
            ],
            "rows": [
                [
                    "Val",
                    "<italic>Original</italic>",
                    "42.1%",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Val",
                    "Random",
                    "50.7%",
                    "+8.6",
                    "16",
                    "26"
                ],
                [
                    "Val",
                    "DA",
                    "51.4%",
                    "+9.3",
                    "13",
                    "23"
                ],
                [
                    "Val",
                    "SSE",
                    "45.8%",
                    "+3.7",
                    "18",
                    "22"
                ],
                [
                    "Val",
                    "ESIM",
                    "39.3%",
                    "-2.8",
                    "23",
                    "20"
                ],
                [
                    "Val",
                    "InferSent",
                    "38.3%",
                    "-3.8",
                    "24",
                    "20"
                ],
                [
                    "[EMPTY]",
                    "BERT",
                    "28.0%",
                    "-14.1",
                    "25",
                    "10"
                ],
                [
                    "Test",
                    "<italic>Original</italic>",
                    "26.0%",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Test",
                    "ESIM",
                    "29.0%",
                    "+3.0",
                    "11",
                    "14"
                ]
            ],
            "title": "Table 2: Fraction of incorrect summaries at first position after reranking with different NLI models. \u2191 and \u2193 show the absolute number of improved (incorrect replaced by correct) and worsened (vice versa) instances."
        },
        "insight": "For 107 out of the 200 documents, an incorrect and correct summary is among the 5 alternatives. Table 2 shows that in this sample from the validation data, the fraction of incorrect summaries at first position, when the 5 alternatives are ranked as during beam search, is at 42.1%. [CONTINUE] Using entailment probabilities of ESIM and InferSent, we can slightly improve upon that and reduce incorrect summaries. However, with DA and SSE, more incorrect summaries end up in the first position. Note that these results are not in line with the model's NLI accuracies, underlining that performance on NLI does not directly transfer to our task. Only for BERT, which outperforms the other models on NLI by a large margin, we also see substantially better reranking performance. But even for this powerful model, more than half of the errors still remain in the summaries.5 Interestingly, we also find that for ESIM and InferSent, reranking hurts in many cases, leaving just a few cases of net improvement. [CONTINUE] Given the validation results, we then applied reranking to the CNN-DM test data followed by a post-hoc correctness evaluation as in Section 4. We used the ESIM model and reranked all 10 [CONTINUE] beam hypotheses generated by FAS.6 In contrast to the validation sample, the fraction of incorrect summaries increases from 26% to 29% (Table 2), demonstrating that the slight improvement on the validation data does not transfer to the test set."
    },
    {
        "id": "479",
        "table": {
            "header": [
                "\u00e0 le \u2192 au",
                "de lequel \u2192 duquel"
            ],
            "rows": [
                [
                    "\u00e0 les \u2192 aux",
                    "de lesquels \u2192 desquels"
                ],
                [
                    "\u00e0 lequel \u2192 auquel",
                    "de lesquelles \u2192 desquelles"
                ],
                [
                    "\u00e0 lesquels \u2192 auxquels",
                    "en les \u2192 \u00e8s"
                ],
                [
                    "\u00e0 lesquelles \u2192 auxquelles",
                    "vois ci \u2192 voici"
                ],
                [
                    "de le \u2192 du",
                    "vois l\u00e0 \u2192 voil\u00e0"
                ],
                [
                    "de les \u2192 des",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 2: French contraction rules."
        },
        "insight": "The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2http://universaldependencies.org/ techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). [CONTINUE] Each of these approaches has their advantages and limitations (Table 2)."
    },
    {
        "id": "480",
        "table": {
            "header": [
                "[EMPTY]",
                "Setup: Full DBLESS",
                "Setup: Full DBLESS",
                "Setup: Full WBLESS",
                "Setup: Full WBLESS",
                "Setup: Full BIBLESS",
                "Setup: Full BIBLESS",
                "Setup: Disjoint DBLESS",
                "Setup: Disjoint DBLESS",
                "Setup: Disjoint WBLESS",
                "Setup: Disjoint WBLESS",
                "Setup: Disjoint BIBLESS",
                "Setup: Disjoint BIBLESS"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL"
                ],
                [
                    "lear [Vulic:2018naaclps]",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    ".528",
                    ".531",
                    ".555",
                    ".529",
                    ".381",
                    ".389"
                ],
                [
                    "postle dffn",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    ".898",
                    ".825",
                    ".754",
                    ".746",
                    ".696",
                    ".677"
                ],
                [
                    "postle adv",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    "<bold>.942</bold>",
                    "<bold>.888</bold>",
                    "<bold>.832</bold>",
                    "<bold>.766</bold>",
                    "<bold>.757</bold>",
                    "<bold>.690</bold>"
                ]
            ],
            "title": "Table 1: Accuracy of postle models on *BLESS datasets, for two different sets of English distributional vectors: Skip-Gram (SG) and GloVe (GL). lear reports highest scores on *BLESS datasets in the literature."
        },
        "insight": "The accuracy scores on *BLESS test sets are provided in Table 1.8 Our POSTLE models display exactly the same performance as LEAR in the FULL setting: this is simply because all words found in *BLESS datasets are covered by the lexical constraints, and POSTLE does not generalize the initial LEAR transformation to unseen test words. In the DISJOINT setting, however, LEAR is left \"blind\" as it has not seen a single test word in the constraints: it leaves distributional vectors of *BLESS test words identical. In this setting, LEAR performance is equivalent to the original distributional space. In contrast, learning to generalize the LE specialization function from LEAR-specializations of other words, POSTLE models are able to successfully LE-specialize vectors of test *BLESS words. As in the graded LE, the adversarial POSTLE architecture outperforms the simpler DFFN model."
    },
    {
        "id": "481",
        "table": {
            "header": [
                "<bold>Random</bold>",
                "Target: Spanish .498",
                "Target: Spanish .498",
                "Target: Spanish .498",
                "Target: French .515",
                "Target: French .515",
                "Target: French .515"
            ],
            "rows": [
                [
                    "<bold>Distributional</bold>",
                    ".362",
                    ".362",
                    ".362",
                    ".387",
                    ".387",
                    ".387"
                ],
                [
                    "[EMPTY]",
                    "Ar",
                    "Co",
                    "Sm",
                    "Ar",
                    "Co",
                    "Sm"
                ],
                [
                    "postle dffn",
                    "<bold>.798</bold>",
                    ".740",
                    ".728",
                    ".688",
                    ".735",
                    ".742"
                ],
                [
                    "postle adv",
                    ".768",
                    "<bold>.790</bold>",
                    "<bold>.782</bold>",
                    "<bold>.746</bold>",
                    "<bold>.770</bold>",
                    "<bold>.786</bold>"
                ]
            ],
            "title": "Table 2: Average precision (AP) of postle models in cross-lingual transfer. Results are shown for both postle models (dffn and adv), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar [Artetxe:2018acl], Co [Conneau:2018iclr], and Sm [Smith:2017iclr]."
        },
        "insight": "The average precision (AP) ranking scores achieved via cross-lingual transfer of POSTLE are shown in Table 2. We report AP scores using three methods for cross-lingual word embedding induction, and compare their performance to two baselines: 1) random word pair scoring, and 2) the original (FASTTEXT) vectors. The results uncover the inability of distributional vectors to capture LE \u2013 they yield lower performance than the random baseline, which strongly emphasizes the need for the LE-specialization. The transferred POSTLE yields an immense improve"
    },
    {
        "id": "482",
        "table": {
            "header": [
                "[EMPTY]",
                "CoNLL max",
                "CoNLL MINA",
                "CoNLL head",
                "LEA max",
                "LEA MINA",
                "LEA head"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set"
                ],
                [
                    "Stanford rule-based",
                    "55.60 (8)",
                    "57.55 (8)",
                    "57.38 (8)",
                    "47.31 (8)",
                    "49.65 (8)",
                    "49.44 (8)"
                ],
                [
                    "cort",
                    "63.03 (7)",
                    "64.60 (6)",
                    "64.51 (6)",
                    "56.10 (6)",
                    "58.05 (6)",
                    "57.93 (6)"
                ],
                [
                    "Peng et al.",
                    "63.05 (6)",
                    "63.50 (7)",
                    "63.54 (7)",
                    "55.22 (7)",
                    "55.76 (7)",
                    "55.80 (7)"
                ],
                [
                    "deep-coref ranking",
                    "65.59 (5)",
                    "67.29 (5)",
                    "67.09 (5)",
                    "59.58 (5)",
                    "61.70 (5)",
                    "61.43 (5)"
                ],
                [
                    "deep-coref RL",
                    "65.81 (4)",
                    "67.50 (4)",
                    "67.36 (4)",
                    "59.76 (4)",
                    "61.84 (4)",
                    "61.64 (4)"
                ],
                [
                    "Lee et al. 2017 single",
                    "67.23 (3)",
                    "68.55 (3)",
                    "68.53 (3)",
                    "61.24 (3)",
                    "62.87 (3)",
                    "62.82 (3)"
                ],
                [
                    "Lee et al. 2017 ensemble",
                    "68.87 (2)",
                    "70.12 (2)",
                    "70.05 (2)",
                    "63.19 (2)",
                    "64.76 (2)",
                    "64.64 (2)"
                ],
                [
                    "Lee et al. 2018",
                    "72.96 (1)",
                    "74.26 (1)",
                    "75.29 (1)",
                    "67.73 (1)",
                    "69.32 (1)",
                    "70.40 (1)"
                ],
                [
                    "[EMPTY]",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref"
                ],
                [
                    "Stanford rule-based",
                    "51.78 (4)",
                    "53.79 (5)",
                    "57.10 (4)",
                    "43.28 (5)",
                    "45.48 (6)",
                    "49.28 (4)"
                ],
                [
                    "deep-coref ranking",
                    "52.90 (3)",
                    "55.16 (2)",
                    "57.13 (3)",
                    "44.40 (3)",
                    "46.98 (3)",
                    "49.05 (5)"
                ],
                [
                    "deep-coref RL",
                    "50.73 (5)",
                    "54.26 (4)",
                    "57.16 (2)",
                    "41.98 (6)",
                    "46.02 (4)",
                    "49.29 (3)"
                ],
                [
                    "Lee et al. 2017 single",
                    "50.38 (6)",
                    "52.16 (6)",
                    "54.02 (6)",
                    "43.86 (4)",
                    "45.75 (5)",
                    "47.69 (6)"
                ],
                [
                    "Lee et al. 2017 ensemble",
                    "53.63 (2)",
                    "55.03 (3)",
                    "56.80 (5)",
                    "47.50 (2)",
                    "48.98 (2)",
                    "50.87 (2)"
                ],
                [
                    "Lee et al. 2018",
                    "57.89 (1)",
                    "59.90 (1)",
                    "61.33 (1)",
                    "52.42 (1)",
                    "54.63 (1)",
                    "56.19 (1)"
                ]
            ],
            "title": "Table 4: Evaluations based on maximum span, MINA, and head spans on the CoNLL-2012 test set and WikiCoref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted."
        },
        "insight": "The reinforcement learning model of deep-coref, i.e., deep-coref RL, has the most significant difference when it is evaluated based on maximum vs. minimum spans (about 4 points). The ensemble model of e2e-coref, on the other hand, has the least difference between maximum and minimum span scores (1.4 points), which indicates it better recognizes maximum span boundaries in out-of-domain data. The ranking of systems is very different by using maximum vs. min [CONTINUE] Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). [CONTINUE] The coreference resolver of Peng et al. (2015) has the smallest difference between its maximum and minimum span evaluation scores. [CONTINUE] Based on maximum spans, Peng et al. (2015) performs on-par with cort while cort outperforms it by about one percent when they are evaluated based on minimum spans."
    },
    {
        "id": "483",
        "table": {
            "header": [
                "[EMPTY]",
                "MSE",
                "R2"
            ],
            "rows": [
                [
                    "Experiment 1",
                    "0.03448",
                    "0.12238"
                ],
                [
                    "Experiment 2",
                    "0.03068",
                    "0.17576"
                ],
                [
                    "Experiment 3, random regression",
                    "0.17112",
                    "-3.39857"
                ]
            ],
            "title": "Table 1: Quality of the regression model\u2019s predictions on the test set."
        },
        "insight": "Table 1 shows and compares MSE and R2 reported from these experiments. The results show that in experiment 2, the mean squared error is reduced and the r2 score is increased."
    },
    {
        "id": "484",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "rows": [
                [
                    "ST-CNN",
                    "22.8",
                    "10.08",
                    "48.64",
                    "42.91"
                ],
                [
                    "POS-CNN",
                    "61.40",
                    "40.98",
                    "68.26",
                    "54.85"
                ],
                [
                    "POS-HAN",
                    "[BOLD] 63.14",
                    "[BOLD] 41.30",
                    "[BOLD] 69.32",
                    "[BOLD] 57.76"
                ]
            ],
            "title": "TABLE III: The Accuracy of Different Syntactic Representations"
        },
        "insight": "Table III reports the accuracy of different syntactic representations for all the benchmark datasets. In ST encoding, the [CONTINUE] the corresponding paper. The experimental results demon strate that our proposed syntactic representation (POS-CNN outperforms the previously proposed method (ST-CNN) by the benchmark datasets (38.6% in a large margin in all CCAT10, 30.80% in CCAT50, 19.62% in BLOGS10, 11.94% in BLOGS50). This improvement in performance can be due [CONTINUE] syntactic representations are kept identical. According to Table III, POS-HAN outperforms POS-CNN model consistently across all the benchmark datasets (1.74% in CCAT10, 0.32% in CCAT50, 1.06% in BLOGS10, 2.91% in BLOGS50). This"
    },
    {
        "id": "485",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "rows": [
                [
                    "Syntactic-HAN",
                    "63.14",
                    "41.30",
                    "69.32",
                    "57.76"
                ],
                [
                    "Lexical-HAN",
                    "86.04",
                    "79.50",
                    "70.81",
                    "59.77"
                ],
                [
                    "Style-HAN",
                    "[BOLD] 90.58",
                    "[BOLD] 82.35",
                    "[BOLD] 72.83",
                    "[BOLD] 61.19"
                ]
            ],
            "title": "TABLE IV: The Accuracy of Syntactic (Syntactic-HAN), Lexical (Lexical-HAN),and combined (Style-HAN) Models"
        },
        "insight": "the table, lexical model consistently outperforms the syntactic model across all the benchmark datasets. Moreover, combining the two representations further improves the performance results. Based on the observation, we realize that even if Syntactic-HAN achieves a comparable performance results combining it with Lexical-HAN, slightly improves the overall performance (Style-HAN). As shown in Table IV, the performance improvement in terms of accuracy is consistent across all the benchmark datasets (4.54% in CCAT10, 2.85% in CCAT50, 2.02% in BLOGS10, 1.42% in BLOGS50)"
    },
    {
        "id": "486",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Combined",
                "[BOLD] Parallel"
            ],
            "rows": [
                [
                    "CCAT10",
                    "88.36",
                    "90.58"
                ],
                [
                    "CCAT50",
                    "81.21",
                    "82.35"
                ],
                [
                    "BLOG10",
                    "67.38",
                    "72.83"
                ],
                [
                    "BLOG50",
                    "58.81",
                    "61.19"
                ]
            ],
            "title": "TABLE V: The accuracy of Different Fusion Approaches"
        },
        "insight": "Table V reports the accuracy of the combined and the parallel fusion approaches. According to these results, training two parallel networks for lexical and syntax encoding achieves higher accuracy when compared to training the same network with combined embeddings."
    },
    {
        "id": "487",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "rows": [
                [
                    "SVM-affix-punctuation 3-grams",
                    "78.8",
                    "69.3",
                    "#",
                    "#"
                ],
                [
                    "CNN-char",
                    "#",
                    "#",
                    "61.2",
                    "49.4"
                ],
                [
                    "Continuous n-gram",
                    "74.8",
                    "72.6",
                    "61.34",
                    "52.82"
                ],
                [
                    "N-gram CNN",
                    "86.8",
                    "76.5",
                    "63.74",
                    "53.09"
                ],
                [
                    "Syntax-CNN",
                    "88.20",
                    "81.00",
                    "64.10",
                    "56.73"
                ],
                [
                    "Style-HAN",
                    "[BOLD] 90.58",
                    "[BOLD] 82.35",
                    "[BOLD] 72.83",
                    "[BOLD] 61.19"
                ]
            ],
            "title": "TABLE VI: Test Accuracy of models for each dataset"
        },
        "insight": "Table VI reports the accuracy of the models on the four benchmark datasets. All the results are bold. It shows that Style-HAN outperforms the baselines by 2.38%, 1.35%, 8.73%, and 4.46% over the CCAT10, CCAT50, BLOGs10, and BLOGS50 datasets, respectively."
    },
    {
        "id": "488",
        "table": {
            "header": [
                "Experiment",
                "[ITALIC] S P",
                "[ITALIC] S R",
                "[ITALIC] S F1",
                "[ITALIC] NS P",
                "[ITALIC] NS R",
                "[ITALIC] NS F1"
            ],
            "rows": [
                [
                    "SVM [ITALIC] rbl",
                    "65.55",
                    "66.67",
                    "66.10",
                    "66.10",
                    "64.96",
                    "65.52"
                ],
                [
                    "SVM [ITALIC] c+ [ITALIC] rbl",
                    "63.32",
                    "61.97",
                    "62.63",
                    "62.77",
                    "64.10",
                    "63.5"
                ],
                [
                    "LSTM [ITALIC] r",
                    "67.90",
                    "66.23",
                    "67.1",
                    "67.08",
                    "[BOLD] 68.80",
                    "67.93"
                ],
                [
                    "LSTM [ITALIC] c+LSTM [ITALIC] r",
                    "66.19",
                    "79.49",
                    "72.23",
                    "74.33",
                    "59.40",
                    "66.03"
                ],
                [
                    "LSTM [ITALIC] conditional",
                    "[BOLD] 70.03",
                    "76.92",
                    "[BOLD] 73.32",
                    "74.41",
                    "67.10",
                    "[BOLD] 70.56"
                ],
                [
                    "LSTM [ITALIC] ras",
                    "69.45",
                    "70.94",
                    "70.19",
                    "70.30",
                    "68.80",
                    "69.45"
                ],
                [
                    "LSTM [ITALIC] cas+LSTM [ITALIC] ras",
                    "66.90",
                    "[BOLD] 82.05",
                    "[BOLD] 73.70",
                    "[BOLD] 76.80",
                    "59.40",
                    "66.99"
                ],
                [
                    "LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s",
                    "65.90",
                    "74.35",
                    "69.88",
                    "70.59",
                    "61.53",
                    "65.75"
                ]
            ],
            "title": "Table 2: Experimental results for the discussion forum dataset (bold are best scores)"
        },
        "insight": "Table 2 shows the classification results on the discussion forum dataset. Although a vast majority of the context posts contain 3-4 sentences, around 100 context posts have more than ten sentences and thus we set a cutoff to a maximum of ten sentences for context modeling. For the reply r we considered the entire reply. [CONTINUE] The SV Mbl models that are based on discrete features did not perform very well, and adding context actually hurt the performance. Regarding the performance of the neural network models, we observe that modeling context improves the performance using all types of LSTM architectures that read both context (c) and reply (r) [CONTINUE] The highest performance when considering both the S and [CONTINUE] S classes is achieved by the LSTMconditional model (73.32% F1 for S class and 70.56% F1 for [CONTINUE] S, showing a 6% and 3% improvement over LSTMr for S and [CONTINUE] S classes, respectively). The LSTM model with sentence-level attentions on both context and reply (LSTMcas +LSTMras ) gives the best F1 score of 73.7% for the S class. For the [CONTINUE] S class, while we notice an improvement in precision we notice a drop in recall when compared to the LSTM model with sentence level attention only on reply (LSTMras ). [CONTINUE] We observe the performance (69.88% F1 for S category) deteriorates, probably due to the lack of enough training data."
    },
    {
        "id": "489",
        "table": {
            "header": [
                "Experiment",
                "[ITALIC] S P",
                "[ITALIC] S R",
                "[ITALIC] S F1",
                "[ITALIC] NS P",
                "[ITALIC] NS R",
                "[ITALIC] NS F1"
            ],
            "rows": [
                [
                    "SVM [ITALIC] rbl",
                    "64.20",
                    "64.95",
                    "64.57",
                    "69.0",
                    "68.30",
                    "68.7"
                ],
                [
                    "SVM [ITALIC] c+ [ITALIC] rbl",
                    "65.64",
                    "65.86",
                    "65.75",
                    "70.11",
                    "69.91",
                    "70.0"
                ],
                [
                    "LSTM [ITALIC] r",
                    "73.25",
                    "58.72",
                    "65.19",
                    "61.47",
                    "75.44",
                    "67.74"
                ],
                [
                    "LSTM [ITALIC] c+LSTM [ITALIC] r",
                    "70.89",
                    "67.95",
                    "69.39",
                    "64.94",
                    "68.03",
                    "66.45"
                ],
                [
                    "LSTM [ITALIC] conditional",
                    "76.08",
                    "[BOLD] 76.53",
                    "[BOLD] 76.30",
                    "[BOLD] 72.93",
                    "72.44",
                    "[BOLD] 72.68"
                ],
                [
                    "LSTM [ITALIC] ras",
                    "76.00",
                    "73.18",
                    "74.56",
                    "70.52",
                    "73.52",
                    "71.9"
                ],
                [
                    "LSTM [ITALIC] cas+LSTM [ITALIC] ras",
                    "[BOLD] 77.25",
                    "75.51",
                    "[BOLD] 76.36",
                    "72.65",
                    "[BOLD] 74.52",
                    "[BOLD] 73.57"
                ],
                [
                    "LSTM [ITALIC] caw+LSTM [ITALIC] raw",
                    "76.74",
                    "69.77",
                    "73.09",
                    "68.63",
                    "75.77",
                    "72.02"
                ],
                [
                    "LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s",
                    "76.42",
                    "71.37",
                    "73.81",
                    "69.50",
                    "74.77",
                    "72.04"
                ]
            ],
            "title": "Table 3: Experimental results for Twitter dataset (bold are best scores)"
        },
        "insight": "Table 3 shows the results on the Twitter dataset. [CONTINUE] As for discussion forums, adding context using the SVM models does not show a statistically significant improvement. For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply outperform the LSTM model that reads only the reply (LSTMr). The best performing architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ). LSTMconditional model shows an improvement of 11% F1 on the S class and 4-5%F1 on the [CONTINUE] NS class, compared to LSTMr. For the attentionbased models, the improvement using context is smaller (\u223c2% F1)."
    },
    {
        "id": "490",
        "table": {
            "header": [
                "Algorithm",
                "Modality",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "rows": [
                [
                    "Majority",
                    "-",
                    "25.0",
                    "50.0",
                    "33.3"
                ],
                [
                    "Random",
                    "-",
                    "49.5",
                    "49.5",
                    "49.8"
                ],
                [
                    "SVM",
                    "T",
                    "65.1",
                    "64.6",
                    "64.6"
                ],
                [
                    "SVM",
                    "A",
                    "65.9",
                    "64.6",
                    "64.6"
                ],
                [
                    "SVM",
                    "V",
                    "68.1",
                    "67.4",
                    "67.4"
                ],
                [
                    "SVM",
                    "T+A",
                    "66.6",
                    "66.2",
                    "66.2"
                ],
                [
                    "SVM",
                    "T+V",
                    "[BOLD] 72.0",
                    "[BOLD] 71.6",
                    "[BOLD] 71.6"
                ],
                [
                    "SVM",
                    "A+V",
                    "66.2",
                    "65.7",
                    "65.7"
                ],
                [
                    "SVM",
                    "T+A+V",
                    "71.9",
                    "71.4",
                    "71.5"
                ],
                [
                    "\u0394 [ITALIC] multi\u2212 [ITALIC] unimodal",
                    "[EMPTY]",
                    "\u21913.9%",
                    "\u21914.2%",
                    "\u21914.2%"
                ],
                [
                    "Error rate reduction",
                    "[EMPTY]",
                    "\u219112.2%",
                    "\u219112.9%",
                    "\u219112.9%"
                ]
            ],
            "title": "Table 2: Speaker-dependent setup. All results are averaged across five folds where each fold present weighted F-score across both sarcastic and non-sarcastic classes."
        },
        "insight": "Table 2 presents the classification results for sarcasm prediction in the speaker-dependent setup. [CONTINUE] The lowest performance is obtained with the Majority baseline which achieves 33.3% weighted Fscore (66.7% F-score for non-sarcastic class and 0% for sarcastic). The pre-trained features for the visual modality provide the best performance among the unimodal variants. The addition of textual features through concatenation improves the unimodal baseline and achieves the best performance. The tri-modal variant is unable to achieve the best score due to a slightly sub-optimal performance from the audio modality. Overall, the combination of visual and textual signals significantly improves over the unimodal variants, with a relative error rate reduction of up to 12.9%. [CONTINUE] In multiple evaluations, the multimodal variants were shown to significantly outperform their unimodal counterparts, with relative error rate reductions of up to 12.9%."
    },
    {
        "id": "491",
        "table": {
            "header": [
                "Algorithm",
                "Modality",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "rows": [
                [
                    "Majority",
                    "-",
                    "32.8",
                    "57.3",
                    "41.7"
                ],
                [
                    "Random",
                    "-",
                    "51.1",
                    "50.2",
                    "50.4"
                ],
                [
                    "SVM",
                    "T",
                    "60.9",
                    "59.6",
                    "59.8"
                ],
                [
                    "SVM",
                    "A",
                    "[BOLD] 65.1",
                    "62.6",
                    "62.7"
                ],
                [
                    "SVM",
                    "V",
                    "54.9",
                    "53.4",
                    "53.6"
                ],
                [
                    "SVM",
                    "T+A",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ],
                [
                    "SVM",
                    "T+V",
                    "62.2",
                    "61.5",
                    "61.7"
                ],
                [
                    "SVM",
                    "A+V",
                    "64.1",
                    "61.8",
                    "61.9"
                ],
                [
                    "SVM",
                    "T+A+V",
                    "64.3",
                    "62.6",
                    "62.8"
                ],
                [
                    "\u0394 [ITALIC] multi\u2212 [ITALIC] unimodal",
                    "[EMPTY]",
                    "\u21930.4%",
                    "\u21910.3%",
                    "\u21910.4%"
                ],
                [
                    "Error rate reduction",
                    "[EMPTY]",
                    "\u21931.1%",
                    "\u21910.8%",
                    "\u21911.1%"
                ]
            ],
            "title": "Table 3: Multimodal sarcasm classification. Evaluated using an speaker-independent setup. Note: T=text, A=audio, V=video."
        },
        "insight": "Table 3 presents the performance of our baselines in the speaker-independent setup. In this case, the multimodal variants do not greatly outperform the unimodal counterparts. Unlike Table 2, the audio channel plays a more important role, and it is slightly improved by adding text. [CONTINUE] In multiple evaluations, the multimodal variants were shown to significantly outperform their unimodal counterparts, with relative error rate reductions of up to 12.9%."
    },
    {
        "id": "492",
        "table": {
            "header": [
                "Setup",
                "Features",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "rows": [
                [
                    "Speaker Dependent",
                    "T",
                    "65.1",
                    "64.6",
                    "64.6"
                ],
                [
                    "Speaker Dependent",
                    "+ context",
                    "65.5",
                    "65.1",
                    "65.0"
                ],
                [
                    "Speaker Dependent",
                    "+ speaker",
                    "[BOLD] 67.7",
                    "[BOLD] 67.2",
                    "[BOLD] 67.3"
                ],
                [
                    "Speaker Dependent",
                    "Best (T + V)",
                    "72.0",
                    "71.6",
                    "[BOLD] 71.8"
                ],
                [
                    "Speaker Dependent",
                    "+ context",
                    "71.9",
                    "71.4",
                    "71.5"
                ],
                [
                    "Speaker Dependent",
                    "+ speaker",
                    "[BOLD] 72.1",
                    "[BOLD] 71.7",
                    "[BOLD] 71.8"
                ],
                [
                    "Speaker Independent",
                    "T",
                    "[BOLD] 60.9",
                    "59.6",
                    "59.8"
                ],
                [
                    "Speaker Independent",
                    "+ context",
                    "57.9",
                    "54.5",
                    "54.1"
                ],
                [
                    "Speaker Independent",
                    "+ speaker",
                    "60.7",
                    "[BOLD] 60.7",
                    "[BOLD] 60.7"
                ],
                [
                    "Speaker Independent",
                    "Best (T + A)",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ],
                [
                    "Speaker Independent",
                    "+ context",
                    "[BOLD] 65.2",
                    "[BOLD] 62.9",
                    "63.0"
                ],
                [
                    "Speaker Independent",
                    "+ speaker",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ]
            ],
            "title": "Table 4: Role of context and utterance\u2019s speaker. Note: T=text, A=audio, V=video."
        },
        "insight": "Table 4 shows the results for both evaluation settings for the textual baseline and the best multimodal variant. For the context features, we see a slight improvement in the best variant of the speaker independent setup (text plus audio); however, in other models, there is no improvement."
    },
    {
        "id": "493",
        "table": {
            "header": [
                "system",
                "Alchemy 3utts",
                "Alchemy 5utts",
                "Tangrams 3utts",
                "Tangrams 5utts",
                "Scene 3utts",
                "Scene 5utts"
            ],
            "rows": [
                [
                    "Long+16",
                    "56.8",
                    "52.3",
                    "64.9",
                    "27.6",
                    "23.2",
                    "14.7"
                ],
                [
                    "REINFORCE",
                    "58.3",
                    "44.6",
                    "[BOLD] 68.5",
                    "[BOLD] 37.3",
                    "47.8",
                    "33.9"
                ],
                [
                    "BS-MML",
                    "58.7",
                    "47.3",
                    "62.6",
                    "32.2",
                    "53.5",
                    "32.5"
                ],
                [
                    "RandoMer",
                    "[BOLD] 66.9",
                    "[BOLD] 52.9",
                    "65.8",
                    "37.1",
                    "[BOLD] 64.8",
                    "[BOLD] 46.2"
                ]
            ],
            "title": "Table 2: Comparison to prior work. Long+16 results are directly from Long et\u00a0al. (2016). Hyperparameters are chosen by best performance on validation set (see Appendix\u00a0A)."
        },
        "insight": "Table 2 compares RANDOMER to results from Long et al. (2016) as well as two baselines, REINFORCE and BSMML [CONTINUE] Our approach achieves new state-of-the-art results by a significant margin, especially on the SCENE domain, [CONTINUE] We report the results for REINFORCE, BS-MML, and RANDOMER on the seed and hyperparameters that achieve the best validation accuracy. [CONTINUE] We note that REINFORCE performs very well on TANGRAMS but worse on ALCHEMY and very poorly on SCENE."
    },
    {
        "id": "494",
        "table": {
            "header": [
                "random",
                "beam",
                "Alchemy 3utts",
                "Alchemy 5utts",
                "Tangrams 3utts",
                "Tangrams 5utts",
                "Scene 3utts",
                "Scene 5utts"
            ],
            "rows": [
                [
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search",
                    "[BOLD] classic beam search"
                ],
                [
                    "None",
                    "32",
                    "30.3",
                    "23.2",
                    "0.0",
                    "0.0",
                    "33.4",
                    "20.1"
                ],
                [
                    "None",
                    "128",
                    "59.0",
                    "46.4",
                    "60.9",
                    "28.6",
                    "24.5",
                    "13.9"
                ],
                [
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search",
                    "[BOLD] randomized beam search"
                ],
                [
                    "[ITALIC] \u03f5=0.05",
                    "32",
                    "58.7",
                    "45.5",
                    "61.1",
                    "32.5",
                    "33.4",
                    "23.0"
                ],
                [
                    "[ITALIC] \u03f5=0.15",
                    "32",
                    "[BOLD] 61.3",
                    "48.3",
                    "[BOLD] 65.2",
                    "[BOLD] 34.3",
                    "50.8",
                    "33.5"
                ],
                [
                    "[ITALIC] \u03f5=0.25",
                    "32",
                    "60.5",
                    "[BOLD] 48.6",
                    "60.0",
                    "27.3",
                    "[BOLD] 54.1",
                    "[BOLD] 35.7"
                ]
            ],
            "title": "Table 3: Randomized beam search. All listed models use gradient weight qMML and Tokens to represent execution history."
        },
        "insight": "Table 3 shows that (cid:15)-greedy randomized beam search consistently outperforms classic beam search. Even when we increase the beam size of classic beam [CONTINUE] search to 128, it still does not surpass randomized beam search with a beam of 32, and further increases yield no additional improvement."
    },
    {
        "id": "495",
        "table": {
            "header": [
                "[ITALIC] q( [BOLD] z)",
                "Alchemy 3utts",
                "Alchemy 5utts",
                "Tangrams 3utts",
                "Tangrams 5utts",
                "Scene 3utts",
                "Scene 5utts"
            ],
            "rows": [
                [
                    "[ITALIC] qRL",
                    "0.2",
                    "0.0",
                    "0.9",
                    "0.6",
                    "0.0",
                    "0.0"
                ],
                [
                    "[ITALIC] qMML( [ITALIC] q\u03b2=1)",
                    "61.3",
                    "48.3",
                    "[BOLD] 65.2",
                    "[BOLD] 34.3",
                    "50.8",
                    "33.5"
                ],
                [
                    "[ITALIC] q\u03b2=0.25",
                    "[BOLD] 64.4",
                    "[BOLD] 48.9",
                    "60.6",
                    "29.0",
                    "42.4",
                    "29.7"
                ],
                [
                    "[ITALIC] q\u03b2=0",
                    "63.6",
                    "46.3",
                    "54.0",
                    "23.5",
                    "[BOLD] 61.0",
                    "[BOLD] 42.4"
                ]
            ],
            "title": "Table 4: \u03b2-meritocratic updates. All listed models use randomized beam search, \u03f5=0.15 and Tokens to represent execution history."
        },
        "insight": "Table 4 evaluates the impact of \u03b2-meritocratic parameter updates (gradient weight q\u03b2). More uniform upweighting across reward-earning programs leads to higher accuracy and fewer spurious programs, especially in SCENE. However, no single value of \u03b2 performs best over all domains."
    },
    {
        "id": "496",
        "table": {
            "header": [
                "Hyperparameter",
                "Environments (a)",
                "Environments (b)"
            ],
            "rows": [
                [
                    "[ITALIC] \u03bbQ1 factor",
                    "1.0",
                    "1.0"
                ],
                [
                    "[ITALIC] \u03bbBC factor",
                    "1.0",
                    "1.0"
                ],
                [
                    "[ITALIC] \u03bbA factor",
                    "1.0",
                    "1.0"
                ],
                [
                    "[ITALIC] \u03bbL2 factor",
                    "1.0 [ITALIC] e\u22125",
                    "1.0 [ITALIC] e\u22125"
                ],
                [
                    "Batch size",
                    "512",
                    "512"
                ],
                [
                    "Actor learning rate",
                    "1.0 [ITALIC] e\u22123",
                    "1.0 [ITALIC] e\u22123"
                ],
                [
                    "Critic learning rate",
                    "1.0 [ITALIC] e\u22124",
                    "1.0 [ITALIC] e\u22124"
                ],
                [
                    "Memory size",
                    "5.0 [ITALIC] e5",
                    "5.0 [ITALIC] e5"
                ],
                [
                    "Expert trajectories",
                    "20",
                    "10"
                ],
                [
                    "Pre-training steps",
                    "2.0 [ITALIC] e4",
                    "2.0 [ITALIC] e4"
                ],
                [
                    "Training steps",
                    "5.0 [ITALIC] e6",
                    "5.0 [ITALIC] e5"
                ],
                [
                    "Discount factor  [ITALIC] \u03b3",
                    "0.99",
                    "0.99"
                ],
                [
                    "Hidden layers",
                    "3",
                    "3"
                ],
                [
                    "Neurons per layer",
                    "128",
                    "128"
                ],
                [
                    "Activation function",
                    "ELU",
                    "ELU"
                ]
            ],
            "title": "Table 2: Cycle-of-Learning hyperparemeters for each environment: (a) LunarLanderContinuous-v2 and (b) Microsoft AirSim."
        },
        "insight": "rel The hyperparameters used in CoL for each environment are described in the Supplementary Material. [CONTINUE] mentation to improve reproducibility of the research work. Algorithm 1 describes all steps of the proposed method and Table 2 summarize all CoL hyperparameters used on the LunarLanderContinuous-v2 and Microsoft AirSim experiments."
    },
    {
        "id": "497",
        "table": {
            "header": [
                "Model",
                "250 labeled 73257 unlabeled",
                "500 labeled 73257 unlabeled",
                "1000 labeled 73257 unlabeled"
            ],
            "rows": [
                [
                    "Supervised",
                    "40.62\u00b10.95",
                    "22.93\u00b10.67",
                    "15.54\u00b10.61"
                ],
                [
                    "Supervised (Mixup)",
                    "33.73\u00b11.79",
                    "21.08\u00b10.61",
                    "13.70\u00b10.47"
                ],
                [
                    "Supervised ( Manifold Mixup)",
                    "31.75\u00b11.39",
                    "20.57\u00b10.63",
                    "13.07\u00b10.53"
                ],
                [
                    "\u03a0 model\u00a0(Laine & Aila,  2016 )",
                    "9.93\u00b11.15",
                    "6.65\u00b10.53",
                    "4.82\u00b10.17"
                ],
                [
                    "TempEns\u00a0(Laine & Aila,  2016 )",
                    "12.62\u00b12.91",
                    "5.12\u00b10.13",
                    "4.42\u00b10.16"
                ],
                [
                    "MT\u00a0(Tarvainen & Valpola,  2017 )",
                    "4.35\u00b10.50",
                    "4.18\u00b10.27",
                    "3.95\u00b10.19"
                ],
                [
                    "VAT\u00a0(Miyato et\u00a0al.,  2018 )",
                    "\u2013",
                    "\u2013",
                    "5.42\u00b1NA"
                ],
                [
                    "VAT+Ent\u00a0(Miyato et\u00a0al.,  2018 )",
                    "\u2013",
                    "\u2013",
                    "3.86\u00b1NA"
                ],
                [
                    "VAdD\u00a0(Park et\u00a0al.,  2018 )",
                    "\u2013",
                    "\u2013",
                    "4.16\u00b10.08"
                ],
                [
                    "SNTG\u00a0(Luo et\u00a0al.,  2018 )",
                    "[BOLD] 4.29\u00b1 [BOLD] 0.23",
                    "[BOLD] 3.99\u00b1 [BOLD] 0.24",
                    "[BOLD] 3.86\u00b1 [BOLD] 0.27"
                ],
                [
                    "ICT",
                    "4.78\u00b10.68",
                    "4.23\u00b10.15",
                    "3.89\u00b10.04"
                ]
            ],
            "title": "Table 2: Error rates (%) on SVHN using CNN-13 architecture. We ran three trials for ICT."
        },
        "insight": "For SVHN, the test errors obtained by ICT are competitive with other state-of-the-art SSL methods (Table 2)."
    },
    {
        "id": "498",
        "table": {
            "header": [
                "Models",
                "Natural",
                "0.1",
                "FGSM 0.2",
                "0.3",
                "0.1",
                "BIM 0.2",
                "0.3",
                "0.1",
                "C&W 0.2",
                "0.3"
            ],
            "rows": [
                [
                    "Natural Model",
                    "99.1",
                    "67.3",
                    "12.9",
                    "4.7",
                    "22.5",
                    "0.0",
                    "0.0",
                    "21.6",
                    "0.0",
                    "0.0"
                ],
                [
                    "AdvTrain",
                    "99.1",
                    "73.0",
                    "52.7",
                    "10.9",
                    "62.0",
                    "6.5",
                    "0.0",
                    "71.09",
                    "17.0",
                    "2.1"
                ],
                [
                    "CrossEntropy",
                    "99.2",
                    "91.6",
                    "60.4",
                    "18.3",
                    "87.9",
                    "19.9",
                    "0.0",
                    "88.09",
                    "20.0",
                    "0.0"
                ],
                [
                    "Ours",
                    "98.4",
                    "91.6",
                    "70.3",
                    "41.6",
                    "88.1",
                    "64.9",
                    "26.7",
                    "89.2",
                    "72.6",
                    "37.6"
                ]
            ],
            "title": "Table 1: Test Accuracy of adversarial examples on MNIST dataset (%)"
        },
        "insight": "On MNIST dataset, our proposed gradient regularization method can achieve \u223c 90% accuracy under all considered attacks within (cid:96)\u221e = 0.1 constraints. Cross entropy gradient regularization (Ross & Doshi-Velez (2018)) achieves similar robustness as ours within (cid:96)\u221e = 0.1, but their robustness performance drops very fast when adversarial attacking strengths increases, e.g. under (cid:96)\u221e = 0.2, 0.3 attacks. The reason is that the softmax and cross entropy operation introduces unnecessary nonlinearity, in which case the gradients from cross entropy loss is already very small. [CONTINUE] Final results are shown in Table. 1 and 2."
    },
    {
        "id": "499",
        "table": {
            "header": [
                "Models",
                "Natural",
                "3",
                "FGSM 6",
                "9",
                "3",
                "BIM 6",
                "9",
                "3",
                "C&W 6",
                "9"
            ],
            "rows": [
                [
                    "Natural Model",
                    "87.2",
                    "5.8",
                    "2.4",
                    "1.6",
                    "0.7",
                    "0.0",
                    "0.0",
                    "0.6",
                    "0.0",
                    "0.0"
                ],
                [
                    "AdvTrain*",
                    "84.5",
                    "10.2",
                    "5.8",
                    "2.6",
                    "1.4",
                    "0.0",
                    "0.0",
                    "0.0",
                    "0.0",
                    "0.0"
                ],
                [
                    "CrossEntropy*",
                    "86.2",
                    "19.1",
                    "9.5",
                    "6.1",
                    "2.6",
                    "0.7",
                    "0.4",
                    "2.1",
                    "1.5",
                    "1.4"
                ],
                [
                    "Ours",
                    "84.2",
                    "59.8",
                    "41.9",
                    "31.0",
                    "54.6",
                    "29.5",
                    "20.3",
                    "53.7",
                    "29.8",
                    "20.1"
                ],
                [
                    "Ours+AdvTrain",
                    "83.1",
                    "68.5",
                    "48.5",
                    "38.2",
                    "62.7",
                    "39.3",
                    "30.3",
                    "60.5",
                    "39.0",
                    "30.3"
                ],
                [
                    "MinMax*",
                    "79.4",
                    "65.8",
                    "55.6",
                    "47.4",
                    "64.2",
                    "49.3",
                    "41.1",
                    "62.9",
                    "48.5",
                    "40.7"
                ]
            ],
            "title": "Table 2: Test Accuracy of adversarial examples on CIFAR10 dataset (%)"
        },
        "insight": "Improving robustness on CIFAR10 dataset is much harder than MNIST. State-of-the-art MinMax training achieves \u223c 40% accuracy under strongest attacks in considered settings. But this method highly relies on huge amounts of adversarial data augmentation methods, which takes over 10 times of overhead during training process. By contrast, our method doesn't need adversarial example [CONTINUE] Final results are shown in Table. 1 and 2. [CONTINUE] generation and can achieves comparable robustness under (cid:96)\u221e = 3 constraints. We test the average time of each epoch for both natural training and gradient regularized training. Our time consumption is average 2.1 times than natural training per epoch. Notice that the robustness enhancement of our method becomes lower when (cid:96)\u221e becomes larger. This shows one limitation of gradient regularization methods: Our gradient regularization approach is based on Taylor approximation in a small neighborhood. When the adversarial examples exceeds the reasonable approximating range, the gradient regularization effect also exhausts."
    },
    {
        "id": "500",
        "table": {
            "header": [
                "[BOLD] Sample number",
                "1",
                "2",
                "3"
            ],
            "rows": [
                [
                    "\u0394 [ITALIC] J",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "IP",
                    "0.91",
                    "0.88",
                    "0.72"
                ],
                [
                    "Bethe",
                    "0.57",
                    "0.58",
                    "0.60"
                ],
                [
                    "SM",
                    "1.23",
                    "1.17",
                    "1.19"
                ],
                [
                    "TRW",
                    "0.40",
                    "0.39",
                    "0.49"
                ]
            ],
            "title": "TABLE I: Distances of inferred interaction matrices from gradient ascent results"
        },
        "insight": "for all three datasets, the developed TRW approximation gave the smallest distance from the gradient ascent results. Somewhat surprisingly, SM approximation always gives the worst result."
    },
    {
        "id": "501",
        "table": {
            "header": [
                "Data sets",
                "Nodes",
                "Training",
                "Testing",
                "Bits",
                "Accuracy",
                "Time"
            ],
            "rows": [
                [
                    "[ITALIC] USPS",
                    "2",
                    "7291",
                    "2007",
                    "5",
                    "0.916",
                    "45.82\u00a0 [ITALIC] secs"
                ],
                [
                    "[ITALIC] USPS",
                    "2",
                    "7291",
                    "2007",
                    "25",
                    "0.936",
                    "227.17\u00a0 [ITALIC] secs / 3.78\u00a0 [ITALIC] mins"
                ],
                [
                    "[ITALIC] USPS",
                    "2",
                    "7291",
                    "2007",
                    "45",
                    "0.941",
                    "408.66\u00a0 [ITALIC] secs / 6.90\u00a0 [ITALIC] mins"
                ],
                [
                    "[ITALIC] Mnist",
                    "2",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "5",
                    "0.826",
                    "1690.46\u00a0 [ITALIC] secs / 28.17\u00a0 [ITALIC] mins"
                ],
                [
                    "[ITALIC] Mnist",
                    "2",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "25",
                    "0.960",
                    "8406.04\u00a0 [ITALIC] secs / 140.08\u00a0 [ITALIC] mins / 2.33\u00a0 [ITALIC] hours"
                ],
                [
                    "[ITALIC] Mnist",
                    "2",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "45",
                    "0.969",
                    "4253.28\u00a0 [ITALIC] secs / 251.85\u00a0 [ITALIC] mins / 4.20\u00a0 [ITALIC] hours"
                ],
                [
                    "[ITALIC] Mnist",
                    "10",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "5",
                    "0.839",
                    "487.88\u00a0 [ITALIC] secs / 8.13\u00a0 [ITALIC] mins"
                ],
                [
                    "[ITALIC] Mnist",
                    "10",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "25",
                    "0.962",
                    "2361.07\u00a0 [ITALIC] secs / 39.35\u00a0 [ITALIC] mins"
                ],
                [
                    "[ITALIC] Mnist",
                    "10",
                    "60 [ITALIC] K",
                    "10 [ITALIC] K",
                    "45",
                    "0.964",
                    "4253.28\u00a0 [ITALIC] secs / 70.89\u00a0 [ITALIC] mins / 1.18\u00a0 [ITALIC] hours"
                ],
                [
                    "[ITALIC] Mnist1M",
                    "10",
                    "1 [ITALIC] M",
                    "100 [ITALIC] K",
                    "5",
                    "0.824",
                    "7918.38\u00a0 [ITALIC] secs / 131.97\u00a0 [ITALIC] mins / 2.20\u00a0 [ITALIC] hours"
                ],
                [
                    "[ITALIC] Mnist1M",
                    "10",
                    "1 [ITALIC] M",
                    "100 [ITALIC] K",
                    "25",
                    "0.927",
                    "28066.37\u00a0 [ITALIC] secs / 467.77\u00a0 [ITALIC] mins / 7.80\u00a0 [ITALIC] hours"
                ],
                [
                    "[ITALIC] Mnist1M",
                    "10",
                    "1 [ITALIC] M",
                    "100 [ITALIC] K",
                    "45",
                    "0.934665",
                    "48574.96\u00a0 [ITALIC] secs / 809.58\u00a0 [ITALIC] mins / 13.49\u00a0 [ITALIC] hours"
                ]
            ],
            "title": "TABLE I: Top-10 retrieval results and running time for *SHL for various data sets. Here, running time secs means seconds, mins means minutes and hours means hours."
        },
        "insight": "As reported in Table 1, *SHL provides competitive retrieval results using about 13 hours for the 45 bits codeword. Secondly, as shown in the second and third row in the table, a larger cluster will benefit more from parallel computing. Here, Mnist run on the 10 nodes cluster need almost 25% running time compared with the same data set on a 2 node cluster. Thus, when confronting with even larger data sets, a larger cluster with more powerful nodes can be a solution."
    },
    {
        "id": "502",
        "table": {
            "header": [
                "Model",
                "SVQA",
                "TGIF-QA (*) Action",
                "TGIF-QA (*) Trans.",
                "TGIF-QA (*) Frame",
                "TGIF-QA (*) Count"
            ],
            "rows": [
                [
                    "Linguistic only",
                    "42.6",
                    "51.5",
                    "52.8",
                    "46.0",
                    "4.77"
                ],
                [
                    "Ling.+S.Frame",
                    "44.6",
                    "51.3",
                    "53.4",
                    "50.4",
                    "4.63"
                ],
                [
                    "S.Frame+MAC",
                    "58.1",
                    "67.8",
                    "76.1",
                    "57.1",
                    "4.41"
                ],
                [
                    "Avg.Pool+MAC",
                    "67.4",
                    "70.1",
                    "77.7",
                    "58.0",
                    "4.31"
                ],
                [
                    "TRN+MAC",
                    "70.8",
                    "69.0",
                    "78.4",
                    "58.7",
                    "4.33"
                ],
                [
                    "CRN+MLP",
                    "49.3",
                    "51.5",
                    "53.0",
                    "53.5",
                    "4.53"
                ],
                [
                    "[BOLD] CRN+MAC",
                    "[BOLD] 75.8",
                    "[BOLD] 71.3",
                    "[BOLD] 78.7",
                    "[BOLD] 59.2",
                    "[BOLD] 4.23"
                ]
            ],
            "title": "Table 1: Ablation studies. (*) For count, the lower the better."
        },
        "insight": "The ablation results are presented in Table 1, 2 showing progressive improvements, which justify the added complexity. [CONTINUE] From Table 1, it is clear that TGIF-QA is highly linguistically [CONTINUE] biased while the problem is mitigated with SVQA dataset to some extent. [CONTINUE] Ling.+S.Frame: This is a very basic model of VQA that combines the encoded question vector with CNN features of a random frame from a given video. As expected, this baseline gives modest improvements over the model using only linguistic features. S.Frame+MAC: To demonstrate the significance of multi-step reasoning in Video QA, we randomly select one video frame and then use its CNN feature maps as the knowledge base of the multi-step centralized reasoning module MAC. In other words, this experiment reflects the importance of multi-step reasoning in a question answering problem. As SVQA dataset contains questions with compositional sequences, it greatly benefits from the multi-step centralized reasoning module. Avg.Pool+MAC: A baseline to assess the significance of temporal information in the simplest form of average temporal pooling comparing to ones with a single frame. We follow  to sparely sample 8 frames which are the middle frames of the equal size segments from a given video. As can be seen, this model is able to achieve significant improvements in both SVQA and TGIF-QA. Due to the linguistic bias, the contribution of visual information to the overall performance on TGIF-QA is much less than that of SVQA. TRN+MAC: This baseline is a special case of ours where we flatten the hierarchy, and the relation network is applied at the frame level, similar to what proposed in . The model mitigates the limit of feature engineering process of using only one single frame for video representation as well as simply temporal pooling over the whole sequence of frames. Apparently, using a single frame loses crucial temporal information of the video and is likely to fail when strong temporal reasoning is needed, particularly in state transition and counting. We use visual features processed in the Avg.Pool+MAC experiment to input into a TRN module for fair comparisons. TRN improves by more than 12% of overall performance from one with a single video frame, while that of state transition task on TGIF-QA is more than 2%, around 1.5% for both repeating action and frame QA and 0.08 MSE in case of repetition count. Although this baseline produces great increments on SVQA comparing to the experiment Avg.Pool+MAC, it barely outperforms those on TGIF-QA. CRN+MLP: In order to evaluate how the reasoning module affects the overall performance, we conduct this experiment by using a feed-forward network as the reasoning module with the proposed visual representation CRN. CRN+MAC: This is our proposed method in which we opt CRN as the knowledge base of the compositional attention module. We experience significant improvements on all tasks over the simplistic TRN on SVQA whilst that of [CONTINUE] TGIF-QA dataset is more moderate. The results reveal the strong CRN's capability as well as a better suit of video representation for reasoning over the simplistic TRN, especially in case of compositional reasoning. The results are also consistent with our earlier arguments that sparsely sampled frames from the video are insufficient to embrace fast-pace actions/events such as repeating action and repetition count."
    },
    {
        "id": "503",
        "table": {
            "header": [
                "[EMPTY]",
                "Exist",
                "Count",
                "Integer Comparison More",
                "Integer Comparison Equal",
                "Integer Comparison Less",
                "Attribute Comparison Color",
                "Attribute Comparison Size",
                "Attribute Comparison Type",
                "Attribute Comparison Dir",
                "Attribute Comparison Shape",
                "Query Color",
                "Query Size",
                "Query Type",
                "Query Dir",
                "Query Shape",
                "All"
            ],
            "rows": [
                [
                    "SA(S) ",
                    "51.7",
                    "36.3",
                    "72.7",
                    "54.8",
                    "58.6",
                    "52.2",
                    "53.6",
                    "52.7",
                    "53.0",
                    "52.3",
                    "29.0",
                    "54.0",
                    "55.7",
                    "38.1",
                    "46.3",
                    "43.1"
                ],
                [
                    "TA-GRU(T) ",
                    "54.6",
                    "36.6",
                    "73.0",
                    "57.3",
                    "57.7",
                    "53.8",
                    "53.4",
                    "54.8",
                    "55.1",
                    "52.4",
                    "22.0",
                    "54.8",
                    "55.5",
                    "41.7",
                    "42.9",
                    "44.2"
                ],
                [
                    "SA+TA-GRU ",
                    "52.0",
                    "38.2",
                    "74.3",
                    "57.7",
                    "61.6",
                    "56.0",
                    "55.9",
                    "53.4",
                    "57.5",
                    "53.0",
                    "23.4",
                    "63.3",
                    "62.9",
                    "43.2",
                    "41.7",
                    "44.9"
                ],
                [
                    "[BOLD] CRN+MAC",
                    "[BOLD] 72.8",
                    "[BOLD] 56.7",
                    "[BOLD] 84.5",
                    "[BOLD] 71.7",
                    "[BOLD] 75.9",
                    "[BOLD] 70.5",
                    "[BOLD] 76.2",
                    "[BOLD] 90.7",
                    "[BOLD] 75.9",
                    "[BOLD] 57.2",
                    "[BOLD] 76.1",
                    "[BOLD] 92.8",
                    "[BOLD] 91.0",
                    "[BOLD] 87.4",
                    "[BOLD] 85.4",
                    "[BOLD] 75.8"
                ]
            ],
            "title": "Table 3: Comparison with the state-of-the-art method on SVQA."
        },
        "insight": "We also compare our proposed model with other state-ofthe-art methods on both two datasets, as shown in Table 3 (SVQA, synthetic) [CONTINUE] For the SVQA, Table 1 and Table 3 reveal that the contribution of visual information to the overall performance of the best known results is very little. [CONTINUE] We establish new qualitatively different SOTAs, jumping massively from 44.9% accuracy to 75.8% accuracy overall."
    },
    {
        "id": "504",
        "table": {
            "header": [
                "Model",
                "Action",
                "Trans.",
                "Frame",
                "Count"
            ],
            "rows": [
                [
                    "VIS+LSTM (aggr)",
                    "46.8",
                    "56.9",
                    "34.6",
                    "5.09"
                ],
                [
                    "VIS+LSTM (avg)",
                    "48.8",
                    "34.8",
                    "35.0",
                    "4.80"
                ],
                [
                    "VQA-MCB (aggr)",
                    "58.9",
                    "24.3",
                    "25.7",
                    "5.17"
                ],
                [
                    "VQA-MCB (avg)",
                    "29.1",
                    "33.0",
                    "15.5",
                    "5.54"
                ],
                [
                    "Yu et al.",
                    "56.1",
                    "64.0",
                    "39.6",
                    "5.13"
                ],
                [
                    "ST(R+C)",
                    "60.1",
                    "65.7",
                    "48.2",
                    "4.38"
                ],
                [
                    "ST-SP(R+C)",
                    "57.3",
                    "63.7",
                    "45.5",
                    "4.28"
                ],
                [
                    "ST-SP-TP(R+C)",
                    "57.0",
                    "59.6",
                    "47.8",
                    "4.56"
                ],
                [
                    "ST-TP(R+C)",
                    "60.8",
                    "67.1",
                    "49.3",
                    "4.40"
                ],
                [
                    "ST-TP(R+F)",
                    "62.9",
                    "69.4",
                    "49.5",
                    "4.32"
                ],
                [
                    "Co-memory(R+F)",
                    "68.2",
                    "74.3",
                    "51.5",
                    "[BOLD] 4.10"
                ],
                [
                    "PSAC(R)",
                    "70.4",
                    "76.9",
                    "55.7",
                    "4.27"
                ],
                [
                    "[BOLD] CRN+MAC(R)",
                    "[BOLD] 71.3",
                    "[BOLD] 78.7",
                    "[BOLD] 59.2",
                    "4.23"
                ]
            ],
            "title": "Table 4: Comparison with the state-of-the-art method on TGIF-QA dataset. For count, the lower the better. R: ResNet, C: C3D features, F: flow features."
        },
        "insight": "Table 4 (TGIF-QA, real). [CONTINUE] For the TGIF-QA dataset, Jang et al.  extended winner models of the VQA 2016 challenge to evaluate on Video QA task, namely VIS+LSTM  and VQA-MCB . Early fusion and late fusion are applied to both two approaches. We also list some other methods provided by  including those proposed by  and . Interestingly, none of the previous work reported ablation study of utilizing only textual cues as the input of the system to assess the linguistic bias of the dataset, and the fact that some of the reported methods produced worse performance than this baseline. We suspect that improper integrating of visual information caused confusion to the system giving such low performance. In  , Jang et al. also proposed their own methods of leveraging spatial and temporal attention. In Table 4, SP indicates spatial attention, ST presents temporal attention while \"R\" and \"C\" mean ResNet features and C3D features, respectively. Later, Gao et al.  greatly advanced the performance on this dataset with a co-memory mechanism on two video feature streams. Li et al.  recently set the state of the art performance on TGIF-QA with only ResNet features by using a novel self-attention mechanism. Our method, similarly in the sense that we only use ResNet features, is able to achieve significant improvements on three tasks including repeating action, state transition and frame QA while we are slightly advanced on repetition count task compared to our competitor  using the same video features. [CONTINUE] Specifically, we gain better performance than  of around 1% for repeating action, approximately 2% for state transition, 3.5% for frame QA, and 0.04 MSE for repetition count. [CONTINUE] For repetition counting in TGIF-QA, although we have not outperformed , it is not directly comparable since they utilized motion in addition to appearance, whilst in our case motion is not explicitly used and thus the action boundaries are not clearly detected. We hypothesize that counting task needs a specific network, as evident in recent work [21, 33]."
    },
    {
        "id": "505",
        "table": {
            "header": [
                "Methods",
                "MAML",
                "MR-MAML (A) (ours)",
                "MR-MAML (W) (ours)",
                "CNP",
                "MR-CNP (A) (ours)",
                "MR-CNP (W) (ours)"
            ],
            "rows": [
                [
                    "5 shot",
                    "0.46 (0.04)",
                    "[BOLD] 0.17 (0.03)",
                    "[BOLD] 0.16 (0.04) ",
                    "0.91 (0.10)",
                    "[BOLD] 0.10 (0.01)",
                    "[BOLD] 0.11 (0.02)"
                ],
                [
                    "10 shot",
                    "0.13 (0.01)",
                    "[BOLD] 0.07 (0.02)",
                    "[BOLD] 0.06 (0.01)",
                    "0.92 (0.05)",
                    "[BOLD] 0.09 (0.01)",
                    "[BOLD] 0.09 (0.01)"
                ]
            ],
            "title": "Table 1: Test MSE for the non-mutually-exclusive sinusoid regression problem. We compare MAML and CNP against meta-regularized MAML (MR-MAML) and meta-regularized CNP (MR-CNP) where regularization is either on the activations (A) or the weights (W). We report the mean over 5 trials and the standard deviation in parentheses."
        },
        "insight": "once we add the additional amplitude input which indicates the task identity, we find that both MAML and CNP converge to the complete memorization solution and fail to generalize well to test data (Table 1 and Appendix Figures 7 and 8). Both meta-regularized MAML and CNP (MR-MAML) and (MR-CNP) instead converge to a solution that adapts to the data, and as a result, greatly outperform the unregularized methods. [CONTINUE] For CNP, although the task training set contains sufficient information to infer the correct amplitude, this information is ignored and the regression curve at test-time is determined by the one-hot vector. As a result, CNP can only generalize to points from the curves it has seen in the training (Figure 7 first row). On the other hand, MAML does use the task training data (Figure 5, 8 and Table 1), however, its performance is much worse than in the mutually-exclusive task. MR-MAML and MR-CNP avoid converging to a memorization solution and achieve excellent test performance on sinusoid task."
    },
    {
        "id": "506",
        "table": {
            "header": [
                "Method",
                "MAML",
                "MR-MAML (W) (ours)",
                "CNP",
                "MR-CNP (W) (ours)",
                "FT",
                "FT + Weight Decay"
            ],
            "rows": [
                [
                    "MSE",
                    "5.39 (1.31)",
                    "[BOLD] 2.26 (0.09)",
                    "8.48 (0.12)",
                    "2.89 (0.18)",
                    "7.33 (0.35)",
                    "6.16 (0.12)"
                ]
            ],
            "title": "Table 2: Meta-test MSE for the pose prediction problem. We compare MR-MAML (ours) with conventional MAML and fine-tuning (FT). We report the average over 5 trials and standard deviation in parentheses."
        },
        "insight": "We compare MAML and CNP with their meta-regularized versions (Table 2). We additionally include fine-tuning as baseline, which trains a single network on all the instances jointly, and then fine-tunes on the task training data. Meta-learning with meta-regularization (on weights) outperforms all competing methods by a large margin."
    },
    {
        "id": "507",
        "table": {
            "header": [
                "[ITALIC] NME Omniglot",
                "20-way 1-shot",
                "20-way 5-shot"
            ],
            "rows": [
                [
                    "MAML",
                    "7.8 (0.2)%",
                    "50.7 (22.9)%"
                ],
                [
                    "TAML\u00a0(Jamal & Qi,  2019 )",
                    "9.6 (2.3)%",
                    "67.9 (2.3)%"
                ],
                [
                    "MR-MAML (W) (ours)",
                    "[BOLD] 83.3 (0.8)%",
                    "[BOLD] 94.1 (0.1)%"
                ]
            ],
            "title": "Table 4: Meta-test accuracy on non-mutually-exclusive (NME) classification. The fine-tuning and nearest-neighbor baseline results for MiniImagenet are from (Ravi & Larochelle, 2016)."
        },
        "insight": "We evaluate MAML, TAML (Jamal & Qi, 2019), and MR-MAML (ours)  on non-mutually-exclusive classification tasks (Table 4). We find that MR-MAML significantly outperforms previous methods on all of these tasks."
    },
    {
        "id": "508",
        "table": {
            "header": [
                "[ITALIC] NME MiniImagenet",
                "5-way 1-shot",
                "5-way 5-shot"
            ],
            "rows": [
                [
                    "Fine-tuning",
                    "28.9 (0.5))%",
                    "49.8 (0.8))%"
                ],
                [
                    "Nearest-neighbor",
                    "41.1 (0.7)%",
                    "51.0 (0.7) %"
                ],
                [
                    "MAML",
                    "26.3 (0.7)%",
                    "41.6 (2.6)%"
                ],
                [
                    "TAML\u00a0(Jamal & Qi,  2019 )",
                    "26.1 (0.6)%",
                    "44.2 (1.7)%"
                ],
                [
                    "MR-MAML (W) (ours)",
                    "[BOLD] 43.6 (0.6)%",
                    "[BOLD] 53.8 (0.9)%"
                ]
            ],
            "title": "Table 4.2: Meta-test accuracy on non-mutually-exclusive (NME) classification. The fine-tuning and nearest-neighbor baseline results for MiniImagenet are from (Ravi & Larochelle, 2016)."
        },
        "insight": "We evaluate MAML, TAML (Jamal & Qi, 2019), MR-MAML (ours), fine-tuning, and a nearest neighbor baseline on non-mutually-exclusive classification tasks (Table 4). We find that MR-MAML significantly outperforms previous methods on all of these tasks."
    },
    {
        "id": "509",
        "table": {
            "header": [
                "Method",
                "Average error rate Food-101N",
                "Average error rate Clothing-1M"
            ],
            "rows": [
                [
                    "Supervised",
                    "Supervised",
                    "Supervised"
                ],
                [
                    "MLP",
                    "10.42",
                    "16.09"
                ],
                [
                    "Label Prop ",
                    "13.24",
                    "17.81"
                ],
                [
                    "Label Spread ",
                    "12.03",
                    "17.71"
                ],
                [
                    "CleanNet ",
                    "6.99",
                    "15.77"
                ],
                [
                    "Weakly-Supervised",
                    "Weakly-Supervised",
                    "Weakly-Supervised"
                ],
                [
                    "Cls. Filt.",
                    "16.60",
                    "23.55"
                ],
                [
                    "Avg. Base. ",
                    "16.20",
                    "30.56"
                ],
                [
                    "Unsupervised",
                    "Unsupervised",
                    "Unsupervised"
                ],
                [
                    "DRAE ",
                    "18.70",
                    "38.95"
                ],
                [
                    "unsup-kNN",
                    "26.63",
                    "43.31"
                ],
                [
                    "NoiseRank",
                    "24.02",
                    "23.54"
                ],
                [
                    "[BOLD] NoiseRank (I)",
                    "[BOLD] 18.43",
                    "[BOLD] 22.81"
                ]
            ],
            "title": "Table 2: Label noise detection accuracy. Left: average error rate over all the classes (%) Right: Label noise recall, F1 and macro-F1 (%). NoiseRank(I) is iterative NoiseRank"
        },
        "insight": "We report the effectiveness of our proposed method on detecting label noise in Table 2, in terms of averaged detection error rate over all classes in Food101-N and Clothing-1M. Table 2 details the average error rate of label noise detection on the verified validation set compared against a wide range of baselines. [CONTINUE] On Food101-N the estimated noise is 19.66% and the avg error rate of iterative NoiseRank and DRAE are comparable."
    },
    {
        "id": "510",
        "table": {
            "header": [
                "Method",
                "Type",
                "Recall",
                "F1",
                "MacroF1"
            ],
            "rows": [
                [
                    "Food-101N (19.66% estimated noise)",
                    "Food-101N (19.66% estimated noise)",
                    "Food-101N (19.66% estimated noise)",
                    "Food-101N (19.66% estimated noise)",
                    "Food-101N (19.66% estimated noise)"
                ],
                [
                    "CleanNet",
                    "sup.",
                    "71.06",
                    "[BOLD] 74.01",
                    "[BOLD] 84.04"
                ],
                [
                    "Avg. Base.",
                    "weakly",
                    "47.70",
                    "59.57",
                    "76.08"
                ],
                [
                    "unsup-kNN",
                    "unsup.",
                    "22.02",
                    "24.23",
                    "54.03"
                ],
                [
                    "NoiseRank (I)",
                    "unsup.",
                    "[BOLD] 85.61",
                    "64.42",
                    "76.06"
                ],
                [
                    "Clothing-1M (38.46% estimated noise)",
                    "Clothing-1M (38.46% estimated noise)",
                    "Clothing-1M (38.46% estimated noise)",
                    "Clothing-1M (38.46% estimated noise)",
                    "Clothing-1M (38.46% estimated noise)"
                ],
                [
                    "CleanNet",
                    "sup.",
                    "69.40",
                    "[BOLD] 73.99",
                    "[BOLD] 79.65"
                ],
                [
                    "Avg. Base.",
                    "weakly",
                    "43.92",
                    "55.14",
                    "67.65"
                ],
                [
                    "unsup-kNN",
                    "unsup.",
                    "10.85",
                    "16.60",
                    "44.26"
                ],
                [
                    "NoiseRank (I)",
                    "unsup.",
                    "[BOLD] 74.18",
                    "71.74",
                    "76.52"
                ]
            ],
            "title": "Table 2: Label noise detection accuracy. Left: average error rate over all the classes (%) Right: Label noise recall, F1 and macro-F1 (%). NoiseRank(I) is iterative NoiseRank"
        },
        "insight": "we further measure recall, F1 and macro-F1 scores for label noise detection in Table 2. NoiseRank has state-of-the-art recall of 85.61% on Food-101N and 74.18% on Clothing-1M. NoiseRank F1/MacroF1 is competitive with the best supervised method in noise detection CleanNet  which requires verified labels in training and validation, and thus has a significant advantage compared to unsupervised and weakly-supervised methods."
    },
    {
        "id": "511",
        "table": {
            "header": [
                "[BOLD] #",
                "Method",
                "Training",
                "Pre-training",
                "Top-1"
            ],
            "rows": [
                [
                    "1",
                    "None ",
                    "noisy train",
                    "ImageNet",
                    "81.44"
                ],
                [
                    "2",
                    "CleanNet ",
                    "noisy(+verified)",
                    "ImageNet",
                    "83.95"
                ],
                [
                    "3",
                    "DeepSelf ",
                    "noisy train",
                    "ImageNet",
                    "85.11"
                ],
                [
                    "4",
                    "NoiseRank",
                    "cleaned train",
                    "ImageNet",
                    "85.20"
                ],
                [
                    "5",
                    "[EMPTY]",
                    "cleaned train",
                    "noisy train #1",
                    "[BOLD] 85.78"
                ]
            ],
            "title": "Table 3: Image classification on Food-101N results in terms of top-1 accuracy (%). Train data (310k) and test data (25k). CleanNet is trained with an additional 55k/5k (tr/va) verification labels to provide the required supervision on noise detection"
        },
        "insight": "In results table 3 and 4, in each row, the model is fine-tuned with the mentioned training examples on the specified pre-trained model (eg. \"noisy train # 1\" refers to the model # 1 referenced in the table that was trained using noisy training samples on ImageNet pre-trained Resnet-50). [CONTINUE] In Table 3 Food101-N, NoiseRank achieves state of the art 85.78% in top-1 accuracy compared to unsupervised 's 85.11%, and 11% error reduction over supervised noise reduction method CleanNet."
    },
    {
        "id": "512",
        "table": {
            "header": [
                "#",
                "Method",
                "Training",
                "Pre-training",
                "Top-1"
            ],
            "rows": [
                [
                    "1",
                    "None ",
                    "clean50k",
                    "ImageNet",
                    "75.19"
                ],
                [
                    "2",
                    "None ",
                    "noisy train",
                    "ImageNet",
                    "68.94"
                ],
                [
                    "3",
                    "[EMPTY]",
                    "clean50k",
                    "noisy train # 2",
                    "79.43"
                ],
                [
                    "4",
                    "loss cor.",
                    "noisy(+verified)",
                    "ImageNet",
                    "69.84"
                ],
                [
                    "5",
                    "[EMPTY]",
                    "clean50k",
                    "# 4",
                    "80.38"
                ],
                [
                    "6",
                    "Joint opt. ",
                    "noisy train",
                    "ImageNet",
                    "72.16"
                ],
                [
                    "7",
                    "PENCIL ",
                    "noisy train",
                    "ImageNet",
                    "73.49"
                ],
                [
                    "8",
                    "CleanNet ",
                    "noisy(+verified)",
                    "ImageNet",
                    "74.69"
                ],
                [
                    "9",
                    "[EMPTY]",
                    "clean50k",
                    "# 8",
                    "79.90"
                ],
                [
                    "10",
                    "DeepSelf ",
                    "noisy train",
                    "ImageNet",
                    "74.45"
                ],
                [
                    "11",
                    "[EMPTY]",
                    "clean50k",
                    "# 10",
                    "[BOLD] 81.16"
                ],
                [
                    "12",
                    "[EMPTY]",
                    "cleaned train",
                    "ImageNet",
                    "73.77"
                ],
                [
                    "13",
                    "NoiseRank",
                    "cleaned train",
                    "noisy train #2",
                    "73.82"
                ],
                [
                    "14",
                    "[EMPTY]",
                    "clean50k",
                    "# 13",
                    "79.57"
                ]
            ],
            "title": "Table 4: Image classification on Clothing-1M results in terms of top-1 accuracy (%). Train data (1M) and test data (10k). CleanNet and Loss Correction are trained with an additional 25k/7k (train/validation) verification labels to provide required supervision on noise detection/correction"
        },
        "insight": "In results table 3 and 4, in each row, the model is fine-tuned with the mentioned training examples on the specified pre-trained model (eg. \"noisy train # 1\" refers to the model # 1 referenced in the table that was trained using noisy training samples on ImageNet pre-trained Resnet-50). [CONTINUE] in Table 4, \"# 4\" in the pre-training column, refers to model # 4 indicated in the table. [CONTINUE] In Table 4 Clothing-1M, NoiseRank used to reduce label noise in noisy train (\u223c40% estimated noise) is effective in improving classification from 68.94% to 73.82% (16% error reduction), even [CONTINUE] without supervision from clean set in high noise regime, and performs comparable to recent unsupervised  and marginally outperforms unsupervised PENCIL . [CONTINUE] Lastly, we also reported results of fine-tuning each method with an additional clean 50k set, as per the setting followed in .  achieves best result of 81.16% with clean 50k sample set. We note that even without noise correction, the inclusion of the clean set boosts accuracy from 68.94% to 79.43% and may shadow the benefit of noise removal; with CleanNet  at 79.90% and ours at 79.57% being comparable in this setting."
    },
    {
        "id": "513",
        "table": {
            "header": [
                "Layer Name",
                "conv1_1",
                "conv1_2",
                "pool1",
                "conv2_1",
                "conv2_2",
                "pool2",
                "conv3_1",
                "conv3_2",
                "conv3_3",
                "pool3"
            ],
            "rows": [
                [
                    "#Neuron",
                    "64",
                    "64",
                    "64",
                    "128",
                    "128",
                    "128",
                    "256",
                    "256",
                    "256",
                    "256"
                ],
                [
                    "#Left Eye",
                    "1",
                    "-",
                    "-",
                    "-",
                    "2",
                    "3",
                    "4",
                    "2",
                    "3",
                    "2"
                ],
                [
                    "#Right Eye",
                    "1",
                    "-",
                    "-",
                    "-",
                    "3",
                    "3",
                    "4",
                    "3",
                    "2",
                    "3"
                ],
                [
                    "#Nose",
                    "1",
                    "-",
                    "-",
                    "-",
                    "1",
                    "3",
                    "2",
                    "-",
                    "1",
                    "3"
                ],
                [
                    "#Mouth",
                    "1",
                    "-",
                    "-",
                    "-",
                    "3",
                    "2",
                    "4",
                    "3",
                    "15",
                    "7"
                ],
                [
                    "#Left Eye & Right Eye",
                    "1",
                    "-",
                    "-",
                    "-",
                    "2",
                    "3",
                    "3",
                    "1",
                    "-",
                    "-"
                ],
                [
                    "#Left Eye & Nose",
                    "1",
                    "-",
                    "-",
                    "-",
                    "1",
                    "3",
                    "2",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Left Eye & Mouth",
                    "1",
                    "-",
                    "-",
                    "-",
                    "2",
                    "1",
                    "2",
                    "1",
                    "1",
                    "-"
                ],
                [
                    "#Right Eye & Nose",
                    "1",
                    "-",
                    "-",
                    "-",
                    "1",
                    "3",
                    "1",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Right Eye & Mouth",
                    "1",
                    "-",
                    "-",
                    "-",
                    "3",
                    "1",
                    "2",
                    "2",
                    "1",
                    "1"
                ],
                [
                    "#Nose & Mouth",
                    "1",
                    "-",
                    "-",
                    "-",
                    "1",
                    "1",
                    "1",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Shared",
                    "1",
                    "-",
                    "-",
                    "-",
                    "1",
                    "1",
                    "1",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Layer Name",
                    "conv4_1",
                    "conv4_2",
                    "conv4_3",
                    "pool4",
                    "conv5_1",
                    "conv5_2",
                    "conv5_3",
                    "pool5",
                    "fc6",
                    "fc7"
                ],
                [
                    "#Neuron",
                    "512",
                    "512",
                    "512",
                    "512",
                    "512",
                    "512",
                    "512",
                    "512",
                    "4096",
                    "4096"
                ],
                [
                    "#Left Eye",
                    "9",
                    "5",
                    "15",
                    "7",
                    "12",
                    "4",
                    "1",
                    "1",
                    "-",
                    "1"
                ],
                [
                    "#Right Eye",
                    "7",
                    "3",
                    "10",
                    "9",
                    "9",
                    "1",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Nose",
                    "10",
                    "8",
                    "17",
                    "13",
                    "7",
                    "2",
                    "2",
                    "1",
                    "-",
                    "1"
                ],
                [
                    "#Mouth",
                    "19",
                    "12",
                    "12",
                    "11",
                    "8",
                    "2",
                    "1",
                    "2",
                    "1",
                    "1"
                ],
                [
                    "#Left Eye & Right Eye",
                    "5",
                    "1",
                    "3",
                    "4",
                    "2",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Left Eye & Nose",
                    "3",
                    "-",
                    "4",
                    "-",
                    "1",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Left Eye & Mouth",
                    "1",
                    "1",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Right Eye & Nose",
                    "3",
                    "-",
                    "1",
                    "1",
                    "1",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Right Eye & Mouth",
                    "2",
                    "-",
                    "2",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Nose & Mouth",
                    "5",
                    "1",
                    "2",
                    "2",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "#Shared",
                    "1",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ]
            ],
            "title": "Table 1: Number of extracted attribute witnesses in VGG-Face. The first row lists the layers used in VGG-Face. The second row shows the number of neurons at each layer. The following four rows present the number of extracted witnesses for individual attributes. The following six rows denote the pairwise overlap of witnesses between different attributes. The bottom row shows the number of neurons shared by the witness sets of different attributes."
        },
        "insight": "The witnesses extracted can be found in Table 1, which shows the layers (1st row), the number of neurons in each layer (2nd row) and the number of witnesses (remaining rows). Note that although there are 64-4096 neurons in each layer, the number of witnesses extracted is smaller than 20."
    },
    {
        "id": "514",
        "table": {
            "header": [
                "Metric",
                "[BOLD] Detectron",
                "[BOLD] Ours"
            ],
            "rows": [
                [
                    "AP",
                    "0.4759",
                    "[BOLD] 0.477"
                ],
                [
                    "AP50",
                    "0.7687",
                    "[BOLD] 0.7696"
                ],
                [
                    "AP75",
                    "0.539",
                    "[BOLD] 0.541"
                ],
                [
                    "APm",
                    "0.4173",
                    "[BOLD] 0.42"
                ],
                [
                    "APl",
                    "0.4973",
                    "[BOLD] 0.4989"
                ]
            ],
            "title": "Table 2. Results on the MPII validation dataset"
        },
        "insight": "This algorithm allows us to cover up for some of the limitations that the Detectron has, and we show that we achieve slight improvements over the Detectron and Part Affinity Field baselines with absolutely zero retraining. [CONTINUE] Note that the skeleton structure of Detectron and PAFs are different (COCO v/s MPII). We ran on COCO keypoints 2017 validation dataset, and MPII validation dataset."
    },
    {
        "id": "515",
        "table": {
            "header": [
                "Metric",
                "[BOLD] Detectron",
                "[BOLD] PAF test-dev",
                "[BOLD] PAF test-challenge",
                "[BOLD] Ours"
            ],
            "rows": [
                [
                    "AP",
                    "0.6423",
                    "0.618",
                    "0.605",
                    "[BOLD] 0.6465"
                ],
                [
                    "AP50",
                    "0.8643",
                    "0.849",
                    "0.834",
                    "[BOLD] 0.8677"
                ],
                [
                    "AP75",
                    "0.6992",
                    "0.675",
                    "0.664",
                    "[BOLD] 0.7044"
                ],
                [
                    "APm",
                    "0.5854",
                    "0.571",
                    "0.551",
                    "[BOLD] 0.5904"
                ],
                [
                    "APl",
                    "0.7339",
                    "0.682",
                    "0.681",
                    "[BOLD] 0.7361"
                ]
            ],
            "title": "Table 1: Results on the COCO keypoints 2017 validation and test datasets"
        },
        "insight": "This algorithm allows us to cover up for some of the limitations that the Detectron has, and we show that we achieve slight improvements over the Detectron and Part Affinity Field baselines with absolutely zero retraining. [CONTINUE] Note that the skeleton structure of Detectron and PAFs are different (COCO v/s MPII). We ran on COCO keypoints 2017 validation dataset, and MPII validation dataset."
    },
    {
        "id": "516",
        "table": {
            "header": [
                "[EMPTY]",
                "Action Set 6",
                "Action Set 8",
                "Action Set 20",
                "Action Set 27",
                "Action Set 54"
            ],
            "rows": [
                [
                    "Limit Jump Actions",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]"
                ],
                [
                    "Limit Backward Actions",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Limit Forward Actions",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] Ave Floor",
                    "3",
                    "[BOLD] 5",
                    "[BOLD] 5",
                    "[BOLD] 5",
                    "3"
                ]
            ],
            "title": "Table 5: Comparison of action sets and their impact on the maximum floor reached after 10m steps."
        },
        "insight": "We found that the 8, 20, and 27 action sets all saw a similar improvement in performance going from an average of three floors to an average of five floors (see Table 5)."
    },
    {
        "id": "517",
        "table": {
            "header": [
                "Dataset Attribute",
                "VF Parkhi et\u00a0al. ( 2015 ) Left Eye",
                "VF Parkhi et\u00a0al. ( 2015 ) Right Eye",
                "VF Parkhi et\u00a0al. ( 2015 ) Nose",
                "VF Parkhi et\u00a0al. ( 2015 ) Mouth",
                "LFW Huang et\u00a0al. ( 2007 ) Left Eye",
                "LFW Huang et\u00a0al. ( 2007 ) Right Eye",
                "LFW Huang et\u00a0al. ( 2007 ) Nose",
                "LFW Huang et\u00a0al. ( 2007 ) Mouth"
            ],
            "rows": [
                [
                    "Face Descriptor",
                    "0.830",
                    "0.830",
                    "0.955",
                    "0.855",
                    "0.825",
                    "0.835",
                    "0.915",
                    "0.935"
                ],
                [
                    "Attribute Witness",
                    "0.940",
                    "0.935",
                    "0.985",
                    "0.990",
                    "0.870",
                    "0.845",
                    "0.975",
                    "0.965"
                ]
            ],
            "title": "Table 2: Accuracy of attribute detection. Attribute detection using extracted witnesses versus using an existing layer in VGG-Face called face descriptor (i.e., the fc7 layer), whose neurons are considered representing abstract features of human faces Parkhi et\u00a0al. (2015)."
        },
        "insight": "The results are shown in Table 2. For the VF dataset, witnesses consistently achieve higher than 93% accuracy whereas face descriptor has lower than 86% for 3 attributes. Since LFW is a dataset with a different set of persons from those in the training set (from VF), we observe a decrease of accuracy. However, attribute witnesses still have higher accuracy than face descriptor."
    },
    {
        "id": "518",
        "table": {
            "header": [
                "[EMPTY]",
                "Numenta_Standard",
                "F1_T_Front_Flat",
                "F1_T_Back_Flat"
            ],
            "rows": [
                [
                    "Front-Predicted",
                    "0.67",
                    "0.42",
                    "0.11"
                ],
                [
                    "Back-Predicted",
                    "0.63",
                    "0.11",
                    "0.42"
                ]
            ],
            "title": "Table 2: Sensitivity to positional bias"
        },
        "insight": "To further illustrate, we investigated how the two models behave under two contrasting positional bias scenarios: (i) anomaly predictions overlapping with front-end portions of real anomaly ranges (Front-Predicted) and (ii) anomaly predictions overlapping with back-end portions of real anomaly ranges (Back-Predicted) as shown in Table 2. [CONTINUE] We then scored them using Numenta_Standard , F1 _T _Front_Flat, and F1 _T _Back _Flat. [CONTINUE] As shown in Table 2, NAB's scoring function is not sufficiently sensitive to distinguish between the two scenarios. [CONTINUE] However, our model distinguishes between the two scenarios when its positional bias is set appropriately."
    },
    {
        "id": "519",
        "table": {
            "header": [
                "Algorithm",
                "Kosarak",
                "MSNBC",
                "BMS2",
                "BMS1",
                "Kosarak(small)"
            ],
            "rows": [
                [
                    "MPP1",
                    "47",
                    "45",
                    "2",
                    "-",
                    "-"
                ],
                [
                    "MPP2",
                    "106",
                    "103",
                    "4",
                    "-",
                    "-"
                ],
                [
                    "MPP3",
                    "151",
                    "158",
                    "5",
                    "-",
                    "-"
                ],
                [
                    "MPP",
                    "-",
                    "-",
                    "2",
                    "0.5",
                    "4"
                ]
            ],
            "title": "Table 4: Time (in seconds) required for MDD construction and information generation."
        },
        "insight": "Table 4 shows that the time required to construct the MDD database and generate constraint specific information is quite small. This indicates that our MDD database can be used to effectively and efficiently handle constraints such as average and median."
    },
    {
        "id": "520",
        "table": {
            "header": [
                "RiverSwim",
                "E3 3.0",
                "R-Max 3.0",
                "MBIE 3.3",
                "ESSR 3.1",
                "ESSR (0.06)"
            ],
            "rows": [
                [
                    "[0.5pt/2pt] SixArms",
                    "1.8",
                    "2.8",
                    "9.3",
                    "7.3",
                    "(1.2)"
                ]
            ],
            "title": "Table 2: Comparison between ESSR, R-Max, E3, and MBIE. The numbers reported for R-Max, E3, and MBIE were extracted from the histograms presented by Strehl08\u00a0(Strehl08). ESSR\u2019s performance is the average over 100 runs. A 95% confidence interval is reported between parentheses. All numbers are reported in millions (i.e., \u00d7106)."
        },
        "insight": "that performs as well as some theoretically sample-efficient approaches. [CONTINUE] in the tabular case, perform as well as traditional approaches with PAC-MDP guarantees. [CONTINUE] Table 2 depicts the performance of this algorithm, dubbed ESSR, as well as the performance of some algorithms with polynomial sample-complexity bounds. [CONTINUE] These results clearly show that ESSR performs, on average, similarly to other algorithms with PAC-MDP guarantees, suggesting that the norm of the SSR is a promising exploration bonus."
    },
    {
        "id": "521",
        "table": {
            "header": [
                "Freeway",
                "DQN 32.4",
                "DQN (0.3)",
                "DQNMMC [ITALIC] e 29.5",
                "DQNMMC [ITALIC] e (0.1)",
                "DQNMMC\\scriptsize{CTS} 29.2",
                "DQNMMC\\scriptsize{PixelCNN} 29.4",
                "RND -",
                "RND -",
                "DQNMMC [ITALIC] e+SR 29.4",
                "DQNMMC [ITALIC] e+SR (0.1)"
            ],
            "rows": [
                [
                    "[0.5pt/2pt] Gravitar",
                    "118.5",
                    "(22.0)",
                    "1078.3",
                    "(254.1)",
                    "199.8",
                    "275.4",
                    "790.0",
                    "(122.9)",
                    "457.4",
                    "(120.3)"
                ],
                [
                    "[0.5pt/2pt] Mont. Rev.",
                    "0.0",
                    "(0.0)",
                    "0.0",
                    "(0.0)",
                    "2941.9",
                    "1671.7",
                    "524.8",
                    "(314.0)",
                    "1395.4",
                    "(1121.8)"
                ],
                [
                    "[0.5pt/2pt] Private Eye",
                    "1447.4",
                    "(2,567.9)",
                    "113.4",
                    "(42.3)",
                    "32.8",
                    "14386.0",
                    "61.3",
                    "(53.7)",
                    "104.4",
                    "(50.4)"
                ],
                [
                    "[0.5pt/2pt] Solaris",
                    "783.4",
                    "(55.3)",
                    "2244.6",
                    "(378.8)",
                    "1147.1",
                    "2279.4",
                    "1270.3",
                    "(291.0)",
                    "1890.1",
                    "(163.1)"
                ],
                [
                    "[0.5pt/2pt] Venture",
                    "4.4",
                    "(5.4)",
                    "1220.1",
                    "(51.0)",
                    "0.0",
                    "856.2",
                    "953.7",
                    "(167.3)",
                    "1348.5",
                    "(56.5)"
                ]
            ],
            "title": "Table 3: Performance of the proposed algorithm, DQNMMCe+SR, compared to various agents on the \u201chard exploration\u201d subset of Atari 2600 games. The DQN results reported are from Machado18a\u00a0(Machado18a) while the DQNMMC\\scriptsize{CTS}, DQNMMC\\scriptsize{PixelCNN} and RND results were obtained through personal communication with the authors of the corresponding papers. Burda19 did not evaluate RND in Freeway. When available, standard deviation is reported between parentheses. See text for details."
        },
        "insight": "it achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime. [CONTINUE] achieves state-of-the-art performance in hard exploration Atari 2600 games when in a low sample-complexity regime. [CONTINUE] Table 3 summarizes the results after 100 million frames. [CONTINUE] CTS and DQNMMC e We can clearly see that our algorithm achieves scores much higher than those achieved by DQN, which struggles in games that pose hard exploration problems. When comparing our algorithm to DQNMMC PIXELCNN we observe that, on average, DQNMMC +SR at least matches the performance of these algorithms while being simpler by not requiring a density model. Instead, our algorithm requires the SR, which is domain-independent as it is already defined for every problem since it is a component of the value function estimates, as discussed in Section 2. Finally, DQNMMC +SR also outperforms RND (Burda et al. 2019) when it is trained for 100 million frames.4 e [CONTINUE] e the comparison between DQNMMC +SR and Finally, DQNMMC shows that the provided exploration bonus has a big impact in the game MONTEZUMA'S REVENGE, which is probably known as the hardest game among those we used in our evaluation, and the only game where agents do not learn how to achieve scores greater than zero with random e [CONTINUE] exploration. Interestingly, the change in architecture and the use of MMC leads to a big improvement in games such as GRAVITAR and VENTURE, which we cannot fully explain. However, because the change in architecture does not have any effect in MONTEZUMA'S REVENGE, it seems that the proposed exploration bonus is essential in games with very sparse rewards. [CONTINUE] While the results depicted in Table 3 allow us to see the benefit of using an exploration bonus derived from the SR, they do not inform us about the impact of the auxiliary task in the results. [CONTINUE] the comparisons in Table 3 and the results in this section suggest that the exploration bonus we introduced is essential for our approach to achieve state-of-the-art performance."
    },
    {
        "id": "522",
        "table": {
            "header": [
                "RiverSwim",
                "Sarsa 24,770",
                "Sarsa (196)",
                "Sarsa + SR 1,213,544",
                "Sarsa + SR (540,454)"
            ],
            "rows": [
                [
                    "[0.5pt/2pt] SixArms",
                    "247,977",
                    "(4,970)",
                    "1,052,934",
                    "(2,311,617)"
                ]
            ],
            "title": "Table 1: Comparison between Sarsa and Sarsa+SR. A 95% confidence interval is reported between parentheses."
        },
        "insight": "The performance of each algorithm, averaged over 100 runs, is available in Table 1. [CONTINUE] Sarsa obtains an average return of approximately 25,000 while Sarsa+SR obtains an approximate average return of 1.2 million."
    },
    {
        "id": "523",
        "table": {
            "header": [
                "RiverSwim",
                "\u21131-norm 1,213,544",
                "\u21131-norm (540,454)",
                "\u21132-norm 1,192,052",
                "\u21132-norm (507,179)"
            ],
            "rows": [
                [
                    "[0.5pt/2pt] SixArms",
                    "1,052,934",
                    "(2,311,617)",
                    "819,927",
                    "(2,132,003)"
                ]
            ],
            "title": "Table 5: Performance of Sarsa+SR when using the \u21131-norm and \u21132-norm of the SR to generate the exploration bonus. A 95% confidence interval is reported between parentheses."
        },
        "insight": "The results reported for Sarsa+SR when using the (cid:96)2-norm of the SR are the average over 100 runs. The actual numbers are available in Table 5. As before, it seems that it does not make much difference which norm of the SR we use ((cid:96)1-norm or the (cid:96)2-norm)."
    },
    {
        "id": "524",
        "table": {
            "header": [
                "Detector",
                "FP",
                "Targeted Patch",
                "Targeted Patch",
                "Targeted Glasses",
                "Targeted Glasses",
                "Targeted C&W0",
                "Targeted C&W0",
                "Targeted C&W2",
                "Targeted C&W2",
                "Targeted C&W\u221e",
                "Targeted C&W\u221e",
                "Untargeted FGSM",
                "Untargeted BIM"
            ],
            "rows": [
                [
                    "Detector",
                    "FP",
                    "First",
                    "Next",
                    "First",
                    "Next",
                    "First",
                    "Next",
                    "First",
                    "Next",
                    "First",
                    "Next",
                    "FGSM",
                    "BIM"
                ],
                [
                    "FS Xu et\u00a0al. ( 2018 )",
                    "23.32%",
                    "0.77",
                    "0.71",
                    "0.73",
                    "0.58",
                    "0.68",
                    "0.65",
                    "0.60",
                    "0.50",
                    "0.42",
                    "0.37",
                    "0.36",
                    "0.20"
                ],
                [
                    "AS",
                    "20.41%",
                    "0.96",
                    "0.98",
                    "0.97",
                    "0.97",
                    "0.93",
                    "0.99",
                    "0.99",
                    "1.00",
                    "0.96",
                    "1.00",
                    "0.85",
                    "0.76"
                ],
                [
                    "AP",
                    "30.61%",
                    "0.89",
                    "0.96",
                    "0.69",
                    "0.75",
                    "0.96",
                    "0.94",
                    "0.99",
                    "0.97",
                    "0.95",
                    "0.99",
                    "0.87",
                    "0.89"
                ],
                [
                    "WKN",
                    "7.87%",
                    "0.94",
                    "0.97",
                    "0.71",
                    "0.76",
                    "0.83",
                    "0.89",
                    "0.99",
                    "0.97",
                    "0.97",
                    "0.96",
                    "0.86",
                    "0.87"
                ],
                [
                    "STN",
                    "2.33%",
                    "0.08",
                    "0.19",
                    "0.16",
                    "0.19",
                    "0.90",
                    "0.94",
                    "0.97",
                    "1.00",
                    "0.76",
                    "0.87",
                    "0.46",
                    "0.41"
                ],
                [
                    "AmI",
                    "9.91%",
                    "0.97",
                    "0.98",
                    "0.85",
                    "0.85",
                    "0.91",
                    "0.95",
                    "0.99",
                    "0.99",
                    "0.97",
                    "1.00",
                    "0.91",
                    "0.90"
                ],
                [
                    "w/o Left Eye",
                    "18.37%",
                    "0.97",
                    "0.99",
                    "0.75",
                    "0.79",
                    "0.88",
                    "0.92",
                    "0.99",
                    "0.95",
                    "0.97",
                    "0.98",
                    "0.89",
                    "0.90"
                ],
                [
                    "w/o Right Eeye",
                    "18.08%",
                    "0.93",
                    "0.96",
                    "0.73",
                    "0.80",
                    "0.86",
                    "0.91",
                    "0.99",
                    "0.96",
                    "0.98",
                    "0.98",
                    "0.86",
                    "0.87"
                ],
                [
                    "w/o Nose",
                    "27.41%",
                    "0.97",
                    "0.99",
                    "0.78",
                    "0.84",
                    "0.91",
                    "0.94",
                    "0.98",
                    "0.97",
                    "0.99",
                    "0.98",
                    "0.94",
                    "0.90"
                ],
                [
                    "w/o Mouth",
                    "20.99%",
                    "0.91",
                    "0.97",
                    "0.74",
                    "0.79",
                    "0.86",
                    "0.95",
                    "1.00",
                    "0.95",
                    "0.99",
                    "0.98",
                    "0.86",
                    "0.87"
                ]
            ],
            "title": "Table 3: Detecting adversarial samples. We have two settings for targeted attacks. \u2018First\u2019 denotes that the first label is the target whereas \u2018Next\u2019 denotes the next label of the correct label is the target. FS stands feature squeezing; AS attribute substitution only; AP attribute preservation only; WKN non-witness weakening only; STN witness strengthening only; and AmI our final result. The bottom four rows denote detection accuracy of AmI using witnesses excluding some certain attribute (e.g., w/o Nose)."
        },
        "insight": "From Table 3 it can be observed that AmI can effectively detect adversarial samples with more than 90% accuracy for most attacks. The same tests are conducted on feature squeezing (FS) . The best result achieved by FS is 77%. FS especially does not preform well on FGSM and BIM, which is consistent with the observation of Xu et al. . [CONTINUE] The false positive rate (the FP column in Table 3) is 9.91% for AmI and 23.32% for FS on the VF dataset. [CONTINUE] we conducted a few additional experiments: (1) we use only attribute substitution to extract witnesses and then build the attribute-steered model for detection with results shown in the AS row of Table 3; (2) we use only attribute preservation (AP); (3) we only weaken non-witnesses (WKN); and (4) we only strengthen witnesses (STN). Our results show that although AS or AP alone can detect adversarial samples with good accuracy, their false positive rates (see the FP column) are very high (i.e., 20.41% and 30.61%). [CONTINUE] Specifically, we evaluate on four subsets of attribute witnesses extracted using bi-directional reasoning as shown in Table 3: (1) we exclude witnesses of left eye (w/o Left Eye); (2) exclude witnesses of right eye (w/o Right Eye); (3) exclude witnesses of nose (w/o Nose); and (4) exclude witnesses of mouth (w/o Mouth). We observe that the detection accuracy degrades a bit (less than 5% in most cases)."
    },
    {
        "id": "525",
        "table": {
            "header": [
                "Dataset",
                "Local ELM testing error",
                "Local ELM running time",
                "MTFL testing error",
                "MTFL running time",
                "GO-MTL testing error",
                "GO-MTL running time",
                "MTL-ELM testing error",
                "MTL-ELM running time",
                "DGSP testing error",
                "DGSP running time",
                "DNSP testing error",
                "DNSP running time",
                "DMTL-ELM testing error",
                "DMTL-ELM running time",
                "FO-DMTL-ELM testing error",
                "FO-DMTL-ELM running time"
            ],
            "rows": [
                [
                    "USPS",
                    "4.26",
                    "0.009",
                    "4.67",
                    "0.10",
                    "6.30",
                    "7.52",
                    "[BOLD] 3.49",
                    "226.1",
                    "5.05",
                    "0.03",
                    "4.47",
                    "0.04",
                    "[BOLD] 3.54",
                    "184.2",
                    "3.89",
                    "22.5"
                ],
                [
                    "MNIST",
                    "6.58",
                    "0.004",
                    "6.84",
                    "0.20",
                    "9.76",
                    "8.10",
                    "[BOLD] 5.90",
                    "244.2",
                    "7.9",
                    "0.04",
                    "7.35",
                    "0.07",
                    "[BOLD] 5.96",
                    "192.6",
                    "6.20",
                    "19.7"
                ]
            ],
            "title": "TABLE I: Comparison of testing error(%) and running time (s) for different learning approaches and training data sets."
        },
        "insight": "The testing errors of Local ELM, MTL-ELM and DMTL-ELM and FO-DMTL-ELM in Table 1 are obtained with L = 300, where \u03c4t = 20 + dt, \u03b6t = 40 for DMTL-ELM and \u03c4(cid:48) = 40 t for FO-DMTL-ELM. = 30 + dt, \u03b6 (cid:48) t [CONTINUE] To [CONTINUE] Table 1 demonstrates that the testing error achieved by MTL-ELM, DMTL-ELM and FO-DMTL-ELM can outperform other MTL methods over the tested data sets. [CONTINUE] Meanwhile, the proposed DMTL-ELM algorithm can achieve almost the same testing error compared with centralized MTL-ELM. Though with decayed generalization performance with same iteration k, the running time of FO-DMTL-ELM is much faster than MTL-ELM and DMTL-ELM. Among all the tested methods, the Loal ELM has the shortest running time."
    },
    {
        "id": "526",
        "table": {
            "header": [
                "[EMPTY]",
                "Larsson et al.",
                "Iizuka et al.",
                "Zhang et al.",
                "Ours"
            ],
            "rows": [
                [
                    "Top1",
                    "0.152",
                    "0.13",
                    "0.14",
                    "0.51"
                ],
                [
                    "Top2",
                    "0.34",
                    "0.159",
                    "0.18",
                    "0.19"
                ],
                [
                    "Top3",
                    "0.156",
                    "0.36",
                    "0.156",
                    "0.12"
                ],
                [
                    "Top4",
                    "0.17",
                    "0.153",
                    "0.42",
                    "0.18"
                ],
                [
                    "Avg. rank",
                    "2.39\u00b11.03",
                    "2.68\u00b10.93",
                    "2.95\u00b11.16",
                    "1.98\u00b11.36"
                ]
            ],
            "title": "Table 6: User study result on comparison with the automatic video colorization"
        },
        "insight": "Table 6 shows details of the user study result on this task. Our approach is 50.66% more likely to be chosen as the 1st-rank result which significantly outperforms all three in terms of average rank: 1.98 \u00b1 1.36 vs. 2.39 \u00b1 1.03 for , 2.68 \u00b1 0.93 for , and 2.95 \u00b1 1.16 for ."
    },
    {
        "id": "527",
        "table": {
            "header": [
                "[EMPTY]",
                "STN",
                "VPN",
                "Ours"
            ],
            "rows": [
                [
                    "Top1",
                    "0.13",
                    "0.07",
                    "0.8"
                ],
                [
                    "Top2",
                    "0.66",
                    "0.17",
                    "0.17"
                ],
                [
                    "Top3",
                    "0.151",
                    "0.76",
                    "0.05"
                ],
                [
                    "Avg. rank",
                    "2.07\u00b10.33",
                    "2.69\u00b10.36",
                    "1.24\u00b10.156"
                ]
            ],
            "title": "Table 7: User study result on comparison with the video color propagation"
        },
        "insight": "Table 7 shows these results. Again, our method achieves the highest 1st-rank percentage at 79.67%. Overall, it achieves the highest average rank: 1.24 \u00b1 0.156 vs. 2.07 \u00b1 0.33 for STN, and 2.69 \u00b1 0.36 for VPN."
    },
    {
        "id": "528",
        "table": {
            "header": [
                "Approach",
                "LC-QuAD",
                "QALD-6",
                "QALD-7"
            ],
            "rows": [
                [
                    "SENNA",
                    "0.10",
                    "0.02",
                    "0.06"
                ],
                [
                    "LSTM",
                    "0.28",
                    "0.20",
                    "0.15"
                ],
                [
                    "Bi-LSTM",
                    "0.25",
                    "0.06",
                    "0.03"
                ],
                [
                    "SIBKB*",
                    "0.14",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "ReMatch*",
                    "0.16",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "EARL",
                    "0.26",
                    "0.18",
                    "0.15"
                ],
                [
                    "Falcon",
                    "0.38",
                    "0.30",
                    "0.13"
                ],
                [
                    "MDP-Parser",
                    "[BOLD] 0.45",
                    "[BOLD] 0.34",
                    "[BOLD] 0.25"
                ],
                [
                    "EARL+MDP-Parser",
                    "0.34",
                    "0.22",
                    "0.24"
                ]
            ],
            "title": "Table 2: Accuracies for relation linking task."
        },
        "insight": "Table 2 summarizes the results in terms of accuracy. Despite the simplicity of the linking component in our approach, MDP-Parser achieves better results compared to the baselines, and again, improves the performance of EARL by almost 5% on all the datasets."
    },
    {
        "id": "529",
        "table": {
            "header": [
                "Approach",
                "Entity linking",
                "Relation linking"
            ],
            "rows": [
                [
                    "EARL",
                    "0.006",
                    "0.164"
                ],
                [
                    "Falcon",
                    "0.453",
                    "0.152"
                ],
                [
                    "MDP-Parser",
                    "[BOLD] 0.612",
                    "[BOLD] 0.443"
                ]
            ],
            "title": "Table 3: Accuracies on LC-QuAD in lowercase"
        },
        "insight": "Table 3 shows that the performance of both baselines remarkably falls down on the modified dataset. The accuracy of EARL drops by 60% and 10%, and Falcon by 29% and 23% for entity linking and relation linking tasks, respectively (cmp. Table 1 and Table2). On the other hand, MDP-Parser performs almost the same as before for relation linking (only 1% decrease). However, the accuracy of entity linking drops by 15%."
    },
    {
        "id": "530",
        "table": {
            "header": [
                "[BOLD] hyper-parameter",
                "[BOLD] value"
            ],
            "rows": [
                [
                    "iteration",
                    "1000"
                ],
                [
                    "fitness threshold",
                    "0.999"
                ],
                [
                    "evolution size",
                    "6"
                ],
                [
                    "activation",
                    "relu"
                ],
                [
                    "episode steps",
                    "500"
                ],
                [
                    "episode generation",
                    "20"
                ]
            ],
            "title": "TABLE II: Hyper-parameters in the Cartpole v0."
        },
        "insight": "As a classical continuous control environment , the Cartpole-v0  environment is controlled by bringing to bear a force of +1 or \u22121 to the cart. A pendulum starts upright, and the goal is to prevent it from toppling over. An accumulated reward of +1 would be given before a terminated environment (e.g., falling 15 degrees from vertical, or a cart shifting more than 2.4 units from the center). As experimental settings, we select a sample size of 1000, and use relu activation for neural network output to select an adaptive action in Tab. II. To solve the problem, we conduct and fine-tune both NEAT and FS-NEAT as baseline results for accessing targeted accumulated rewards of 195.0 in 200 episode steps . [CONTINUE] Here, we have improved the requirements of the fitness threshold (499.5 rewards in 500 episode steps) and normalized the fitness threshold as episode steps . See Tab. II. rewards [CONTINUE] continuous control,"
    },
    {
        "id": "531",
        "table": {
            "header": [
                "task",
                "method",
                "fall rate",
                "Avg.gen",
                "StDev.gen"
            ],
            "rows": [
                [
                    "IMPLY",
                    "NEAT",
                    "0.1%",
                    "7.03",
                    "1.96"
                ],
                [
                    "IMPLY",
                    "FS-NEAT",
                    "0.0%",
                    "6.35",
                    "2.21"
                ],
                [
                    "IMPLY",
                    "Bi-NEAT",
                    "0.0%",
                    "5.00",
                    "2.50"
                ],
                [
                    "IMPLY",
                    "GS-NEAT",
                    "0.0%",
                    "5.82",
                    "2.88"
                ],
                [
                    "NAND",
                    "NEAT",
                    "0.1%",
                    "13.02",
                    "3.87"
                ],
                [
                    "NAND",
                    "FS-NEAT",
                    "0.0%",
                    "12.50",
                    "4.34"
                ],
                [
                    "NAND",
                    "Bi-NEAT",
                    "0.0%",
                    "10.26",
                    "5.26"
                ],
                [
                    "NAND",
                    "GS-NEAT",
                    "0.0%",
                    "11.74",
                    "5.82"
                ],
                [
                    "NOR",
                    "NEAT",
                    "0.1%",
                    "13.13",
                    "4.18"
                ],
                [
                    "NOR",
                    "FS-NEAT",
                    "0.0%",
                    "12.83",
                    "4.58"
                ],
                [
                    "NOR",
                    "Bi-NEAT",
                    "0.0%",
                    "10.60",
                    "5.64"
                ],
                [
                    "NOR",
                    "GS-NEAT",
                    "0.0%",
                    "11.86",
                    "6.29"
                ],
                [
                    "XOR",
                    "NEAT",
                    "0.1%",
                    "103.42",
                    "56.02"
                ],
                [
                    "XOR",
                    "FS-NEAT",
                    "0.1%",
                    "101.19",
                    "50.72"
                ],
                [
                    "XOR",
                    "Bi-NEAT",
                    "0.0%",
                    "84.15",
                    "30.58"
                ],
                [
                    "XOR",
                    "GS-NEAT",
                    "0.0%",
                    "88.11",
                    "36.13"
                ]
            ],
            "title": "TABLE V: Result statistics in the experiments of logic gates."
        },
        "insight": "After restraining the influence of hyper-parameters, the tasks from Tab. V describe the influence of task complexity on evolutionary strategies. [CONTINUE] The results show that with the increase in task difficulty, our algorithm can make the population evolve faster. In the IMPLY task, the difference between the average end generation is 1 to 2 generations. When the average of end generations in XOR tasks is counted, the gap between our proposed strategies and the baselines widens to nearly 20 generations. Additionally, the average node number in the final neural network and the task complexity seem to have a potentially positive correlation."
    },
    {
        "id": "532",
        "table": {
            "header": [
                "task",
                "method",
                "fall rate",
                "Avg.gen",
                "StDev.gen"
            ],
            "rows": [
                [
                    "CartPole v0",
                    "NEAT",
                    "26.5%",
                    "147.33",
                    "99.16"
                ],
                [
                    "CartPole v0",
                    "FS-NEAT",
                    "4.8%",
                    "72.86",
                    "85.08"
                ],
                [
                    "CartPole v0",
                    "Bi-NEAT",
                    "0.0%",
                    "29.35",
                    "18.86"
                ],
                [
                    "CartPole v0",
                    "GS-NEAT",
                    "0.0%",
                    "31.95",
                    "22.56"
                ],
                [
                    "LunarLander v2",
                    "NEAT",
                    "4.9%",
                    "144.21",
                    "111.87"
                ],
                [
                    "LunarLander v2",
                    "FS-NEAT",
                    "3.3%",
                    "152.91",
                    "108.61"
                ],
                [
                    "LunarLander v2",
                    "Bi-NEAT",
                    "0.0%",
                    "48.66",
                    "44.57"
                ],
                [
                    "LunarLander v2",
                    "GS-NEAT",
                    "0.0%",
                    "44.57",
                    "50.29"
                ]
            ],
            "title": "TABLE VI: Result statistics in the complex experiments."
        },
        "insight": "The tasks in the continuous control and game environments Bi-NEAT and GS-NEAT still show amazing potential. See Tab. VI. [CONTINUE] Unlike in the case of the logical experiments, the results show that the two proposed strategies are superior both in terms of evolutionary speed and stability. The enhanced evolutionary speed is reflected in the fact that the baselines require two to three times the average end generation as our strategies for the tested tasks. In addition, the smaller standard variance of end generation shows the evolutionary stability of our strategies."
    },
    {
        "id": "533",
        "table": {
            "header": [
                "[BOLD] Feature Selection",
                "Filters",
                "[BOLD] Method CC",
                "[BOLD] # features 290",
                "[BOLD] Accuracy 50.90%"
            ],
            "rows": [
                [
                    "[BOLD] Feature Selection",
                    "Filters",
                    "MI",
                    "400",
                    "68.44%"
                ],
                [
                    "[BOLD] Feature Selection",
                    "Filters",
                    "[ITALIC] \u03c72 Statistics",
                    "400",
                    "67.46%"
                ],
                [
                    "[BOLD] Feature Selection",
                    "Filters",
                    "FCBF",
                    "15",
                    "31.10%"
                ],
                [
                    "[BOLD] Feature Selection",
                    "Wrappers",
                    "SFS",
                    "400",
                    "86.67%"
                ],
                [
                    "[BOLD] Feature Selection",
                    "Wrappers",
                    "PSO",
                    "403",
                    "59.42%"
                ],
                [
                    "[BOLD] Feature Selection",
                    "Wrappers",
                    "GA",
                    "396",
                    "61.80%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "PCA",
                    "5",
                    "60.80%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "Kernel PCA",
                    "5",
                    "9.2%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "MDS",
                    "5",
                    "61.66%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "Isomap",
                    "5",
                    "75.30%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "LLE",
                    "5",
                    "65.56%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "LE",
                    "5",
                    "77.04%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "AE",
                    "5",
                    "83.20%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Unsupervised",
                    "[ITALIC] t-SNE",
                    "3",
                    "89.62%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Supervised",
                    "FLDA",
                    "5",
                    "76.04%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Supervised",
                    "Kernel FLDA",
                    "5",
                    "21.34%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Supervised",
                    "SPCA",
                    "5",
                    "55.68%"
                ],
                [
                    "[BOLD] Feature Extraction",
                    "Supervised",
                    "ML",
                    "5",
                    "56.98%"
                ],
                [
                    "\u2013",
                    "\u2013",
                    "Original data",
                    "784",
                    "53.50%"
                ]
            ],
            "title": "TABLE II: Performance of feature selection and extraction methods on MNIST dataset."
        },
        "insight": "As reported in Table II, the accuracy of original data without applying any feature selection or extraction method is 53.50%."
    },
    {
        "id": "534",
        "table": {
            "header": [
                "Sequence -10",
                "Sequence -112",
                "Sequence 68",
                "Sequence -39",
                "Sequence -123",
                "Sequence 35",
                "Sequence -45",
                "Sequence 66",
                "Sequence -28",
                "Sequence 62",
                "Average NIST Value",
                "Average NIST Value 0.24"
            ],
            "rows": [
                [
                    "22",
                    "-113",
                    "34",
                    "-111",
                    "44",
                    "42",
                    "63",
                    "114",
                    "-63",
                    "-41",
                    "[EMPTY]",
                    "0.57"
                ],
                [
                    "-48",
                    "-111",
                    "20",
                    "-102",
                    "10",
                    "-18",
                    "55",
                    "11",
                    "80",
                    "62",
                    "[EMPTY]",
                    "0.16"
                ]
            ],
            "title": "Table 1: Sequences in decimal representation of one trained PRNG with seed=0 and B=80, alongside their average NIST value."
        },
        "insight": "NIST test suite is used to compute the average value of all eligible tests in the battery, run on the generated PRNs sequences. [CONTINUE] This value is then used as a reward function [CONTINUE] Please notice tsince test statistic values are probabilities according to statistical hypothesis definition, rewards belong to [0, 1]. [CONTINUE] In table 1 some generated sequences are shown in decimal representation. All the generated sequences are results of one trained PRNG with one seed state. Since any fixed length sequence can be considered the period of the generator and there are multiple sequences given by just one generator,"
    },
    {
        "id": "535",
        "table": {
            "header": [
                "[EMPTY]",
                "MountainCar",
                "DoublePendulum",
                "Hopper",
                "Walker2d",
                "HalfCheetah",
                "Ant"
            ],
            "rows": [
                [
                    "S / A",
                    "R4 / {0,1}",
                    "R11 / R1",
                    "R11 / R3",
                    "R17 / R6",
                    "R17 / R6",
                    "R111 / R8"
                ],
                [
                    "Setting / Demo",
                    "[BOLD] S1 / 81.25",
                    "[BOLD] S3 / 1488.28",
                    "[BOLD] S2 / 969.71",
                    "[BOLD] S2 / 1843.75",
                    "[BOLD] S2 / 2109.80",
                    "[BOLD] S2 / 1942.05"
                ],
                [
                    "PPO",
                    "-0.74\u00b19.61",
                    "302.77\u00b137.09",
                    "17.09\u00b113.54",
                    "1.54\u00b15.75",
                    "978.84\u00b1665.61",
                    "-2332.95\u00b12193.85"
                ],
                [
                    "MMD-IL",
                    "82.99\u00b14.57",
                    "218.43\u00b113.72",
                    "118.66\u00b10.38",
                    "8.88\u00b16.07",
                    "161.74\u00b1219.85",
                    "967.83\u00b10.87"
                ],
                [
                    "Pre-train",
                    "83.35\u00b16.32",
                    "8928.79\u00b1388.61",
                    "1356.47\u00b1470.43",
                    "2607.38\u00b1301.94",
                    "3831.96\u00b1150.30",
                    "-5377.25\u00b11682.56"
                ],
                [
                    "POfD",
                    "45.01\u00b128.16",
                    "628.47\u00b169.36",
                    "32.13\u00b124.23",
                    "-1.48\u00b10.03",
                    "2801.59\u00b166.03",
                    "-68.59\u00b119.17"
                ],
                [
                    "Penalty",
                    "-120.29\u00b148.30",
                    "1902.95\u00b1210.41",
                    "1225.03\u00b1296.52",
                    "286.23\u00b112.46",
                    "1517.68\u00b135.85",
                    "-3711.12\u00b1794.97"
                ],
                [
                    "Penalty + Ann.",
                    "79.00\u00b11.04",
                    "1671.78\u00b1108.80",
                    "1220.10\u00b1112.74",
                    "282.00\u00b16.70",
                    "2592.94\u00b1870.04",
                    "-116.89\u00b188.01"
                ],
                [
                    "Ours",
                    "[BOLD] 83.46\u00b11.42",
                    "[BOLD] 9331.40\u00b15.95",
                    "[BOLD] 2329.89\u00b1125.85",
                    "[BOLD] 3483.78\u00b1269.59",
                    "[BOLD] 4106.69\u00b195.47",
                    "[BOLD] 2645.58\u00b1118.55"
                ]
            ],
            "title": "Table 1: Comparative results (with only 1 imperfect demonstration). All results are measured in the original exact reward."
        },
        "insight": "In comparative evaluations, we carry out several RLfD baselines, including Pre-train (Silver et al. 2016) and POfD (Kang, Jie, and Feng 2018). In particular, we introduce another two baselines of penalty method3 with MMD as discrepancy measure, denoted by Penalty and Penalty + Ann., and the later one also employ an annealing strategy described in Sec. 4. We also run two non-RLfD baselines PPO and MMD-Imitation (denoted as MMD-IL) to verify the reward sparsification and the imperfect expert setting respectively. PPO will run with the sparse reward while MMD-IL will directly optimize the objective defined in (9) with provided imperfect demonstrations. [CONTINUE] The results of our compar [CONTINUE] ative evaluations are summarized in Table 1, which averaged 50 trials under the learned policies. [CONTINUE] The results overall read that our method achieves comparable performances with the baselines on relatively simple tasks (such as MountainCar) and outperforms them with a large margin on difficult tasks (such as Hopper, Walker2d and Ant). During policy optimization, our method can converge faster than other RLfD counterparts as well as obtains better final results. Comparing with the strong baseline of Pre-train, we can see that although convergence efficiency of proposed method during the early phase of training may not have significant advantages, but as it continues, the performance of our method can be improved persistently like Hopper(+973.42) and Walker2d(+876.40), while Pre-train struggles on achieving higher return, which demonstrates that our method could benefit more from the exploration guidance offered by the soft constraint during the whole policy optimization procedure than by only imitating at the beginning. [CONTINUE] On the other hand, we also find that our algorithm exhibits a more stable and efficient behavior over all the baselines using the penalty method. From the learning curve and numerical results, it can be seen that adopting penalty with imperfect demonstrations will induce a noisy and misleading gradient update, which will prevent the performances from improving further while our method with a soft constraint will not suffer from this. This essentially accounts for the performance gap between our method and all baselines with penalty departures. Moreover, the complex training strategies and auxiliary model in POfD also leads to unstable and [CONTINUE] inefficient training across different tasks and environment specifications. [CONTINUE] From the results of PPO and MMD-Imitation, the experiment settings of reward sparsification and imperfect demonstrations can be verified. As it illustrates, under sparse environmental feedback, pure PPO fails to find an optimal policy on most of the tested tasks, which indicates the impact of ineffective exploration. While with few imperfect demonstrations, MMD-Imitation also cannot learn promising policies. It suggests that combining the demonstrations and environmental feedback would be essential for the designated tasks. Furthermore, as similar MMD-Imitation update may happen in our method when the optimization just starts (mentioned in Sec. 3.3), these results also show how can our method benefit from the follow-up solving of the constrained optimization problem."
    },
    {
        "id": "536",
        "table": {
            "header": [
                "[BOLD] type_points",
                "[BOLD] kernel",
                "[BOLD] method",
                "[BOLD] 1",
                "[BOLD] 2",
                "[BOLD] 3",
                "[BOLD] 4",
                "[BOLD] 5",
                "[BOLD] 6"
            ],
            "rows": [
                [
                    "inliers",
                    "linear",
                    "split",
                    "17",
                    "56",
                    "1064",
                    "49",
                    "1075",
                    "9"
                ],
                [
                    "inliers",
                    "rbf",
                    "split",
                    "14",
                    "53",
                    "1064",
                    "111",
                    "1092",
                    "276"
                ],
                [
                    "outliers",
                    "rbf",
                    "split",
                    "23",
                    "63",
                    "1080",
                    "61",
                    "178",
                    "279"
                ],
                [
                    "inliers",
                    "linear",
                    "keep",
                    "46",
                    "60",
                    "82",
                    "7",
                    "177",
                    "49"
                ],
                [
                    "inliers",
                    "rbf",
                    "keep",
                    "100",
                    "166",
                    "349",
                    "26",
                    "199",
                    "56"
                ],
                [
                    "outliers",
                    "rbf",
                    "keep",
                    "48",
                    "93",
                    "90",
                    "1",
                    "150",
                    "59"
                ],
                [
                    "inliers",
                    "linear",
                    "keep_reset",
                    "13",
                    "52",
                    "344",
                    "39",
                    "704",
                    "12"
                ],
                [
                    "inliers",
                    "rbf",
                    "keep_reset",
                    "21",
                    "56",
                    "354",
                    "7",
                    "552",
                    "47"
                ],
                [
                    "outliers",
                    "rbf",
                    "keep_reset",
                    "15",
                    "60",
                    "937",
                    "49",
                    "119",
                    "157"
                ]
            ],
            "title": "Table 2: Number of rules for clustering methods and K-means clustering."
        },
        "insight": "Table 2 shows that the number of rules for anomalous data points is always inferior to those for non-anomalous ones. [CONTINUE] Regarding hypothesis 2, in most of the cases DT generates less rules than our algorithm (with the exception of anomalous data points in dataset 4). However, results may be considered similar. [CONTINUE] We also compared the results of our algorithm with other surrogate methods such as using a DT model trained over the same features and using as target variable the anomalous or nonanomalous labels. In most scenarios both methods yielded similar results, thus showing that our approach is a feasible technique for XAI for anomaly detection."
    },
    {
        "id": "537",
        "table": {
            "header": [
                "[BOLD] Algorithm",
                "Beijing (15/ 30/ 45 min) MAE",
                "Beijing (15/ 30/ 45 min) RMSE",
                "Shenzhen (15/ 30/ 45 min) MAE",
                "Shenzhen (15/ 30/ 45 min) RMSE"
            ],
            "rows": [
                [
                    "LR",
                    "29.90 / 30.27 / 30.58",
                    "69.74 / 70.95 / 72.00",
                    "24.59 / 24.80 / 25.09",
                    "51.31 / 52.36 / 52.80"
                ],
                [
                    "GBRT",
                    "17.29 / 17.81 / 18.40",
                    "44.60 / 48.50 / 51.59",
                    "13.90 / 14.67 / 14.71",
                    "35.05 / 37.98 / 38.09"
                ],
                [
                    "GRU",
                    "18.51 / 18.78 / 19.73",
                    "55.43 / 55.92 / 58.64",
                    "16.73 / 16.88 / 17.14",
                    "46.92 / 47.26 / 47.56"
                ],
                [
                    "Google-Parking",
                    "21.49 / 21.68 / 22.85",
                    "57.26 / 59.25 / 60.48",
                    "17.10 / 18.33 / 18.69",
                    "47.30 / 48.45 / 49.34"
                ],
                [
                    "Du-Parking",
                    "17.67 / 17.70 / 18.03",
                    "50.17 / 50.63 / 51.75",
                    "13.91 / 14.17 / 14.39",
                    "42.66 / 43.24 / 43.56"
                ],
                [
                    "STGCN",
                    "16.57 / 16.44 / 17.10",
                    "50.79 / 51.04 / 52.61",
                    "13.46 / 13.59 / 13.88",
                    "39.26 / 39.96 / 40.29"
                ],
                [
                    "DCRNN",
                    "15.66 / 15.97 / 16.30",
                    "46.28 / 47.80 / 48.87",
                    "13.11 / 13.19 / 13.89",
                    "42.74 / 43.37 / 44.27"
                ],
                [
                    "CxtGNN\u00a0(ours)",
                    "15.29 / 15.69 / 16.15",
                    "45.55 / 46.69 / 47.78",
                    "12.39 / 12.73 / 13.09",
                    "36.31 / 36.92 / 37.46"
                ],
                [
                    "CAGNN\u00a0(ours)",
                    "12.45 / 12.77 / 13.20",
                    "39.99 / 40.81 / 41.31",
                    "10.50 / 10.62 / 10.98",
                    "31.86 / 32.12 / 32.83"
                ],
                [
                    "[BOLD] SHARE\u00a0(ours)",
                    "[BOLD] 10.68 / 10.97 / 11.43",
                    "[BOLD] 32.00 / 32.78 / 33.78",
                    "[BOLD] 9.23 / 9.41 / 9.66",
                    "[BOLD] 30.44 / 30.90 / 31.70"
                ]
            ],
            "title": "Table 2: Parking availability prediction error given by MAE and RMSE on Beijing and Shenzhen."
        },
        "insight": "we observe significant improvement by comparing SHARE with its variants (i.e., CxtGNN and CAGNN). For example, by adding the PA approximation module, CAGNN achieves (18.6%, 18.6%, 18.3%) lower MAE and (12.2%, 12.6%, 13.5%) lower RMSE than CxtGNN on BEIJING, respectively. By further adding the SCConv block, SHARE achieves (14.2%, 14.1%, 13.4%) lower MAE and (20%, 19.7%, 18.2%) lower RMSE than CAGNN on BEIJING. [CONTINUE] Table 2 reports the overall results of our methods and all the compared baselines on two datasets with respect to MAE and RMSE. [CONTINUE] Specifically, SHARE achieves (31.8%, 31.3%, 29.9%) and (30.9%, 31.5%, 30.9%) improvements beyond the state-ofthe-art approach (DCRNN) on MAE and RMSE on BEIJING for (15min, 30min, 45min) prediction, respectively. Similarity, the improvement of MAE and RMSE on SHENZHEN are (29.6%, 28.7%, 30.5%) and (28.8%, 28.8%, 28.4%). [CONTINUE] we observe all graph convolution based models (i.e., STGCN, DCRNN and SHARE) outperform other deep learning based approaches (i.e., Google-Parking and Du-parking), [CONTINUE] GBRT outperforms Google-parking, GRU, LR, and achieves a similar result with Du-parking, [CONTINUE] One extra interesting finding is that both MAE and RMSE of all methods on SHENZHEN is relatively smaller than on BEIJING."
    },
    {
        "id": "538",
        "table": {
            "header": [
                "[EMPTY]",
                "TDD accuracy Fixed",
                "TDD accuracy Random",
                "Used as training data in 10 GD steps Rand. real",
                "Used as training data in 10 GD steps Optim. real",
                "Used as training data in 10 GD steps  [ITALIC] k-means",
                "Used as training data in 10 GD steps Avg. real",
                "Used in K-NN Rand. real",
                "Used in K-NN k-means"
            ],
            "rows": [
                [
                    "IMDB",
                    "[BOLD] 75.0",
                    "[BOLD] 73.4 \u00b1 3.3",
                    "49.7 \u00b1 0.9",
                    "49.9 \u00b1 0.8",
                    "49.9 \u00b1 0.6",
                    "50.0 \u00b1 0.1",
                    "50.0 \u00b1 0.1",
                    "50.0 \u00b1 0.0"
                ],
                [
                    "SST5",
                    "[BOLD] 37.5",
                    "[BOLD] 36.3 \u00b1 1.5",
                    "21.2 \u00b1 4.9",
                    "24.6 \u00b1 2.6",
                    "19.6 \u00b1 4.5",
                    "21.3 \u00b1 4.1",
                    "23.1 \u00b1 0.0",
                    "20.9 \u00b1 2.1"
                ],
                [
                    "TREC6",
                    "[BOLD] 79.2",
                    "[BOLD] 77.3 \u00b1 2.9",
                    "37.5 \u00b1 10.1",
                    "44.6 \u00b1 7.5",
                    "34.4 \u00b1 13.0",
                    "28.0 \u00b1 9.5",
                    "31.5 \u00b1 9.9",
                    "50.5 \u00b1 6.8"
                ],
                [
                    "TREC50",
                    "[BOLD] 57.6",
                    "11.0 \u00b1 0.0",
                    "8.2 \u00b1 6.0",
                    "9.9 \u00b1 6.6",
                    "14.7 \u00b1 5.5",
                    "12.5 \u00b1 6.4",
                    "15.4 \u00b1 5.1",
                    "[BOLD] 45.1 \u00b1 6.6"
                ],
                [
                    "TREC502",
                    "67.4",
                    "42.1 \u00b1 2.1",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 3: Means and standard deviations of TDD and baseline accuracies on text data using TextConvNet. All values are percentages. The first four baselines are used to train the same neural network as in the distillation experiments. The last two baselines are used to train a K-Nearest Neighbors classifier. Each result uses 10 GD steps aside from IMDB with k-means and TREC50 which had to be done with 2 GD steps due to GPU memory constraints and also insufficient training samples for some classes in TREC50. The second TREC50 row uses TDD with 5 GD steps with 4 images per class. Experiments with random initializations have their results listed in the form [mean \u00b1 standard deviation] and are based on the resulting performance of 200 randomly initialized networks."
        },
        "insight": "The baseline results are shown in Table 3. [CONTINUE] For example, TDD can produce 20 distilled sentences that train the CNN model up to a distillation ratio of 91.62% on the IMDB dataset. Even for far more difficult language tasks, TDD still has impressive results but with larger distilled datasets. For example, for the 50-class TREC50 task, it can learn 1000 sentences that train the model to a distillation ratio of 79.86%. [CONTINUE] Some examples of TDD results are detailed in Table 3. [CONTINUE] TDD can learn 20 images that train a network with random initialization up to a distillation ratio of 84.27% on IMDB, only slightly lower than in the fixed initialization case. Similarly, for TREC6 there is only a 2.07% difference between the 60-sample distillation ratio for fixed and random initializations; the accuracies were within a standard deviation of each other. [CONTINUE] Only in the case of the much tougher TREC50 task, was there a large decrease in performance when working with random initializations. [CONTINUE] All the mean and standard deviation results for random initializations in Table 3 are derived by testing with 200 randomly initialized networks."
    },
    {
        "id": "539",
        "table": {
            "header": [
                "Layer",
                "Filter",
                "Stride",
                "Activation"
            ],
            "rows": [
                [
                    "no.",
                    "size",
                    "[EMPTY]",
                    "function"
                ],
                [
                    "1",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "2",
                    "32\u00d73\u00d73",
                    "1",
                    "ReLU"
                ],
                [
                    "3",
                    "16\u00d73\u00d73",
                    "2",
                    "ReLU"
                ],
                [
                    "4",
                    "32\u00d73\u00d73",
                    "0.5",
                    "ReLU"
                ],
                [
                    "5",
                    "3\u00d73\u00d73",
                    "1",
                    "tanh"
                ],
                [
                    "6",
                    "3\u00d7\u23088 [ITALIC] \u03c3c\u2309\u00d7\u23088 [ITALIC] \u03c3c\u2309",
                    "1",
                    "identity"
                ]
            ],
            "title": "Table 1: The architecture of the CNN for learning. From the first to the fifth layers constituted a CAE. The sixth layer had the weights obtained from the Laplacian filter bank in which three filters were used."
        },
        "insight": "The architecture of the CNN for learning is shown in Tab.1. [CONTINUE] From the first layer to the fifth layer constituted a CAE. Since the activation function of the fifth layer was tanh, the pixel values of the original images were scaled to [\u22121, 1]. The resolution of the feature maps of the third layer was the half of the resolution of the original images, because the stride of the third layer was 2. The stride of the fourth layer was 0.5 that means upsampling carried out by the bilinear interpolation. The sixth layer had the weights obtained from the Laplacian filter bank. Since the three filters with the scales \u03c3c = 0.8, 1.6 and 3.2 were used for the filter bank, the sixth layer had the three channels [CONTINUE] corresponding to the subbands. Although the size of the Laplacian filter was basically decided by \u23088\u03c3c\u2309, the size was changed to an odd number by adding 1 if \u23088\u03c3c\u2309 was an even number."
    },
    {
        "id": "540",
        "table": {
            "header": [
                "Model",
                "[ITALIC] L>=1 SR(%)",
                "[ITALIC] L>=1 SPL(%)",
                "[ITALIC] L>=5 SR(%)",
                "[ITALIC] L>=5 SPL(%)"
            ],
            "rows": [
                [
                    "Random",
                    "11.2",
                    "5.1",
                    "1.1",
                    "0.50"
                ],
                [
                    "Baseline [zhu2017target]",
                    "35.0",
                    "10.3",
                    "25.0",
                    "10.5"
                ],
                [
                    "Scene-prior [yang2018visual]",
                    "35.4",
                    "10.9",
                    "23.8",
                    "10.7"
                ],
                [
                    "SAVN [Wortsman_2019_CVPR]",
                    "35.7",
                    "9.3",
                    "23.9",
                    "9.4"
                ],
                [
                    "[BOLD] MJOLNIR-r (our)",
                    "54.8",
                    "19.2",
                    "41.7",
                    "18.9"
                ],
                [
                    "[BOLD] MJOLNIR-o (our)",
                    "[BOLD] 65.3",
                    "[BOLD] 21.1",
                    "[BOLD] 50.0",
                    "[BOLD] 20.9"
                ]
            ],
            "title": "TABLE I: Comparison with state-of-the-art visual navigation algorithms"
        },
        "insight": "Table [CONTINUE] and Figure 4a shows the performance of each of the tested models in terms of Success Rate (SR) and SPL. It can be seen that both of our models significantly outperform the current state-of-the-art. MJOLNIR-o has \u2248 30% increase in SR. [CONTINUE] It is also important to note here, that the GCN network of , which does not include the context vector as its node feature, performs poorer than even the baseline  and SAVN ."
    },
    {
        "id": "541",
        "table": {
            "header": [
                "DONE action",
                "Model",
                "[ITALIC] L>=1 SR(%)",
                "[ITALIC] L>=1 SPL(%)",
                "[ITALIC] L>=5 SR(%)",
                "[ITALIC] L>=5 SPL(%)"
            ],
            "rows": [
                [
                    "only sampled",
                    "MJOLNIR-r",
                    "54.8",
                    "19.2",
                    "41.7",
                    "18.9"
                ],
                [
                    "only sampled",
                    "MJOLNIR-o",
                    "65.3",
                    "21.1",
                    "50.0",
                    "20.9"
                ],
                [
                    "only sampled",
                    "MJOLNIR-o (no_g)",
                    "59.0",
                    "16.6",
                    "41.0",
                    "16.9"
                ],
                [
                    "only sampled",
                    "MJOLNIR-o (w)",
                    "64.7",
                    "21.6",
                    "46.4",
                    "20.6"
                ],
                [
                    "sampled + env",
                    "SAVN",
                    "54.4",
                    "35.55",
                    "37.87",
                    "23.47"
                ],
                [
                    "sampled + env",
                    "[BOLD] MJOLNIR-o",
                    "[BOLD] 83.1",
                    "[BOLD] 53.9",
                    "[BOLD] 71.6",
                    "[BOLD] 36.9"
                ]
            ],
            "title": "TABLE III: Ablation study for MJOLNIR"
        },
        "insight": "We compare our model with the SAVN  model using this stopping criteria. The results show that we perform significantly better."
    },
    {
        "id": "542",
        "table": {
            "header": [
                "Algorithm",
                "F1-Score"
            ],
            "rows": [
                [
                    "Na\u00efve persistence",
                    "0.629"
                ],
                [
                    "Na\u00efve majority",
                    "[ITALIC] 0.640"
                ],
                [
                    "Vanilla TrueSkill",
                    "0.400"
                ],
                [
                    "Multi skill KT",
                    "0.259"
                ],
                [
                    "TrueLearn",
                    "[BOLD] 0.677"
                ]
            ],
            "title": "Table 1: Mean F1-Score with the full VLN dataset"
        },
        "insight": "We implement four baseline models to compare TrueLearn against: i) Na\u00a8\u0131ve persistence, which assumes a static behaviour for all users, i.e. if the learner is engaged, they will remain engaged and vice versa; ii) Na\u00a8\u0131ve majority, which predicts future engagement based solely on mean past engagement of users; iii) KT model (Multi-Skill KT) according to (Bishop, Winn, and Diethe 2015); and iv) Vanilla TrueSkill (Herbrich, Minka, and Graepel 2007). [CONTINUE] The results in Table 1 show evidence that TrueLearn outperforms the baselines while retaining a transparent learner model."
    },
    {
        "id": "543",
        "table": {
            "header": [
                "[BOLD] Augment Size",
                "[BOLD] Top-1 Error (%)",
                "[BOLD] Top-3 Error (%)"
            ],
            "rows": [
                [
                    "0",
                    "10.94",
                    "4.01"
                ],
                [
                    "430",
                    "9.75",
                    "3.75"
                ],
                [
                    "860",
                    "9.35",
                    "3.68"
                ],
                [
                    "1290",
                    "9.09",
                    "3.69"
                ],
                [
                    "1720",
                    "9.05",
                    "3.66"
                ]
            ],
            "title": "TABLE I: Effect of Data Augmentation on the UTD-MHAD dataset."
        },
        "insight": "The results are summarized in Table I. As can be seen, data augmentation regularizes the model training and helps model avoid overfitting. As a result, when we [CONTINUE] increase the training dataset by data augmentation, the top1 error rate is consistently reduced. We also notice that data augmentation does not have much effect on top-3 error rate, indicating that data augmentation mainly boosts correct answers from top-3 positions to top-1 positions."
    },
    {
        "id": "544",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] 16",
                "[BOLD] 32",
                "[BOLD] 64",
                "[BOLD] 128",
                "[BOLD] Average"
            ],
            "rows": [
                [
                    "No  [ITALIC] Lcyc",
                    "65.58",
                    "24.03",
                    "28.37",
                    "35.19",
                    "38.29"
                ],
                [
                    "No  [ITALIC] Lsem",
                    "40.15",
                    "11.60",
                    "[BOLD] 9.75",
                    "12.01",
                    "18.38"
                ],
                [
                    "No UNet",
                    "82.89",
                    "24.78",
                    "17.54",
                    "16.54",
                    "35.44"
                ],
                [
                    "With Style Embedding",
                    "[BOLD] 26.85",
                    "10.87",
                    "9.89",
                    "13.06",
                    "15.16"
                ],
                [
                    "No Style Embedding",
                    "29.23",
                    "[BOLD] 9.21",
                    "10.51",
                    "[BOLD] 10.41",
                    "[BOLD] 14.84"
                ]
            ],
            "title": "Table 2: Sliced Wasserstein Score"
        },
        "insight": "We measure those benefits both quantitatively using Sliced Wasserstein Score proposed in . [CONTINUE] In our experiments, we observed that having UNet encourages the network to find more local correspondences. Without UNet, the network failed to preserve correspondence between semantic parts and there were common error patterns such as the face direction becoming mirrored after translation \u2013 which is technically allowed by all our loss terms but is judged as being unnatural by human. Note that similar error patterns are observed in our experiment with MUNIT 1, which does not use UNet. Adding cycle loss and semantic consistency loss both resulted in higher Sliced Wasserstein Score and better output. Adding Style embedding increased the Sliced Wasserstein Score by a little, but it gave the user the ability to control some features such as hair color and eye color. However we argue that those features should perhaps belong to the content and the style embeddings failed to catch the more subtle yet important style information, such as eye-to-face ratio, texture of the hair, etc., that varies from painter to painter. We thus made the style embedding optional and did not use that for the final results."
    },
    {
        "id": "545",
        "table": {
            "header": [
                "[BOLD] Source Image  [BOLD] TwinGAN",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            "rows": [],
            "title": "Table 3: Human to Cat Translation"
        },
        "insight": "In order to show the general applicability of our model, here we show our results on the task of translating human faces to cat faces. For human face we collected 200k images from the CELEBA dataset. We extracted around 10k cat faces from the CAT dataset by cropping the cat faces using [CONTINUE] the eye and ear positions. The network, the CELEBA dataset, and training setup is the same as in 4.1."
    },
    {
        "id": "546",
        "table": {
            "header": [
                "[BOLD] Dataset (Mortality task)",
                "[BOLD] 1h-24h PR-AUC",
                "[BOLD] 1h-24h ROC",
                "[BOLD] 24h-720h PR-AUC",
                "[BOLD] 24h-720h ROC"
            ],
            "rows": [
                [
                    "PSV (Code+Signal)",
                    "62.46 (\u00b1 0.20)",
                    "[BOLD] 90.06 (\u00b1 0.12)",
                    "48.88 (\u00b1 0.13)",
                    "[BOLD] 85.90 (\u00b1 0.09)"
                ],
                [
                    "PSV (Code)",
                    "45.35 (\u00b1 0.22)",
                    "81.69 (\u00b1 0.29)",
                    "30.50 (\u00b1 0.22)",
                    "77.98 (\u00b1 0.14)"
                ],
                [
                    "PSV (Signal)",
                    "49.42 (\u00b1 0.18)",
                    "82.27 (\u00b1 0.04)",
                    "31.32 (\u00b10.10)",
                    "82.27 (\u00b1 0.04)"
                ],
                [
                    "PSV (Semi-supervised)",
                    "[BOLD] 65.40 (\u00b1 0.35)",
                    "89.10 (\u00b1 0.59)",
                    "[BOLD] 53.76 (\u00b1 0.26)",
                    "85.15 (\u00b1 0.66)"
                ],
                [
                    "Seq2Seq (lyu2018improving)",
                    "7.73 (\u00b1 0.60)",
                    "51.32 (\u00b1 0.31)",
                    "8.17 (\u00b1 0.05)",
                    "61.90 (\u00b1 0.19)"
                ],
                [
                    "Seq2Seq (Semi-supervised)",
                    "8.58(\u00b10.32)",
                    "51.23(\u00b1 0.22)",
                    "19.22 (\u00b1 0.06)",
                    "61.63 (\u00b1 61.63)"
                ],
                [
                    "Transformer (darabi2019taper)2",
                    "11.18 (\u00b1 0.35)",
                    "53.01 (\u00b1 0.64)",
                    "10.67 (\u00b1 0.30)",
                    "50.45 (\u00b1 0.88)"
                ]
            ],
            "title": "Table 2: Mortality downstream task ablation, and comparison with baselines. Results reported are average of 5 runs on a test split of 15% and the std are reported in parenthesis."
        },
        "insight": "The results for both downstream tasks are summarized in Table 2&3. [CONTINUE] In the tables, the ablation of the different components of the PSV model is done by first pretraining the model in an unsupervised fashion and freezing the pre-trained network for downstream tasks. Empirically, from both tables, it is evident the network benefits from both code and signal representations on the defined tasks."
    },
    {
        "id": "547",
        "table": {
            "header": [
                "[BOLD] Dataset (Readmission task)",
                "[BOLD] 1h-24h PR-AUC",
                "[BOLD] 1h-24h ROC",
                "[BOLD] 24h-720h PR-AUC",
                "[BOLD] 24h-720h ROC"
            ],
            "rows": [
                [
                    "PSV (Code+Signal)",
                    "61.25 (\u00b1 0.26)",
                    "80.99 (\u00b1 0.11)",
                    "57.86 (\u00b1 0.19)",
                    "80.94 (\u00b1 0.09)"
                ],
                [
                    "PSV (Code)",
                    "57.24 (\u00b1 0.56)",
                    "79.58 (\u00b1 0.12)",
                    "49.63 (\u00b1 0.38)",
                    "76.15 (\u00b1 0.24)"
                ],
                [
                    "PSV (Signal)",
                    "30.47 (\u00b1 0.13)",
                    "59.35 (\u00b1 0.15)",
                    "30.34 (\u00b1 0.10)",
                    "59.22 (\u00b1 0.14)"
                ],
                [
                    "PSV (Semi-supervised)",
                    "[BOLD] 69.02 (\u00b1 0.42)",
                    "[BOLD] 83.40 (\u00b1 0.23)",
                    "[BOLD] 68.04 (\u00b1 0.51)",
                    "[BOLD] 82.25 (\u00b1 0.24)"
                ],
                [
                    "Seq2Seq (lyu2018improving)2",
                    "26.43 (\u00b1 0.56)",
                    "51.45 (\u00b1 1.13)",
                    "20.30 (\u00b1 0.18)",
                    "52.35 (\u00b1 0.34)"
                ],
                [
                    "Seq2Seq (Semi-supervised)",
                    "26.68(\u00b1 0.19)",
                    "51.80 (\u00b10.54)",
                    "22.31 (\u00b1 0.11)",
                    "52.18 (\u00b1 0.42)"
                ],
                [
                    "Transformer (darabi2019taper)2",
                    "28.22 (\u00b1 0.60)",
                    "58.82 (\u00b1 0.43)",
                    "27.70 (\u00b1 0.79)",
                    "59.45 (\u00b1 0.91)"
                ]
            ],
            "title": "Table 3: Readmission downstream task ablation, and comparison with baselines. Results reported are average of 5 runs on a test split of 15% and the std are reported in parenthesis."
        },
        "insight": "The results for both downstream tasks are summarized in Table 2&3. [CONTINUE] In the tables, the ablation of the different components of the PSV model is done by first pretraining the model in an unsupervised fashion and freezing the pre-trained network for downstream tasks. Empirically, from both tables, it is evident the network benefits from both code and signal representations on the defined tasks."
    },
    {
        "id": "548",
        "table": {
            "header": [
                "Place",
                "Bot",
                "Frags",
                "F/D ratio",
                "Kills",
                "Suicides",
                "Deaths"
            ],
            "rows": [
                [
                    "1",
                    "F1",
                    "[BOLD] 559",
                    "1.35",
                    "[BOLD] 597",
                    "[BOLD] 38",
                    "413"
                ],
                [
                    "2",
                    "Arnold",
                    "413",
                    "[BOLD] 1.90",
                    "532",
                    "119",
                    "[BOLD] 217"
                ],
                [
                    "3",
                    "Clyde",
                    "393",
                    "0.77",
                    "476",
                    "83",
                    "509"
                ],
                [
                    "4",
                    "TUHO",
                    "312",
                    "0.67",
                    "424",
                    "112",
                    "465"
                ],
                [
                    "5",
                    "5vision",
                    "142",
                    "0.28",
                    "206",
                    "64",
                    "497"
                ],
                [
                    "6",
                    "ColbyMules",
                    "131",
                    "0.25",
                    "222",
                    "91",
                    "516"
                ],
                [
                    "7",
                    "AbyssII",
                    "118",
                    "0.21",
                    "217",
                    "99",
                    "542"
                ],
                [
                    "8",
                    "WallDestroyerXxx",
                    "-130",
                    "-0.41",
                    "[ITALIC] 13",
                    "143",
                    "315"
                ],
                [
                    "9",
                    "Ivomi",
                    "[ITALIC] -578",
                    "[ITALIC] -0.68",
                    "149",
                    "[ITALIC] 727",
                    "[ITALIC] 838"
                ]
            ],
            "title": "Table II: Results of the 2016 Competition: Track 1. \u2018Frags\u2019 is the number of opponent kills decreased by the number of suicides of the agent. \u2018F/D\u2019 denotes Frags/Death. \u2018Deaths\u2019 include suicides."
        },
        "insight": "The results of the competition are shown in Tables II and III, for Track 1 and 2, respectively. [CONTINUE] The bots in 7 out of 9 submissions were competent enough to systematically eliminate the opponents. Among them, four bots stand out by scoring more than 300 frags: F1, Arnold, Clyde and TUHO. The difference between the second (Arnold) and the third (Clyde) place was minuscule (413 vs. 393 frags) and it is questionable whether the order remained the same if games were repeated. There is no doubt, however, that F1 was the best bot beating the forerunner (Arnold) by a large margin. F1 was also characterized by the least number of suicides. Note, however, that generally, the number of suicides is high for all the agents. Interestingly, despite the fact that F1 scored the best, it was Arnold who was gunned down the least often. [CONTINUE] Notice also that F1, the winner of Track 1 of the previous competition, took the fifth place with 164 frags and is again characterized by the least number of suicides, which, in general, did not decrease compared to the 2016 competition and is still high for all the agents. [CONTINUE] Similarly to the result of Track 1, the gap between the first two bots was tiny."
    },
    {
        "id": "549",
        "table": {
            "header": [
                "Place",
                "Bot",
                "Total Frags",
                "F/D ratio",
                "Kills M1",
                "Kills M2",
                "Kills M3",
                "Kills T",
                "Suicides M1",
                "Suicides M2",
                "Suicides M3",
                "Suicides T",
                "Deaths M1",
                "Deaths M2",
                "Deaths M3",
                "Deaths T"
            ],
            "rows": [
                [
                    "1",
                    "IntelAct",
                    "[BOLD] 256",
                    "3.08",
                    "[BOLD] 113",
                    "[BOLD] 49",
                    "[BOLD] 135",
                    "[BOLD] 297",
                    "[ITALIC] 19",
                    "[ITALIC] 17",
                    "5",
                    "[ITALIC] 41",
                    "47",
                    "24",
                    "12",
                    "83"
                ],
                [
                    "2",
                    "Arnold",
                    "164",
                    "[BOLD] 32.8",
                    "76",
                    "37",
                    "53",
                    "167",
                    "2",
                    "[BOLD] 1",
                    "[BOLD] 0",
                    "[BOLD] 3",
                    "[BOLD] 3",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 5"
                ],
                [
                    "3",
                    "TUHO",
                    "51",
                    "0.66",
                    "51",
                    "9",
                    "13",
                    "73",
                    "7",
                    "15",
                    "[BOLD] 0",
                    "22",
                    "31",
                    "29",
                    "17",
                    "77"
                ],
                [
                    "4",
                    "ColbyMules",
                    "18",
                    "0.13",
                    "8",
                    "5",
                    "13",
                    "26",
                    "1",
                    "7",
                    "[BOLD] 0",
                    "8",
                    "60",
                    "27",
                    "44",
                    "129"
                ],
                [
                    "5",
                    "5vision",
                    "12",
                    "0.09",
                    "12",
                    "10",
                    "4",
                    "26",
                    "3",
                    "8",
                    "3",
                    "14",
                    "45",
                    "[ITALIC] 37",
                    "47",
                    "131"
                ],
                [
                    "6",
                    "Ivomi",
                    "-2",
                    "-0.01",
                    "6",
                    "5",
                    "2",
                    "13",
                    "2",
                    "13",
                    "[BOLD] 0",
                    "15",
                    "[ITALIC] 69",
                    "33",
                    "35",
                    "137"
                ],
                [
                    "7",
                    "WallDestroyerXxx",
                    "[ITALIC] -9",
                    "[ITALIC] -0.06",
                    "[ITALIC] 2",
                    "[ITALIC] 0",
                    "[ITALIC] 0",
                    "[ITALIC] 2",
                    "[BOLD] 0",
                    "5",
                    "[ITALIC] 6",
                    "11",
                    "48",
                    "30",
                    "[ITALIC] 78",
                    "[ITALIC] 156"
                ]
            ],
            "title": "Table III: Results of the 2016 Competition: Track 2. \u2018M\u2019 denotes map and \u2018T\u2019 denotes a total statistic."
        },
        "insight": "In Track 2, IntelAct was the best bot, significantly surpassing its competitors on all maps. Arnold, who finished in the second place, was killed the least frequently. Compared to Track 1, the numbers of kills and suicides (see Tables II and III) are significantly lower,"
    },
    {
        "id": "550",
        "table": {
            "header": [
                "Place",
                "Bot",
                "Frags",
                "F/D ratio",
                "Kills",
                "Suicides",
                "Deaths"
            ],
            "rows": [
                [
                    "1",
                    "Marvin",
                    "[BOLD] 248",
                    "[BOLD] 1.16",
                    "[BOLD] 315",
                    "67",
                    "[BOLD] 213"
                ],
                [
                    "2",
                    "Arnold2",
                    "245",
                    "0.84",
                    "314",
                    "69",
                    "291"
                ],
                [
                    "3",
                    "Axon",
                    "215",
                    "0.77",
                    "252",
                    "37",
                    "278"
                ],
                [
                    "4",
                    "TBoy",
                    "198",
                    "0.60",
                    "229",
                    "31",
                    "[ITALIC] 330"
                ],
                [
                    "5",
                    "F1",
                    "164",
                    "0.57",
                    "179",
                    "[BOLD] 15",
                    "290"
                ],
                [
                    "6",
                    "YanShi",
                    "158",
                    "0.58",
                    "246",
                    "[ITALIC] 88",
                    "273"
                ],
                [
                    "7",
                    "DoomNet",
                    "139",
                    "0.50",
                    "179",
                    "40",
                    "280"
                ],
                [
                    "8",
                    "Turmio",
                    "132",
                    "0.47",
                    "209",
                    "77",
                    "280"
                ],
                [
                    "9",
                    "AlphaDoom",
                    "[ITALIC] 109",
                    "[ITALIC] 0.39",
                    "[ITALIC] 139",
                    "30",
                    "281"
                ]
            ],
            "title": "Table V: Results of the 2017 Competition: Track 1. \u2018F/D\u2019 denotes Frags/Death. Deaths include suicides."
        },
        "insight": "The results of the competition were shown in Tables V and VI for Track 1 and 2, respectively. [CONTINUE] The level of the submitted bots was significantly higher in 2017 than in 2016. There were no weak bots in Track 1. The spread of the frag count was rather small: the worst bot scored 109 while the best one 248. The track was won by Marvin, which scored 248 frags, only 3 more than the runner-up, Arnold2, and 33 more then Axon. Interestingly, Marvin did not stand out with his accuracy or ability to avoid rockets; it focused on gathering resources: medkits and armors, which greatly increased his chances of survival. Marvin was hit the largest number of times but, at the same time, it was killed the least frequently. Arnold2, on the other hand, was better at aiming (shooting and detection precision). [CONTINUE] all could move, aim, and shoot opponents. [CONTINUE] The competition was won by Arnold4, who scored 275 frags and was closely followed up by YanShi, who scored 273. Arnold4 was the most accurate bot in the whole competition and the only bot that did not commit any suicide. This turned out to be crucial to win against YanShi, who had the same number of 275 kills but committed two suicides. YanShi, however, achieved the highest frags/death ratio by being the best at avoiding being killed and had the highest detection precision. These two were definitely the best compared to the other agents. The next runner-up, IntelAct, the winner of Track 2 in the previous competition, scored substantially fewer, 221 frags. Fewer items on the maps in Track 2 possibly contributed to the lower position of Marvin, which ended up in the fourth place with 193 frags."
    },
    {
        "id": "551",
        "table": {
            "header": [
                "Objective",
                "[BOLD] G",
                "L",
                "Random",
                "LDP",
                "FV",
                "ERes",
                "Greedy",
                "RNet\u2013DQN  [ITALIC] avg",
                "RNet\u2013DQN  [ITALIC] best"
            ],
            "rows": [
                [
                    "F [ITALIC] random",
                    "BA",
                    "2",
                    "0.018\u00b10.001",
                    "0.036",
                    "0.051",
                    "0.053",
                    "0.033",
                    "0.051\u00b10.001",
                    "[BOLD] 0.057"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "5",
                    "0.049\u00b10.002",
                    "0.089",
                    "0.098",
                    "0.106",
                    "0.079",
                    "0.124\u00b10.001",
                    "[BOLD] 0.130"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "10",
                    "0.100\u00b10.003",
                    "0.158",
                    "0.176",
                    "0.180",
                    "0.141",
                    "0.211\u00b10.001",
                    "[BOLD] 0.222"
                ],
                [
                    "[EMPTY]",
                    "ER",
                    "2",
                    "0.029\u00b10.001",
                    "0.100",
                    "0.103",
                    "0.103",
                    "0.082",
                    "0.098\u00b10.001",
                    "[BOLD] 0.104"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "5",
                    "0.071\u00b10.002",
                    "0.168",
                    "0.172",
                    "[BOLD] 0.175",
                    "0.138",
                    "0.164\u00b10.001",
                    "0.173"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "10",
                    "0.138\u00b10.002",
                    "0.238",
                    "0.252",
                    "[BOLD] 0.253",
                    "0.217",
                    "0.240\u00b10.001",
                    "0.249"
                ],
                [
                    "F [ITALIC] targeted",
                    "BA",
                    "2",
                    "0.010\u00b10.001",
                    "0.022",
                    "0.018",
                    "0.018",
                    "0.045",
                    "0.042\u00b10.001",
                    "[BOLD] 0.047"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "5",
                    "0.025\u00b10.001",
                    "0.091",
                    "0.037",
                    "0.077",
                    "0.077",
                    "0.108\u00b10.001",
                    "[BOLD] 0.117"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "10",
                    "0.054\u00b10.003",
                    "0.246",
                    "0.148",
                    "0.232",
                    "0.116",
                    "0.272\u00b10.002",
                    "[BOLD] 0.289"
                ],
                [
                    "[EMPTY]",
                    "ER",
                    "2",
                    "0.020\u00b10.002",
                    "0.103",
                    "0.090",
                    "0.098",
                    "[BOLD] 0.149",
                    "0.122\u00b10.001",
                    "0.128"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "5",
                    "0.050\u00b10.002",
                    "0.205",
                    "0.166",
                    "0.215",
                    "[BOLD] 0.293",
                    "0.268\u00b10.001",
                    "0.279"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "10",
                    "0.098\u00b10.003",
                    "0.306",
                    "0.274",
                    "0.299",
                    "0.477",
                    "0.461\u00b10.003",
                    "[BOLD] 0.482"
                ]
            ],
            "title": "Table 1: Mean cumulative reward per episode obtained by the agents on graphs with |V|=20, grouped by objective function, graph family, and number of edge additions L."
        },
        "insight": "In Table 1, we present the main results of our experimental evaluation. For random, the policies learned by RNet\u2013DQN yield solutions that outperform the greedy approach in both the ER and BA cases. Results for improving targeted are not as strong but still considerably better than random. [CONTINUE] Across all types of graphs and performance measures, RNet\u2013 DQN performed statistically significantly better than random."
    },
    {
        "id": "552",
        "table": {
            "header": [
                "[BOLD] Object",
                "[BOLD] Training Set Size",
                "[BOLD] ex/cm",
                "[BOLD] ey/cm",
                "[BOLD] e [ITALIC] \u03c8 [BOLD] /\u00b0"
            ],
            "rows": [
                [
                    "Purple detergent bottle",
                    "1200",
                    "1.029",
                    "0.975",
                    "3.43"
                ],
                [
                    "Blue watering pot",
                    "600",
                    "1.140",
                    "0.987",
                    "1.75"
                ],
                [
                    "Black and white mug",
                    "600",
                    "1.190",
                    "1.182",
                    "5.98"
                ],
                [
                    "Green water ladle",
                    "600",
                    "1.092",
                    "1.123",
                    "3.35"
                ]
            ],
            "title": "TABLE I: Testing Set Performance"
        },
        "insight": "For translation estimation, most estimation fall within 2 cm error range for both x and y translations. For rotation estimation, the exactly correct class is obtained most of the time; most wrong estimations fall in closest neighboring classes."
    },
    {
        "id": "553",
        "table": {
            "header": [
                "Accuracy:",
                "[BOLD] 99.84%",
                "[BOLD] 99.83%",
                "99.82%"
            ],
            "rows": [
                [
                    "Not Learnable",
                    "0",
                    "4",
                    "1,183"
                ],
                [
                    "Random Init.",
                    "0",
                    "21",
                    "2,069"
                ],
                [
                    "Ones Init.",
                    "1",
                    "19",
                    "1,292"
                ]
            ],
            "title": "Table 3: Ensembles"
        },
        "insight": "We matched the previous state of the art with 4,544 ensembles. We surpassed this with 44 ensembles that achieved an accuracy of 99.83% and established a new state of the art of 99.84% with one ensemble. See Table 3."
    },
    {
        "id": "554",
        "table": {
            "header": [
                "Datasets",
                "accuracy(%)",
                "ranking(%)"
            ],
            "rows": [
                [
                    "whole dataset",
                    "93.95 \u00b1 0.11",
                    "0.02"
                ],
                [
                    "sub dataset",
                    "[BOLD] 94.02 \u00b1 0.14",
                    "[BOLD] 0.01"
                ]
            ],
            "title": "Table 7: Predictors trained and evaluated with the whole NASBench dataset and sub dataset. The experiments are repeated 20 times to alleviate the randomness of the results."
        },
        "insight": "we may form a better search space of NASBench dataset by using only 68552 models with 3 \u00d7 3 operation and skip-connect between input node and output node. An experiment of training and evaluating performance predictor is conducted on this sub dataset and the results show that the predictor trained and evaluated with sub dataset performs better than the previous one as shown in Tab. 7. It shows that a better search space helps to produce a better performance predictor."
    },
    {
        "id": "555",
        "table": {
            "header": [
                "Network",
                "Data Source",
                "Algorithm",
                "# iterations",
                "[ITALIC] Finit\u00d710\u22123",
                "[ITALIC] Fopt\u00d710\u22123",
                "Ratio to CG",
                "Deep/Shallow"
            ],
            "rows": [
                [
                    "[ITALIC] B1",
                    "[ITALIC] B1",
                    "Adadelta",
                    "2000",
                    "332.2",
                    "6.748",
                    "578.43",
                    "\u2013"
                ],
                [
                    "-",
                    "[ITALIC] B1",
                    "RMSprop",
                    "2000",
                    "332.2",
                    "0.098",
                    "8.40",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] B1",
                    "SGD",
                    "2000",
                    "332.2",
                    "90.402",
                    "7748.77",
                    "\u2013"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] B1",
                    "CG",
                    "821",
                    "332.2",
                    "0.012",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B3",
                    "Adadelta",
                    "2000",
                    "143.3",
                    "6.446",
                    "173.67",
                    "\u2013"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B3",
                    "RMSprop",
                    "2000",
                    "143.3",
                    "0.243",
                    "6.54",
                    "\u2013"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B3",
                    "SGD",
                    "2000",
                    "143.3",
                    "50.485",
                    "1360.22",
                    "\u2013"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B3",
                    "CG",
                    "2200",
                    "143.3",
                    "0.037",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B5",
                    "Adadelta",
                    "2000",
                    "83.8",
                    "4.915",
                    "41.04",
                    "\u2013"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B5",
                    "RMSprop",
                    "2000",
                    "83.8",
                    "0.277",
                    "2.31",
                    "\u2013"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B5",
                    "SGD",
                    "2000",
                    "83.8",
                    "33.233",
                    "277.51",
                    "\u2013"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B5",
                    "CG",
                    "1490",
                    "83.8",
                    "0.120",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B3",
                    "Adadelta",
                    "2000",
                    "235.8",
                    "2.577",
                    "70.41",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B3",
                    "RMSprop",
                    "2000",
                    "235.8",
                    "0.075",
                    "2.04",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B3",
                    "SGD",
                    "2000",
                    "235.8",
                    "45.396",
                    "1240.02",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B3",
                    "CG",
                    "420",
                    "235.8",
                    "0.037",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B5",
                    "Adadelta",
                    "2000",
                    "206.7",
                    "1.594",
                    "52.44",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B5",
                    "RMSprop",
                    "2000",
                    "206.7",
                    "0.070",
                    "2.29",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B5",
                    "SGD",
                    "2000",
                    "206.7",
                    "32.404",
                    "1065.83",
                    "\u2013"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B5",
                    "CG",
                    "333",
                    "206.7",
                    "0.030",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B1",
                    "Adadelta",
                    "2000",
                    "237.8",
                    "28.331",
                    "6.75",
                    "11.0"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B1",
                    "RMSprop",
                    "2000",
                    "237.8",
                    "4.415",
                    "1.05",
                    "59.2"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B1",
                    "SGD",
                    "2000",
                    "237.8",
                    "118.896",
                    "28.34",
                    "2.6"
                ],
                [
                    "[ITALIC] B3",
                    "[ITALIC] B1",
                    "CG",
                    "1072",
                    "237.8",
                    "4.195",
                    "\u2013",
                    "114.6"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B1",
                    "Adadelta",
                    "2000",
                    "208.6",
                    "44.980",
                    "5.32",
                    "28.2"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B1",
                    "RMSprop",
                    "2000",
                    "208.6",
                    "9.629",
                    "1.14",
                    "138.1"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B1",
                    "SGD",
                    "2000",
                    "208.6",
                    "136.118",
                    "16.11",
                    "4.2"
                ],
                [
                    "[ITALIC] B5",
                    "[ITALIC] B1",
                    "CG",
                    "2125",
                    "208.6",
                    "8.451",
                    "\u2013",
                    "278.0"
                ]
            ],
            "title": "Table 2: Detailed results for problems of size B."
        },
        "insight": "The results for the optimization methods for problem size class B are given in Table 2. [CONTINUE] The focus of this table is on showing the performance on shallow and deep networks for all optimization methods. [CONTINUE] The first three blocks of the table show the optimization results for shallow and deep networks individually. [CONTINUE] The following four blocks represent the cross check itself. [CONTINUE] following four blocks represent the cross check itself. The column Network denotes the network architecture used for the applied optimization. [CONTINUE] The column Data Source points to the architecture for which the training set has been generated, with a known error optimum of zero. [CONTINUE] For the training runs presented in these rows, the optimum is not known. [CONTINUE] It is only known that it would be zero for the architecture given in the column Data Source, but not necessarily also for the architecture of the column Network, used for the fitting. [CONTINUE] For example, for the first of these sixteen rows, the network architecture trained is B1 (i.e., a shallow net with a single hidden layer). [CONTINUE] It is optimized to fit the training set for which it is known that a zero error can be reached by the architecture B3 (i.e., a deep net with three hidden layers). [CONTINUE] The column Ratio to CG displays how many times the error function value attained by the Keras methods was higher than that reached by the conjugate gradient. [CONTINUE] The column Deep/Shallow shows the ratio of the following error function values for the deep network and the shallow network. [CONTINUE] Shallow networks have been superior for all test problems and all optimizing algorithms. [CONTINUE] However, it is interesting to observe that the gap, although always large, was relatively smaller for weakly performing optimization methods (SGD and Adadelta) as well as for large networks."
    },
    {
        "id": "556",
        "table": {
            "header": [
                "Shallow",
                "Deep",
                "Data deep \u2013 NN shallow \u00d710\u22123",
                "Data shallow \u2013 NN deep \u00d710\u22123",
                "Ratio Deep/Shallow"
            ],
            "rows": [
                [
                    "[ITALIC] A1",
                    "[ITALIC] A3",
                    "0.038",
                    "5.368",
                    "140.5"
                ],
                [
                    "[ITALIC] A1",
                    "[ITALIC] A5",
                    "0.035",
                    "11.353",
                    "320.5"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B3",
                    "0.075",
                    "4.415",
                    "59.2"
                ],
                [
                    "[ITALIC] B1",
                    "[ITALIC] B5",
                    "0.070",
                    "9.629",
                    "138.1"
                ],
                [
                    "[ITALIC] C1",
                    "[ITALIC] C3",
                    "0.182",
                    "4.663",
                    "25.7"
                ],
                [
                    "[ITALIC] C1",
                    "[ITALIC] C5",
                    "0.148",
                    "9.777",
                    "66.1"
                ]
            ],
            "title": "Table 3: Results for all given problems and their ratio between shallow and deep networks."
        },
        "insight": "The mean square error minima (MSE) attained by shallow networks for problems having a zero MSE for some deep network are essentially lower than in the opposite situation. [CONTINUE] Additionally, Table 3 shows mean square minima for all size classes using the Keras optimization method RMSprop. [CONTINUE] This table elucidates the development of the performance (MSE) with shallow and deep networks for varying network sizes. [CONTINUE] Each row shows the performance of a pair of a shallow and a deep network with a comparable number of parameters. [CONTINUE] The average performance of a shallow network for a problem for which a zero error minimum is known to be attainable by a deep network is given in the column Data deep \u2013 NN shallow. [CONTINUE] The average performance of a deep network for a problem for which a zero error minimum is known to be attainable by a shallow network is given in the column Data shallow \u2013 NN deep. [CONTINUE] The ratio of both average performances is shown in the column Ratio Deep/Shallow. [CONTINUE] The difference tends to slightly decrease with the problem size. [CONTINUE] The by far weakest method was the SGD, while the best was the conjugate gradient (CG). [CONTINUE] Adadelta and RMSprop were performing between the both, with RMSprop sometimes approaching the CG performance. [CONTINUE] The difference between the performance with a shallow network on one hand and deep network on the other hand grows with the performance of the optimization method: the difference is relatively small for the worst performing SGD and very large for the best performing CG. [CONTINUE] The computing experiments seem to essentially show the superiority of shallow networks in attaining low mean square minima for given mapping problems."
    },
    {
        "id": "557",
        "table": {
            "header": [
                "Teacher: baseline",
                "Student: baseline",
                "Normal KD (T\u2192S)",
                "Re-KD (S\u2192T)"
            ],
            "rows": [
                [
                    "ResNet18: 75.87",
                    "MobileNetV2: 68.38",
                    "71.05\u00b10.16 ( [BOLD] +2.67)",
                    "77.28\u00b10.28 ( [BOLD] +1.41)"
                ],
                [
                    "ResNet18: 75.87",
                    "ShuffleNetV2: 70.34",
                    "72.05\u00b10.13 ( [BOLD] +1.71)",
                    "77.35\u00b10.32 ( [BOLD] +1.48)"
                ],
                [
                    "ResNet50: 78.16",
                    "MobileNetV2: 68.38",
                    "71.04\u00b10.20 ( [BOLD] +2.66)",
                    "79.30\u00b10.11 ( [BOLD] +1.14)"
                ],
                [
                    "ResNet50: 78.16",
                    "ShuffleNetV2: 70.34",
                    "72.15\u00b10.18 ( [BOLD] +1.81)",
                    "79.43\u00b10.39 ( [BOLD] +1.27)"
                ],
                [
                    "DenseNet121: 79.04",
                    "MobileNetV2: 68.38",
                    "71.29\u00b10.23 ( [BOLD] +2.91)",
                    "79.55\u00b10.11 ( [BOLD] +0.51)"
                ],
                [
                    "DenseNet121: 79.04",
                    "ShuffleNetV2: 70.34",
                    "72.32\u00b10.25 ( [BOLD] +1.98)",
                    "79.83\u00b10.05 ( [BOLD] +0.79)"
                ],
                [
                    "ResNeXt29: 81.03",
                    "MobileNetV2: 68.38",
                    "71.65\u00b10.41 ( [BOLD] +3.27)",
                    "81.53\u00b10.14 ( [BOLD] +0.50)"
                ],
                [
                    "ResNeXt29: 81.03",
                    "ResNet18: 75.87",
                    "77.84\u00b10.15 ( [BOLD] +1.97)",
                    "81.62\u00b10.22 ( [BOLD] +0.59)"
                ]
            ],
            "title": "Table 1: Normal KD and Re-KD experiment results on CIFAR100. We report mean\u00b1std (in %) over 3 runs. The number in parenthesis means increased accuracy over baseline (T: teacher, S: student)."
        },
        "insight": "In Tab. 1, the teacher models are improved significantly by learning from students, especially for teacher models ResNet18 and ResNet50."
    },
    {
        "id": "558",
        "table": {
            "header": [
                "Teacher: baseline",
                "Student: baseline",
                "Normal KD (T\u2192S)",
                "Re-KD (S\u2192T)"
            ],
            "rows": [
                [
                    "ResNet18: 95.12",
                    "Plain CNN: 87.14",
                    "87.67\u00b10.17 ( [BOLD] +0.53)",
                    "95.33\u00b10.12  [BOLD] (+0.21)"
                ],
                [
                    "ResNet18: 95.12",
                    "MobileNetV2: 90.98",
                    "91.69\u00b10.14  [BOLD] (+0.71)",
                    "95.71\u00b10.11  [BOLD] (+0.59)"
                ],
                [
                    "MobileNetV2: 90.98",
                    "Plain CNN: 87.14",
                    "87.45\u00b10.18 ( [BOLD] +0.31)",
                    "91.81\u00b10.23 ( [BOLD] +0.92)"
                ],
                [
                    "ResNeXt29: 95.76",
                    "ResNet18: 95.12",
                    "95.80\u00b10.13 ( [BOLD] +0.68)",
                    "96.49\u00b10.15 ( [BOLD] +0.73)"
                ]
            ],
            "title": "Table 2: Re-KD experiment results (accuracy, mean\u00b1std over 3 runs in %) on CIFAR10."
        },
        "insight": "However, in some cases, we can find Re-KD outperforms Normal KD. For instance, in Tab. 2 (3rd row), the student model (plain CNN) can only be improved by 0.31% when taught by MobileNetV2, but the teacher (MobileNetV2) can be improved by 0.92% by learning from the student. We have similar observations for ResNeXt29 and ResNet18 (4th row in Tab. 2)."
    },
    {
        "id": "559",
        "table": {
            "header": [
                "Dataset",
                "Pt-Teacher: baseline",
                "Student: baseline",
                "De-KD"
            ],
            "rows": [
                [
                    "CIFAR100",
                    "ResNet18: 15.48",
                    "MobileNetV2: 68.38",
                    "70.65\u00b10.35 ( [BOLD] +2.27)"
                ],
                [
                    "CIFAR100",
                    "ResNet18: 15.48",
                    "ShuffleNetV2: 70.34",
                    "71.82\u00b10.11 ( [BOLD] +1.48)"
                ],
                [
                    "CIFAR100",
                    "ResNet50: 45.82",
                    "MobileNetV2: 68.38",
                    "71.45\u00b10.23 ( [BOLD] +3.09)"
                ],
                [
                    "CIFAR100",
                    "ResNet50: 45.82",
                    "ShuffleNetV2: 70.34",
                    "72.11\u00b10.09 ( [BOLD] +1.77)"
                ],
                [
                    "CIFAR100",
                    "ResNet50: 45.82",
                    "ResNet18: 75.87",
                    "77.23\u00b10.11 ( [BOLD] +1.23)"
                ],
                [
                    "CIFAR100",
                    "ResNeXt29: 51.94",
                    "MobileNetV2: 68.38",
                    "71.52\u00b10.27 ( [BOLD] +3.14)"
                ],
                [
                    "CIFAR100",
                    "ResNeXt29: 51.94",
                    "ShuffleNetV2:70.34",
                    "72.26\u00b10.36 ( [BOLD] +1.92)"
                ],
                [
                    "CIFAR100",
                    "ResNeXt29: 51.94",
                    "ResNet18: 75.87",
                    "77.28\u00b10.17 ( [BOLD] +1.41)"
                ],
                [
                    "Tiny-ImageNet",
                    "ResNet18: 9.41",
                    "MobileNetV2: 55.06",
                    "56.22 ( [BOLD] +1.16)"
                ],
                [
                    "Tiny-ImageNet",
                    "ResNet18: 9.41",
                    "ShuffleNetV2: 60.51",
                    "60.66 ( [BOLD] +0.15)"
                ],
                [
                    "Tiny-ImageNet",
                    "ResNet50: 31.01",
                    "MobileNetV2:55.06",
                    "56.02 ( [BOLD] +0.96)"
                ],
                [
                    "Tiny-ImageNet",
                    "ResNet50: 31.01",
                    "ShuffleNetV2: 60.51",
                    "61.09 ( [BOLD] +0.58)"
                ]
            ],
            "title": "Table 4: De-KD accuracy (in %) on two datasets. Pt-Teacher is \u201cPoorly-trained Teacher\u201d. Refer to the \u201cNormal KD\u201d in Tabs.\u00a01 to\u00a03 for the accuracy of students taught by \u201cfully-trained teacher\u201d."
        },
        "insight": "From De-KD experiment results on CIFAR100 in Tab. 4, we observe that the student can be greatly promoted even when distilled by a poorly-trained teacher. [CONTINUE] From the De-KD experiment results on Tiny-ImageNet in Tab. 4, we find ResNet18 with 9.14% accuracy can still enhance the teacher model MobileNetV2 by 1.16%."
    },
    {
        "id": "560",
        "table": {
            "header": [
                "Model",
                "Baseline",
                "Tf-KD [ITALIC] self",
                "Normal KD [Teacher]"
            ],
            "rows": [
                [
                    "MobileNetV2",
                    "55.06",
                    "56.77 ( [BOLD] +1.71)",
                    "56.70 ( [BOLD] +1.64) [ResNet18]"
                ],
                [
                    "ShuffleNetV2",
                    "60.51",
                    "61.36 ( [BOLD] +0.85)",
                    "61.19 ( [BOLD] +0.68) [ResNet18]"
                ],
                [
                    "ResNet50",
                    "67.47",
                    "68.18 ( [BOLD] +0.71)",
                    "68.23 ( [BOLD] +0.76) [DenseNet121]"
                ],
                [
                    "DenseNet121",
                    "68.15",
                    "68.29 ( [BOLD] +0.14)",
                    "68.31 ( [BOLD] +0.16) [ResNeXt29]"
                ]
            ],
            "title": "Table 5: Tf-KDself experiment results on Tiny-ImageNet (in %)."
        },
        "insight": "As a comparison, we also use DenseNet121 to teach ResNet18 on ImageNet, and ResNet18 obtains 0.56% improvement, which is comparable with our self-training implementation (Tab. 7)."
    },
    {
        "id": "561",
        "table": {
            "header": [
                "Model",
                "Baseline",
                "Tf-KD [ITALIC] self"
            ],
            "rows": [
                [
                    "ResNet18",
                    "69.84",
                    "70.42 ( [BOLD] +0.58)"
                ],
                [
                    "ResNet50",
                    "75.77",
                    "76.41 ( [BOLD] +0.64)"
                ],
                [
                    "DenseNet121",
                    "75.28",
                    "75.72 ( [BOLD] +0.44)"
                ],
                [
                    "ResNeXt101",
                    "79.28",
                    "79.56 ( [BOLD] +0.28)"
                ]
            ],
            "title": "Table 6: Tf-KDself experiment results on ImageNet (Top1 accuracy, in %)."
        },
        "insight": "From Tab. 8 and Tab. 9, we can observe with no teacher used and just a regularization term added, Tf-KDreg achieves comparable performance with Normal KD on both CIFAR100 and Tiny-ImageNet."
    },
    {
        "id": "562",
        "table": {
            "header": [
                "Architecture",
                "Backbone model",
                "Pre-trained on",
                "# Parameters (millions)",
                "Best  [ITALIC] T",
                "AUC"
            ],
            "rows": [
                [
                    "FbF CNN",
                    "-",
                    "-",
                    "1.6",
                    "1",
                    "0.815"
                ],
                [
                    "FbF FT",
                    "VGG19 ",
                    "ImageNET",
                    "144.1",
                    "1",
                    "0.809"
                ],
                [
                    "FbF FT",
                    "MobileNet ",
                    "ImageNET",
                    "3.7",
                    "1",
                    "0.779"
                ],
                [
                    "FbF FT",
                    "Inceptionresnet ",
                    "ImageNET",
                    "56",
                    "1",
                    "0.617"
                ],
                [
                    "FbF FT",
                    "NASNet ",
                    "ImageNET",
                    "89.5",
                    "1",
                    "0.738"
                ],
                [
                    "FbF FT",
                    "Xception ",
                    "ImageNET",
                    "23.1",
                    "1",
                    "0.683"
                ],
                [
                    "FbF FT",
                    "ResNet50 ",
                    "ImageNET",
                    "25.8",
                    "1",
                    "0.861"
                ],
                [
                    "FbF SMT+CNN",
                    "YOLOv3 ",
                    "COCO",
                    "63.6",
                    "1",
                    "0.854"
                ],
                [
                    "FbF SMT+CNN",
                    "Mask RCNN ",
                    "COCO",
                    "65.8",
                    "1",
                    "0.853"
                ],
                [
                    "CNN+LSTM",
                    "-",
                    "-",
                    "0.6",
                    "50",
                    "0.888"
                ],
                [
                    "FT+LSTM",
                    "VGG19 ",
                    "ImageNET",
                    "143.9",
                    "50",
                    "0.886"
                ],
                [
                    "FT+LSTM",
                    "MobileNet ",
                    "ImageNET",
                    "3.6",
                    "10",
                    "0.844"
                ],
                [
                    "FT+LSTM",
                    "Inceptionresnet ",
                    "ImageNET",
                    "56",
                    "20",
                    "0.5"
                ],
                [
                    "FT+LSTM",
                    "NASNet ",
                    "ImageNET",
                    "89.3",
                    "5",
                    "0.761"
                ],
                [
                    "FT+LSTM",
                    "Xception ",
                    "ImageNET",
                    "23",
                    "50",
                    "0.768"
                ],
                [
                    "[BOLD] Best 3",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "FT+LSTM",
                    "ResNet50 ",
                    "ImageNET",
                    "25.8",
                    "20",
                    "0.910"
                ],
                [
                    "SMT+CNN+LSTM",
                    "YOLOv3 ",
                    "COCO",
                    "62.5",
                    "50",
                    "0.927"
                ],
                [
                    "[BOLD] SMT+CNN+LSTM",
                    "[BOLD] Mask R-CNN",
                    "COCO",
                    "64.8",
                    "50",
                    "[BOLD] 0.937"
                ]
            ],
            "title": "TABLE I: Classification performances on the lane change dataset. Our method, smt+cnn+lstm, achieved the best AUC score."
        },
        "insight": "Table [CONTINUE] summarizes the experimental results, where network architectures are shown in the first column. The backbone model column indicates the base transferred very deep network if there was any. Not all architectures used transfer learning, namely FbF CNN and CNN+LSTM. Datasets that the transferred networks were pre-trained on are given in column three. It should be noted again that all architectures were trained with our data for the final classification task. The total number of network parameters for each architecture is shown in column four for assessing the computational load. Lower parameter amount correlates with faster inference time\\. , the fixed number of frames, were changed between 5 to 100 for each configuration. The best scoring [CONTINUE] in terms of AUC of each row is given in column five. The final column is the AUC score, the main performance metric of this study. All spatiotemporal architectures with an LSTM layer outperformed their spatial counterparts, except the configuration with the Inceptionresnet  base model, which had the lowest performance. These results underline the importance of the temporal dimension. However, a very large dependence on temporal information is also undesired because; it swells the network, increases the input data size and slows the inference time. [CONTINUE] The best result was obtained with the proposed SMT+CNN+LSTM framework which used a Mask R-CNN  semantic mask extractor. We believe this result was due to the masked-contrasted temporal compositions' aptitude for relaying semantic information. The third best result was obtained with an FT+LSTM architecture which used ResNet50  as its backbone model. The rest of the architectures fell behind the top three by a noticeable margin. For example, the proposed SMT+CNN+LSTM's risky lane change detection performance was 25% better than the FbF FT with an Xception backbone."
    },
    {
        "id": "563",
        "table": {
            "header": [
                "Schemes CNN Image Classifier  [BOLD] (naive baseline)",
                "# of Lane Acc. 71.8%",
                "Gain -",
                "Road Type Acc. 89.1%",
                "Gain -",
                "ALE 0.374",
                "Reduction -"
            ],
            "rows": [
                [
                    "- with smoothing post-processing",
                    "[BOLD] 74.1%",
                    "[BOLD] 2.3%",
                    "90.6%",
                    "1.5%",
                    "[BOLD] 0.337",
                    "[BOLD] 9.8%"
                ],
                [
                    "- with MRF post-processing",
                    "73.7%",
                    "1.9%",
                    "92.2%",
                    "3.1%",
                    "0.355",
                    "5.1%"
                ],
                [
                    "CNN Image Classifier (1.5x receptive field)",
                    "71.8%",
                    "0.0%",
                    "90.1%",
                    "1.0%",
                    "0.367",
                    "1.9%"
                ],
                [
                    "- with smoothing post-processing",
                    "74.0%",
                    "2.2%",
                    "91.1%",
                    "2.0%",
                    "0.340",
                    "9.1%"
                ],
                [
                    "- with MRF post-processing",
                    "[BOLD] 74.1%",
                    "[BOLD] 2.3%",
                    "[BOLD] 92.9%",
                    "[BOLD] 3.8%",
                    "0.340",
                    "9.1%"
                ],
                [
                    "CNN Image Classifier (2.0x receptive field)",
                    "68.8%",
                    "-2.0%",
                    "89.1%",
                    "0.0%",
                    "0.393",
                    "-5.1%"
                ],
                [
                    "- with smoothing post-processing",
                    "70.6%",
                    "-1.2%",
                    "89.9%",
                    "0.8%",
                    "0.371",
                    "0.8%"
                ],
                [
                    "- with MRF post-processing",
                    "70.2%",
                    "-1.6%",
                    "91.6%",
                    "2.5%",
                    "0.386",
                    "-3.2%"
                ],
                [
                    "[BOLD] RoadTagger\u00a0(ours)",
                    "[BOLD] 77.2%",
                    "[BOLD] 5.4%",
                    "[BOLD] 93.1%",
                    "[BOLD] 4.0%",
                    "[BOLD] 0.291",
                    "[BOLD] 22.2%"
                ]
            ],
            "title": "Table 1: Performance of RoadTagger\u00a0and different CNN image classifier baselines. In the table, we highlight both the best and the second best results."
        },
        "insight": "We report the overall accuracy of the two types of road attributes and the absolute lane error for the CNN image classifier baselines and RoadTagger in table 1. [CONTINUE] As shown in the table, RoadTagger surpasses all the CNN image classifier based baselines. Compared with the baseline using only CNN image classifier, RoadTagger improves the inference accuracy of the number of lanes from 71.8% to 77.2%, and of the road type from 89.1% to 93.1%. This improvement comes with a reduction of the absolute lane detection error of 22.2%. Compared with the best baseline (with MRF post-processing and 1.5x larger receptive field), RoadTagger still improves the accuracy of the lane count inference by 3.1 points, which comes with a reduction of the absolute lane detection error of 14.4%, and achieves similar accuracy in road type inference."
    },
    {
        "id": "564",
        "table": {
            "header": [
                "Scheme",
                "# of Lane",
                "Road Type",
                "ALE"
            ],
            "rows": [
                [
                    "RoadTagger\u00a0with",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "- Raw",
                    "74.0%",
                    "91.2%",
                    "0.332"
                ],
                [
                    "- Road",
                    "75.5%",
                    "92.3%",
                    "0.327"
                ],
                [
                    "- Road(D)",
                    "75.6%",
                    "92.0%",
                    "0.324"
                ],
                [
                    "- Raw+Road(D)+Aux",
                    "77.2%",
                    "93.1%",
                    "0.291"
                ]
            ],
            "title": "Table 2: Impact of different graph structures used in RoadTagger. Here, we use abbreviations to denote different graphs. We use Raw for the original road network graph, Road for the road extraction graph, Road(D) for the road extraction graph with directional decomposition and Aux for the auxiliary graph for parallel roads."
        },
        "insight": "We show results of RoadTagger with different graph structures in Table 2. [CONTINUE] For a single graph structure, we find adding more restrictions into the graph structure can yield better performance, e.g., the performance of using road extraction graph is better than the performance of using the original raw road network graph. [CONTINUE] We find using the combination of the Raw graph, Road(D) graph and Aux graph can yield better performance compared with the performance of using a single graph structure."
    },
    {
        "id": "565",
        "table": {
            "header": [
                "Scheme",
                "# of Lane",
                "Road Type",
                "ALE"
            ],
            "rows": [
                [
                    "RoadTagger",
                    "77.2%",
                    "93.1%",
                    "0.291"
                ],
                [
                    "No Vertex Dropout",
                    "74.7%",
                    "92.7%",
                    "0.325"
                ],
                [
                    "No Regularization.",
                    "76.5%",
                    "90.8%",
                    "0.300"
                ]
            ],
            "title": "Table 3: Impact of random vertex dropout and graph Laplace regularization."
        },
        "insight": "We show the comparison results in Table 3. We find both of these two techniques are critical to the performance improvement of [CONTINUE] the random vertex dropout has more impact on the number of lane inference and the graph Laplace regularization has more impact on the road type inference."
    },
    {
        "id": "566",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] FID score  [BOLD] VAE",
                "[BOLD] FID score  [ITALIC] \u03b2 [BOLD] -VAE ( [ITALIC] \u03b2=3)",
                "[BOLD] FID score  [BOLD] RKM",
                "[BOLD] FID score  [BOLD] Ro. Gen-RKM"
            ],
            "rows": [
                [
                    "[BOLD] MNIST",
                    "142.54\u00b10.73",
                    "187.21\u00b10.11",
                    "134.95\u00b11.61",
                    "[BOLD] 87.32\u00b11.92"
                ],
                [
                    "[BOLD] F-MNIST",
                    "245.84\u00b10.43",
                    "291.11\u00b11.6",
                    "163.51\u00b11.24",
                    "[BOLD] 153.32\u00b10.05"
                ],
                [
                    "[BOLD] SVHN",
                    "168.21\u00b10.23",
                    "234.87\u00b11.45",
                    "112.45\u00b11.4",
                    "[BOLD] 98.14 \u00b11.2"
                ],
                [
                    "[BOLD] CIFAR-10",
                    "201.21\u00b10.71",
                    "241.23\u00b10.34",
                    "187.08\u00b10.58",
                    "[BOLD] 132.6\u00b10.21"
                ],
                [
                    "[BOLD] Dsprites",
                    "234.51\u00b11.10",
                    "298.21\u00b11.5",
                    "182.65\u00b10.57",
                    "[BOLD] 160.56\u00b10.96"
                ],
                [
                    "[BOLD] 3Dshapes",
                    "233.18\u00b10.94",
                    "252.41\u00b10.38",
                    "177.29\u00b11.60",
                    "[BOLD] 131.18\u00b11.45"
                ]
            ],
            "title": "Table 1: FID Scores\u00a0[13] over 10 iterations for 4000 randomly generated samples when the training data is contaminated with 20% outliers. (smaller is better)."
        },
        "insight": "The images are generated by random sampling from a fitted Gaussian distribution on the learned latent variables. When using a robust training procedure, the model does not encode the noisy images. As a consequence, no noisy images are generated and the generation quality is considerably better. This is confirmed by the Frechet Inception Distance (FID) scores [43] in Table 1, which quantify the quality of generation. We repeat the above experiment on the Fashion-MNIST dataset, where only the FID scores are reported in Table 1."
    },
    {
        "id": "567",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[ITALIC] hdim",
                "[BOLD] Algorithm",
                "[BOLD] Lasso  [BOLD] Disent.",
                "[BOLD] Lasso  [BOLD] Comple.",
                "[BOLD] Lasso  [BOLD] Inform.",
                "[BOLD] Random Forest  [BOLD] Disent.",
                "[BOLD] Random Forest  [BOLD] Comple.",
                "[BOLD] Random Forest  [BOLD] Inform."
            ],
            "rows": [
                [
                    "[BOLD] DSprites",
                    "2",
                    "[ITALIC] \u03b2-VAE ( [ITALIC] \u03b2=3)",
                    "0.19",
                    "0.16",
                    "6.42",
                    "0.13",
                    "0.32",
                    "[BOLD] 1.39"
                ],
                [
                    "[BOLD] DSprites",
                    "2",
                    "Gen-RKM",
                    "0.07",
                    "0.07",
                    "[BOLD] 5.82",
                    "0.25",
                    "0.27",
                    "5.91"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "Ro. Gen-RKM",
                    "[BOLD] 0.21",
                    "[BOLD] 0.21",
                    "9.13",
                    "[BOLD] 0.36",
                    "[BOLD] 0.38",
                    "5.95"
                ],
                [
                    "[BOLD] 3DShapes",
                    "3",
                    "[ITALIC] \u03b2-VAE ( [ITALIC] \u03b2=3)",
                    "0.24",
                    "0.28",
                    "[BOLD] 2.72",
                    "0.12",
                    "0.13",
                    "2.15"
                ],
                [
                    "[BOLD] 3DShapes",
                    "3",
                    "Gen-RKM",
                    "0.14",
                    "0.14",
                    "3.03",
                    "0.15",
                    "0.15",
                    "1.09"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "Ro. Gen-RKM",
                    "[BOLD] 0.47",
                    "[BOLD] 0.49",
                    "3.13",
                    "[BOLD] 0.44",
                    "[BOLD] 0.45",
                    "[BOLD] 1.02"
                ]
            ],
            "title": "Table 2: Disentanglement Metric on DSprites and 3D Shapes dataset. The training subset is contaminated with extra generating factors (20% of the data is considered as outliers). The framework of [9] with Lasso and Random Forest regressor\u00a0[9] is used to evaluate the learned representation. For disentanglement and completeness higher score is better, for informativeness, lower is better."
        },
        "insight": "The disentanglement of the latent representation is measured quantitatively using the proposed framework\u2020 of , which consists out of 3 measures: disentanglement, completeness and informativeness. The results are shown in Table 2, where the robust method outperforms the Gen-RKM. [CONTINUE] We only show the quantitative results in Table 2."
    },
    {
        "id": "568",
        "table": {
            "header": [
                "[EMPTY]",
                "mean IoU (w/o kNN)",
                "mean IoU (+kNN)",
                "Number of Parameters"
            ],
            "rows": [
                [
                    "SalsaNet\u00a0",
                    "44.2",
                    "45.4",
                    "6.58M"
                ],
                [
                    "+ extra layers",
                    "45.2",
                    "46.6",
                    "26.38M"
                ],
                [
                    "+ context module",
                    "46.4",
                    "48.2",
                    "26.40M"
                ],
                [
                    "+ new dropoout",
                    "49.6",
                    "52.0",
                    "26.40M"
                ],
                [
                    "+ average pooling",
                    "50.4",
                    "52.9",
                    "23.26M"
                ],
                [
                    "+  [ITALIC] Lov\u00e1sz-Softmax loss",
                    "51.5",
                    "54.5",
                    "23.26M"
                ]
            ],
            "title": "TABLE II: Ablative analysis."
        },
        "insight": "In this ablative analysis, we investigate the individual contribution of each improvements over the original SalsaNet model. Table II shows the total number of model parameters together with the obtained mIoU scores on the SemanticKITTI test set before and after applying the kNN-based post processing step (see section III-E). [CONTINUE] As depicted in table II, each operation on SalsaNet has a unique improvement on the accuracy. The post processing step leads to certain jump in the accuracy. The highest jump in the model parameters is observed when the new layers are inserted. We can achieve the highest accuracy score of 54.5% by having almost 50% less model parameters (i.e. 23.26M) in contrast to RangeNet++  which has 50M parameters. Overall improvement on the original SalsaNet model is more than 10% (54.5% versus 44.2%)."
    },
    {
        "id": "569",
        "table": {
            "header": [
                "[EMPTY]",
                "Mean (msec)",
                "Std (msec)",
                "Speed (fps)"
            ],
            "rows": [
                [
                    "RangeNet++\u00a0",
                    "14.27",
                    "2.86",
                    "70 Hz"
                ],
                [
                    "SalsaNet\u00a0",
                    "10.38",
                    "1.72",
                    "96 Hz"
                ],
                [
                    "SalsaNext [Ours]",
                    "12.00",
                    "1.99",
                    "83 Hz"
                ]
            ],
            "title": "TABLE III: Runtime performance on the Semantic-KITTI dataset [2]"
        },
        "insight": "Runtime performance is crucial in autonomous driving. Table III shows the single forward pass runtime performance of SalsaNext in comparison to the other networks. For a fair comparison, we measure the inference time of all scans in a sample test scenario (sequence 13) using the same NVIDIA Quadro P6000-24GB GPU card. [CONTINUE] Obtained mean runtime [CONTINUE] values and standard deviations are reported in Table III. The propsed SalsaNext clearly exhibits better performance compared to the RangeNet++  and has a comparable performance with the original SalsaNet model. Note also that the standard deviation of the SalsaNext runtime is also relatively less than the RangeNet++ , which plays a crucial role in the stability of the self-driving perception modules. Recall also Fig. 1 which shows the overall runtime versus accuracy plot for the state-of-the-art methods and SalsaNext. Consequently, our proposed SalsaNext network inference (a single forward pass) time can reach up to 83 Hz while providing the highest accuracy 54.5%. We here emphasize that this achieved high speed is significantly faster than the sampling rate of mainstream LiDAR scanners which typically work at 10Hz."
    },
    {
        "id": "570",
        "table": {
            "header": [
                "[EMPTY]",
                "Mean perturb, with edge detection",
                "Random perturb, with edge detection",
                "Mean perturb, excluding edge detection",
                "Random perturb, excluding edge detection"
            ],
            "rows": [
                [
                    "Faithfulness, F",
                    "0.18 (0.17\u20140.20)",
                    "0.10 (0.09\u20140.12)",
                    "0.04 (0.03\u20140.05)",
                    "0.09 (0.08\u20140.10)"
                ],
                [
                    "AOPCMoRF,  [ITALIC] L=20",
                    "0.29 (0.27\u20140.31)",
                    "0.15 (0.14\u20140.17)",
                    "0.19 (0.17\u20140.20)",
                    "0.07 (0.06\u20140.08)"
                ],
                [
                    "AOPCMoRF,  [ITALIC] L=40",
                    "0.38 (0.36\u20140.39)",
                    "0.18 (0.17\u20140.20)",
                    "0.27 (0.25\u20140.28)",
                    "0.09 (0.08\u20140.11)"
                ],
                [
                    "AOPCMoRF,  [ITALIC] L=60",
                    "0.42 (0.40\u20140.44)",
                    "0.20 (0.18\u20140.21)",
                    "0.31 (0.29\u20140.33)",
                    "0.11 (0.10\u20140.12)"
                ],
                [
                    "AOPCMoRF,  [ITALIC] L=80",
                    "0.46 (0.44\u20140.48)",
                    "0.21 (0.20\u20140.23)",
                    "0.34 (0.32\u20140.36)",
                    "0.12 (0.11\u20140.14)"
                ],
                [
                    "AOPCMoRF,  [ITALIC] L=100",
                    "0.48 (0.46\u20140.50)",
                    "0.22 (0.21\u20140.24)",
                    "0.37 (0.35\u20140.39)",
                    "0.13 (0.12\u20140.15)"
                ],
                [
                    "AOPCLeRF,  [ITALIC] L=20",
                    "0.05 (0.04\u20140.06)",
                    "0.13 (0.11\u20140.15)",
                    "0.06 (0.05\u20140.07)",
                    "0.15 (0.13\u20140.17)"
                ],
                [
                    "AOPCLeRF,  [ITALIC] L=40",
                    "0.11 (0.09\u20140.12)",
                    "0.18 (0.16\u20140.20)",
                    "0.11 (0.09\u20140.12)",
                    "0.20 (0.18\u20140.22)"
                ],
                [
                    "AOPCLeRF,  [ITALIC] L=60",
                    "0.16 (0.14\u20140.17)",
                    "0.22 (0.20\u20140.24)",
                    "0.15 (0.14\u20140.17)",
                    "0.23 (0.21\u20140.25)"
                ],
                [
                    "AOPCLeRF,  [ITALIC] L=80",
                    "0.20 (0.18\u20140.22)",
                    "0.25 (0.23\u20140.27)",
                    "0.19 (0.17\u20140.21)",
                    "0.26 (0.23\u20140.28)"
                ],
                [
                    "AOPCLeRF,  [ITALIC] L=100",
                    "0.24 (0.22\u20140.25)",
                    "0.28 (0.26\u20140.30)",
                    "0.23 (0.21\u20140.25)",
                    "0.28 (0.26\u20140.30)"
                ]
            ],
            "title": "Table 1: Krippendorf\u2019s \u03b1 under different perturbation functions for the saliency metrics, measured using saliency method ranking across images. Numbers in brackets are 99.9% confidence intervals estimated with 10,000 bootstrap samples."
        },
        "insight": "Table 1 lists the Krippendorf \u03b1 statistics for the saliency metrics (with varying numbers of deleted pixels L for AOPC). These are calculated on the image-wise rankings of saliency maps, and test \"inter-rater reliability\" as described above. Low \u03b1 values indicate that the saliency method rankings on different images are inconsistent. The left two columns of table 1 show \u03b1 values when the baseline edge detection method is included in the ranking. This baseline is ranked more consistently (low rankings for AOPCMoRF and F, high rankings for AOPCLeRF) than the true saliency methods over all data set images, producing higher \u03b1 values than when it is excluded from the rankings (the right-hand two columns). [CONTINUE] Perturbing with random RGB values reduces \u03b1 for Fidelity and AOPCMoRF compared with mean perturbation, which may be due to the increased stochasticity in the perturbations from the random colour choices. However, it is difficult to understand why AOPCLeRF at smaller numbers of perturbation steps (L = 20 and L = 40) produce greater \u03b1 values with random RGB perturbation that with mean perturbation. A \"low\" \u03b1 value is not strictly defined, but \u03b1 < 0.65 are often considered to indicate unreliability inter-rater reliability. The largest value in the table, 0.48 for AOPCMoRF at L = 100 using mean perturbation and including edge detection in the ranking, is substantially less than 0.65, indicating low inter-rater reliability whatever the specifics of the metric. We can conclude from this that the metrics will not produce consistent rankings of saliency maps when applied to new test images."
    },
    {
        "id": "571",
        "table": {
            "header": [
                "[EMPTY]",
                "Sensitivity",
                "gradient\u2299input",
                "SHAP",
                "Deep Taylor",
                "Edge detection"
            ],
            "rows": [
                [
                    "Faithfulness, F vs AOPCMoRF,",
                    "0.34",
                    "0.11",
                    "0.00",
                    "0.24",
                    "0.17"
                ],
                [
                    "Faithfulness vs AOPCLeRF,",
                    "-0.11",
                    "-0.14",
                    "-0.09",
                    "-0.22",
                    "-0.09"
                ],
                [
                    "AOPCMoRF vs AOPCLeRF",
                    "0.28",
                    "0.53",
                    "0.77",
                    "0.16",
                    "0.11"
                ],
                [
                    "AOPCMoRF, random RGB perturb vs AOPCMoRF, mean perturb",
                    "0.62",
                    "0.71",
                    "0.77",
                    "0.58",
                    "0.57"
                ]
            ],
            "title": "Table 2: Spearman correlations between pairs of metrics, measured over all images, for each saliency method. AOPC was taken at L=100 perturbation steps. Unless otherwise stated, mean perturbation was used."
        },
        "insight": "Finally, we assess internal consistency reliability by measuring the correlation between different metrics over the data [CONTINUE] set images, for each saliency method. A selection of pairs of correlations for different metrics is shown in table 2. The correlation values indicate that the metrics are, in general, not measuring the same underlying quantity across the different saliency methods. Faithfulness is only weakly correlated with AOPCMoRF, and in fact slightly anti-correlated with AOPCLeRF across all saliency methods. The correlation between AOPCMoRF and AOPCLeRF is highly variable across saliency methods, while the most consistent correlations across methods are between AOPCMoRF applied with random RGB versus mean perturbation. This is to be expected as both methods perturb the same pixels \u2014 though there is still reasonably high variability even in this case (from \u03c1 = 0.58 for deep Taylor decomposition to \u03c1 = 0.77 for SHAP)."
    },
    {
        "id": "572",
        "table": {
            "header": [
                "[BOLD] Na\u00efve Cholesky decomposition 1 seed point",
                "[BOLD] Na\u00efve Cholesky decomposition 1 seed point",
                "[BOLD] Na\u00efve Cholesky decomposition 100 seed points",
                "[BOLD] Na\u00efve Cholesky decomposition 100 seed points"
            ],
            "rows": [
                [
                    "Iteration",
                    "Accuracy",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "82",
                    "-5.23",
                    "168",
                    "-0.52"
                ],
                [
                    "126",
                    "-4.5",
                    "169",
                    "-0.41"
                ],
                [
                    "386",
                    "-4.3",
                    "170",
                    "-0.30"
                ],
                [
                    "479",
                    "-4.1",
                    "[BOLD] 232",
                    "[BOLD] -0.04"
                ],
                [
                    "[BOLD] Optimized Cholesky decomposition",
                    "[BOLD] Optimized Cholesky decomposition",
                    "[BOLD] Optimized Cholesky decomposition",
                    "[BOLD] Optimized Cholesky decomposition"
                ],
                [
                    "1 seed point",
                    "1 seed point",
                    "100 seed points",
                    "100 seed points"
                ],
                [
                    "Iteration",
                    "Accuracy",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "103",
                    "-1.3",
                    "648",
                    "-0.95"
                ],
                [
                    "139",
                    "-0.77",
                    "673",
                    "-0.61"
                ],
                [
                    "156",
                    "-0.33",
                    "731",
                    "-0.23"
                ],
                [
                    "297",
                    "-0.18",
                    "838",
                    "-0.19"
                ],
                [
                    "318",
                    "-0.14",
                    "972",
                    "-0.18"
                ],
                [
                    "[BOLD] 611",
                    "[BOLD] -0.01",
                    "975",
                    "-0.09"
                ]
            ],
            "title": "Table 1: Accuracy improvements of our approach and the na\u00efve Cholesky decomposition for the 5D Levy function."
        },
        "insight": "The second important aspect of every hyperparameter optimization method is the time to convergence. Thus, we make 1000 iterations with both methods and evaluate whether and if so when the optimal solution, i.e. f (x) = 0 is [CONTINUE] approximated reasonable well; see Tab. 1. [CONTINUE] In the first one, we provided only 1 random seed and continued for 1000 iterations of the optimization procedure. In the second one, we generated 100 random seeds and used this as a broad initialization for the remaining 1000 iterations. [CONTINUE] our approach converges to the global maximum in 611 iterations. In the scenario of 100 seed points, our approach still converges to the optimal parameter setting, but needs more iterations"
    },
    {
        "id": "573",
        "table": {
            "header": [
                "[BOLD] Na\u00efve Cholesky decomposition Iteration",
                "[BOLD] Na\u00efve Cholesky decomposition Accuracy"
            ],
            "rows": [
                [
                    "13",
                    "0.25"
                ],
                [
                    "15",
                    "0.67"
                ],
                [
                    "24",
                    "0.83"
                ],
                [
                    "26",
                    "0.88"
                ],
                [
                    "63",
                    "0.90"
                ],
                [
                    "198",
                    "0.93"
                ],
                [
                    "239",
                    "0.96"
                ],
                [
                    "732",
                    "[BOLD] 0.97"
                ],
                [
                    "[BOLD] Optimized Cholesky decomposition",
                    "[BOLD] Optimized Cholesky decomposition"
                ],
                [
                    "Iteration",
                    "Accuracy"
                ],
                [
                    "1",
                    "0.11"
                ],
                [
                    "4",
                    "0.12"
                ],
                [
                    "11",
                    "0.96"
                ],
                [
                    "[BOLD] 168",
                    "[BOLD] 0.97"
                ]
            ],
            "title": "Table 2: Accuracy improvements of our approach and the na\u00efve Cholesky decomposition for LeNet5 and MNIST."
        },
        "insight": "Hence, we also compared both approaches in terms of test accuracy; see Tab. 2. [CONTINUE] Indeed, our method can find the optimal solution in 168 iterations, while the na\u00efve approach needs 732 optimization steps."
    },
    {
        "id": "574",
        "table": {
            "header": [
                "[BOLD] Na\u00efve Cholesky decomposition Iteration",
                "[BOLD] Na\u00efve Cholesky decomposition Accuracy"
            ],
            "rows": [
                [
                    "16",
                    "0.74"
                ],
                [
                    "26",
                    "0.75"
                ],
                [
                    "43",
                    "0.77"
                ],
                [
                    "50",
                    "0.78"
                ],
                [
                    "176",
                    "0.79"
                ],
                [
                    "[BOLD] Optimized Cholesky decomposition",
                    "[BOLD] Optimized Cholesky decomposition"
                ],
                [
                    "Iteration",
                    "Accuracy"
                ],
                [
                    "24",
                    "0.77"
                ],
                [
                    "53",
                    "0.78"
                ],
                [
                    "62",
                    "0.79"
                ],
                [
                    "117",
                    "0.80"
                ],
                [
                    "237",
                    "[BOLD] 0.81"
                ]
            ],
            "title": "Table 3: Accuracy improvements of our approach and the na\u00efve Cholesky decomposition for ResNet32 and CIFAR10."
        },
        "insight": "our approach identifies a better parameter setting resulting in an accuracy of 0.81 after 10 epochs, see Tab. 3. We reach a competing parameter setting to the na\u00efve approach on average already after 62 iterations in 194.2min - the original algorithm takes on average 176 iterations with a duration of 567min,"
    },
    {
        "id": "575",
        "table": {
            "header": [
                "[BOLD] Optimized Cholesky decomposition Iteration",
                "[BOLD] Optimized Cholesky decomposition Accuracy"
            ],
            "rows": [
                [
                    "1",
                    "0.11"
                ],
                [
                    "16",
                    "0.51"
                ],
                [
                    "21",
                    "0.77"
                ],
                [
                    "35",
                    "0.79"
                ],
                [
                    "61",
                    "[BOLD] 0.80"
                ]
            ],
            "title": "Table 4: Accuracy improvements of our approach decomposition for ResNet32 and CIFAR10 in a parallel setting."
        },
        "insight": "While the original approach needs 176 iterations, we hit the same accuracy after 35 optimization steps, [CONTINUE] approach needs 176 iterations, we hit the same accuracy after 35 optimization steps, i.e. We even outperform our approach and reach our final result of 0.80 after 61 iterations,"
    },
    {
        "id": "576",
        "table": {
            "header": [
                "Network",
                "Dataset",
                "Steps Baseline",
                "Steps Ours",
                "Accuracy Baseline",
                "Accuracy Ours"
            ],
            "rows": [
                [
                    "ResNet-44",
                    "CIFAR10",
                    "156K",
                    "[BOLD] 44K",
                    "93.24%",
                    "93%"
                ],
                [
                    "WRN-28-10",
                    "CIFAR100",
                    "156K",
                    "[BOLD] 80K",
                    "82.26%",
                    "82.2%"
                ]
            ],
            "title": "Table 1: Test accuracy (Top-1) results for CIFAR10/100. We compare model accuracy using our training scheme and early learning-rate drop as described in section\u00a04.3. We emphasize the reduces number of steps required reaching this accuracy using our MMS method."
        },
        "insight": "As described in table 1, training with our selection scheme almost reached final model accuracy in considerably less training steps as originally suggested."
    },
    {
        "id": "577",
        "table": {
            "header": [
                "Task",
                "Category",
                "MLP",
                "LR",
                "XGB",
                "RF",
                "Ensemble"
            ],
            "rows": [
                [
                    "Det",
                    "S",
                    "0.91",
                    "0.86",
                    "0.96",
                    "0.96",
                    "[BOLD] 0.97"
                ],
                [
                    "Det",
                    "SS",
                    "0.92",
                    "0.85",
                    "0.96",
                    "0.96",
                    "[BOLD] 0.96"
                ],
                [
                    "Det",
                    "SK",
                    "0.86",
                    "0.75",
                    "0.90",
                    "0.90",
                    "[BOLD] 0.91"
                ],
                [
                    "Pred",
                    "S",
                    "0.84",
                    "0.76",
                    "0.88",
                    "0.88",
                    "[BOLD] 0.90"
                ],
                [
                    "Pred",
                    "SS",
                    "0.86",
                    "0.76",
                    "0.90",
                    "0.90",
                    "[BOLD] 0.91"
                ],
                [
                    "Pred",
                    "SK",
                    "0.85",
                    "0.73",
                    "0.89",
                    "0.89",
                    "[BOLD] 0.90"
                ]
            ],
            "title": "Table 1: AUC using different classifiers for three sepsis gold standards. Here, Det = Detection, Pred = Prediction, S = Sepsis, SS = Severe Sepsis, SK = Septic Shock, MLP = Multilayer Perceptron, LR = Logistic Regression, XGB = XGBoost, RF = Random Forest."
        },
        "insight": "The results have been reported in Table 1. As we can see, from the results, out of the 4 base classifiers, RF is the clear winner in most of the cases while LR did the worst. XGB and MLP lie in between."
    },
    {
        "id": "578",
        "table": {
            "header": [
                "Task",
                "Category",
                "SOFA",
                "qSOFA",
                "MEWS",
                "Ensemble"
            ],
            "rows": [
                [
                    "Det",
                    "S",
                    "0.62",
                    "0.66",
                    "0.72",
                    "[BOLD] 0.97"
                ],
                [
                    "Det",
                    "SS",
                    "0.66",
                    "0.72",
                    "0.76",
                    "[BOLD] 0.96"
                ],
                [
                    "Det",
                    "SK",
                    "0.63",
                    "0.61",
                    "0.66",
                    "[BOLD] 0.91"
                ],
                [
                    "Pred",
                    "S",
                    "0.54",
                    "0.44",
                    "0.49",
                    "[BOLD] 0.90"
                ],
                [
                    "Pred",
                    "SS",
                    "0.60",
                    "0.56",
                    "0.59",
                    "[BOLD] 0.91"
                ],
                [
                    "Pred",
                    "SK",
                    "0.64",
                    "0.57",
                    "0.63",
                    "[BOLD] 0.90"
                ]
            ],
            "title": "Table 2: Comparison with rule-based scoring systems in term of AUC. Here, Det = Detection, Pred = Prediction, S = Sepsis, SS = Severe Sepsis, SK = Septic Shock."
        },
        "insight": "Out of the three systems, MEWS did the best in detection task (0.72, 0.76 and 0.66 for sepsis, severe sepsis, and septic shock respectively) and SOFA did the best in prediction task (0.54, 0.60 and 0.64 for sepsis, severe sepsis and septic shock respectively)."
    },
    {
        "id": "579",
        "table": {
            "header": [
                "Number of nodes",
                "Mini-batch size",
                "Regular loader (%)",
                "Locality-aware loader(%)"
            ],
            "rows": [
                [
                    "16",
                    "8,192",
                    "76.67",
                    "76.81"
                ],
                [
                    "32",
                    "16,384",
                    "75.33",
                    "75.12"
                ],
                [
                    "64",
                    "32,768",
                    "68.69",
                    "69.54"
                ]
            ],
            "title": "TABLE I: Imagenet-1K ResNet50 validation accuracy comparison between the regular data loader and the locality-aware data loader."
        },
        "insight": "In Table I, we present the results. Using the localityaware data loader resulted in comparable validation accuracy with that of the regular PyTorch data loader, as the differences are below 1%."
    },
    {
        "id": "580",
        "table": {
            "header": [
                "Feature No.",
                "Sepsis Detection",
                "Septic Shock Prediction"
            ],
            "rows": [
                [
                    "1",
                    "0.85",
                    "0.75"
                ],
                [
                    "2",
                    "0.82",
                    "0.70"
                ],
                [
                    "3",
                    "0.87",
                    "0.74"
                ],
                [
                    "4",
                    "0.80",
                    "[BOLD] 0.79"
                ],
                [
                    "5",
                    "0.81",
                    "0.69"
                ],
                [
                    "6",
                    "[BOLD] 0.90",
                    "0.71"
                ]
            ],
            "title": "Table 3: Features ranking in term of AUC. Systolic blood pressure and temperature are the most important vital signs for sepsis prediction and detection respectively."
        },
        "insight": "We found feature 4 (systolic blood pressure) and feature 6 (temperature) as the most important vital signs for prediction and detection respectively. [CONTINUE] Also a close observation reveals, feature 3 (respiratory rate) and feature 1 (heart rate) as the next most important features."
    },
    {
        "id": "581",
        "table": {
            "header": [
                "# of Features",
                "Sepsis Detection",
                "Septic Shock Prediction"
            ],
            "rows": [
                [
                    "1",
                    "0.91",
                    "0.79"
                ],
                [
                    "2",
                    "0.90",
                    "0.80"
                ],
                [
                    "3",
                    "0.91",
                    "0.80"
                ],
                [
                    "4",
                    "0.92",
                    "0.83"
                ],
                [
                    "5",
                    "0.92",
                    "0.83"
                ],
                [
                    "6",
                    "[BOLD] 0.97",
                    "[BOLD] 0.90"
                ]
            ],
            "title": "Table 4: Features ablation in term of AUC. Sepsis is highly correlated with six vital signs. All models perform significantly better when six vital signs are used."
        },
        "insight": "In Table 4, the number of vitals for set 2-6 has been chosen in a way so that the gradual change, upon addition of new vital, becomes clear. We find that there is no particular trend up to five vital signs. [CONTINUE] we find that there is a universal improvement in AUC when six vital signs are used for both detection and prediction tasks."
    },
    {
        "id": "582",
        "table": {
            "header": [
                "Method",
                "Supervision",
                "Dataset",
                "Cap",
                "Abs Rel",
                "Sq Rel",
                "RMSE",
                "RMSE log",
                "[ITALIC] \u03b4<1.25",
                "[ITALIC] \u03b4<1.252",
                "[ITALIC] \u03b4<1.253"
            ],
            "rows": [
                [
                    "Train set mean",
                    "-",
                    "K",
                    "80m",
                    "0.361",
                    "4.826",
                    "8.102",
                    "0.377",
                    "0.638",
                    "0.804",
                    "0.894"
                ],
                [
                    "Eigen  Coarse",
                    "Depth",
                    "K",
                    "80m",
                    "0.214",
                    "1.605",
                    "6.563",
                    "0.292",
                    "0.673",
                    "0.884",
                    "0.957"
                ],
                [
                    "Eigen  Fine",
                    "Depth",
                    "K",
                    "80m",
                    "0.203",
                    "1.548",
                    "6.307",
                    "0.282",
                    "0.702",
                    "0.890",
                    "0.958"
                ],
                [
                    "Liu ",
                    "Depth",
                    "K",
                    "80m",
                    "0.201",
                    "1.584",
                    "6.471",
                    "0.273",
                    "0.680",
                    "0.898",
                    "0.967"
                ],
                [
                    "SfMLearner\u00a0",
                    "-",
                    "K",
                    "80m",
                    "0.208",
                    "1.768",
                    "6.856",
                    "0.283",
                    "0.678",
                    "0.885",
                    "0.957"
                ],
                [
                    "Vid2Depth\u00a0",
                    "-",
                    "K",
                    "80m",
                    "0.163",
                    "1.240",
                    "6.220",
                    "0.250",
                    "0.762",
                    "0.916",
                    "0.968"
                ],
                [
                    "GeoNet\u00a0",
                    "-",
                    "K",
                    "80m",
                    "0.155",
                    "1.296",
                    "5.857",
                    "0.233",
                    "0.793",
                    "0.931",
                    "0.973"
                ],
                [
                    "Zhan ",
                    "Stereo",
                    "K",
                    "80m",
                    "[BOLD] 0.135",
                    "1.132",
                    "5.585",
                    "0.229",
                    "0.820",
                    "0.933",
                    "0.971"
                ],
                [
                    "Ours",
                    "-",
                    "K",
                    "80m",
                    "0.150",
                    "[BOLD] 1.127",
                    "[BOLD] 5.564",
                    "[BOLD] 0.229",
                    "[BOLD] 0.823",
                    "[BOLD] 0.936",
                    "[BOLD] 0.974"
                ],
                [
                    "Garg ",
                    "Stereo",
                    "K",
                    "50m",
                    "0.169",
                    "1.080",
                    "5.104",
                    "0.273",
                    "0.740",
                    "0.904",
                    "0.962"
                ],
                [
                    "SfMLearner\u00a0",
                    "-",
                    "K",
                    "50m",
                    "0.201",
                    "1.391",
                    "5.181",
                    "0.264",
                    "0.696",
                    "0.900",
                    "0.966"
                ],
                [
                    "Vid2Depth\u00a0",
                    "-",
                    "K",
                    "50m",
                    "0.155",
                    "0.927",
                    "4.549",
                    "0.231",
                    "0.781",
                    "0.931",
                    "0.975"
                ],
                [
                    "GeoNet\u00a0",
                    "-",
                    "K",
                    "50m",
                    "0.147",
                    "0.936",
                    "4.348",
                    "0.218",
                    "0.810",
                    "0.941",
                    "0.977"
                ],
                [
                    "Zhan ",
                    "Stereo",
                    "K",
                    "50m",
                    "[BOLD] 0.128",
                    "[BOLD] 0.815",
                    "4.204",
                    "0.216",
                    "[BOLD] 0.835",
                    "0.941",
                    "0.975"
                ],
                [
                    "Ours",
                    "-",
                    "K",
                    "50m",
                    "0.146",
                    "0.927",
                    "[BOLD] 4.107",
                    "[BOLD] 0.216",
                    "0.819",
                    "[BOLD] 0.943",
                    "[BOLD] 0.981"
                ],
                [
                    "SfMLearner\u00a0",
                    "-",
                    "CS+K",
                    "80m",
                    "0.198",
                    "1.836",
                    "6.565",
                    "0.275",
                    "0.718",
                    "0.901",
                    "0.960"
                ],
                [
                    "Vid2Depth\u00a0",
                    "-",
                    "CS+K",
                    "80m",
                    "0.159",
                    "1.231",
                    "5.912",
                    "0.243",
                    "0.784",
                    "0.923",
                    "0.970"
                ],
                [
                    "GeoNet\u00a0",
                    "-",
                    "CS+K",
                    "80m",
                    "0.153",
                    "1.328",
                    "5.737",
                    "0.232",
                    "0.802",
                    "0.934",
                    "0.972"
                ],
                [
                    "Ours",
                    "-",
                    "CS+K",
                    "80m",
                    "[BOLD] 0.136",
                    "[BOLD] 1.064",
                    "[BOLD] 5.176",
                    "[BOLD] 0.289",
                    "[BOLD] 0.830",
                    "[BOLD] 0.942",
                    "[BOLD] 0.976"
                ]
            ],
            "title": "Table 1: Monodular depth estimation results on KITTI dataset by the split of Eigen [10]. K and CS refer to KITTI and Cityscapes datasets, respectively. As for supervision, \u2018Depth\u2019 means the ground truth depth is used during training, \u2018Stereo\u2019 means stereo image sequences with known baselines between two cameras are used during training, and \u2018-\u2019 means no supervision is provided. The results are capped at 80m and 50m, respectively. As for error metrics Abs Rel, Seq Rel, RMSE and RMSE log, lower value is better; as for accuracy metrics \u03b4<1.25, \u03b4<1.252 and \u03b4<1.253, higher value is better."
        },
        "insight": "As shown in Table 1, our method outperforms all self-supervised methods and achieves comparable results with supervised ones. [CONTINUE] Results in the bottom rows of Table 1 show that our method generalizes well in different environments."
    },
    {
        "id": "583",
        "table": {
            "header": [
                "Method",
                "Seq.09",
                "Seq.10"
            ],
            "rows": [
                [
                    "ORB-SLAM\u00a0 (short)",
                    "0.064\u00b10.141",
                    "0.064\u00b10.130"
                ],
                [
                    "ORB-SLAM\u00a0 (full)",
                    "0.014\u00b10.008",
                    "0.012\u00b10.011"
                ],
                [
                    "SfMLearner\u00a0",
                    "0.021\u00b10.017",
                    "0.020\u00b10.015"
                ],
                [
                    "SfMLearner\u00a0 modified",
                    "0.016\u00b10.009",
                    "0.013\u00b10.009"
                ],
                [
                    "Zhan ",
                    "0.013\u00b10.009",
                    "0.013\u00b10.008"
                ],
                [
                    "Vid2Depth\u00a0",
                    "0.013\u00b10.010",
                    "0.012\u00b10.011"
                ],
                [
                    "GeoNet\u00a0",
                    "0.012\u00b10.007",
                    "0.012\u00b10.009"
                ],
                [
                    "Ours",
                    "[BOLD] 0.0030\u00b10.0014",
                    "[BOLD] 0.0029\u00b10.0012"
                ]
            ],
            "title": "Table 2: Absolute Trajectory Error (ATE) on sequence 09 and 10 in KITTI odometry dataset. Our method outperforms all the other baselines by a large margin."
        },
        "insight": "As shown in Table 2, our method significantly outperforms all the other baselines, [CONTINUE] although only a limited number of frames can be processed by LSTM, our method still performs better than ORB-SLAM (full) without any need of global optimization (such as loop closure, bundle adjustment and re-localization)"
    },
    {
        "id": "584",
        "table": {
            "header": [
                "Method",
                "Dataset",
                "Cap",
                "Abs Rel",
                "Sq Rel",
                "RMSE",
                "RMSE log",
                "[ITALIC] \u03b4<1.25",
                "[ITALIC] \u03b4<1.252",
                "[ITALIC] \u03b4<1.253"
            ],
            "rows": [
                [
                    "Baseline",
                    "K",
                    "50m",
                    "0.218",
                    "1.462",
                    "5.837",
                    "0.275",
                    "0.723",
                    "0.908",
                    "0.967"
                ],
                [
                    "Baseline+code",
                    "K",
                    "50m",
                    "0.162",
                    "1.178",
                    "4.533",
                    "0.236",
                    "0.811",
                    "0.933",
                    "0.973"
                ],
                [
                    "Baseline+code+GAN",
                    "K",
                    "50m",
                    "0.152",
                    "0.937",
                    "4.120",
                    "0.217",
                    "0.816",
                    "0.939",
                    "0.979"
                ],
                [
                    "Baseline+code+LSTM",
                    "K",
                    "50m",
                    "0.148",
                    "0.939",
                    "4.271",
                    "0.217",
                    "0.816",
                    "0.941",
                    "0.977"
                ],
                [
                    "Baseline+code+GAN+LSTM",
                    "K",
                    "50m",
                    "0.150",
                    "0.931",
                    "4.116",
                    "0.216",
                    "0.819",
                    "0.943",
                    "0.979"
                ],
                [
                    "Baseline+code+GAN+LSTM+TC",
                    "K",
                    "50m",
                    "[BOLD] 0.146",
                    "[BOLD] 0.927",
                    "[BOLD] 4.107",
                    "[BOLD] 0.216",
                    "[BOLD] 0.819",
                    "[BOLD] 0.943",
                    "[BOLD] 0.981"
                ]
            ],
            "title": "Table 3: Ablation study on depth estimation for various versions of our method. Baseline denotes our framework without code, LSTM, discriminator (GAN) and trajectory consistency (TC) loss."
        },
        "insight": "All the experiments are conducted on KITTI dataset and results are shown in Table 3, 4 [CONTINUE] In addition, adversarial learning gives the performance a further boost, and the temporal information actually improves depth."
    },
    {
        "id": "585",
        "table": {
            "header": [
                "Method",
                "Seq.09",
                "Seq.10"
            ],
            "rows": [
                [
                    "Baseline",
                    "0.0072\u00b10.0025",
                    "0.0070\u00b10.0023"
                ],
                [
                    "B+code",
                    "0.0069\u00b10.0021",
                    "0.0065\u00b10.0020"
                ],
                [
                    "B+code+GAN",
                    "0.0064\u00b10.0019",
                    "0.0062\u00b10.0019"
                ],
                [
                    "B+code+LSTM",
                    "0.0045\u00b10.0015",
                    "0.0043\u00b10.0015"
                ],
                [
                    "B+code+GAN+LSTM",
                    "0.0036\u00b10.0013",
                    "0.0036\u00b10.0012"
                ],
                [
                    "B+code+GAN+LSTM+TC",
                    "[BOLD] 0.0030\u00b10.0014",
                    "[BOLD] 0.0029\u00b10.0012"
                ]
            ],
            "title": "Table 4: Ablation study on pose estimation for various versions of our method on KITTI sequence 09 and 10. B denotes baseline."
        },
        "insight": "As for pose estimation in Table 4, our baseline method performs much better than the other self-supervised VO approaches in literature (Table 2). This may mainly because of the joint use of depth and image for pose estimation (Eq. (5)). In addition, the accuracy is significantly improved by LSTM which incorporates historical information of multiple frames. The enforcement of trajectory consistency also brings about promising improvements in that it enforces geometric consistency among multiple pose estimations. Since depth is improved mainly on edges and details which takes up a small proportion, the accuracy gain is therefore limited. Yet the improved details are very important to RGBD matching for pose regression. Therefore, a slight increase in depth accuracy causes a big improvement in pose estimation."
    },
    {
        "id": "586",
        "table": {
            "header": [
                "Data of each sequence",
                "1 hour data"
            ],
            "rows": [
                [
                    "Time-step length",
                    "15 mins"
                ],
                [
                    "Sequence length",
                    "4"
                ],
                [
                    "Number of regions",
                    "64"
                ],
                [
                    "Number of features",
                    "68"
                ],
                [
                    "Number of hidden layers",
                    "2"
                ],
                [
                    "Number of neurons in each hidden layer",
                    "1500-2000"
                ],
                [
                    "Activation function of hidden recurrent layers",
                    "tanh"
                ],
                [
                    "Loss function",
                    "Mean squared error"
                ]
            ],
            "title": "Table 3: Experimental Parameters"
        },
        "insight": "Although recurrent neural networks can accept sequences with any length as input, because of the nature of our problem we had to choose a constant sequence length. Due to the constrained computational power we had, we used every hour data as a sequence. Because the time interval for each data point is 15 minutes, each sequence consists of four data points. Since the data contains records for 110 days, the shape of data would be (110*24, 4, 68). Table 3 includes the list of parameters used in the experiment for all three types of RNNs."
    },
    {
        "id": "587",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] RMSE",
                "[BOLD] MAPE (%)",
                "[BOLD] Training time"
            ],
            "rows": [
                [
                    "DEMA",
                    "4.37",
                    "48.54",
                    "-"
                ],
                [
                    "LASSO",
                    "3.87",
                    "41.42",
                    "4 mins/37 secs"
                ],
                [
                    "XGBoost",
                    "3.78",
                    "40.80",
                    "120 mins/53 secs"
                ],
                [
                    "LSTM",
                    "3.46",
                    "39.04",
                    "146 mins/43 secs"
                ],
                [
                    "Simple RNN",
                    "3.22",
                    "37.42",
                    "16 mins/40 secs"
                ],
                [
                    "GRU",
                    "3.21",
                    "37.50",
                    "119 mins/19 secs"
                ]
            ],
            "title": "Table 4: Errors over the entire city"
        },
        "insight": "According error rates both RMSE and MAPE, it can be observed that RNNs demonstrate considerably better performance in comparison to the others. Table 4 shows the detailed values of errors over the entire city for each method. Training was performed on a core-i7-7700HQ CPU with 16 GBs of RAM."
    },
    {
        "id": "588",
        "table": {
            "header": [
                "[BOLD] Statement  [BOLD] Level",
                "[BOLD] Opinion  [BOLD] Agree",
                "[BOLD] Opinion  [BOLD] Partially Agree",
                "[BOLD] Opinion  [BOLD] Disagree"
            ],
            "rows": [
                [
                    "Legibility",
                    "37",
                    "0",
                    "3"
                ],
                [
                    "Fluency",
                    "28",
                    "8",
                    "4"
                ],
                [
                    "Effectiveness",
                    "30",
                    "6",
                    "4"
                ],
                [
                    "UI",
                    "35",
                    "4",
                    "1"
                ],
                [
                    "Experience",
                    "31",
                    "7",
                    "2"
                ]
            ],
            "title": "TABLE II: Related Attributes of Bullet Data"
        },
        "insight": "For each statement, each volunteer can select one of the three choices, 'Agree', 'Partially Agree' or 'Disagree' to express his/her opinion on that statement. The survey result is shown in Table 2. [CONTINUE] From Table II, we can learn that 28 out 40 volunteers claim that the webpage loading is still fluent when the SmartDanmu extension is enabled, which means the latency caused by communicating with cloud server is tolerant and even negligible for the video watchers. There are still 12 volunteers declare that the inconsistency of they can feel [CONTINUE] the video page loading to an extent. We note that a server with more computational and communication resource will improve this delay. Meanwhile, around 75% participants think SmartDanmu could effectively remove low-quality bullets and increase the overall watching experience compared with the original danmaku videos."
    },
    {
        "id": "589",
        "table": {
            "header": [
                "[ITALIC] method",
                "BlogCatalog mic.",
                "BlogCatalog mac.",
                "PubMed mic.",
                "PubMed mac.",
                "Cora mic.",
                "Cora mac.",
                "Reddit mic.",
                "Reddit mac.",
                "Flickr mic.",
                "Flickr mac.",
                "Youtube mic.",
                "Youtube mac.",
                "CoCit mic.",
                "CoCit mac."
            ],
            "rows": [
                [
                    "DeepWalk",
                    "42.15",
                    "28.48",
                    "73.96",
                    "71.34",
                    "64.98",
                    "51.53",
                    "94.40",
                    "92.01",
                    "[BOLD] 42.20",
                    "[BOLD] 31.00",
                    "47.09",
                    "39.89",
                    "41.92",
                    "30.07"
                ],
                [
                    "Node2vec",
                    "42.46",
                    "[BOLD] 29.16",
                    "72.36",
                    "68.54",
                    "65.74",
                    "49.12",
                    "94.11",
                    "91.73",
                    "42.11",
                    "30.57",
                    "[BOLD] 48.41",
                    "[BOLD] 42.04",
                    "41.64",
                    "28.18"
                ],
                [
                    "Verse",
                    "35.51",
                    "21.77",
                    "71.24",
                    "68.68",
                    "60.87",
                    "45.52",
                    "92.87",
                    "89.69",
                    "35.70",
                    "23.00",
                    "45.12",
                    "37.28",
                    "40.17",
                    "27.56"
                ],
                [
                    "APP",
                    "20.60",
                    "5.39",
                    "69.00",
                    "65.20",
                    "64.58",
                    "47.03",
                    "77.11",
                    "56.28",
                    "24.26",
                    "4.21",
                    "45.04",
                    "36.61",
                    "40.34",
                    "28.06"
                ],
                [
                    "HOPE",
                    "n.a",
                    "n.a",
                    "63.00",
                    "54.6",
                    "26.23",
                    "1.22",
                    "n.a",
                    "n.a",
                    "n.a",
                    "n.a",
                    "n.a",
                    "n.a",
                    "16.66",
                    "1.91"
                ],
                [
                    "NetMF",
                    "[BOLD] 43.29",
                    "29.04",
                    "73.66",
                    "71.11",
                    "63.38",
                    "46.16",
                    "91.99",
                    "86.92",
                    "37.44",
                    "21.55",
                    "\u2717",
                    "\u2717",
                    "40.42",
                    "28.7"
                ],
                [
                    "LINE-1+2",
                    "41.01",
                    "25.02",
                    "62.29",
                    "59.79",
                    "54.04",
                    "41.83",
                    "[BOLD] 94.50",
                    "[BOLD] 92.08",
                    "41.46",
                    "27.65",
                    "48.22",
                    "41.51",
                    "37.71",
                    "26.75"
                ],
                [
                    "LINE-1",
                    "41.54",
                    "24.28",
                    "55.65",
                    "53.83",
                    "62.36",
                    "47.19",
                    "94.31",
                    "91.96",
                    "40.92",
                    "26.19",
                    "47.49",
                    "41.17",
                    "36.10",
                    "25.70"
                ],
                [
                    "LINE-2",
                    "36.70",
                    "18.80",
                    "56.81",
                    "51.71",
                    "51.05",
                    "35.37",
                    "94.30",
                    "91.81",
                    "40.49",
                    "24.24",
                    "47.46",
                    "39.97",
                    "31.4",
                    "20.59"
                ],
                [
                    "GraphSAGE",
                    "19.28",
                    "5.07",
                    "77.90",
                    "76.39",
                    "67.07",
                    "44.78",
                    "89.94",
                    "82.28",
                    "25.52",
                    "5.84",
                    "40.45",
                    "29.97",
                    "43.71",
                    "30.52"
                ],
                [
                    "GraphSAGE-GCN",
                    "26.76",
                    "10.82",
                    "[BOLD] 79.19",
                    "[BOLD] 77.85",
                    "69.64",
                    "51.64",
                    "91.65",
                    "86.88",
                    "29.66",
                    "9.69",
                    "42.54",
                    "32.54",
                    "44.08",
                    "30.73"
                ],
                [
                    "SDNE",
                    "26.40",
                    "12.29",
                    "46.41",
                    "32.32",
                    "32.43",
                    "8.27",
                    "\u2717",
                    "\u2717",
                    "29.10",
                    "10.53",
                    "\u2717",
                    "\u2717",
                    "21.67",
                    "9.53"
                ],
                [
                    "Max-Vote",
                    "32.71",
                    "19.60",
                    "76.81",
                    "75.25",
                    "[BOLD] 71.96",
                    "[BOLD] 57.21",
                    "93.26",
                    "90.11",
                    "34.60",
                    "22.48",
                    "28.96",
                    "25.65",
                    "[BOLD] 44.66",
                    "[BOLD] 33.39"
                ]
            ],
            "title": "TABLE VI: Multilabel Node Classification results in terms of Micro-F1 and Macro-F1. All results are mean of 5-fold cross validations. \u2717\u00a0indicates the corresponding method failed to finish for the given dataset. \u2019n.a\u2019 indicates the given method is \u2019not applicable\u2019 to the corresponding graph."
        },
        "insight": "We report the Micro-F1 and Macro-F1 scores after a 5-fold multi-label classification using one-vs-rest logistic regression. [CONTINUE] In this section, we look at the results from node classification (Table 6)."
    },
    {
        "id": "590",
        "table": {
            "header": [
                "Software Engineer",
                "Experience with IoT Development (None/Low/ Medium/ High)",
                "Solution Performance (Fitness Average)",
                "Does the solution work?"
            ],
            "rows": [
                [
                    "1",
                    "High",
                    "55.48",
                    "Y"
                ],
                [
                    "2",
                    "None",
                    "26.99",
                    "N"
                ],
                [
                    "3",
                    "High",
                    "62.88",
                    "Y"
                ],
                [
                    "4",
                    "Low",
                    "62.49",
                    "Y"
                ],
                [
                    "5",
                    "None",
                    "30.50",
                    "N"
                ],
                [
                    "6",
                    "Low",
                    "51.09",
                    "Y"
                ],
                [
                    "7",
                    "Medium",
                    "54.37",
                    "Y"
                ],
                [
                    "8",
                    "None",
                    "16.59",
                    "N"
                ],
                [
                    "9",
                    "High",
                    "28.62",
                    "N"
                ],
                [
                    "10",
                    "None",
                    "61.60",
                    "Y"
                ],
                [
                    "11",
                    "None",
                    "29.67",
                    "N"
                ],
                [
                    "12",
                    "Medium",
                    "47.81",
                    "Y"
                ],
                [
                    "13",
                    "None",
                    "30.32",
                    "N"
                ],
                [
                    "14",
                    "Low",
                    "56.91",
                    "Y"
                ],
                [
                    "[BOLD] Learning",
                    "[EMPTY]",
                    "[BOLD] 59.53",
                    "[BOLD] Y"
                ],
                [
                    "zeroed",
                    "[EMPTY]",
                    "28.33",
                    "N"
                ]
            ],
            "title": "TABLE III: Correlation between participants expertises in the Internet of Things with their solution results."
        },
        "insight": "After executing the solution proposed by each participant, we connect that solution's results with the participant's knowledge in the IoT domain, as shown in Table 3. [CONTINUE] Thus, we performed statistical analyses, as described by Peck and Devore , of the measures presented in Table 3."
    },
    {
        "id": "591",
        "table": {
            "header": [
                "Variable",
                "n samples",
                "Highest value",
                "Mean \u00af\u00af\u00af [ITALIC] x",
                "Median",
                "Standard deviation  [ITALIC] \u03c3",
                "Degrees of freedom (n-1)",
                "t critical value (.99%)"
            ],
            "rows": [
                [
                    "Software Engineers",
                    "14",
                    "62.88",
                    "43.95",
                    "49.45",
                    "16.00",
                    "13",
                    "2.65"
                ],
                [
                    "Software Engineers with IoT knowledge",
                    "8",
                    "62.88",
                    "52.46",
                    "54.92",
                    "10.91",
                    "7",
                    "3.00"
                ],
                [
                    "Software Engineers without IoT knowledge",
                    "6",
                    "61.60",
                    "32.61",
                    "30.00",
                    "15.15",
                    "5",
                    "3.37"
                ],
                [
                    "Machine- learning based approach",
                    "1",
                    "59.53",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "TABLE IV: Data to perform test statistic."
        },
        "insight": "As shown in Table 4, we separated the results of the experiments into two groups: i) software engineers with IoT knowledge and ii) software engineers without IoT knowledge. Then, we calculated the mean and the standard deviation of [CONTINUE] Accordingly, for a distribution with 7 degrees of freedom (see Table 4) and a confidence level of 99%, the negative tcriticalvalue is -3.00. [CONTINUE] As shown in Table 4, the performance mean (x) of the IoT expert software engineers' solutions is 52.46 and the standard deviation (\u03c3) is 10.91. [CONTINUE] As shown in Table 4, the performance mean (x) of the solutions from software engineers without experience in IoT development is 32.61 and the standard deviation (\u03c3) is 15.15. [CONTINUE] As shown in Table 4, this T-distribution has 5 degrees of freedom."
    },
    {
        "id": "592",
        "table": {
            "header": [
                "[EMPTY]",
                "Energy%",
                "People%",
                "Trip%",
                "[BOLD] Fitness"
            ],
            "rows": [
                [
                    "Average Participant 12",
                    "50.52",
                    "100",
                    "38.14",
                    "[BOLD] 56.90"
                ],
                [
                    "Average Learning",
                    "8.46",
                    "100",
                    "46.29",
                    "[BOLD] 68.83"
                ]
            ],
            "title": "TABLE VI: Using the same solution in a different environment - day average."
        },
        "insight": "As shown in Table 6, when considering the whole day, the machine-learning approach presented the best result. Because the average time for the trip was a little higher using the machine-learning approach, the difference in energy consumption between the two solutions is considerably higher."
    },
    {
        "id": "593",
        "table": {
            "header": [
                "# [ITALIC] Batch",
                "[ITALIC] Training",
                "[ITALIC] Validation",
                "[ITALIC] Test",
                "[ITALIC] Thr(2)",
                "[ITALIC] Thr(1)"
            ],
            "rows": [
                [
                    "100",
                    "0.8954",
                    "6.1547",
                    "9.1574",
                    "71.52%",
                    "65.14%"
                ],
                [
                    "300",
                    "0.8521",
                    "5.9856",
                    "7.1259",
                    "74.32%",
                    "70.78%"
                ],
                [
                    "500",
                    "0.7485",
                    "5.0198",
                    "5.1245",
                    "78.91%",
                    "73.15%"
                ],
                [
                    "700",
                    "0.6574",
                    "3.1497",
                    "4.1547",
                    "85.47%",
                    "80.19%"
                ],
                [
                    "1000",
                    "0.5782",
                    "1.8643",
                    "3.0214",
                    "91.71%",
                    "89.18%"
                ]
            ],
            "title": "TABLE III: Batch number sensitivity analyze Batch."
        },
        "insight": "Looking at table.III in more details, by increasing the number of batches training and test errors have been decreased. it is also noticeable that the validation data fall by increasing the number of batches. during the learning process the main aim is to decrease the validation data error which have been satisfied properly.moreover, increment of the batches directly influence the accuracy of the deep learning process."
    },
    {
        "id": "594",
        "table": {
            "header": [
                "[ITALIC] k",
                "[ITALIC] Thr(2)",
                "[ITALIC] Thr(1)",
                "[ITALIC] Thr(0.5)"
            ],
            "rows": [
                [
                    "2.125\u221710\u22122",
                    "87.56%",
                    "82.39%",
                    "79.45%"
                ],
                [
                    "2.125\u221710\u22125",
                    "87.84%",
                    "83.05%",
                    "80.25%"
                ],
                [
                    "2.125\u221710\u22127",
                    "88.69%",
                    "84.58%",
                    "82.12%"
                ],
                [
                    "2.125\u221710\u221210",
                    "80.32%",
                    "78.73%",
                    "77.45%"
                ],
                [
                    "2.125\u221710\u221213",
                    "64.78%",
                    "63.58%",
                    "61.03%"
                ]
            ],
            "title": "TABLE IV: Accuracy analyze of deep learning solution based on changing coefficie."
        },
        "insight": "The variable parameters in this part are k and De which play a central role in the behavior of the equation. Table.IV demonstrate the accuracy of deep learning solution by changing the k and De values, where C0 and L are 75.5 and 0.05 respectively. [CONTINUE] According to the table.IV, it is understood that the accuracy of the solution strongly depends on the correlation of Rate of reaction and Diffusion coefficient. In the case of constant Diffusion coefficient where k value is less than the particular amount the accuracy of the solution drop considerably. It is also similar for the case of the constant Rate of reaction where if De value chose larger than specific value, the solution will not be reliable. When it comes to physics of the phenomena, reduction of the accuracy of the deep learning solution become"
    },
    {
        "id": "595",
        "table": {
            "header": [
                "[ITALIC] De",
                "[ITALIC] Thr(2)",
                "[ITALIC] Thr(1)",
                "[ITALIC] Thr(0.5)"
            ],
            "rows": [
                [
                    "2.6\u221710\u22125",
                    "73.38%",
                    "66.39%",
                    "61.45%"
                ],
                [
                    "2.6\u221710\u22127",
                    "84.29%",
                    "82.48%",
                    "76.21%"
                ],
                [
                    "2.6\u221710\u221210",
                    "89.29%",
                    "87.78%",
                    "82.27%"
                ],
                [
                    "2.6\u221710\u221212",
                    "88.57%",
                    "86.54%",
                    "82.12%"
                ],
                [
                    "2.6\u221710\u221215",
                    "81.33%",
                    "75.91%",
                    "70.67%"
                ]
            ],
            "title": "TABLE IV: Accuracy analyze of deep learning solution based on changing coefficie."
        },
        "insight": "It is needed to note that based on the Damkohler concept, the values for Damkohler must be chosen carefully between specific intervals. In the table.V it is clear that the accuracy of deep learning solution dramatically decreases when the Damkohler number gets smaller from a specific value."
    },
    {
        "id": "596",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] HAR Model",
                "[BOLD] Interpolant",
                "[BOLD] Input",
                "[BOLD] Window Size",
                "[BOLD] Precision [ITALIC] m",
                "[BOLD] Recall [ITALIC] m",
                "[BOLD] F-score [ITALIC] m"
            ],
            "rows": [
                [
                    "(clinical room)",
                    "[EMPTY]",
                    "(acceleration)",
                    "[EMPTY]",
                    "( [ITALIC] \u03b4t)",
                    "(mean\u00b1std)",
                    "(mean\u00b1std)",
                    "(mean\u00b1std)"
                ],
                [
                    "[ITALIC] Roomset1",
                    "SVM [ITALIC] lin\u2217",
                    "Cubic",
                    "Hand-crafted features",
                    "4 seconds",
                    "87.87\u00b12.55",
                    "83.44\u00b11.72",
                    "84.96\u00b11.23"
                ],
                [
                    "[ITALIC] Roomset1",
                    "SVM [ITALIC] rbf\u2217",
                    "None",
                    "Hand-crafted features",
                    "8 seconds",
                    "90.39\u00b12.70",
                    "87.42\u00b11.42",
                    "88.45\u00b11.68"
                ],
                [
                    "[ITALIC] Roomset1",
                    "CRF\u2217",
                    "Linear",
                    "Hand-crafted features",
                    "2 seconds",
                    "85.97\u00b12.43",
                    "82.35\u00b13.08",
                    "83.73\u00b12.40"
                ],
                [
                    "[ITALIC] Roomset1",
                    "Bi-LSTM",
                    "Linear",
                    "Raw sensor readings",
                    "2 seconds",
                    "89.97\u00b10.78",
                    "85.11\u00b10.99",
                    "86.96\u00b11.06"
                ],
                [
                    "[ITALIC] Roomset1",
                    "DeepCNN",
                    "Quadratic",
                    "Raw sensor readings",
                    "4 seconds",
                    "92.43\u00b11.21",
                    "87.93\u00b11.74",
                    "89.73\u00b11.55"
                ],
                [
                    "[ITALIC] Roomset1",
                    "DeepConvLSTM",
                    "Linear",
                    "Raw sensor readings",
                    "4 seconds",
                    "91.87\u00b11.43",
                    "88.88\u00b11.79",
                    "90.42\u00b11.54"
                ],
                [
                    "[ITALIC] Roomset1",
                    "[BOLD] (Ours) SparseSense",
                    "None",
                    "Raw sensor readings",
                    "2 seconds",
                    "[BOLD] 95.0\u00b10.75",
                    "[BOLD] 94.08\u00b10.78",
                    "[BOLD] 94.51\u00b10.62"
                ],
                [
                    "[ITALIC] Roomset2",
                    "SVM [ITALIC] lin\u2217",
                    "Cubic",
                    "Hand-crafted features",
                    "2 seconds",
                    "87.06\u00b14.10",
                    "84.00\u00b12.90",
                    "84.97\u00b13.74"
                ],
                [
                    "[ITALIC] Roomset2",
                    "SVM [ITALIC] rbf\u2217",
                    "None",
                    "Hand-crafted features",
                    "8 seconds",
                    "90.97\u00b14.11",
                    "83.88\u00b12.04",
                    "85.53\u00b12.86"
                ],
                [
                    "[ITALIC] Roomset2",
                    "CRF\u2217",
                    "None",
                    "Hand-crafted features",
                    "16 seconds",
                    "83.68\u00b16.50",
                    "78.29\u00b13.58",
                    "79.99\u00b14.76"
                ],
                [
                    "[ITALIC] Roomset2",
                    "Bi-LSTM",
                    "Previous",
                    "Raw sensor readings",
                    "2 seconds",
                    "92.38\u00b10.91",
                    "91.4\u00b10.62",
                    "91.78\u00b10.58"
                ],
                [
                    "[ITALIC] Roomset2",
                    "DeepCNN",
                    "Linear",
                    "Raw sensor readings",
                    "4 seconds",
                    "93.11\u00b10.94",
                    "91.7\u00b11.18",
                    "92.36\u00b10.99"
                ],
                [
                    "[ITALIC] Roomset2",
                    "DeepConvLSTM",
                    "Previous",
                    "Raw sensor readings",
                    "4 seconds",
                    "94.16\u00b10.52",
                    "93.05\u00b10.78",
                    "93.77\u00b10.63"
                ],
                [
                    "[ITALIC] Roomset2",
                    "[BOLD] (Ours) SparseSense",
                    "None",
                    "Raw sensor readings",
                    "2 seconds",
                    "[BOLD] 97.07\u00b10.52",
                    "[BOLD] 96.88\u00b10.34",
                    "[BOLD] 96.97\u00b10.37"
                ]
            ],
            "title": "Table 1: A comparison of the best performing activity recognition models for the naturally sparse clinical room datasets. Numbers and design choices for the baselines with asterisks are quoted from [Wickramasinghe and Ranasinghe2015]. The remaining baselines are replicated following their original paper descriptions. For a fair comparison, the reported results are for the best performing configuration of interpolants and window durations for each baseline."
        },
        "insight": "In Table 1, we report the mean F-measure (F-scorem) as the widely adopted evaluation metric [CONTINUE] report the highest achieving configurations in Table 1. [CONTINUE] From the outlined results, we observe that the SparseSense network outperforms all the baseline models with a large margin in the task of sparse data-stream classification. [CONTINUE] Taking into account the superior performance of DeepConvLSTM among the baselines in Table 1, here we only present comparisons with this model."
    },
    {
        "id": "597",
        "table": {
            "header": [
                "Decoder",
                "R",
                "B",
                "T",
                "C (\u2030) 0-3s",
                "Centroid @ 3s L2",
                "Centroid @ 3s NLL",
                "Centroid @ 3s H",
                "Heading @ 3s L2",
                "Heading @ 3s NLL",
                "Heading @ 3s H"
            ],
            "rows": [
                [
                    "MLP",
                    "\u2717",
                    "\u2717",
                    "\u2717",
                    "9.79",
                    "127",
                    "2.56",
                    "0.47",
                    "5.68",
                    "-3.32",
                    "0.15"
                ],
                [
                    "MLP",
                    "\u2713",
                    "\u2717",
                    "\u2717",
                    "2.23",
                    "118",
                    "1.50",
                    "0.47",
                    "4.97",
                    "-7.01",
                    "1.49"
                ],
                [
                    "GNN",
                    "\u2713",
                    "\u2717",
                    "\u2717",
                    "2.91",
                    "122",
                    "1.73",
                    "0.72",
                    "5.09",
                    "-6.65",
                    "2.37"
                ],
                [
                    "GNN",
                    "\u2713",
                    "G",
                    "\u2717",
                    "2.14",
                    "116",
                    "1.42",
                    "0.50",
                    "5.14",
                    "[BOLD] -7.31",
                    "1.17"
                ],
                [
                    "GNN",
                    "\u2713",
                    "R",
                    "\u2717",
                    "1.32",
                    "109",
                    "1.14",
                    "0.39",
                    "4.77",
                    "-7.12",
                    "-1.97"
                ],
                [
                    "GNN",
                    "\u2713",
                    "R",
                    "R",
                    "[BOLD] 0.78",
                    "[BOLD] 105",
                    "[BOLD] 1.08",
                    "0.24",
                    "[BOLD] 4.75",
                    "-6.99",
                    "-1.87"
                ]
            ],
            "title": "TABLE III: [ATG4D] Ablation study of the different contributions at 95% recall"
        },
        "insight": "We show the advantages of projecting the output state of node u to the local coordinate system of node v when computing the message m(k) u\u2192v in the ablation study in Table III. [CONTINUE] However, we conduct ablation studies for our model at 95% recall in the next sections, which is closer to the demand that self-driving cars should meet. [CONTINUE] Ablation Study: We first study the per-actor feature extraction mechanism of our model by comparing simple feature indexing versus Rotated RoI Align. We do this study in a model that does not contemplate interactions for better isolation. In particular, we omit our SPAGNN from Fig. 2 and use the initial trajectories predicted by CNNR as the final output. As shown in the first 2 rows of Table III, Rotated RoI Align clearly outperforms simple feature indexing. Our RRoI pooled features provide better information about the surroundings of the target vehicle since they contain features in a region spanning by 25 meters whereas the feature indexing variant consists of just accessing the feature map at the anchor location associated to the detection. [CONTINUE] e) Graph Neural Network architectures: We evaluate several GNN architectures with different levels of spatial [CONTINUE] awareness to demonstrate the effectiveness of our SPAGNN. [CONTINUE] The second to third rows of Table III show that adding a standard GNN without spatial awareness does not improve performance. From the third to the forth row we observe an improvement by including the detection bounding boxes in global (G) coordinate frame as part of the state at every message passing iteration of the GNN. Then, we make these bounding boxes relative (R) to the actor receiving the message, which gives us a boost across most metrics. Finally, we add the parameters of the predicted probability distributions of the future trajectories to the message passing algorithm to recover our SPAGNN. Interestingly, the models become more certain, i.e. lower entropy (H), as we add better spatial-awareness mechanisms."
    },
    {
        "id": "598",
        "table": {
            "header": [
                "Model",
                "Col. (\u2030) 0-1s",
                "Col. (\u2030) 0-3s",
                "L2 x,y (cm) 0s",
                "L2 x,y (cm) 1s",
                "L2 x,y (cm) 3s",
                "Heading err (deg) 0s",
                "Heading err (deg) 1s",
                "Heading err (deg) 3s"
            ],
            "rows": [
                [
                    "D+T+S-LSTM ",
                    "1.43",
                    "16.31",
                    "22",
                    "147",
                    "607",
                    "4.06",
                    "5.14",
                    "8.07"
                ],
                [
                    "D+T+CSP ",
                    "1.64",
                    "20.78",
                    "22",
                    "95",
                    "282",
                    "4.06",
                    "4.70",
                    "6.20"
                ],
                [
                    "D+T+CAR-Net ",
                    "0.28",
                    "12.30",
                    "22",
                    "46",
                    "149",
                    "4.06",
                    "4.87",
                    "6.14"
                ],
                [
                    "FaF ",
                    "1.12",
                    "17.41",
                    "30",
                    "54",
                    "183",
                    "4.71",
                    "4.98",
                    "6.43"
                ],
                [
                    "IntentNet ",
                    "0.28",
                    "7.03",
                    "26",
                    "45",
                    "146",
                    "4.21",
                    "4.40",
                    "5.64"
                ],
                [
                    "NMP ",
                    "0.05",
                    "3.06",
                    "23",
                    "36",
                    "114",
                    "4.10",
                    "4.24",
                    "5.09"
                ],
                [
                    "E2E S-LSTM ",
                    "0.06",
                    "1.14",
                    "22",
                    "36",
                    "106",
                    "4.97",
                    "4.85",
                    "5.61"
                ],
                [
                    "E2E CSP ",
                    "0.06",
                    "4.47",
                    "23",
                    "38",
                    "114",
                    "4.82",
                    "5.04",
                    "5.84"
                ],
                [
                    "E2E CAR-Net ",
                    "0.07",
                    "1.15",
                    "22",
                    "35",
                    "105",
                    "4.44",
                    "4.41",
                    "5.12"
                ],
                [
                    "SpAGNN (Ours)",
                    "[BOLD] 0.03",
                    "[BOLD] 0.42",
                    "[BOLD] 22",
                    "[BOLD] 33",
                    "[BOLD] 96",
                    "[BOLD] 3.92",
                    "[BOLD] 3.89",
                    "[BOLD] 4.55"
                ]
            ],
            "title": "TABLE I: [ATG4D] Social interaction and motion forecasting metrics at 80% detection recall"
        },
        "insight": "Comparison Against the State-of-the-art: We benchmark our method against a variety of baselines, which can be classified in three groups. [CONTINUE] Methods that use past trajectory as their main motion cues: SocialLSTM , Convolutional Social Pooling , CAR-Net . [CONTINUE] we employ our object detector and a vehicle tracker consisting of an Interactive Multiple Model  with Unscented Kalman Filter  and Hungarian Matching [CONTINUE] FaF , IntentNet  and NMP . [CONTINUE] Tables [CONTINUE] and II show the cumulative collision rate, the centroid L2 error and the absolute heading error for different future horizons for the ATG4D and NUSCENES datasets respectively. The results reveal a clear improvement in detection and interaction understanding, as well as a solid gain in long-term motion forecasting. Our model substantially improves both in collision rate and centroid error on both datasets. [CONTINUE] We obtain the lowest heading error on ATG4D [CONTINUE] Note that we perform this comparison at 80% recall at IoU 0.5 in ATG4D because some of the baselines do not reach higher recalls. [CONTINUE] Our approach resulted in significant improvements over the state-of-theart on the challenging ATG4D and NUSCENES autonomous driving datasets."
    },
    {
        "id": "599",
        "table": {
            "header": [
                "Model",
                "Col. (\u2030) 0-1s",
                "Col. (\u2030) 0-3s",
                "L2 x,y (cm) 0s",
                "L2 x,y (cm) 1s",
                "L2 x,y (cm) 3s",
                "Heading err (deg) 0s",
                "Heading err (deg) 1s",
                "Heading err (deg) 3s"
            ],
            "rows": [
                [
                    "E2E S-LSTM ",
                    "0.84",
                    "9.64",
                    "24",
                    "71",
                    "185",
                    "3.08",
                    "3.59",
                    "4.63"
                ],
                [
                    "E2E CSP ",
                    "0.41",
                    "5.77",
                    "24",
                    "70",
                    "174",
                    "3.14",
                    "3.51",
                    "4.64"
                ],
                [
                    "E2E CAR-Net ",
                    "0.36",
                    "4.90",
                    "23",
                    "61",
                    "158",
                    "[BOLD] 2.84",
                    "[BOLD] 3.07",
                    "4.06"
                ],
                [
                    "SpAGNN (Ours)",
                    "[BOLD] 0.25",
                    "[BOLD] 2.22",
                    "[BOLD] 22",
                    "[BOLD] 58",
                    "[BOLD] 145",
                    "2.99",
                    "3.12",
                    "[BOLD] 3.96"
                ]
            ],
            "title": "TABLE II: [nuScenes] Social interaction and motion forecasting metrics at 60% recall"
        },
        "insight": "we get results that are on par with the best baseline in NUSCENES. [CONTINUE] In NUSCENES we use 60% recall"
    },
    {
        "id": "600",
        "table": {
            "header": [
                "Reso-",
                "Method",
                "Inception",
                "FID",
                "Accu-"
            ],
            "rows": [
                [
                    "lution",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "racy"
                ],
                [
                    "64x64",
                    "Real Images",
                    "16.3\u00b10.4",
                    "0",
                    "54.5"
                ],
                [
                    "64x64",
                    " GT Layout",
                    "7.3\u00b10.1",
                    "86.5",
                    "33.9"
                ],
                [
                    "64x64",
                    " GT Layout",
                    "9.1\u00b10.1",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "64x64",
                    "Ours GT Layout",
                    "10.3\u00b10.1",
                    "48.7",
                    "46.1"
                ],
                [
                    "64x64",
                    "",
                    "6.7\u00b10.1",
                    "103.4",
                    "28.8"
                ],
                [
                    "[EMPTY]",
                    "Ours",
                    "7.9\u00b10.2",
                    "65.3",
                    "43.3"
                ],
                [
                    "128x128",
                    "Real Images",
                    "24.2\u00b1 0.9",
                    "0",
                    "59.3"
                ],
                [
                    "128x128",
                    "Ours GT Layout",
                    "12.5\u00b10.3",
                    "59.5",
                    "44.6"
                ],
                [
                    "128x128",
                    "Ours",
                    "10.4\u00b10.4",
                    "75.4",
                    "42.8"
                ],
                [
                    "256x256",
                    "Real Images",
                    "30.7\u00b11.2",
                    "0",
                    "62.4"
                ],
                [
                    "256x256",
                    "Ours GT Layout",
                    "16.4\u00b10.7",
                    "65.2",
                    "45.3"
                ],
                [
                    "256x256",
                    "Ours",
                    "14.5\u00b10.7",
                    "81.0",
                    "42.2"
                ]
            ],
            "title": "Table 1: A quantitative comparison using various image generation scores. In order to support a fair comparison, our model does not use location attributes and employs random appearance attributes."
        },
        "insight": "Tab. 1 compares our method with the baselines and the real test images using the inception, FID, and classification accuracy scores. We make sure not to use information that the baseline method of  is not using and use zero location attributes and appearance attributes that are randomly sam [CONTINUE] pled (see 3.2).  employs bounding boxes and not masks. However, we follow the same comparison (to masked based methods) given in their paper."
    },
    {
        "id": "601",
        "table": {
            "header": [
                "Res",
                "Method",
                "Diversity"
            ],
            "rows": [
                [
                    "64x64",
                    "Johnson ",
                    "0.15\u00b10.08"
                ],
                [
                    "[EMPTY]",
                    "Zhao  GT layout",
                    "0.15\u00b10.06"
                ],
                [
                    "[EMPTY]",
                    "Ours fixed appearance attributes",
                    "0.23\u00b10.01"
                ],
                [
                    "[EMPTY]",
                    "and zeroed location attributes",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Ours zeroed location attributes",
                    "0.35\u00b10.01"
                ],
                [
                    "[EMPTY]",
                    "Ours fixed appearance attributes",
                    "0.37\u00b10.01"
                ],
                [
                    "[EMPTY]",
                    "Our full method",
                    "0.43\u00b10.07"
                ],
                [
                    "256x256",
                    "Ours fixed appearance attributes",
                    "0.48\u00b10.09"
                ],
                [
                    "[EMPTY]",
                    "and zeroed location attributes",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Ours zeroed location attributes",
                    "0.61\u00b10.07"
                ],
                [
                    "[EMPTY]",
                    "Ours fixed appearance attributes",
                    "0.62\u00b10.05"
                ],
                [
                    "[EMPTY]",
                    "Our full method",
                    "0.67\u00b10.05"
                ]
            ],
            "title": "Table 2: The diversity score of\u00a0[27]. The results of\u00a0[9] are computed by us and are considerably higher than those reported for the same method by\u00a0[28]. The results of\u00a0[28] are from their paper."
        },
        "insight": "Tab. 2 reports the diversity of our method in comparison to the two baseline methods. The source of stochasticity we employ (the random vector zi used in Eq. 2) produces a higher diversity than the two baseline methods (which also include a random element), even when not changing the location vector l1 or appearance attributes ai. Varying either one of these factors adds a sizable amount of diversity. In the experiments of the table, the location attributes, when varied, are sampled using per-class Gaussian distribution that fit to the location vectors of the training set images."
    },
    {
        "id": "602",
        "table": {
            "header": [
                "[EMPTY]",
                "IoU",
                "R@0.5",
                "R@0.3"
            ],
            "rows": [
                [
                    "Johnson ",
                    "0.37",
                    "0.32",
                    "0.52"
                ],
                [
                    "Ours (w/o location attributes)",
                    "0.41",
                    "0.37",
                    "0.62"
                ],
                [
                    "Ours (w/ location attributes)",
                    "0.61",
                    "0.66",
                    "0.86"
                ]
            ],
            "title": "Table 3. Comparison of predicted bounding boxes"
        },
        "insight": "Tab. 3 presents a comparison with the method of , regarding the placement accuracy of the bounding boxes. Even when not using the location attribute vectors li, our bounding box placement better matches the test images. As expected, adding the location vectors improves the results."
    },
    {
        "id": "603",
        "table": {
            "header": [
                "User Study",
                "",
                "Ours"
            ],
            "rows": [
                [
                    "More realistic output",
                    "16.7%",
                    "83.3%"
                ],
                [
                    "Better adherence to scene graph",
                    "19.3%",
                    "80.7%"
                ],
                [
                    "Ratio of observed objects",
                    "27.31%",
                    "45.38%"
                ],
                [
                    "among all COCO objects",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Ratio of observed objects",
                    "46.49%",
                    "65.23%"
                ],
                [
                    "among all COCO stuff",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 4. User study results"
        },
        "insight": "User study Following , we perform a user study to compare with the baseline method the realism of the generated image, the adherence to the scene graph, as well as to verify that the objects in the scene graph appear in the output image. The user study involved n = 20 computer graphics and computer vision students. Each student was shown the output images for 30 random test scene-graphs from the COCO-stuff dataset and was asked to select the preferable method, according to two criteria: \"which image is more realistic\" and \"which image better reflects the scene graph\". In addition, the list of objects in the scene graph was presented, and the users were asked to count the number of objects that appear in each of the images. The two images, one for the method of  and one for our method, were presented in a random order. To allow for a fair comparison, the appearance archetypes were selected at random, the location vectors were set to zero for all objects, and we have used images from our 64 \u00d7 64 resolution model. The results, listed in Tab. 4, show that our method significantly outperforms the baseline method in all aspects tested."
    },
    {
        "id": "604",
        "table": {
            "header": [
                "Model",
                "Inception",
                "FID"
            ],
            "rows": [
                [
                    "Full method",
                    "10.4\u00b10.4",
                    "75.4"
                ],
                [
                    "No Lperceptual",
                    "6.2\u00b10.1",
                    "125.1"
                ],
                [
                    "No LD-mask",
                    "5.2\u00b10.1",
                    "183.6"
                ],
                [
                    "No LD-image",
                    "7.4\u00b10.2",
                    "122.5"
                ],
                [
                    "No LD-object",
                    "8.7\u00b10.1",
                    "94.5"
                ],
                [
                    "Using  [ITALIC] Dimage of ",
                    "8.1\u00b10.3",
                    "114.2"
                ]
            ],
            "title": "Table 5: Ablation Study"
        },
        "insight": "Ablation analysis The relative importance of the various losses is approximated, by removing it from the method and training the 128x128 model. For this study, we use both the inception and the FID scores. The results are reported in Tab. 5. As can be seen, removing each of the losses results in a noticeable degradation. Removing the perceptual loss is extremely detrimental. Out of the three discriminators, removing the mask discriminator is the most damaging, since, due to the random component zi, we do not have a direct loss on the mask. Finally, replacing our image discriminator with the one in , results in some loss of accuracy."
    },
    {
        "id": "605",
        "table": {
            "header": [
                "Hyperparameter",
                "Lower bound",
                "Upper bound",
                "Step size"
            ],
            "rows": [
                [
                    "#Layers",
                    "1",
                    "6",
                    "1"
                ],
                [
                    "#Neurons per layer",
                    "50",
                    "600",
                    "25"
                ],
                [
                    "DR method",
                    "1",
                    "11",
                    "1"
                ],
                [
                    "DR ratio",
                    "1",
                    "20",
                    "0.1"
                ],
                [
                    "Quantization",
                    "bin 1",
                    "bin 4",
                    "1"
                ]
            ],
            "title": "TABLE II: General search space for FFNNs"
        },
        "insight": "Table 2 shows the hyperparameters we considered for FFNNs. For each hyperparameter, we show the lower bound, upper bound, and step size. For the number of layers, we considered one to six hidden layers in the architecture search space. with a step size of one."
    },
    {
        "id": "606",
        "table": {
            "header": [
                "Dataset",
                "SenDrive",
                "HAR",
                "Musk",
                "Pendigits",
                "SatIm",
                "Letter",
                "Seizure",
                "SHAR",
                "DNA"
            ],
            "rows": [
                [
                    "Baseline",
                    "93.53%",
                    "95.01%",
                    "98.68%",
                    "97.22%",
                    "91.30%",
                    "95.24%",
                    "87.53%",
                    "90.66%",
                    "94.86%"
                ],
                [
                    "SCANN [hassantabar2019scann]",
                    "97.10%",
                    "95.52%",
                    "99.09%",
                    "97.22%",
                    "90.10%",
                    "92.60%",
                    "96.96%",
                    "93.78%",
                    "95.86%"
                ],
                [
                    "DR + SCANN [hassantabar2019scann]",
                    "99.34%",
                    "95.28%",
                    "98.08%",
                    "97.93%",
                    "89.40%",
                    "92.70%",
                    "97.62%",
                    "94.84%",
                    "93.76%"
                ],
                [
                    "STEERAGE (GS)",
                    "99.07%",
                    "95.90%",
                    "98.90%",
                    "97.30%",
                    "90.35%",
                    "[BOLD] 97.20%",
                    "95.50%",
                    "94.86%",
                    "95.56%"
                ],
                [
                    "STEERAGE (GS+LS)",
                    "[BOLD] 99.36%",
                    "[BOLD] 96.43%",
                    "[BOLD] 99.19%",
                    "[BOLD] 98.05%",
                    "[BOLD] 92.00%",
                    "95.10%",
                    "[BOLD] 97.72%",
                    "[BOLD] 95.50%",
                    "[BOLD] 95.95%"
                ]
            ],
            "title": "TABLE III: Highest test accuracy comparison on small- to medium-size datasets (the highest number is highlighted)"
        },
        "insight": "Table 3 shows the test accuracy results. There are two rows associated with STEERAGE. The first one shows the result obtained by just using the global search module, and the second one when both the global and local search modules are used."
    },
    {
        "id": "607",
        "table": {
            "header": [
                "Method",
                "Error rate (%)",
                "Weights"
            ],
            "rows": [
                [
                    "Baseline",
                    "0.80",
                    "62.0k"
                ],
                [
                    "Network pruning [han2015learning]",
                    "0.77",
                    "34.5k"
                ],
                [
                    "NeST [dai2017nest]",
                    "0.77%",
                    "5.8k"
                ],
                [
                    "SCANN Scheme A[hassantabar2019scann]",
                    "0.68%",
                    "186.4k"
                ],
                [
                    "SCANN Scheme C[hassantabar2019scann]",
                    "0.72%",
                    "9.3k"
                ],
                [
                    "STEERAGE (GS) (14\u00d714)",
                    "4.96%",
                    "38.9k"
                ],
                [
                    "STEERAGE (GS+LS) (14\u00d714)",
                    "1.2%",
                    "5.0k"
                ],
                [
                    "STEERAGE (GS) (21\u00d721)",
                    "1.9%",
                    "207.2k"
                ],
                [
                    "[BOLD] STEERAGE (GS+LS) (21\u00d721)",
                    "[BOLD] 0.72%",
                    "[BOLD] 5.2k"
                ],
                [
                    "[BOLD] STEERAGE (GS+LS) (21\u00d721)",
                    "[BOLD] 0.66%",
                    "[BOLD] 7.2k"
                ],
                [
                    "STEERAGE (GS) (28\u00d728)",
                    "0.86%",
                    "131.1k"
                ],
                [
                    "STEERAGE (GS+LS) (28\u00d728)",
                    "0.68%",
                    "9.9k"
                ]
            ],
            "title": "TABLE VII: Comparison of results for LeNet-5 (the size of the image is shown in parentheses for STEERAGE)"
        },
        "insight": "Table 7 presents the results. The most accurate architecture based on the LeNet-5 model has an error rate of 0.66%. It is based on using both global and local search modules and corresponds to images of size 21 \u00d7 21. This model only has 7.2k parameters. Moreover, another model based on global+local search only has 5.2k parameters, with an error rate of 0.72%. These two results are highlighted in bold."
    },
    {
        "id": "608",
        "table": {
            "header": [
                "[EMPTY]",
                "Mean",
                "Median"
            ],
            "rows": [
                [
                    "SVR-DQN",
                    "[BOLD] 139.75%",
                    "[BOLD] 118.02%"
                ],
                [
                    "Double DQN",
                    "92.48%",
                    "63.13%"
                ]
            ],
            "title": "Table 1: Mean and median normalized scores."
        },
        "insight": "Table 1: Mean and median normalized scores. [CONTINUE] We also give the summary statistics in terms of mean and median score in Table 1. Compared to Adam, the median performance across 20 games increases from 63.13% to 118.02% and the mean performance increases from 92.48% to 139.75%. Noteworthy examples include Seaquest (from 27.42% to 267.94%), Gopher (from 53.22% to 145.42%)."
    },
    {
        "id": "609",
        "table": {
            "header": [
                "G",
                "Training Loss",
                "Evaluation Metric  [ITALIC] L1",
                "Evaluation Metric  [ITALIC] L2",
                "Evaluation Metric Percept.",
                "Evaluation Metric SSIM",
                "Evaluation Metric MS-"
            ],
            "rows": [
                [
                    "G",
                    "Training Loss",
                    "[ITALIC] L1",
                    "[ITALIC] L2",
                    "Percept.",
                    "SSIM",
                    "SSIM"
                ],
                [
                    "none",
                    "[ITALIC] L1",
                    ".0452",
                    ".0067",
                    ".2564",
                    ".1707",
                    ".1144"
                ],
                [
                    "none",
                    "[ITALIC] L2",
                    ".0516",
                    ".0082",
                    ".2663",
                    ".1911",
                    ".1369"
                ],
                [
                    "none",
                    "Percept.",
                    ".0424",
                    ".0062",
                    "[BOLD] .1868",
                    ".1440",
                    ".0992"
                ],
                [
                    "none",
                    "SSIM",
                    "[BOLD] .0406",
                    "[BOLD] .0055",
                    ".2138",
                    "[BOLD] .1378",
                    ".0930"
                ],
                [
                    "none",
                    "MS-SSIM",
                    ".0422",
                    ".0058",
                    ".2358",
                    ".1547",
                    "[BOLD] .0913"
                ],
                [
                    "partial",
                    "[ITALIC] L1",
                    ".0366",
                    ".0050",
                    ".2312",
                    ".1409",
                    ".0861"
                ],
                [
                    "partial",
                    "[ITALIC] L2",
                    ".0410",
                    ".0056",
                    ".2641",
                    ".1628",
                    ".0968"
                ],
                [
                    "partial",
                    "Percept.",
                    ".0363",
                    ".0047",
                    "[BOLD] .1658",
                    ".1282",
                    ".0785"
                ],
                [
                    "partial",
                    "SSIM",
                    "[BOLD] .0336",
                    "[BOLD] .0040",
                    ".1962",
                    "[BOLD] .1159",
                    "[BOLD] .0689"
                ],
                [
                    "partial",
                    "MS-SSIM",
                    ".0366",
                    ".0044",
                    ".2257",
                    ".1399",
                    ".0758"
                ],
                [
                    "full",
                    "[ITALIC] L1",
                    ".0406",
                    ".0055",
                    ".2237",
                    ".1484",
                    ".0913"
                ],
                [
                    "full",
                    "[ITALIC] L2",
                    ".0415",
                    ".0056",
                    ".2302",
                    ".1547",
                    ".0953"
                ],
                [
                    "full",
                    "Percept.",
                    ".0365",
                    ".0048",
                    "[BOLD] .1701",
                    ".1308",
                    ".0803"
                ],
                [
                    "full",
                    "SSIM",
                    "[BOLD] .0362",
                    "[BOLD] .0045",
                    ".2008",
                    "[BOLD] .1270",
                    "[BOLD] .0793"
                ],
                [
                    "full",
                    "MS-SSIM",
                    ".0410",
                    ".0055",
                    ".2165",
                    ".1470",
                    ".0910"
                ]
            ],
            "title": "Table 1: Loss Selection. We explore the influence of different training losses and evaluation metrics on 3 models with different degrees of structured guidance. For each class, we show validation scores for all pairwise combinations of 5 training losses (rows) and the same 5 evaluation metrics (columns). The best model for each evaluation metric is shown in bold. SSIM/MS-SSIM are expressed as dissimilarities\u00a0(Eq. 7)."
        },
        "insight": "Table 1: Loss Selection. We explore the influence of different training losses and evaluation metrics on 3 models with different degrees of structured guidance. For each class, we show validation scores for all pairwise combinations of 5 training losses (rows) and the same 5 evaluation metrics (columns). The best model for each evaluation metric is shown in bold. SSIM/MS-SSIM are expressed as dissimilarities (Eq. (7))."
    },
    {
        "id": "610",
        "table": {
            "header": [
                "L",
                "Model/ Guidance",
                "Evaluation Metric  [ITALIC] L1",
                "Evaluation Metric  [ITALIC] L2",
                "Evaluation Metric Percept.",
                "Evaluation Metric SSIM",
                "Evaluation Metric MS-"
            ],
            "rows": [
                [
                    "L",
                    "Model/ Guidance",
                    "[ITALIC] L1",
                    "[ITALIC] L2",
                    "Percept.",
                    "SSIM",
                    "SSIM"
                ],
                [
                    "[EMPTY]",
                    "PMS",
                    ".0391",
                    ".0047",
                    ".1630",
                    ".1125",
                    ".0561"
                ],
                [
                    "with",
                    "SfSNet (R)",
                    ".0636",
                    ".0121",
                    ".2508",
                    ".1840",
                    ".1277"
                ],
                [
                    "with",
                    "no guidance (P2P)",
                    ".0668",
                    ".0144",
                    ".2430",
                    ".1832",
                    ".1328"
                ],
                [
                    "with",
                    "partial guidance",
                    "[BOLD] .0590",
                    "[BOLD] .0118",
                    ".2195",
                    "[BOLD] .1609",
                    "[BOLD] .1111"
                ],
                [
                    "with",
                    "full guidance",
                    ".0609",
                    ".0123",
                    "[BOLD] .2144",
                    ".1618",
                    ".1138"
                ],
                [
                    "w/o",
                    "SfSNet (P)",
                    ".1359",
                    ".0424",
                    ".4703",
                    ".3221",
                    ".3121"
                ],
                [
                    "w/o",
                    "no guidance (P2P)",
                    ".0815",
                    ".0189",
                    ".2783",
                    ".2076",
                    ".1623"
                ],
                [
                    "w/o",
                    "partial guidance",
                    ".0695",
                    ".0144",
                    "[BOLD] .2325",
                    ".1801",
                    ".1353"
                ],
                [
                    "w/o",
                    "full guidance",
                    "[BOLD] .0684",
                    "[BOLD] .0142",
                    ".2273",
                    "[BOLD] .1763",
                    "[BOLD] .1316"
                ]
            ],
            "title": "Table 2: Baseline Comparison. We show a quantitative comparison of our approach to baseline methods. Performance is reported under the assumption of both known (\u2018with\u2019) and unknown (\u2018w/o\u2019) source illumination. All models have been trained with the SSIM loss."
        },
        "insight": "Table 2: Baseline Comparison. We show a quantitative comparison of our approach to baseline methods. Performance is reported under the assumption of both known ('with') and unknown ('w/o') source illumination. All models have been trained with the SSIM loss."
    },
    {
        "id": "611",
        "table": {
            "header": [
                "[BOLD] KSC update (KSC initialisation) Strategies",
                "[BOLD] KSC update (KSC initialisation)  [ITALIC] K=2",
                "[BOLD] KSC update (KSC initialisation)  [ITALIC] K=3",
                "[BOLD] KSC update (KSC initialisation)  [ITALIC] K=5",
                "[BOLD] KSC update (KSC initialisation)  [ITALIC] K=8",
                "[BOLD] KSC update (KSC initialisation)  [ITALIC] K=10"
            ],
            "rows": [
                [
                    "[ITALIC] SCAL",
                    "[BOLD] 21.09%",
                    "[BOLD] 19.79%",
                    "[BOLD] 24.06%",
                    "[BOLD] 63.48%",
                    "[BOLD] 53.91%"
                ],
                [
                    "[ITALIC] SCAL",
                    "[BOLD] 96.10%",
                    "[BOLD] 98.28%",
                    "[BOLD] 93.80%",
                    "80.02%",
                    "86.45%"
                ],
                [
                    "[ITALIC] MinMargin",
                    "64.84%",
                    "36.46%",
                    "83.13%",
                    "98.44%",
                    "99.84%"
                ],
                [
                    "[ITALIC] MinMargin",
                    "90.30%",
                    "97.31%",
                    "91.83%",
                    "[BOLD] 81.92%",
                    "[BOLD] 88.95%"
                ],
                [
                    "[ITALIC] MaxResid",
                    "96.88%",
                    "95.31%",
                    "99.69%",
                    "99.81%",
                    "99.84%"
                ],
                [
                    "[ITALIC] MaxResid",
                    "77.19%",
                    "85.85%",
                    "77.43%",
                    "79.01%",
                    "82.22%"
                ],
                [
                    "[ITALIC] Random",
                    "97.66%",
                    "96.35%",
                    "99.69%",
                    "97.85%",
                    "97.97%"
                ],
                [
                    "[ITALIC] Random",
                    "77.59%",
                    "88.08%",
                    "76.27%",
                    "77.77%",
                    "83.46%"
                ]
            ],
            "title": "TABLE III: Performance on Yale Faces data sets."
        },
        "insight": "From the results shown in Table III, we see that the percentage of data needed goes up as K increases. [CONTINUE] The performance results are shown in the last section of Table III."
    },
    {
        "id": "612",
        "table": {
            "header": [
                "training set test set",
                "CUB",
                "mini-ImageNet",
                "mini-ImageNet CUB",
                "CUB mini-ImageNet"
            ],
            "rows": [
                [
                    "MatchingNet",
                    "83.75 \u00b1 0.60",
                    "69.14 \u00b1 0.69",
                    "52.59 \u00b1 0.71",
                    "48.95 \u00b1 0.67"
                ],
                [
                    "ProtoNet",
                    "85.70 \u00b1 0.52",
                    "73.77 \u00b1 0.64",
                    "59.22 \u00b1 0.74",
                    "53.58 \u00b1 0.73"
                ],
                [
                    "RelationNet",
                    "82.67 \u00b1 0.61",
                    "69.97 \u00b1 0.68",
                    "54.36 \u00b1 0.71",
                    "45.27 \u00b1 0.66"
                ],
                [
                    "[BOLD] SubspaceNet (ours)",
                    "[BOLD] 87.45 \u00b1 0.48",
                    "[BOLD] 74.03 \u00b1 0.68",
                    "[BOLD] 62.71 \u00b1 0.71",
                    "[BOLD] 56.66 \u00b1 0.68"
                ]
            ],
            "title": "Table 1: Identical domain and domain shift accuracies for 5-way 5-shot classification using a ResNet-10 backbone and mini-ImageNet and CUB datasets. The best-performing method is highlighted."
        },
        "insight": "(See Table 1 for detailed statistics and 95% confidence intervals.) [CONTINUE] Table 1 and Figure 3 present our results in the context of existing state-of-the-art distance-metric learning based methods. [CONTINUE] The intra-domain difference between classes is higher in mini-ImageNet than in CUB (Chen et al. ), which is reflected by a drop of the test accuracies of all methods. It can also be seen that when the domain difference between the training and test stage classes increases (left to right in Figure 3) the performance of all few-shot algorithms lowers. [CONTINUE] As expected, the classification accuracy lowers slightly when going from the mini-ImageNet \u2192 CUB to the CUB \u2192 mini-ImageNet setting. Interestingly though, it does not decrease substantially. This shows that the different feature embeddings generalize well, at least across these two different datasets. Compared to other distance-metric learning based methods, subspace networks show significantly better performance in both the mini-ImageNet \u2192 CUB setting and CUB \u2192 mini-ImageNet setting."
    },
    {
        "id": "613",
        "table": {
            "header": [
                "Metric",
                "Model",
                "Dish washer",
                "Fridge",
                "Microwave",
                "Washing machine",
                "Average",
                "Average improvement"
            ],
            "rows": [
                [
                    "MAE",
                    "FHMM ",
                    "101.30",
                    "98.67",
                    "87.00",
                    "66.76",
                    "88.43",
                    "-"
                ],
                [
                    "MAE",
                    "DAE ",
                    "29.38",
                    "76.62",
                    "21.31",
                    "31.35",
                    "39.66",
                    "-"
                ],
                [
                    "MAE",
                    "Seq2Seq ",
                    "27.07",
                    "26.03",
                    "16.57",
                    "22.72",
                    "23.10",
                    "0.00 %"
                ],
                [
                    "MAE",
                    "SGN",
                    "[BOLD] 14.97",
                    "23.89",
                    "17.52",
                    "20.07",
                    "19.09",
                    "17.34 %"
                ],
                [
                    "MAE",
                    "SGN-sp",
                    "15.96",
                    "22.89",
                    "[BOLD] 15.98",
                    "20.61",
                    "[BOLD] 18.86",
                    "18.36 %"
                ],
                [
                    "MAE",
                    "Hard SGN",
                    "21.27",
                    "24.45",
                    "17.38",
                    "[BOLD] 18.24",
                    "20.33",
                    "11.97 %"
                ],
                [
                    "MAE",
                    "Hard SGN-sp",
                    "24.29",
                    "[BOLD] 22.86",
                    "17.16",
                    "21.94",
                    "21.56",
                    "6.67 %"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "FHMM",
                    "93.64",
                    "46.73",
                    "65.03",
                    "58.77",
                    "66.04",
                    "-"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "DAE",
                    "29.21",
                    "20.48",
                    "17.86",
                    "27.64",
                    "23.80",
                    "-"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "Seq2Seq",
                    "26.93",
                    "11.67",
                    "11.43",
                    "16.82",
                    "16.71",
                    "0.00 %"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "SGN",
                    "[BOLD] 11.74",
                    "[BOLD] 10.62",
                    "14.84",
                    "10.70",
                    "[BOLD] 11.97",
                    "28.34 %"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "SGN-sp",
                    "12.07",
                    "12.26",
                    "[BOLD] 11.31",
                    "13.28",
                    "12.23",
                    "26.81 %"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "Hard SGN",
                    "20.72",
                    "14.51",
                    "16.53",
                    "[BOLD] 8.47",
                    "12.40",
                    "25.77 %"
                ],
                [
                    "[ITALIC] SAE\u03b4",
                    "Hard SGN-sp",
                    "27.63",
                    "10.99",
                    "15.52",
                    "15.66",
                    "13.31",
                    "20.37 %"
                ]
            ],
            "title": "Table 2: Experiment results for REDD data."
        },
        "insight": "Table 2 and 3 show the performance of the previous works and SGN for the REDD and UK-DALE dataset. [CONTINUE] In the two tables, the bold font denotes the best performing algorithm for each appliance, and the shade denotes that SGN has outperformed the best of the previous works."
    },
    {
        "id": "614",
        "table": {
            "header": [
                "Grid Size",
                "F-score (%)"
            ],
            "rows": [
                [
                    "1\u00d71",
                    "67.3"
                ],
                [
                    "8\u00d78",
                    "70.9"
                ],
                [
                    "15\u00d715",
                    "71.7"
                ],
                [
                    "20\u00d720",
                    "70.4"
                ]
            ],
            "title": "Table 1: Results achieved by our SummaryNet model for different optical flow grid sizes. We choose to use a grid size of 15\u00d715 for the remaining experiments in this paper. The values in this table correspond to the TvSum50 dataset."
        },
        "insight": "From Table 1, we see that grid size in the dense optical flow computation has a notable effect on accuracy. There is a trade-off between having too small a grid size (which would result in too much noise in the optical flow videos), and too large a grid size (which would result in not enough signal being present in the optical flow videos). We choose the grid size value that results in the best performance (i.e. 15 \u00d7 15) for all subsequent experiments."
    },
    {
        "id": "615",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Features",
                "[BOLD] RGB",
                "[BOLD] Opt. Flow",
                "[BOLD] RGB+Opt. Flow"
            ],
            "rows": [
                [
                    "Baseline",
                    "ResNet",
                    "-",
                    "-",
                    "50.48"
                ],
                [
                    "Baseline",
                    "I3D",
                    "67.30",
                    "67.28",
                    "68.64"
                ],
                [
                    "ConvNet",
                    "I3D",
                    "70.00",
                    "67.40",
                    "68.40"
                ],
                [
                    "ConvLSTM",
                    "I3D",
                    "70.18",
                    "70.44",
                    "69.10"
                ],
                [
                    "SummaryNet",
                    "I3D",
                    "70.06",
                    "71.70",
                    "[BOLD] 72.02"
                ]
            ],
            "title": "Table 2: Comparison of video summarisation methods using F-score (%) on TvSum50 dataset. These results clearly show the advantage of using spatio-temporal features."
        },
        "insight": "In Table 2, we see the results of baseline-ResNet is more than 3% lower than the results obtained by  who used GoogLeNet features with a similar model architecture. In this table we also clearly see the advantage of using spatiotemporal features. [CONTINUE] Since we are using a two-stream network, we get two separate predictions for a video. The results are combined and the average is taken as the final prediction. These scores are represented by the last column in the table. [CONTINUE] Our proposed method, SummaryNet is shown in the last row of Table 2. [CONTINUE] In Table 2, we see that I3D-based features serve as a better, more rich representation of the videos than ResNet features. This is evident from the respective accuracies of both approaches\u2013with the baseline I3D approach achieving an accuracy of 68.64%, compared to the 50.48% from the ResNet baseline approach. [CONTINUE] This large difference can be attributed to the fact that the ResNet features only incorporate 2D spatial information into the representation, whereas the I3D features incorporate 3D spatial information (through the RGB stream), as well as explicitly modelling temporal information (through the optical flow stream). This shows that separately modelling spatial / appearance and temporal / motion information in videos is useful. [CONTINUE] We use a simple averaging approach whereby the RGB and optical flow outputs of the sigmoid regression network are simply averaged. This simple scheme is likely a suboptimal way to fuse the information from the two streams and more complex information fusion methods can likely yield higher performance. The ConvLSTM I3D model improves upon the previous, standard ConvNet model on all metrics (RGB, optical flow, and combined). [CONTINUE] We attribute this performance increase to the encoder-decoder model, which is able to effectively capture the most salient information from the I3D representations, and remove any noise from the core signal that could potentially be detrimental in the sigmoid network's learning process. [CONTINUE] Thus, the change points supplied by using the I3D features clearly demonstrate its superiority over using simple 2D spatial features, such as those from ResNet."
    },
    {
        "id": "616",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Features",
                "[BOLD] RGB",
                "[BOLD] Opt. Flow",
                "[BOLD] RGB+Opt. Flow"
            ],
            "rows": [
                [
                    "Baseline",
                    "ResNet",
                    "-",
                    "-",
                    "38.06"
                ],
                [
                    "Baseline",
                    "I3D",
                    "33.86",
                    "41.45",
                    "40.64"
                ],
                [
                    "ConvNet",
                    "I3D",
                    "33.90",
                    "41.77",
                    "42.85"
                ],
                [
                    "ConvLSTM",
                    "I3D",
                    "33.88",
                    "42.58",
                    "43.75"
                ],
                [
                    "SummaryNet",
                    "I3D",
                    "35.83",
                    "43.07",
                    "[BOLD] 44.60"
                ]
            ],
            "title": "Table 3: Comparison of video summarisation methods using F-score (%) on SumMe dataset. These results clearly show the advantage of using spatio-temporal features."
        },
        "insight": "In Table 3, we see a similar trend to that of Table 2. The baseline I3D model outperforms the baseline ResNet model, due to the incorporation of 3D spatial and temporal information into the representations. The trends follows that of the TvSum50 dataset for the other I3D-based models as well, with the combined SummaryNet model performing best."
    },
    {
        "id": "617",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] SumMe",
                "[BOLD] TvSum50"
            ],
            "rows": [
                [
                    "",
                    "39.7",
                    "-"
                ],
                [
                    "",
                    "-",
                    "51.3"
                ],
                [
                    "",
                    "38.6",
                    "54.7"
                ],
                [
                    "",
                    "41.7",
                    "56.3"
                ],
                [
                    "",
                    "42.1",
                    "58.1"
                ],
                [
                    "",
                    "[BOLD] 47.5",
                    "56.8"
                ],
                [
                    "",
                    "-",
                    "57"
                ],
                [
                    "",
                    "44.4",
                    "[BOLD] 61"
                ],
                [
                    "",
                    "40.1",
                    "56.3"
                ],
                [
                    "",
                    "41.9",
                    "57.6"
                ],
                [
                    "",
                    "[BOLD] 51.3",
                    "58.8"
                ],
                [
                    "SummaryNet (ours)",
                    "[BOLD] 44.6",
                    "[BOLD] 72.02"
                ]
            ],
            "title": "Table 4: Comparison of video summarisation methods using F-score (%). The best results are shown in dark blue and second best is shown in light blue."
        },
        "insight": "In Table 4, we compare video summarisation methods. All the methods shown in this table use deep learning, often leveraging deep spatial features (e.g. GoogLeNet features). For the TvSum50 dataset, SummaryNet outperforms the state-of-the-art  by more than 11%. This significant increase in accuracy is attractive since it demonstrates the power of SummaryNet. For the SumMe dataset we achieve competitive results, where the state-of-the-art  is around 6.7% better than SummaryNet. However, SummaryNet outperforms  on TvSum50 dataset by a large margin of more than 13%. [CONTINUE] Overall, SummaryNet achieves the state-of-the-art performance on TvSum50 dataset, and is competitive with state-of-the-art for the SumMe dataset."
    },
    {
        "id": "618",
        "table": {
            "header": [
                "Model",
                "Y RMSE @ 10 s (m)",
                "Velocity RMSE @ 10 s (m/s)"
            ],
            "rows": [
                [
                    "Fully-connected (FC)",
                    "2.89",
                    "0.526"
                ],
                [
                    "GCN base",
                    "3.52",
                    "0.622"
                ],
                [
                    "GAT",
                    "4.13",
                    "0.688"
                ],
                [
                    "EGCN",
                    "[BOLD] 1.40",
                    "[BOLD] 0.258"
                ],
                [
                    "DGCN",
                    "1.91",
                    "0.360"
                ],
                [
                    "LSTM",
                    "[BOLD] 1.61",
                    "0.331"
                ],
                [
                    "GCN with LSTM",
                    "3.40",
                    "0.653"
                ],
                [
                    "GAT with LSTM",
                    "4.09",
                    "0.728"
                ],
                [
                    "EGCN with LSTM",
                    "1.86",
                    "0.321"
                ],
                [
                    "DGCN with LSTM",
                    "1.63",
                    "[BOLD] 0.256"
                ]
            ],
            "title": "TABLE II: RMSE Analysis"
        },
        "insight": "The Y -coordinate RMSE column in Table II denotes the displacement in Y -coordinate between simulated trajectories and their corresponding true trajectories."
    },
    {
        "id": "619",
        "table": {
            "header": [
                "Model",
                "Jerk Sign Inversions",
                "Negative Headway Occurrence Rate"
            ],
            "rows": [
                [
                    "Fully-connected (FC)",
                    "7.5",
                    "0.08"
                ],
                [
                    "GCN base",
                    "7.5",
                    "0.17"
                ],
                [
                    "GAT",
                    "[BOLD] 5.9",
                    "0.27"
                ],
                [
                    "EGCN",
                    "7.5",
                    "[BOLD] 0"
                ],
                [
                    "DGCN",
                    "7.3",
                    "0.03"
                ],
                [
                    "LSTM",
                    "13.7",
                    "0.02"
                ],
                [
                    "GCN with LSTM",
                    "[BOLD] 6.7",
                    "0.17"
                ],
                [
                    "GAT with LSTM",
                    "0.0",
                    "0.27"
                ],
                [
                    "EGCN with LSTM",
                    "9.5",
                    "0.01"
                ],
                [
                    "DGCN with LSTM",
                    "7.3",
                    "[BOLD] 0"
                ],
                [
                    "True trajectory",
                    "6.3",
                    "/"
                ]
            ],
            "title": "TABLE III: Jerk Sign Inversions Per Trajectory"
        },
        "insight": "Table III shows the number of negative headway occurrences over number of simulated trajectories for all models. [CONTINUE] Consistent with RMSE analysis, the results [CONTINUE] from Table III demonstrates that original GCN models often produce poor acceleration predictions which lead to unrealistic states."
    },
    {
        "id": "620",
        "table": {
            "header": [
                "Category CD\u2193",
                "Category 3D-R2N2 ",
                "table 1.116",
                "car 0.845",
                "chair 1.432",
                "plane 0.895",
                "couch 1.135",
                "firearm 0.993",
                "lamp 4.009",
                "watercraft 1.215",
                "bench 1.891",
                "speaker 1.507",
                "cabinet 0.735",
                "monitor 1.707",
                "cellphone 1.137",
                "mean 1.445"
            ],
            "rows": [
                [
                    "CD\u2193",
                    "PSG ",
                    "0.517",
                    "0.333",
                    "0.645",
                    "0.430",
                    "0.549",
                    "0.423",
                    "1.193",
                    "0.633",
                    "0.629",
                    "0.756",
                    "0.439",
                    "0.722",
                    "0.438",
                    "0.593"
                ],
                [
                    "CD\u2193",
                    "Pixel2mesh ",
                    "0.498",
                    "0.268",
                    "0.610",
                    "0.477",
                    "0.490",
                    "0.453",
                    "1.295",
                    "0.670",
                    "0.624",
                    "0.739",
                    "0.381",
                    "0.755",
                    "0.421",
                    "0.591"
                ],
                [
                    "CD\u2193",
                    "Ours (FC)",
                    "0.314",
                    "0.220",
                    "0.333",
                    "0.127",
                    "0.289",
                    "0.128",
                    "0.560",
                    "0.30",
                    "0.211",
                    "0.471",
                    "0.310",
                    "0.275",
                    "0.181",
                    "0.286"
                ],
                [
                    "CD\u2193",
                    "Ours (ResFC)",
                    "0.305",
                    "0.216",
                    "0.321",
                    "0.123",
                    "0.284",
                    "0.123",
                    "0.543",
                    "0.228",
                    "0.204",
                    "0.474",
                    "0.309",
                    "0.272",
                    "0.181",
                    "0.276"
                ],
                [
                    "CD\u2193",
                    "Ours (GraphX)",
                    "0.299",
                    "0.192",
                    "0.317",
                    "0.123",
                    "0.265",
                    "0.127",
                    "0.549",
                    "0.214",
                    "0.202",
                    "0.433",
                    "0.272",
                    "0.258",
                    "0.159",
                    "0.262"
                ],
                [
                    "CD\u2193",
                    "Ours (ResGraphX)",
                    "0.291",
                    "0.188",
                    "0.313",
                    "0.120",
                    "0.259",
                    "0.124",
                    "0.529",
                    "0.214",
                    "0.199",
                    "0.430",
                    "0.275",
                    "0.257",
                    "0.159",
                    "0.259"
                ],
                [
                    "CD\u2193",
                    "Ours (UpResGraphX)",
                    "[BOLD] 0.284",
                    "[BOLD] 0.184",
                    "[BOLD] 0.306",
                    "[BOLD] 0.116",
                    "[BOLD] 0.254",
                    "[BOLD] 0.119",
                    "[BOLD] 0.523",
                    "[BOLD] 0.210",
                    "[BOLD] 0.189",
                    "[BOLD] 0.419",
                    "[BOLD] 0.265",
                    "[BOLD] 0.248",
                    "[BOLD] 0.155",
                    "[BOLD] 0.252"
                ],
                [
                    "IoU\u2191",
                    "3D-R2N2 ",
                    "0.580",
                    "[BOLD] 0.836",
                    "0.550",
                    "0.561",
                    "0.706",
                    "0.600",
                    "0.421",
                    "0.610",
                    "0.527",
                    "0.717",
                    "0.772",
                    "0.565",
                    "0.754",
                    "0.631"
                ],
                [
                    "IoU\u2191",
                    "PSG ",
                    "0.606",
                    "0.831",
                    "0.544",
                    "0.601",
                    "0.708",
                    "0.604",
                    "0.462",
                    "0.611",
                    "0.550",
                    "[BOLD] 0.737",
                    "0.771",
                    "0.552",
                    "0.749",
                    "0.640"
                ],
                [
                    "IoU\u2191",
                    "GAL ",
                    "[BOLD] 0.714",
                    "0.737",
                    "0.700",
                    "0.685",
                    "0.739",
                    "0.715",
                    "[BOLD] 0.670",
                    "0.675",
                    "0.709",
                    "0.698",
                    "0.772",
                    "[BOLD] 0.804",
                    "0.773",
                    "0.712"
                ],
                [
                    "IoU\u2191",
                    "Ours (FC)",
                    "0.676",
                    "0.820",
                    "0.693",
                    "0.779",
                    "0.784",
                    "0.757",
                    "0.552",
                    "0.769",
                    "0.739",
                    "0.713",
                    "0.769",
                    "0.764",
                    "0.846",
                    "0.743"
                ],
                [
                    "IoU\u2191",
                    "Ours (ResFC)",
                    "0.688",
                    "0.821",
                    "[BOLD] 0.704",
                    "[BOLD] 0.791",
                    "0.786",
                    "[BOLD] 0.765",
                    "0.573",
                    "[BOLD] 0.772",
                    "[BOLD] 0.746",
                    "0.715",
                    "0.770",
                    "0.765",
                    "0.848",
                    "[BOLD] 0.750"
                ],
                [
                    "IoU\u2191",
                    "Ours (GraphX)",
                    "0.487",
                    "0.720",
                    "0.550",
                    "0.734",
                    "0.645",
                    "0.715",
                    "0.487",
                    "0.705",
                    "0.592",
                    "0.617",
                    "0.677",
                    "0.680",
                    "0.821",
                    "0.648"
                ],
                [
                    "IoU\u2191",
                    "Ours (ResGraphX)",
                    "0.532",
                    "0.833",
                    "0.689",
                    "0.766",
                    "[BOLD] 0.790",
                    "0.751",
                    "0.532",
                    "0.763",
                    "0.738",
                    "0.724",
                    "[BOLD] 0.781",
                    "0.757",
                    "[BOLD] 0.858",
                    "0.732"
                ],
                [
                    "IoU\u2191",
                    "Ours (UpResGraphX)",
                    "0.605",
                    "0.819",
                    "0.663",
                    "0.758",
                    "0.770",
                    "0.747",
                    "0.516",
                    "0.754",
                    "0.725",
                    "0.708",
                    "0.770",
                    "0.735",
                    "0.857",
                    "0.725"
                ]
            ],
            "title": "Table 1: Quantitative performance of different single image point cloud generation methods on 13 major categories of ShapeNet. \u201c\u2191\u201d\u00a0indicates higher is better. \u201c\u2193\u201d specifies the opposition. Best performance is highlighted in bold."
        },
        "insight": "The metric scores of PCDNet versus others are tabulated in Table 1. As anticipated, all PCDNet variants outrun all the competing methods by a huge gap. Specifically, the average CD scores from our simplest model (FC) is already twice better than the state of the art. For IoU, our method still tops the table and raises the performance bar which was previ [CONTINUE] ously set by GAL. Also, among all the variants of PCDNet, the GraphX family obtains better CD scores than the baseline whose deformation network is made of only FC layers. [CONTINUE] Still and all, the gain in CD comes at the cost of lower IoU. [CONTINUE] To our surprise, the best performance is achieved by the model using UpResGraphX."
    },
    {
        "id": "621",
        "table": {
            "header": [
                "Category CD\u2193",
                "Category Ours (projection)",
                "table 0.637",
                "car 0.284",
                "chair 0.490",
                "plane 0.177",
                "lamp 0.670",
                "mean 0.452"
            ],
            "rows": [
                [
                    "CD\u2193",
                    "Ours (AdaIN)",
                    "0.372",
                    "0.222",
                    "0.703",
                    "0.243",
                    "0.564",
                    "0.421"
                ],
                [
                    "CD\u2193",
                    "Ours (full)",
                    "0.301",
                    "0.195",
                    "0.319",
                    "0.124",
                    "0.550",
                    "0.298"
                ],
                [
                    "IoU\u2191",
                    "Ours (projection)",
                    "0.540",
                    "0.818",
                    "0.657",
                    "0.704",
                    "0.501",
                    "0.644"
                ],
                [
                    "IoU\u2191",
                    "Ours (AdaIN)",
                    "0.651",
                    "0.840",
                    "0.575",
                    "0.667",
                    "0.523",
                    "0.651"
                ],
                [
                    "IoU\u2191",
                    "Ours (full)",
                    "0.694",
                    "0.844",
                    "0.725",
                    "0.750",
                    "0.566",
                    "0.716"
                ]
            ],
            "title": "Table 2: Quantitative performance of PCDNet when different features are ablated."
        },
        "insight": "Table 2 and Figure 9 demonstrate the quantitative and qualitative results of the ablation study, respectively. [CONTINUE] As can be seen from the table, the models that incorporate only one feature (projection or AdaIN) achieves roughly the same performance. The projection feature helps PCDNet in CD score and AdaIN improves IoU. [CONTINUE] When the two are combined, both the two scores are significantly boosted, which validates our design of PCDNet."
    },
    {
        "id": "622",
        "table": {
            "header": [
                "N",
                "I",
                "EO  [ITALIC] E0",
                "EO time",
                "SA  [ITALIC] E0",
                "SA time (s)",
                "GSO ( [ITALIC] Nbs=1)  [ITALIC] E0",
                "GSO ( [ITALIC] Nbs=1) time (s)"
            ],
            "rows": [
                [
                    "256",
                    "5000",
                    "[BOLD] -0.74585(2)",
                    "\u223c268s",
                    "-0.7278(2)",
                    "1.28",
                    "-0.7270(2)",
                    "[BOLD] 0.75"
                ],
                [
                    "512",
                    "2500",
                    "[BOLD] -0.75235(3)",
                    "\u223c1.2h",
                    "-0.7327(2)",
                    "3.20",
                    "-0.7403(2)",
                    "[BOLD] 1.62"
                ],
                [
                    "1024",
                    "1250",
                    "[BOLD] \u20130.7563(2)",
                    "\u223c20h",
                    "-0.7352(2)",
                    "15.27",
                    "-0.7480(2)",
                    "[BOLD] 3.54"
                ],
                [
                    "2048",
                    "400",
                    "-",
                    "-",
                    "-0.7367(2)",
                    "63.27",
                    "[BOLD] -0.7524(1)",
                    "[BOLD] 5.63"
                ],
                [
                    "4096",
                    "200",
                    "-",
                    "-",
                    "-0.73713(6)",
                    "1591.93",
                    "[BOLD] -0.7548(2)",
                    "[BOLD] 8.38"
                ],
                [
                    "8192",
                    "100",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] -0.7566(4)",
                    "[BOLD] 26.54"
                ]
            ],
            "title": "Table 1: Results on optimization of ground state energy of SK model for different system size N and instances I. [27] only reported results of system size up to N=1024. In the implementation of simulated annealing, the program failed to finish within 96 hours for N=8192. We also present the comparison of time consumption."
        },
        "insight": "Although extremal optimization (EO) has obtained better results, this method is limited to system size up to 1024 and is extremely time-consuming."
    },
    {
        "id": "623",
        "table": {
            "header": [
                "N",
                "I",
                "GD (Adam)  [ITALIC] E0",
                "GD (Adam) time (s)",
                "GD (L-BFGS)  [ITALIC] E0",
                "GD (L-BFGS) time (s)",
                "GSO ( [ITALIC] Nbs=1)  [ITALIC] E0",
                "GSO ( [ITALIC] Nbs=1) time (s)",
                "GSO ( [ITALIC] Nbs=128)  [ITALIC] E0",
                "GSO ( [ITALIC] Nbs=128) time (s)"
            ],
            "rows": [
                [
                    "256",
                    "5000",
                    "-0.6433(3)",
                    "2.84",
                    "-0.535(2)",
                    "2.29",
                    "-0.7270(2)",
                    "0.75",
                    "[BOLD] -0.7369(1)",
                    "[BOLD] 0.69"
                ],
                [
                    "512",
                    "2500",
                    "-0.6456(3)",
                    "2.87",
                    "-0.520(3)",
                    "2.56",
                    "-0.7403(2)",
                    "1.62",
                    "[BOLD] -0.7461(2)",
                    "[BOLD] 1.61"
                ],
                [
                    "1024",
                    "1250",
                    "-0.6466(4)",
                    "3.22",
                    "-0.501(5)",
                    "[BOLD] 2.73",
                    "-0.7480(2)",
                    "3.54",
                    "[BOLD] -0.7522(1)",
                    "4.09"
                ],
                [
                    "2048",
                    "400",
                    "-0.6493(2)",
                    "3.53",
                    "-0.495(8)",
                    "[BOLD] 3.06",
                    "-0.7524(1)",
                    "5.63",
                    "[BOLD] -0.75563(5)",
                    "12.19"
                ],
                [
                    "4096",
                    "200",
                    "-0.6496(5)",
                    "4.62",
                    "-0.49(1)",
                    "[BOLD] 3.55",
                    "-0.7548(2)",
                    "8.38",
                    "[BOLD] -0.75692(2)",
                    "39.64"
                ],
                [
                    "8192",
                    "100",
                    "-0.6508(4)",
                    "16.26",
                    "-0.46(2)",
                    "[BOLD] 4.82",
                    "-0.7566(4)",
                    "26.54",
                    "[BOLD] -0.75769(2)",
                    "204.26"
                ]
            ],
            "title": "Table 2: Results on optimization of ground state energy of SK model for different system size N and instances I. We use different optimizer in gradient descent (GD). We also present the results of parallel version of our proposed method where we choose Nbs=128."
        },
        "insight": "We can optimize this objective function through gradient descent (GD) with different optimizer, e.g. Adam or L-BFGS but the results are not satisfying. [CONTINUE] are not satisfying. The reason is that there are too many frustrations and local mi With the implementation of the parallel version, the results can be improved greatly."
    },
    {
        "id": "624",
        "table": {
            "header": [
                "Graph",
                "size",
                "S2V-DQN",
                "GCNs",
                "GD (L-BFGS)",
                "Greedy",
                "GSO"
            ],
            "rows": [
                [
                    "Cora",
                    "2708",
                    "1381",
                    "[BOLD] 1451",
                    "1446",
                    "[BOLD] 1451",
                    "[BOLD] 1451"
                ],
                [
                    "Citeseer",
                    "3327",
                    "1705",
                    "[BOLD] 1867",
                    "1529",
                    "1818",
                    "1802"
                ],
                [
                    "PubMed",
                    "19717",
                    "15709",
                    "[BOLD] 15912",
                    "15902",
                    "[BOLD] 15912",
                    "15861"
                ]
            ],
            "title": "Table 3: Results on MIS problems."
        },
        "insight": "Table 3 presents the performance of MIS problem on three citation networks. Our proposed method has successfully found the global optimal solution on Cora dataset and obtained much better results compared to the sophisticated S2V-DQN on all three dataset. Although our results are not competitive with ,"
    },
    {
        "id": "625",
        "table": {
            "header": [
                "Graph",
                "size",
                "  [ITALIC] Q",
                " No. comms",
                "EO  [ITALIC] Q",
                "EO No. comms",
                "GSO  [ITALIC] Q",
                "GSO No. comms"
            ],
            "rows": [
                [
                    "Zachary",
                    "34",
                    "0.3810",
                    "2",
                    "0.4188",
                    "4",
                    "[BOLD] 0.4198",
                    "4"
                ],
                [
                    "Jazz",
                    "198",
                    "0.4379",
                    "4",
                    "[BOLD] 0.4452",
                    "5",
                    "0.4451",
                    "4"
                ],
                [
                    "C. elegans",
                    "453",
                    "0.4001",
                    "10",
                    "[BOLD] 0.4342",
                    "12",
                    "0.4304",
                    "8"
                ],
                [
                    "E-mail",
                    "1133",
                    "0.4796",
                    "13",
                    "[BOLD] 0.5738",
                    "15",
                    "0.5275",
                    "8"
                ]
            ],
            "title": "Table 4: Results on modularity optimization. We report the maximum modularity and the corresponding number of communities."
        },
        "insight": "On the task of modularity optimization, our method also obtained satisfying results, as shown in Table 4. Comparing to hierarchical agglomeration, our proposed method has achieved much higher modularity on all datasets."
    },
    {
        "id": "626",
        "table": {
            "header": [
                "N",
                "[ITALIC] s,  [ITALIC] \u03bb",
                "Accuracy (%)"
            ],
            "rows": [
                [
                    "10",
                    "0.2, 3.5",
                    "96.80"
                ],
                [
                    "10",
                    "0.2, 3.8",
                    "100"
                ],
                [
                    "30",
                    "0.2, 3.5",
                    "93.11"
                ],
                [
                    "30",
                    "0.2, 3.8",
                    "100"
                ]
            ],
            "title": "Table 5: Results on structural optimization."
        },
        "insight": "Here we present our results on the task of structural optimization in Table 5. [CONTINUE] For a random 4-regular graph with 10 and 30 nodes, our GSO method has obtained very good reconstruction accuracy and the performance obtained on chaotic dynamics is better than that on non-chaotic dynamics."
    },
    {
        "id": "627",
        "table": {
            "header": [
                "Test Case",
                "[ITALIC] Tin",
                "[ITALIC] pout",
                "\u02d9 [ITALIC] q",
                "[ITALIC] G",
                "[ITALIC] A",
                "[ITALIC] AR",
                "[ITALIC] d",
                "[ITALIC] r"
            ],
            "rows": [
                [
                    "[\u2013]",
                    "[K]",
                    "[bar]",
                    "[MWm2]",
                    "[kgm2s]",
                    "[mm2]",
                    "[\u2013]",
                    "[mm]",
                    "[\u00b5m]"
                ],
                [
                    "1",
                    "140",
                    "80",
                    "49",
                    "11700",
                    "1.9",
                    "2.0",
                    "0.83",
                    "2.1"
                ],
                [
                    "2",
                    "131",
                    "217",
                    "81",
                    "11700",
                    "4.1",
                    "4.1",
                    "0.90",
                    "3.0"
                ],
                [
                    "3",
                    "173",
                    "129",
                    "57",
                    "23900",
                    "7.4",
                    "3.7",
                    "1.14",
                    "3.0"
                ],
                [
                    "4",
                    "127",
                    "57",
                    "55",
                    "26000",
                    "6.0",
                    "7.5",
                    "0.96",
                    "14.2"
                ],
                [
                    "5",
                    "290",
                    "51",
                    "14",
                    "10100",
                    "7.4",
                    "3.7",
                    "1.14",
                    "1.7"
                ],
                [
                    "6",
                    "148",
                    "174",
                    "37",
                    "13200",
                    "3.2",
                    "2.3",
                    "1.07",
                    "6.4"
                ]
            ],
            "title": "Table 2: Exemplary boundary conditions for the test dataset"
        },
        "insight": "Table 2 and Table 3 give an overview of mean value, standard deviation and different percentiles of most relevant thermodynamic properties of the coolant, channel geometries and the resulting wall temperature at the hot-gas wall for the training and test datasets."
    },
    {
        "id": "628",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] Tb",
                "[ITALIC] hb",
                "[ITALIC] pb",
                "[ITALIC] vb",
                "[ITALIC] G",
                "\u02d9 [ITALIC] q",
                "[ITALIC] r",
                "[ITALIC] A",
                "[ITALIC] AR",
                "[ITALIC] d",
                "[ITALIC] Tw"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[K]",
                    "[kJkg]",
                    "[bar]",
                    "[ms]",
                    "[kgsm2]",
                    "[MWm2]",
                    "[\u00b5m]",
                    "[mm2]",
                    "[\u2013]",
                    "[mm]",
                    "[K]"
                ],
                [
                    "Mean",
                    "251",
                    "566",
                    "125",
                    "126",
                    "18483",
                    "36",
                    "6.9",
                    "6.7",
                    "4.4",
                    "1.0",
                    "669"
                ],
                [
                    "Std",
                    "84",
                    "317",
                    "42",
                    "78",
                    "8078",
                    "24",
                    "6.1",
                    "3.2",
                    "3.1",
                    "0.1",
                    "302"
                ],
                [
                    "1 \\char 37",
                    "123",
                    "56",
                    "53",
                    "18",
                    "3027",
                    "9",
                    "0.2",
                    "1.0",
                    "1.0",
                    "0.8",
                    "230"
                ],
                [
                    "25 \\char 37",
                    "183",
                    "279",
                    "90",
                    "64",
                    "12500",
                    "10",
                    "1.0",
                    "5.0",
                    "1.7",
                    "1.0",
                    "426"
                ],
                [
                    "50 \\char 37",
                    "240",
                    "572",
                    "119",
                    "109",
                    "17500",
                    "30",
                    "5.0",
                    "5.0",
                    "3.5",
                    "1.0",
                    "620"
                ],
                [
                    "75 \\char 37",
                    "302",
                    "790",
                    "158",
                    "174",
                    "25000",
                    "50",
                    "15.0",
                    "10.0",
                    "9.2",
                    "1.0",
                    "854"
                ],
                [
                    "99 \\char 37",
                    "433",
                    "1175",
                    "215",
                    "357",
                    "35000",
                    "80",
                    "20.0",
                    "10.0",
                    "9.2",
                    "1.2",
                    "1482"
                ]
            ],
            "title": "Table 3: Mean value, standard deviation and percentiles of the training data"
        },
        "insight": "Table 2 and Table 3 give an overview of mean value, standard deviation and different percentiles of most relevant thermodynamic properties of the coolant, channel geometries and the resulting wall temperature at the hot-gas wall for the training and test datasets."
    },
    {
        "id": "629",
        "table": {
            "header": [
                "Index",
                "DB-12",
                "WDB-12",
                "DB-24",
                "WDB-24",
                "DB-48",
                "WDB-48"
            ],
            "rows": [
                [
                    "PSNR",
                    "31.27",
                    "31.28",
                    "31.65",
                    "31.71",
                    "31.82",
                    "31.89"
                ],
                [
                    "SSIM",
                    "0.8911",
                    "0.8916",
                    "0.8956",
                    "0.8957",
                    "0.8970",
                    "0.8975"
                ]
            ],
            "title": "Table 1: Investigations of WDB with different growth rates on Set5 with scaling factor 4."
        },
        "insight": "which weights are fixed and equal to 1. As shown in Table 1, by adopting a group of trainable weights, WDB can consistently achieve higher scores than DB when setting different growth rates. It achieves more apparent PSNR promotion with growth rates increased."
    },
    {
        "id": "630",
        "table": {
            "header": [
                "Index",
                "noSA-16",
                "SA-16",
                "noSA-20",
                "SA-20",
                "noSA-32",
                "SA-32"
            ],
            "rows": [
                [
                    "PSNR",
                    "28.25",
                    "28.39",
                    "28.28",
                    "28.43",
                    "28.43",
                    "28.55"
                ],
                [
                    "SSIM",
                    "0.7784",
                    "0.7820",
                    "0.7788",
                    "0.7830",
                    "0.7827",
                    "0.7848"
                ],
                [
                    "RCIR",
                    "0.172",
                    "0.175",
                    "0.173",
                    "0.183",
                    "0.181",
                    "0.190"
                ]
            ],
            "title": "Table 2: Evaluation of SA with different growth rates on Set14 with 4\u00d7 up-scaling factor."
        },
        "insight": "As shown in Table 2, SA improves [CONTINUE] PSNR more than 0.1db in each growth rate, and it increases output RCIR with a large extent, thus high-frequency details of an image tend to be recovered more clearly. Besides, compared SA (G = 20) with noSA (G = 32), they have the almost same parameters (1.52M and 1.54M), but SA still achieves higher RCIR and SSIM than no SA. Notably, increasing G"
    },
    {
        "id": "631",
        "table": {
            "header": [
                "Dataset",
                "Index",
                "Bicubic",
                "A+",
                "SRCNN",
                "VDSR",
                "LapSRN",
                "SRDense",
                "SR-DDNet",
                "RDN",
                "D-DBPN",
                "ADRD"
            ],
            "rows": [
                [
                    "Set5",
                    "PSNR",
                    "28.42",
                    "30.28",
                    "30.48",
                    "31.35",
                    "31.54",
                    "32.02",
                    "32.21",
                    "32.47",
                    "[BOLD] 32.47",
                    "32.45"
                ],
                [
                    "[EMPTY]",
                    "SSIM",
                    "0.8104",
                    "0.8603",
                    "0.8820",
                    "0.8855",
                    "0.8934",
                    "0.8982",
                    "0.8988",
                    "0.8990",
                    "0.8980",
                    "[BOLD] 0.8999"
                ],
                [
                    "Set14",
                    "PSNR",
                    "26.00",
                    "27.32",
                    "27.50",
                    "28.03",
                    "28.19",
                    "28.50",
                    "28.71",
                    "28.81",
                    "28.82",
                    "[BOLD] 28.84"
                ],
                [
                    "[EMPTY]",
                    "SSIM",
                    "0.7027",
                    "0.7491",
                    "0.7513",
                    "0.7701",
                    "0.7720",
                    "0.7782",
                    "0.7805",
                    "0.7871",
                    "0.7861",
                    "[BOLD] 0.7923"
                ],
                [
                    "BSD100",
                    "PSNR",
                    "25.96",
                    "26.82",
                    "26.90",
                    "27.29",
                    "27.32",
                    "27.53",
                    "27.69",
                    "27.72",
                    "[BOLD] 27.72",
                    "27.69"
                ],
                [
                    "[EMPTY]",
                    "SSIM",
                    "0.6675",
                    "0.7087",
                    "0.7101",
                    "0.7264",
                    "0.7280",
                    "0.7337",
                    "0.7396",
                    "0.7419",
                    "0.7401",
                    "[BOLD] 0.7477"
                ],
                [
                    "Urban100",
                    "PSNR",
                    "23.14",
                    "24.32",
                    "24.52",
                    "25.18",
                    "25.21",
                    "26.05",
                    "26.21",
                    "26.61",
                    "27.08\u22c6",
                    "[BOLD] \\ \\ 27.26\u22c6"
                ],
                [
                    "[EMPTY]",
                    "SSIM",
                    "0.6577",
                    "0.7183",
                    "0.7221",
                    "0.7553",
                    "0.7561",
                    "0.7819",
                    "0.7884",
                    "0.8028",
                    "0.7972",
                    "[BOLD] 0.8041"
                ]
            ],
            "title": "Table 3: Comparisons with the state-of-the-art methods by PSNR and SSIM (4\u00d7). Scores in bold denote the highest values (\u22c6 indicates that the input is divided into four parts and calculated due to computation limitation of large size images)."
        },
        "insight": "We compare ADRD with state-of-the-art methods, as shown in Table 3. Here, the bicubic interpolation is viewed as a baseline for comparisons. A+ [Timofte et al., 2013] is introduced as a conventional machine learning approach. Some CNN based methods, i.e. SRCNN [Dong et al., 2016a], VDSR [Kim et al., 2016a], LapSRN [Lai et al., 2017], and D-DBPN [Haris et al., 2018] are introduced. SRDenseNet [Tong et al., 2017] (denotes as SRDense), SR-DDNet [Zhu et al., 2018], and RDN [Zhang et al., 2018] are three different sizes of dense block based networks are also cited in the comparison list. ADRD achieves the highest SSIM among all methods, it tends to have better quality in human perception [Wang et al., 2004]. Because ADRD is adept at recovering high-frequency information. Additionally, ADRD also outperforms D-DBPN nearly 0.2db PSNR on Urban100 dataset that contains many large-size real-world images. [CONTINUE] Despite D-DBPN surpasses ADRD on Set5 in PSNR as shown in Table 3,"
    },
    {
        "id": "632",
        "table": {
            "header": [
                "Level",
                "Bicubic",
                "LapSRN",
                "RDN",
                "D-DBPN",
                "ADRD"
            ],
            "rows": [
                [
                    "5\u00d710\u22125",
                    "28.38",
                    "30.84",
                    "31.82",
                    "31.86",
                    "[BOLD] 31.90"
                ],
                [
                    "1\u00d710\u22124",
                    "28.35",
                    "30.66",
                    "31.44",
                    "31.45",
                    "[BOLD] 31.47"
                ],
                [
                    "2\u00d710\u22124",
                    "28.27",
                    "30.24",
                    "30.77",
                    "30.86",
                    "[BOLD] 30.86"
                ],
                [
                    "5\u00d710\u22124",
                    "28.04",
                    "29.31",
                    "29.55",
                    "29.55",
                    "[BOLD] 29.69"
                ]
            ],
            "title": "Table 4: PSNR results of different noise levels on Set5."
        },
        "insight": "et al., 2018, RDN Zhang et al., 2018, LapSRN Lai et al., 2017] are introduced for comparisons. The detailed results are shown in Table 4. [CONTINUE] ADRD outperforms all other methods in each noise level. Though RDN is also a dense block based network, it is easy"
    },
    {
        "id": "633",
        "table": {
            "header": [
                "Acc (%)",
                "Bicubic",
                "LapSRN",
                "RDN",
                "D-DBPN",
                "ADRD"
            ],
            "rows": [
                [
                    "Top-1",
                    "53.4",
                    "52.1",
                    "54.6",
                    "55.1",
                    "[BOLD] 55.7"
                ],
                [
                    "Top-5",
                    "82.5",
                    "82.5",
                    "83.6",
                    "83.9",
                    "[BOLD] 84.2"
                ]
            ],
            "title": "Table 5: Recognition accuracy on Pairs & Oxford."
        },
        "insight": "As shown in Table 5, ADRD promotes 2.3% Top-1 accuracy, while the RDN only promotes 1.2%. The results"
    },
    {
        "id": "634",
        "table": {
            "header": [
                "[EMPTY]",
                "User-indep.",
                "Nonsuper",
                "Semi-super, th=90",
                "Semi-super, th=95",
                "Superv."
            ],
            "rows": [
                [
                    "Arm / LDA",
                    "9.3",
                    "7.9",
                    "4.8 (9.9)",
                    "4.8 (10.1)",
                    "2.7"
                ],
                [
                    "Waist / LDA",
                    "14.1",
                    "9.5",
                    "4.1 (14.9)",
                    "3.9 (14.9)",
                    "3.3"
                ],
                [
                    "Wrist / LDA",
                    "12.7",
                    "10.3",
                    "5.5 (10.9)",
                    "4.8 (11.8)",
                    "3.1"
                ],
                [
                    "Arm / QDA",
                    "8.6",
                    "7.0",
                    "3.7 (13.0)",
                    "4.3 (12.3)",
                    "2.1"
                ],
                [
                    "Waist / QDA",
                    "11.1",
                    "9.1",
                    "4.9 (10.7)",
                    "5.0 (10.3)",
                    "3.5"
                ],
                [
                    "Wrist / QDA",
                    "12.2",
                    "7.5",
                    "3.5 (14.9)",
                    "3.5 (15.7)",
                    "2.9"
                ],
                [
                    "Arm / CART",
                    "11.7",
                    "8.7",
                    "5.0 (23.0)",
                    "4.9 (22.8)",
                    "2.1"
                ],
                [
                    "Waist / CART",
                    "18.0",
                    "15.7",
                    "4.4 (27.4)",
                    "4.3 (28.6)",
                    "2.0"
                ],
                [
                    "Wrist / CART",
                    "12.2",
                    "9.9",
                    "2.6 (26.7)",
                    "2.6 (27.0)",
                    "2.5"
                ],
                [
                    "Mean",
                    "12.2",
                    "9.5",
                    "4.3 (16.8)",
                    "4.2 (17.1)",
                    "2.7"
                ]
            ],
            "title": "Table 1: Average error rates of different personalization approaches. The percentage of user inputs required by the semi-supervised approach in parentheses."
        },
        "insight": "Table 1 show error rates from the balanced accuracies averaged over all nine study subjects. [CONTINUE] Model update clearly benefits from user inputs as the error rates using semisupervised approach (4.2/4.3%) are much lower than non-supervised (9.5%), [CONTINUE] On average, the error rate using the semi-supervised approach is less than 2%-units higher than using the supervised approach. [CONTINUE] (26% [CONTINUE] the error rates of non-supervised models were much higher than the one's obtained using the semi-supervised approach (semi-supervised 4.2/4.4% vs. non-supervised 9.5%). [CONTINUE] the average error rate using the semisupervised approach instead of supervised was less than 2%-units. [CONTINUE] al"
    },
    {
        "id": "635",
        "table": {
            "header": [
                "[BOLD] Gender",
                "[BOLD] NLL  [BOLD] fam.",
                "[BOLD] NLL  [BOLD] unf.",
                "[BOLD] Brier  [BOLD] fam.",
                "[BOLD] Brier  [BOLD] unf.",
                "[BOLD] Label Error  [BOLD] fam.",
                "[BOLD] Label Error  [BOLD] unf.",
                "[BOLD] ECE  [BOLD] fam.",
                "[BOLD] ECE  [BOLD] unf."
            ],
            "rows": [
                [
                    "Baseline",
                    "0.083",
                    "0.542",
                    "0.147",
                    "0.352",
                    "0.028",
                    "[BOLD] 0.147",
                    "0.013",
                    "0.109"
                ],
                [
                    "T-scaling",
                    "12%",
                    "26%",
                    "2%",
                    "[BOLD] 4%",
                    "0%",
                    "[BOLD] 0%",
                    "[BOLD] 73%",
                    "20%"
                ],
                [
                    "Ensemble",
                    "[BOLD] 24%",
                    "33%",
                    "[BOLD] 10%",
                    "[BOLD] 6%",
                    "[BOLD] 22%",
                    "[BOLD] 0%",
                    "36%",
                    "[BOLD] 29%"
                ],
                [
                    "Distill",
                    "8%",
                    "33%",
                    "3%",
                    "4%",
                    "3%",
                    "-7%",
                    "41%",
                    "21%"
                ],
                [
                    "G-distill",
                    "13%",
                    "[BOLD] 38%",
                    "5%",
                    "[BOLD] 6%",
                    "9%",
                    "-5%",
                    "31%",
                    "[BOLD] 31%"
                ],
                [
                    "Bayesian",
                    "17%",
                    "26%",
                    "5%",
                    "4%",
                    "6%",
                    "[BOLD] 0%",
                    "[BOLD] 77%",
                    "19%"
                ],
                [
                    "[BOLD] Cat vs. Dog",
                    "[BOLD] Cat vs. Dog",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Baseline",
                    "0.053",
                    "0.423",
                    "0.112",
                    "0.290",
                    "0.016",
                    "0.095",
                    "0.010",
                    "0.078"
                ],
                [
                    "T-scaling",
                    "23%",
                    "30%",
                    "4%",
                    "5%",
                    "0%",
                    "0%",
                    "64%",
                    "23%"
                ],
                [
                    "Ensemble",
                    "[BOLD] 40%",
                    "[BOLD] 46%",
                    "[BOLD] 17%",
                    "[BOLD] 12%",
                    "[BOLD] 22%",
                    "[BOLD] 8%",
                    "[BOLD] 79%",
                    "[BOLD] 46%"
                ],
                [
                    "Distill",
                    "-13%",
                    "22%",
                    "-9%",
                    "1%",
                    "-18%",
                    "-4%",
                    "55%",
                    "26%"
                ],
                [
                    "G-distill",
                    "-18%",
                    "27%",
                    "-14%",
                    "1%",
                    "-33%",
                    "-8%",
                    "41%",
                    "31%"
                ],
                [
                    "Bayesian",
                    "17%",
                    "26%",
                    "3%",
                    "5%",
                    "0%",
                    "3%",
                    "42%",
                    "21%"
                ],
                [
                    "[BOLD] Animals",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Baseline",
                    "0.326",
                    "1.128",
                    "0.199",
                    "0.341",
                    "0.104",
                    "0.291",
                    "0.048",
                    "0.187"
                ],
                [
                    "T-scaling",
                    "13%",
                    "23%",
                    "3%",
                    "5%",
                    "0%",
                    "0%",
                    "[BOLD] 75%",
                    "37%"
                ],
                [
                    "Ensemble",
                    "[BOLD] 22%",
                    "[BOLD] 32%",
                    "[BOLD] 9%",
                    "[BOLD] 8%",
                    "[BOLD] 11%",
                    "[BOLD] 6%",
                    "50%",
                    "[BOLD] 57%"
                ],
                [
                    "Distill",
                    "7%",
                    "24%",
                    "1%",
                    "5%",
                    "-1%",
                    "0%",
                    "66%",
                    "45%"
                ],
                [
                    "G-distill",
                    "14%",
                    "26%",
                    "5%",
                    "7%",
                    "7%",
                    "2%",
                    "56%",
                    "49%"
                ],
                [
                    "Bayesian",
                    "16%",
                    "24%",
                    "5%",
                    "5%",
                    "4%",
                    "1%",
                    "[BOLD] 74%",
                    "39%"
                ],
                [
                    "[BOLD] Objects",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Baseline",
                    "0.086",
                    "0.128",
                    "0.154",
                    "0.186",
                    "0.195",
                    "0.455",
                    "0.005",
                    "0.010"
                ],
                [
                    "T-scaling",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "2%",
                    "2%"
                ],
                [
                    "Ensemble",
                    "[BOLD] 4%",
                    "4%",
                    "[BOLD] 2%",
                    "2%",
                    "[BOLD] 6%",
                    "[BOLD] 3%",
                    "[BOLD] 3%",
                    "7%"
                ],
                [
                    "Distill",
                    "-1%",
                    "[BOLD] 5%",
                    "0%",
                    "[BOLD] 2%",
                    "1%",
                    "0%",
                    "-31%",
                    "[BOLD] 10%"
                ],
                [
                    "G-distill",
                    "-2%",
                    "5%",
                    "-1%",
                    "2%",
                    "-2%",
                    "-1%",
                    "-41%",
                    "7%"
                ],
                [
                    "Bayesian",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "0%",
                    "3%",
                    "1%"
                ]
            ],
            "title": "Table 1: Performance of baseline (single model) for several metrics and percent reduction in error for other methods. All methods except baseline use T-scaling calibration. \u201cT-scaling\u201d is a single calibrated model."
        },
        "insight": "Our main table of results is shown in Table 1. [CONTINUE] Looking at baseline performance in Table 1, we see much higher error rates for novel samples, compared to familiar, for all tasks. The label error and calibration error are both higher, leading to much higher NLL and Brier error. This means the baseline classifier is less accurate and has poor ability to detect its own inaccuracy on novel samples \u2014 it does not know what it does not know. For example, in gender recognition, the label error increases from 2.8% for novel to 14.7%; the calibration error ECE increases from 0.013 to 0.109; and the NLL increases from 0.083 to 0.542. [CONTINUE] The differences between novel and familiar for object presence classification are substantial but smaller than other tasks, [CONTINUE] In Table 1, \"T-scaling\" refers to the T-scaled baseline, and T-scaling is used for all other non-baseline methods as well. [CONTINUE] Finally, considering Table 1, we see that the ensemble of T-scaled models dominates, consistently achieving the lowest label error, calibration error, NLL, and Brier error."
    },
    {
        "id": "636",
        "table": {
            "header": [
                "[BOLD] Gender",
                "[BOLD] NLL  [BOLD] fam.",
                "[BOLD] NLL  [BOLD] unf.",
                "[BOLD] Brier  [BOLD] fam.",
                "[BOLD] Brier  [BOLD] unf.",
                "[BOLD] ECE  [BOLD] fam.",
                "[BOLD] ECE  [BOLD] unf."
            ],
            "rows": [
                [
                    "Single",
                    "0.083",
                    "0.542",
                    "0.148",
                    "0.352",
                    "0.013",
                    "0.109"
                ],
                [
                    "Sin. T-scale",
                    "0.073",
                    "0.400",
                    "0.145",
                    "0.338",
                    "0.004",
                    "0.087"
                ],
                [
                    "Ensemble",
                    "0.062",
                    "0.455",
                    "0.130",
                    "0.344",
                    "0.003",
                    "0.093"
                ],
                [
                    "Ens. T-scale",
                    "0.063",
                    "0.363",
                    "0.130",
                    "0.333",
                    "0.009",
                    "0.077"
                ],
                [
                    "[BOLD] Cat vs. Dog",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Single",
                    "0.053",
                    "0.423",
                    "0.110",
                    "0.290",
                    "0.010",
                    "0.078"
                ],
                [
                    "Sin. T-scale",
                    "0.041",
                    "0.295",
                    "0.105",
                    "0.276",
                    "0.004",
                    "0.060"
                ],
                [
                    "Ensemble",
                    "0.033",
                    "0.286",
                    "0.095",
                    "0.263",
                    "0.002",
                    "0.055"
                ],
                [
                    "Ens. T-scale",
                    "0.032",
                    "0.229",
                    "0.095",
                    "0.255",
                    "0.002",
                    "0.042"
                ],
                [
                    "[BOLD] Animals",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Single",
                    "0.326",
                    "1.128",
                    "0.200",
                    "0.341",
                    "0.048",
                    "0.187"
                ],
                [
                    "Single T-scale",
                    "0.284",
                    "0.866",
                    "0.195",
                    "0.324",
                    "0.012",
                    "0.118"
                ],
                [
                    "Ensemble",
                    "0.256",
                    "0.930",
                    "0.182",
                    "0.322",
                    "0.022",
                    "0.138"
                ],
                [
                    "Ens. T-scale",
                    "0.254",
                    "0.772",
                    "0.182",
                    "0.311",
                    "0.024",
                    "0.080"
                ]
            ],
            "title": "Table 2: T-scaling calibration effectively reduces likelihood error (NLL, Brier) and calibration error (ECE) for many models across tasks for familiar and unfamiliar samples. Without calibration, using an ensemble reduces these errors, but an ensemble of calibrated models (\u201cEns. T-scale\u201d) performs best. Applying T-scaling to an ensemble of uncalibrated classifiers, and creating an ensemble of calibrated classifiers produces nearly identical results."
        },
        "insight": "els outperform uncalibrated models, and ensembles of calibrated models outperform ensembles of uncalibrated models. For example, in cat vs. dog recognition, the baseline NLL drops from 0.423 to 0.295, a 30% reduction; and the ensemble NLL drops from 0.286 to 0.229, a 20% reduction. Table 2 compares performance of the baseline and ensemble methods, both without and with T-scale calibration. Calibrated single mod [CONTINUE] For the object presence task, there is little effect of calibration [CONTINUE] We also found calibration to improve the Bayesian method . Calibration has little effect for distillation and G-distillation, likely because distillation's fitting to soft labels makes it less confident."
    },
    {
        "id": "637",
        "table": {
            "header": [
                "[EMPTY]",
                "dSprites Matthey et\u00a0al. ( 2017 )",
                "3D Faces Paysan et\u00a0al. ( 2009 )"
            ],
            "rows": [
                [
                    "[ITALIC] \u03b2-VAE Higgins et\u00a0al. ( 2017 )",
                    "0.22",
                    "0.54"
                ],
                [
                    "[ITALIC] \u03b2-TCVAE Chen et\u00a0al. ( 2018 )",
                    "0.38",
                    "0.62"
                ],
                [
                    "ICP-ALL",
                    "0.33",
                    "0.26"
                ],
                [
                    "ICP-COM",
                    "0.20",
                    "0.57"
                ],
                [
                    "[BOLD] ICP",
                    "[BOLD] 0.48",
                    "[BOLD] 0.73"
                ]
            ],
            "title": "Table 3: MIG score of disentanglement."
        },
        "insight": "We evaluate the disentanglement performance quantitatively by the Mutual Information Gap (MIG) score  with the 2D shapes (dSprites)  dataset and 3D Faces  dataset. MIG is a classifier-free information-theoretic disentanglement metric and is meaningful for any factorized latent distribution. As shown in Table 3, ICP achieves the state-of-the-art performance on the quantitative evaluation of disentanglement. We also conduct ablation studies as what we do in the supervised setting. [CONTINUE] From the results of ICP-ALL and ICP-COM, we find disentanglement performance decreases without the information diversifying and competing process."
    },
    {
        "id": "638",
        "table": {
            "header": [
                "[ITALIC] K",
                "CARS196 1",
                "CARS196 2",
                "CARS196 4",
                "CARS196 8",
                "CUB-200-2011 1",
                "CUB-200-2011 2",
                "CUB-200-2011 4",
                "CUB-200-2011 8",
                "SOP 1",
                "SOP 10",
                "SOP 100"
            ],
            "rows": [
                [
                    "Without fine-tuning",
                    "35.6",
                    "47.3",
                    "59.4",
                    "72.2",
                    "40.1",
                    "53.2",
                    "66.0",
                    "76.6",
                    "43.7",
                    "60.8",
                    "76.5"
                ],
                [
                    "Fine-tuned with CCE",
                    "48.8",
                    "58.5",
                    "71.0",
                    "78.4",
                    "46.0",
                    "58.0",
                    "69.3",
                    "78.3",
                    "51.7",
                    "69.8",
                    "85.3"
                ],
                [
                    "Triplet Semihard",
                    "51.5",
                    "63.8",
                    "73.5",
                    "82.4",
                    "42.6",
                    "55.0",
                    "66.4",
                    "77.2",
                    "66.7",
                    "82.4",
                    "91.9"
                ],
                [
                    "Lifted Struct",
                    "53.0",
                    "65.7",
                    "76.0",
                    "84.3",
                    "43.6",
                    "56.6",
                    "68.6",
                    "79.6",
                    "62.5",
                    "80.8",
                    "91.9"
                ],
                [
                    "N-pair-mc",
                    "53.9",
                    "66.8",
                    "77.8",
                    "86.4",
                    "45.4",
                    "58.4",
                    "69.5",
                    "79.5",
                    "66.4",
                    "83.2",
                    "93.0"
                ],
                [
                    "Struct Clust",
                    "58.1",
                    "70.6",
                    "80.3",
                    "87.8",
                    "48.2",
                    "61.4",
                    "71.8",
                    "81.9",
                    "67.0",
                    "83.7",
                    "93.2"
                ],
                [
                    "Spectral Clust",
                    "73.1",
                    "82.2",
                    "89.0",
                    "93.0",
                    "53.2",
                    "66.1",
                    "76.7",
                    "85.3",
                    "67.6",
                    "83.7",
                    "93.3"
                ],
                [
                    "Proxy NCA",
                    "73.2",
                    "82.4",
                    "86.4",
                    "88.7",
                    "49.2",
                    "61.9",
                    "67.9",
                    "72.4",
                    "73.7",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "RLL",
                    "74.0",
                    "83.6",
                    "90.1",
                    "94.1",
                    "57.4",
                    "[BOLD] 69.7",
                    "79.2",
                    "[BOLD] 86.9",
                    "76.1",
                    "89.1",
                    "95.4"
                ],
                [
                    "ICE",
                    "[BOLD] 77.0",
                    "[BOLD] 85.3",
                    "[BOLD] 91.3",
                    "[BOLD] 94.8",
                    "[BOLD] 58.3",
                    "69.5",
                    "[BOLD] 79.4",
                    "86.7",
                    "[BOLD] 77.3",
                    "[BOLD] 90.0",
                    "[BOLD] 95.6"
                ],
                [
                    "RLL-(L,M,H)",
                    "82.1",
                    "89.3",
                    "93.7",
                    "96.7",
                    "61.3",
                    "72.7",
                    "82.7",
                    "89.4",
                    "79.8",
                    "91.3",
                    "96.3"
                ],
                [
                    "ICE-(L, M, H)",
                    "82.8",
                    "89.5",
                    "93.7",
                    "96.4",
                    "61.4",
                    "73.2",
                    "82.5",
                    "89.2",
                    "80.1",
                    "91.8",
                    "96.6"
                ]
            ],
            "title": "Table 3: Comparison with the state-of-the-art methods on CARS196, CUB-200-2011 and SOP in terms of Recall@K (%). All the compared methods use GoogLeNet V2 as the backbone architecture. \u2018\u2013\u2019 means the results which are not reported in the original paper. The best results in the first block using single embedding are bolded."
        },
        "insight": "Results. Table 3 compares the results of our ICE and those of the state-of-the-art DML losses. ICE achieves the best Recall@1 performance on all benchmarks. We observe that only RLL achieves comparable performance in a few terms. [CONTINUE] We note that (Wang et al., 2019c) is also complex in designing weighting schemes and contains four control hyper-parameters. However, our Recall@1 on SOP is 77.3%, which is only 0.9% lower than 78.2% of (Wang et al., 2019c). It is also worth mentioning that among these approaches, except fine-tuned models with CCE, only our method has a clear probability interpretation and aims to maximise the joint instance-level matching probability. As observed, apart from being unscalable, CCE's performance is much worse than the state-of-the-art methods."
    },
    {
        "id": "639",
        "table": {
            "header": [
                "[ITALIC] N=180, [ITALIC] s=64",
                "R@1",
                "R@10",
                "R@100"
            ],
            "rows": [
                [
                    "[ITALIC] C\u00d7 [ITALIC] k=90\u00d72",
                    "77.3",
                    "90.0",
                    "95.6"
                ],
                [
                    "[ITALIC] C\u00d7 [ITALIC] k=60\u00d73",
                    "75.2",
                    "88.7",
                    "95.2"
                ],
                [
                    "[ITALIC] C\u00d7 [ITALIC] k=45\u00d74",
                    "74.9",
                    "88.7",
                    "95.3"
                ],
                [
                    "[ITALIC] C\u00d7 [ITALIC] k=36\u00d75",
                    "74.6",
                    "88.7",
                    "95.4"
                ]
            ],
            "title": "Table 4: The impact of batch content C\u00d7k on SOP in terms of Recall@K (%). The batch size is N=180 and the scaling parameter is s=64."
        },
        "insight": "We evaluate the impact of batch content which consists of [CONTINUE] classes and k images per class, i.e., \u2200c, Nc = k. The batch size [CONTINUE] = [CONTINUE] \u00d7 k is set to 180. In our experiments, we change the number of classes [CONTINUE] from 36 to 90, and the number of images k from 2 to 5, while keeping the batch size unchanged. Table 4 shows the results on SOP dataset. We observe that when there are more classes in the mini-batch, the performance is better. We conjecture that as the number of classes increases, the mini-batch training becomes more difficult and helps the model to generalise better."
    },
    {
        "id": "640",
        "table": {
            "header": [
                "[ITALIC] k=2, [ITALIC] s=64",
                "R@1",
                "R@10",
                "R@100"
            ],
            "rows": [
                [
                    "[ITALIC] N=180",
                    "77.3",
                    "90.0",
                    "95.6"
                ],
                [
                    "[ITALIC] N=160",
                    "75.4",
                    "88.8",
                    "95.1"
                ],
                [
                    "[ITALIC] N=140",
                    "75.1",
                    "88.7",
                    "95.2"
                ],
                [
                    "[ITALIC] N=120",
                    "75.1",
                    "88.6",
                    "95.2"
                ],
                [
                    "[ITALIC] N=100",
                    "74.4",
                    "88.2",
                    "95.1"
                ]
            ],
            "title": "Table 5: The results of different batch size N on SOP in terms of Recall@K (%). While changing C, we fix k=2 and s=64. Therefore, N=C\u00d72."
        },
        "insight": "To explore different batch size [CONTINUE] , we fix k = 2 and only change C. In this case, [CONTINUE] = [CONTINUE] \u00d7 2. Table 5 shows that as the number of classes increases, the performance grows. In detail, when the number of classes increases from 50 to 90, the performance raises from 74.4% to 77.3% accordingly. One reason may be that as the number of classes increases, it fits the global structure of the test set better, where there are a large number of classes but only a few positive examples. In addition, the increasing difficulty of mini-batch training can help the model to generalise better."
    },
    {
        "id": "641",
        "table": {
            "header": [
                "180=90\u00d72, [ITALIC] s=64",
                "R@1",
                "R@10",
                "R@100"
            ],
            "rows": [
                [
                    "64",
                    "72.6",
                    "87.1",
                    "94.0"
                ],
                [
                    "128",
                    "74.3",
                    "87.9",
                    "94.5"
                ],
                [
                    "256",
                    "75.2",
                    "88.6",
                    "94.8"
                ],
                [
                    "512",
                    "77.3",
                    "90.0",
                    "95.6"
                ]
            ],
            "title": "Table 6: The results of different embedding size on SOP in terms of Recall@K (%). In all experiments: s=64, C=90,k=2. N=C\u00d7k=90\u00d72."
        },
        "insight": "The dimension of feature representations is an important factor in many DML methods. We conduct experiments on SOP to see the influence of different embedding size. The results are presented in Table 6. We observe that when the embedding size is very small, e.g., 64, the performance is much worse. The performance increases gradually as the embedding size grows."
    },
    {
        "id": "642",
        "table": {
            "header": [
                "Method",
                "Backbone",
                "pretrained",
                "sMOTSA",
                "MOTSA",
                "MOTSP",
                "IDS",
                "TP",
                "FP",
                "FN"
            ],
            "rows": [
                [
                    "TrackR-CNN\u00a0[voigtlaender2019mots]",
                    "TrackR-CNN/ResNet-101",
                    "COCO + MOTS",
                    "76.2",
                    "87.8",
                    "[BOLD] 87.2",
                    "93",
                    "7276",
                    "[BOLD] 134",
                    "753"
                ],
                [
                    "Mask R-CNN + maskprop\u00a0[voigtlaender2019mots]",
                    "Mask R-CNN/ResNet-101",
                    "COCO + MOTS",
                    "75.1",
                    "86.6",
                    "87.1",
                    "\u2013",
                    "\u2013",
                    "\u2013",
                    "\u2013"
                ],
                [
                    "CAMOT\u00a0[ovsep2018track]",
                    "TrackR-CNN/ResNet-101",
                    "COCO + MOTS",
                    "67.4",
                    "78.6",
                    "86.5",
                    "220",
                    "6702",
                    "172",
                    "1327"
                ],
                [
                    "CIWT\u00a0[osep2017combined]",
                    "TrackR-CNN/ResNet-101",
                    "COCO + MOTS",
                    "68.1",
                    "79.4",
                    "86.7",
                    "106",
                    "6815",
                    "333",
                    "1214"
                ],
                [
                    "Ours",
                    "PANet/ResNet-50",
                    "COCO",
                    "66.9",
                    "79.1",
                    "85.2",
                    "(0)",
                    "6647",
                    "291",
                    "1382"
                ],
                [
                    "Ours",
                    "Mask R-CNN/ResNet-101",
                    "COCO",
                    "66.1",
                    "78.1",
                    "85.5",
                    "(0)",
                    "6614",
                    "347",
                    "1415"
                ],
                [
                    "Ours",
                    "Mask R-CNN/ResNet-101",
                    "COCO + MOTS",
                    "77.4",
                    "89.6",
                    "86.6",
                    "(0)",
                    "[BOLD] 7338",
                    "142",
                    "[BOLD] 691"
                ]
            ],
            "title": "Table 2: Validation of our auto-annotation for video instance segmentation on KITTI MOTS validation set (car class)."
        },
        "insight": "Table 2 shows the results on the KITTI Eigen split test, where the proposed method achieves state-of-the-art performance in the single view depth prediction task with unsupervised training."
    },
    {
        "id": "643",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Approach",
                "[BOLD] Acc",
                "[BOLD] DSP",
                "[BOLD] CPU\u00a0\u2021",
                "[BOLD] VPU",
                "[BOLD] FLOPs",
                "[BOLD] Supernet training"
            ],
            "rows": [
                [
                    "[BOLD] Model",
                    "[BOLD] Approach",
                    "[BOLD] Acc",
                    "[BOLD] DSP",
                    "[BOLD] CPU\u00a0\u2021",
                    "[BOLD] VPU",
                    "[BOLD] FLOPs",
                    "[BOLD] Time Reduction (%)"
                ],
                [
                    "[BOLD] Model",
                    "[BOLD] Approach",
                    "[BOLD] (%)",
                    "[BOLD] (ms)",
                    "[BOLD] (ms)",
                    "[BOLD] (ms)",
                    "[BOLD] FLOPs",
                    "[BOLD]  [ITALIC] Stage1 /  [ITALIC] Stage1+ [ITALIC] Stage2 \u2020"
                ],
                [
                    "MobileNetV2\u00a0",
                    "Manual",
                    "72.00",
                    "10.1",
                    "432.4",
                    "45.2",
                    "300M",
                    "-"
                ],
                [
                    "MobileNetV3-Large1.0\u00a0",
                    "RL\u00a0",
                    "75.20",
                    "48.3",
                    "411.4",
                    "43.9",
                    "219M",
                    "-"
                ],
                [
                    "MnasNet-A1\u00a0",
                    "RL",
                    "75.20",
                    "149.0",
                    "1056.1",
                    "52.4",
                    "312M",
                    "-"
                ],
                [
                    "FBNet-iPhoneX\u00a0",
                    "gradient",
                    "73.20",
                    "105.0",
                    "313.0",
                    "45.6",
                    "322M",
                    "-"
                ],
                [
                    "FBNet-S8\u00a0",
                    "gradient",
                    "73.27",
                    "293.0",
                    "369.6",
                    "45.1",
                    "293M",
                    "-"
                ],
                [
                    "Proxyless-R (mobile)\u00a0",
                    "gradient",
                    "74.60",
                    "534.6",
                    "616.5",
                    "53.1",
                    "333M",
                    "-"
                ],
                [
                    "{Singlepath-Oneshot}\u22c6\u00a0",
                    "oneshot",
                    "74.30",
                    "270.6",
                    "455.8",
                    "38.7",
                    "319M",
                    "0"
                ],
                [
                    "[ITALIC]  [BOLD] HURRICANE (DSP)",
                    "oneshot",
                    "[BOLD] 76.67",
                    "[BOLD] 16.5",
                    "576.7",
                    "45.4",
                    "709M",
                    "61.4 ( [ITALIC] 76.57) / 36.4 ( [ITALIC] 76.63)"
                ],
                [
                    "[ITALIC]  [BOLD] HURRICANE (CPU)",
                    "oneshot",
                    "[BOLD] 74.59",
                    "80.1",
                    "[BOLD] 301.3",
                    "38.9",
                    "327M",
                    "57.7 ( [ITALIC] 74.59) / 33.9 ( [ITALIC] 74.59)"
                ],
                [
                    "[ITALIC]  [BOLD] HURRICANE (VPU)",
                    "oneshot",
                    "[BOLD] 75.13",
                    "390.8",
                    "645.3",
                    "[BOLD] 35.6",
                    "409M",
                    "45.0 ( [ITALIC] 74.63) / 20.8 ( [ITALIC] 75.13)"
                ]
            ],
            "title": "Table 3: Compared with various state-of-the-art efficient models on ImageNet, HURRICANE is the only NAS method that achieves high accuracy and low latency on all the target hardware. Results suggest FLOPs is not the accurate metric that indicates actual inference latency. As the hardware measurement settings in related NAS works are different, we measure the latency on our hardware platforms. \u2021: CPU latency is measured on a single CPU core with float32 precision. \u22c6: For fair comparison, we use the block search model instead of the block search+ channel search. \u2020: In the form \u201dx(y)\u201d, where \u201dx\u201d means the training time reduction and \u201dy\u201d means the accuracy achieved."
        },
        "insight": "Ta [CONTINUE] Com [CONTINUE] CANE improves the accuracy by 2.59% to 4.03% on all target hardware platforms. Compared to state-of-the-art models searched by NAS, HURRICANE achieves the lowest inference latency on DSP, CPU, VPU, with better or comparable accuracy. It demonstrates that it's essential to leverage hardware diversity in NAS to achieve the best efficiency on different hardware platforms. [CONTINUE] Finally, Table 3 shows none of existing efficient models are efficient on all the three target hardware. [CONTINUE] Compared with Singlepath-Oneshot, HURRICANE (Stage1 + Stage2) reduces 30.4% supernet training time and finds models with better performance. Furthermore, HURRICANE (Stage1) already achieves better classification accuracy than other NAS methods (loss \u2264 0.5% compared with our final result) but it could reduce an average 54.7% time, which is almost a 2X training time speedup. It demonstrates the effectiveness of exploring more architecture selections in the later layers."
    },
    {
        "id": "644",
        "table": {
            "header": [
                "[BOLD] Task",
                "[BOLD] Dataset",
                "[BOLD] Search Space",
                "[BOLD] Singlepath-Oneshot  [ITALIC] (20  [ITALIC] layers)",
                "[BOLD] Singlepath-Oneshot  [ITALIC] (20  [ITALIC] layers)",
                "[BOLD] HURRICANE  [ITALIC] ( [ITALIC] Stage1: 12 layers)",
                "[BOLD] HURRICANE  [ITALIC] ( [ITALIC] Stage1: 12 layers)",
                "[BOLD] HURRICANE  [ITALIC] ( [ITALIC] Stage1+ [ITALIC] Stage2: 20 layers)",
                "[BOLD] HURRICANE  [ITALIC] ( [ITALIC] Stage1+ [ITALIC] Stage2: 20 layers)"
            ],
            "rows": [
                [
                    "[BOLD] Task",
                    "[BOLD] Dataset",
                    "[BOLD] Search Space",
                    "[BOLD] Acc",
                    "[BOLD] Train",
                    "[BOLD] Acc",
                    "[BOLD] Train",
                    "[BOLD] Acc",
                    "[BOLD] Train"
                ],
                [
                    "[BOLD] Task",
                    "[BOLD] Dataset",
                    "[BOLD] Search Space",
                    "[BOLD] (%)",
                    "[BOLD] iters (#)",
                    "[BOLD] (%)",
                    "[BOLD] iters (#)",
                    "[BOLD] (%)",
                    "[BOLD] iters (#)"
                ],
                [
                    "1",
                    "ImageNet",
                    "Manually-designed\u00a0",
                    "73.72",
                    "144,360",
                    "74.01",
                    "72,180",
                    "74.16",
                    "105,864"
                ],
                [
                    "2",
                    "OUI",
                    "Manually-designed\u00a0",
                    "86.41",
                    "235,800",
                    "86.44",
                    "112,660",
                    "86.90",
                    "133,620"
                ],
                [
                    "3",
                    "OUI",
                    "Hardware-aware (DSP)",
                    "87.22",
                    "569,850",
                    "86.56",
                    "128,380",
                    "87.62",
                    "150,650"
                ],
                [
                    "4",
                    "OUI",
                    "Hardware-aware (CPU)",
                    "87.02",
                    "476,840",
                    "86.75",
                    "144,100",
                    "87.33",
                    "168,990"
                ],
                [
                    "5",
                    "OUI",
                    "Hardware-aware (VPU)",
                    "86.99",
                    "524,000",
                    "86.93",
                    "133,620",
                    "87.07",
                    "157,200"
                ]
            ],
            "title": "Table 5: Compared to Singlepath-Oneshot\u00a0[9], our two-stage search method achieves higher accuracy with much less search cost (26.7%-73.6%) on both manually-designed and hardware-aware search spaces. We list out the training iterations on ImageNet (batchsize=1,024) and OUI (batchsize=64) for search cost comparison."
        },
        "insight": "To further demonstrate the effectiveness of two-stage search method, we compare it with Singlepath-Oneshot on a sequence of tasks in Table 5. [CONTINUE] Table 5 summarizes experiment results. The searched models outperform the manual designed light-weight models, such as MobileNetV2 (top-1 acc: 72.00% on ImageNet, 85.67% on OUI-Adience-Age). For each task, our proposed method could achieve not only higher accuracy but also less search cost. [CONTINUE] As shown in Table 5, in most tasks, only one step search (Stage1) of HURRICANE could achieve a comparable top1 accuracy (or even better in some tasks), but the number of training iterations is significantly reduced (50%-77.5%). [CONTINUE] If the computation budget (e.g. training time) allows, HURRICANE can benefit from the second step search (Stage2). The accuracy is improved by 0.15%-1.06% with an additional cost of only 4.0%-23.3% of training iterations."
    },
    {
        "id": "645",
        "table": {
            "header": [
                "Source of  [ITALIC] wi to form  [ITALIC] x={ [ITALIC] w1, [ITALIC] w2}",
                "Network after the encoder",
                "Accuracy [%]  [ITALIC] y<15",
                "Accuracy [%]  [ITALIC] y\u226515"
            ],
            "rows": [
                [
                    "MNIST training dataset",
                    "SEQL",
                    "92",
                    "87"
                ],
                [
                    "MNIST training dataset",
                    "ReLU",
                    "93",
                    "0.8"
                ],
                [
                    "MNIST test dataset",
                    "SEQL",
                    "91",
                    "83"
                ],
                [
                    "MNIST test dataset",
                    "ReLU",
                    "92",
                    "0.6"
                ]
            ],
            "title": "TABLE II: MNIST Arithmetic Generalization Results"
        },
        "insight": "Generalization results of the network are shown in Table II. [CONTINUE] First, the most significant result is the difference between the accuracy evaluated on pairs y < 15 and pairs y \u2265 15. For the architecture with the EQL network, the accuracy drops by a few percentage points. However, for [CONTINUE] the architecture where the SEQL network is replaced by the commonly used fully-connected network with ReLU activation functions (which we label as \"ReLU\"), the accuracy drops to below 1% showing that the results of the SEQL is able to generalize reasonably well in a regime where the ReLU cannot generalize at all."
    },
    {
        "id": "646",
        "table": {
            "header": [
                "Comparison Between DGD, APC and Algorithm\u00a0 1  with Optimal Parameters on  [ITALIC] \u201cbcsstm07\u201d; regarding (a) Number of Iterations to Attain a Relative Estimation Error 10\u22124,",
                "Comparison Between DGD, APC and Algorithm\u00a0 1  with Optimal Parameters on  [ITALIC] \u201cbcsstm07\u201d; regarding (a) Number of Iterations to Attain a Relative Estimation Error 10\u22124,",
                "Comparison Between DGD, APC and Algorithm\u00a0 1  with Optimal Parameters on  [ITALIC] \u201cbcsstm07\u201d; regarding (a) Number of Iterations to Attain a Relative Estimation Error 10\u22124,",
                "Comparison Between DGD, APC and Algorithm\u00a0 1  with Optimal Parameters on  [ITALIC] \u201cbcsstm07\u201d; regarding (a) Number of Iterations to Attain a Relative Estimation Error 10\u22124,"
            ],
            "rows": [
                [
                    "and (b) Decay rate of the Instantaneous Error-norm. Here,  [ITALIC] cond( [ITALIC] ATA)=5.8\u2217107.",
                    "and (b) Decay rate of the Instantaneous Error-norm. Here,  [ITALIC] cond( [ITALIC] ATA)=5.8\u2217107.",
                    "and (b) Decay rate of the Instantaneous Error-norm. Here,  [ITALIC] cond( [ITALIC] ATA)=5.8\u2217107.",
                    "and (b) Decay rate of the Instantaneous Error-norm. Here,  [ITALIC] cond( [ITALIC] ATA)=5.8\u2217107."
                ],
                [
                    "Algorithm",
                    "DGD",
                    "APC",
                    "Algo.\u00a0 1  ( [ITALIC] \u03b2=5, [ITALIC] K(\u22121)= [ITALIC] On\u00d7 [ITALIC] n)"
                ],
                [
                    "(a) Iterations needed",
                    ">105",
                    "4.85\u2217104",
                    "2.11\u2217104"
                ],
                [
                    "(b) Rate of Decrease",
                    "0.9999",
                    "0.9672",
                    "0.9583+4.3\u2217108\u2217(0.9999) [ITALIC] t+1"
                ]
            ],
            "title": "TABLE I:"
        },
        "insight": "the convergence speed of our proposed algorithm is faster than the accelerated projection-based consensus (APC) method proposed in . [CONTINUE] \u2217(cid:107) ) of 10 We compare the number of iterations by DGD, APC and Algorithm 1 to reach a relative estimation error (defined as (cid:107)x(t ) \u2212 x \u22124 (ref. Table I). (cid:107)x\u2217(cid:107) [CONTINUE] The tuning parameter \u03b2 has been set at 5 for Algorithm 1. The rest of the algorithm parameters have been set the respective algorithms will have their smallest possible convergence rates. Specifically, (\u03b1\u2217 = 3.17 \u2217 10 \u22127, \u03b4\u2217 = 1.95) for Algorithm 1, (\u03b3\u2217 = 1.08, \u03b7\u2217 = 12.03) for APC and \u03b4\u2217 = 3.17 \u2217 10 \u22127 for DGD. such that [CONTINUE] Algorithm 1 performs the fastest among these algorithms. [CONTINUE] test the algorithm on a real data-set with a significant condition number (cond (AT A) = 5.8 \u2217 107) and get much better performance compared to the classical distributed gradient algorithm as well as the recently proposed accelerated projection-consensus algorithm (APC), regarding the number of iterations needed for convergence to the true solution."
    },
    {
        "id": "647",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] LAC128",
                "[ITALIC] LAC256",
                "[ITALIC] LAC512",
                "[ITALIC] LAC1 [ITALIC] K"
            ],
            "rows": [
                [
                    "AlexNet",
                    "2.03",
                    "2.44",
                    "2.92",
                    "1.88"
                ],
                [
                    "GoogLeNet",
                    "1.84",
                    "2.32",
                    "2.76",
                    "1.75"
                ],
                [
                    "VGG_S",
                    "2.43",
                    "3.04",
                    "3.63",
                    "2.31"
                ],
                [
                    "VGG_M",
                    "2.18",
                    "2.73",
                    "3.26",
                    "2.02"
                ],
                [
                    "AlexNet-Sparse",
                    "2.81",
                    "2.91",
                    "3.49",
                    "2.19"
                ],
                [
                    "ResNet-Sparse",
                    "1.69",
                    "2.02",
                    "2.41",
                    "1.61"
                ],
                [
                    "Geomean",
                    "2.13",
                    "2.55",
                    "3.05",
                    "1.95"
                ]
            ],
            "title": "Table 2: Laconic energy efficiency relative to BASE2K."
        },
        "insight": "Table 2 summarizes the energy efficiency of various LAC configurations over BASE2K. On average over all networks LAC128, LAC256, LAC512, and LAC1K are 2.13\u00d7, 2.55\u00d7, 3.05\u00d7, and 1.95\u00d7 more energy efficient than BASE2K."
    },
    {
        "id": "648",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Source \u2192  [BOLD] Target",
                "[BOLD] # labeled",
                "[BOLD] Accuracy"
            ],
            "rows": [
                [
                    "Standard",
                    "MNIST / notMNIST",
                    "50",
                    "34.02%"
                ],
                [
                    "TE",
                    "MNIST / notMNIST",
                    "50",
                    "37.28%"
                ],
                [
                    "Mk-MMD",
                    "MNIST / notMNIST",
                    "50",
                    "46.72%"
                ],
                [
                    "Lautum",
                    "MNIST / notMNIST",
                    "50",
                    "47.96%"
                ],
                [
                    "Lautum + TE",
                    "MNIST / notMNIST",
                    "50",
                    "66.91%"
                ],
                [
                    "Standard",
                    "MNIST / notMNIST",
                    "100",
                    "57.58%"
                ],
                [
                    "TE",
                    "MNIST / notMNIST",
                    "100",
                    "61.45%"
                ],
                [
                    "Mk-MMD",
                    "MNIST / notMNIST",
                    "100",
                    "63.32%"
                ],
                [
                    "Lautum",
                    "MNIST / notMNIST",
                    "100",
                    "65.21%"
                ],
                [
                    "Lautum + TE",
                    "MNIST / notMNIST",
                    "100",
                    "77.32%"
                ],
                [
                    "Standard",
                    "MNIST / notMNIST",
                    "200",
                    "67.78%"
                ],
                [
                    "TE",
                    "MNIST / notMNIST",
                    "200",
                    "74.87%"
                ],
                [
                    "Mk-MMD",
                    "MNIST / notMNIST",
                    "200",
                    "80.35%"
                ],
                [
                    "Lautum",
                    "MNIST / notMNIST",
                    "200",
                    "83.77%"
                ],
                [
                    "Lautum + TE",
                    "MNIST / notMNIST",
                    "200",
                    "85.25%"
                ]
            ],
            "title": "TABLE I: target test set accuracy comparison between standard transfer learning, Temporal Ensembling (TE), Mk-MMD, Lautum regularization and both TE and Lautum regularization for different amounts of labeled training target samples, MNIST \u2192 notMNIST."
        },
        "insight": "We examine five different methods of transfer learning: (1) standard supervised transfer which uses the labeled samples only. (2) Temporal Ensembling semi-supervised learning as outlined in , applied in a transfer learning setting. Temporal Ensembling is applied in the post-transfer training stage. (3) Mk-MMD , which is based on 19 different Gaussian kernels with different standard deviations. Mk-MMD is applied in the pre-transfer training stage. (4) Lautum regularization our technique as described in Section IV. (5) Both Temporal Ensembling and Lautum regularization. Note that Temporal Ensembling is applied in the post-transfer training stage, whereas Lautum regularization is applied in the pre-transfer training stage. [CONTINUE] For each of the five transfer learning methods we examined three different splits of the notMNIST training dataset which consists of 200,000 samples to an unlabeled part and a labeled part: (1) unlabeled part of 199,950 samples and a labeled part of 50 samples; (2) unlabeled part of 199,900 samples and a labeled part of 100 samples; (3) unlabeled part of 199,800 samples and a labeled part of 200 samples. [CONTINUE] Using the settings outlined above we obtained the results shown in Table [CONTINUE] for the MNIST \u2192 notMNIST case. [CONTINUE] The advantage of using Lautum regularization is evident from the results, as it outperforms the other compared methods in all the examined target training set splits. [CONTINUE] In general, the Temporal Ensembling method by itself does not yield very competitive results compared to standard transfer learning. However, the combination of Lautum regularization and Temporal Ensembling provides the best target test set accuracy over all the examined target training set splits."
    },
    {
        "id": "649",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Source \u2192  [BOLD] Target",
                "[BOLD] # labeled",
                "[BOLD] Accuracy"
            ],
            "rows": [
                [
                    "Standard",
                    "CIFAR-10 / 100",
                    "100",
                    "39.90%"
                ],
                [
                    "TE",
                    "CIFAR-10 / 100",
                    "100",
                    "42.20%"
                ],
                [
                    "Mk-MMD",
                    "CIFAR-10 / 100",
                    "100",
                    "45.30%"
                ],
                [
                    "Lautum",
                    "CIFAR-10 / 100",
                    "100",
                    "46.70%"
                ],
                [
                    "Lautum + TE",
                    "CIFAR-10 / 100",
                    "100",
                    "46.90%"
                ],
                [
                    "Standard",
                    "CIFAR-10 / 100",
                    "200",
                    "52.80%"
                ],
                [
                    "TE",
                    "CIFAR-10 / 100",
                    "200",
                    "54.60%"
                ],
                [
                    "Mk-MMD",
                    "CIFAR-10 / 100",
                    "200",
                    "59.30%"
                ],
                [
                    "Lautum",
                    "CIFAR-10 / 100",
                    "200",
                    "60.90%"
                ],
                [
                    "Lautum + TE",
                    "CIFAR-10 / 100",
                    "200",
                    "60.40%"
                ],
                [
                    "Standard",
                    "CIFAR-10 / 100",
                    "500",
                    "64.50%"
                ],
                [
                    "TE",
                    "CIFAR-10 / 100",
                    "500",
                    "66.50%"
                ],
                [
                    "Mk-MMD",
                    "CIFAR-10 / 100",
                    "500",
                    "68.00%"
                ],
                [
                    "Lautum",
                    "CIFAR-10 / 100",
                    "500",
                    "70.80%"
                ],
                [
                    "Lautum + TE",
                    "CIFAR-10 / 100",
                    "500",
                    "70.30%"
                ]
            ],
            "title": "TABLE II: target test set accuracy comparison between standard transfer learning, Temporal Ensembling (TE), Mk-MMD, Lautum regularization and both TE and Lautum regularization for different amounts of labeled training target samples, CIFAR-10 \u2192 CIFAR-100 (10 classes)."
        },
        "insight": "We examined the same five transfer learning techniques as in the MNIST \u2192 notMNIST case, where for each we examined three different splits of the CIFAR-100 (10 classes) training dataset to an unlabeled part and a labeled part: (1) unlabeled part of 4,900 samples and a labeled part of 100 samples; (2) unlabeled part of 4,800 samples and a labeled part of 200 samples; (3) unlabeled part of 4,500 samples and a labeled part of 500 samples. [CONTINUE] Using the settings outlined above we obtained the results shown in Table II for the CIFAR-10 \u2192 CIFAR-100 (10 classes) case. [CONTINUE] It is evident from the results that in the CIFAR10 \u2192 CIFAR-100 (10 classes) case as well, using Lautum regularization improves the post-transfer performance on the target test set and outperforms Temporal Ensembling and Mk-MMD. [CONTINUE] Note, however, that in the case of 200 and 500 labeled samples, applying Lautum regularization only, without Temporal Ensembling, slightly outperforms the combination of the two methods."
    },
    {
        "id": "650",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Simple Traffic Control (4 cars) reward",
                "[BOLD] Simple Traffic Control (4 cars) delay",
                "[BOLD] Simple Traffic Control (4 cars) # C",
                "[BOLD] Simple Traffic Control (4 cars) message",
                "[BOLD] Moderate Traffic Control (8 cars) reward",
                "[BOLD] Moderate Traffic Control (8 cars) delay",
                "[BOLD] Moderate Traffic Control (8 cars) # C",
                "[BOLD] Moderate Traffic Control (8 cars) message",
                "[BOLD] Complex Traffic Control (16 cars) reward",
                "[BOLD] Complex Traffic Control (16 cars) delay",
                "[BOLD] Complex Traffic Control (16 cars) # C",
                "[BOLD] Complex Traffic Control (16 cars) message"
            ],
            "rows": [
                [
                    "CommNet",
                    "-129.2",
                    "45.6",
                    "2.3",
                    "100.0%",
                    "-573.4",
                    "61.6",
                    "6.8",
                    "100.0%",
                    "-4278.8",
                    "82.1",
                    "16.5",
                    "100.0%"
                ],
                [
                    "AMP",
                    "-97.1",
                    "41.8",
                    "[BOLD] 1.2",
                    "100.0%",
                    "-1950.6",
                    "100.0",
                    "[BOLD] 0.9",
                    "100.0%",
                    "-6391.5",
                    "100.0",
                    "[BOLD] 2.1",
                    "100.0%"
                ],
                [
                    "ACML",
                    "[BOLD] -37.6",
                    "[BOLD] 31.2",
                    "1.9",
                    "100.0%",
                    "[BOLD] -103.5",
                    "[BOLD] 43.1",
                    "1.7",
                    "100.0%",
                    "[BOLD] -2824.9",
                    "[BOLD] 66.4",
                    "7.2",
                    "100.0%"
                ],
                [
                    "ACML-mean",
                    "-32.9",
                    "29.3",
                    "2.0",
                    "100.0%",
                    "-96.1",
                    "40.2",
                    "3.9",
                    "100.0%",
                    "-2661.5",
                    "61.4",
                    "9.4",
                    "100.0%"
                ],
                [
                    "ACML-attention",
                    "-24.5",
                    "28.8",
                    "1.6",
                    "100.0%",
                    "-91.8",
                    "39.6",
                    "1.3",
                    "100.0%",
                    "-2359.7",
                    "52.9",
                    "6.8",
                    "100.0%"
                ],
                [
                    "Gated-CommNet",
                    "-88.4",
                    "41.1",
                    "1.7",
                    "22.8%",
                    "-476.7",
                    "47.2",
                    "2.5",
                    "34.3%",
                    "-3529.4",
                    "73.0",
                    "15.7",
                    "29.3%"
                ],
                [
                    "Gated-AMP",
                    "-59.9",
                    "37.1",
                    "[BOLD] 1.3",
                    "[BOLD] 18.6%",
                    "-988.6",
                    "65.3",
                    "[BOLD] 1.6",
                    "[BOLD] 23.7%",
                    "-2870.5",
                    "65.5",
                    "[BOLD] 7.9",
                    "[BOLD] 19.1%"
                ],
                [
                    "Gated-ACML",
                    "[BOLD] -14.6",
                    "[BOLD] 21.0",
                    "2.4",
                    "23.9%",
                    "[BOLD] -69.4",
                    "[BOLD] 32.3",
                    "2.1",
                    "29.8%",
                    "[BOLD] -2101.1",
                    "[BOLD] 48.6",
                    "11.3",
                    "25.8%"
                ],
                [
                    "ATOC",
                    "-19.7",
                    "25.9",
                    "1.9",
                    "37.3%",
                    "-77.5",
                    "35.6",
                    "2.4",
                    "63.7%",
                    "-2481.2",
                    "54.8",
                    "14.9",
                    "[ITALIC] 112.5%"
                ]
            ],
            "title": "Table 1: The average results of 10 experiments on traffic control tasks. For models named as Gated-*, dynamic thresholds with \u03b2=0.8 are used. The \u201cdelay\u201d indicates the timesteps to complete the simulation. The \u201c# C\u201d indicates the number of collisions."
        },
        "insight": "We notice that ACML drives the cars with a relatively larger speed to complete the simulation with the smallest delay (i.e., 31.2, 43.1 and 66.4 for three scenarios, respectively). Besides, although its collision is not the smallest, ACML manages to achieve relatively smaller collisions (i.e., 1.9, 1.7 and 7.2 for three scenarios), since avoiding collision is also in favor of the total reward. [CONTINUE] For traffic control, ACML outperforms CommNet and AMP as shown by the upper part of Table 1. The reason is that ACML has found better tradeoffs between small delay and small collision. [CONTINUE] Both ACML-mean and ACML-attention improve ACML by a clear margin, and they perform much better than CommNet and AMP as shown in Table 1. [CONTINUE] For traffic control scenarios, as shown at the lower part of Table 1, Gated-ACML emits much fewer messages than ACML, which is expected. What surprises us is that GatedACML even obtains more rewards than ACML. [CONTINUE] Consequently, the delay of ACML is larger than that of Gated-ACML as shown in Table 1. [CONTINUE] We also modify ATOC to make it suitable for continuous action and heterogeneous agents. The results are shown at the last row of Table 1 and 2. ATOC works well in traffic control tasks, but performs badly in routing and wifi tasks."
    },
    {
        "id": "651",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Simple Routing reward",
                "[BOLD] Simple Routing message",
                "[BOLD] Moderate Routing reward",
                "[BOLD] Moderate Routing message",
                "[BOLD] Complex Routing reward",
                "[BOLD] Complex Routing message",
                "[BOLD] Simple WAPC. reward",
                "[BOLD] Simple WAPC. message",
                "[BOLD] Complex WAPC. reward",
                "[BOLD] Complex WAPC. message"
            ],
            "rows": [
                [
                    "CommNet",
                    "0.264",
                    "100.0%",
                    "0.164",
                    "100.0%",
                    "-",
                    "100.0%",
                    "0.652",
                    "100.0%",
                    "0.441",
                    "100.0%"
                ],
                [
                    "AMP",
                    "0.266",
                    "100.0%",
                    "0.185",
                    "100.0%",
                    "-",
                    "100.0%",
                    "0.627",
                    "100.0%",
                    "0.418",
                    "100.0%"
                ],
                [
                    "ACML",
                    "[BOLD] 0.317",
                    "100.0%",
                    "[BOLD] 0.263",
                    "100.0%",
                    "-",
                    "100.0%",
                    "[BOLD] 0.665",
                    "100.0%",
                    "[BOLD] 0.480",
                    "100.0%"
                ],
                [
                    "ACML-mean",
                    "0.321",
                    "100.0%",
                    "0.267",
                    "100.0%",
                    "-",
                    "100.0%",
                    "0.673",
                    "100.0%",
                    "0.493",
                    "100.0%"
                ],
                [
                    "ACML-attention",
                    "0.329",
                    "100.0%",
                    "0.271",
                    "100.0%",
                    "-",
                    "100.0%",
                    "0.689",
                    "100.0%",
                    "0.506",
                    "100.0%"
                ],
                [
                    "Gated-CommNet",
                    "0.232",
                    "35.2%",
                    "0.144",
                    "[BOLD] 21.7%",
                    "-",
                    "19.8%",
                    "0.595",
                    "53.1%",
                    "0.386",
                    "41.8%"
                ],
                [
                    "Gated-AMP",
                    "0.241",
                    "46.7%",
                    "0.170",
                    "35.0%",
                    "-",
                    "81.7%",
                    "0.539",
                    "57.2%",
                    "0.350",
                    "[BOLD] 32.3%"
                ],
                [
                    "Gated-ACML",
                    "0.288",
                    "[BOLD] 33.6%",
                    "[BOLD] 0.239",
                    "27.9%",
                    "-",
                    "22.6%",
                    "[BOLD] 0.610",
                    "[BOLD] 41.9%",
                    "[BOLD] 0.411",
                    "37.7%"
                ],
                [
                    "ATOC",
                    "[BOLD] 0.297",
                    "73.7%",
                    "0.102",
                    "[ITALIC] 104.6%",
                    "-",
                    "[ITALIC] 326.1%",
                    "0.418",
                    "[ITALIC] 136.5%",
                    "0.231",
                    "[ITALIC] 393.4%"
                ]
            ],
            "title": "Table 2: The average results of 10 experiments on packet routing and wifi access point configuration tasks. For models named as Gated-*, we adopt dynamic thresholds with \u03b2=0.8. The \u201cWAPC.\u201d is the abbreviation of Wifi Access Point Configuration."
        },
        "insight": "In addition, the results of packet routing and wifi access point configuration shown at the upper part of Table 2 can be analyzed similarly to demonstrate that the performance of ACML is consistent in several multi-agent scenarios. [CONTINUE] For packet routing and access point configuration scenarios, as shown at the lower part of Table 2, Gated-ACML can prune a lot of messages (e.g., about 70% \u2248 1 \u2212 33.6%) with little damage to the reward (e.g., about 10% \u2248 1 \u2212 0.288/0.317)."
    },
    {
        "id": "652",
        "table": {
            "header": [
                "[ITALIC] Tm%",
                "Simple Routing  [ITALIC] pruned message",
                "Simple Routing reward  [ITALIC] decrease",
                "Moderate Routing  [ITALIC] pruned message",
                "Moderate Routing reward  [ITALIC] decrease"
            ],
            "rows": [
                [
                    "10.0%",
                    "12.19%",
                    "[BOLD] -8.46%",
                    "11.60%",
                    "[BOLD] -7.03%"
                ],
                [
                    "20.0%",
                    "24.07%",
                    "[BOLD] -13.59%",
                    "22.77%",
                    "[BOLD] -12.14%"
                ],
                [
                    "30.0%",
                    "27.65%",
                    "[BOLD] -4.88%",
                    "29.98%",
                    "[BOLD] -3.25%"
                ],
                [
                    "70.0%",
                    "66.73%",
                    "9.27%",
                    "68.54%",
                    "10.06%"
                ],
                [
                    "80.0%",
                    "[BOLD] 79.14%",
                    "[BOLD] 14.01%",
                    "76.81%",
                    "13.25%"
                ],
                [
                    "90.0%",
                    "87.22%",
                    "18.60%",
                    "85.11%",
                    "19.50%"
                ],
                [
                    "100.0%",
                    "100.00%",
                    "59.35%",
                    "100.00%",
                    "65.42%"
                ]
            ],
            "title": "Table 3: The results of Gated-ACML in packet routing scenarios. We adopt a fixed threshold T=L\u0394Q(oi)[K\u00d7Tm%]."
        },
        "insight": "The results are shown in Table 3. It can be noticed that the number of pruned messages is close to the desired Tm% for both routing scenarios. [CONTINUE] In addition, as shown at the upper part of the table, when the quantity of pruned messages is smaller than 30%, the reward decrease is a negative value, which means that the reward is increased actually. [CONTINUE] However, as we are pruning more messages, the reward decrease is becoming larger and larger as shown at the lower part of the table, especially when all messages are pruned."
    },
    {
        "id": "653",
        "table": {
            "header": [
                "[EMPTY]",
                "BG+Hair",
                "Face",
                "Eyebr.",
                "Eyes",
                "Mouth"
            ],
            "rows": [
                [
                    "Bald",
                    "[BOLD] 3.675",
                    "3.000",
                    "1.080",
                    "1.400",
                    "[ITALIC] 0.984"
                ],
                [
                    "Bangs",
                    "3.051",
                    "[BOLD] 3.343",
                    "0.997",
                    "[ITALIC] 0.380",
                    "0.641"
                ],
                [
                    "Bla. Hair",
                    "[BOLD] 3.227",
                    "1.494",
                    "0.980",
                    "[ITALIC] 0.360",
                    "0.488"
                ],
                [
                    "Blo. Hair",
                    "[BOLD] 5.155",
                    "2.390",
                    "1.395",
                    "[ITALIC] 0.585",
                    "0.937"
                ],
                [
                    "Bro. Hair",
                    "[BOLD] 1.576",
                    "1.249",
                    "0.423",
                    "[ITALIC] 0.250",
                    "0.414"
                ],
                [
                    "B. Eyebr.",
                    "[BOLD] 1.605",
                    "0.576",
                    "1.086",
                    "[ITALIC] 0.267",
                    "0.303"
                ],
                [
                    "Glasses",
                    "1.421",
                    "[BOLD] 2.422",
                    "1.636",
                    "2.388",
                    "[ITALIC] 0.669"
                ],
                [
                    "Male",
                    "2.305",
                    "[BOLD] 3.052",
                    "1.979",
                    "1.877",
                    "[ITALIC] 0.932"
                ],
                [
                    "M. Open",
                    "0.416",
                    "1.035",
                    "0.663",
                    "[ITALIC] 0.243",
                    "[BOLD] 3.276"
                ],
                [
                    "Mustache",
                    "1.126",
                    "[BOLD] 2.956",
                    "1.495",
                    "[ITALIC] 1.057",
                    "1.557"
                ],
                [
                    "No Beard",
                    "1.520",
                    "[BOLD] 2.549",
                    "1.543",
                    "1.144",
                    "[ITALIC] 1.054"
                ],
                [
                    "Pale Skin",
                    "1.877",
                    "[BOLD] 3.623",
                    "2.000",
                    "[ITALIC] 1.292",
                    "2.609"
                ],
                [
                    "Young",
                    "[BOLD] 1.927",
                    "1.677",
                    "0.990",
                    "1.214",
                    "[ITALIC] 0.386"
                ]
            ],
            "title": "Table 1: L2-norm distances between the center of samples for which an attribute is true and the center of samples for which it is not true (cmp. Fig.\u00a08). The first 3 components of the PCA are used."
        },
        "insight": "The labels considered in this experiment can be found in Tab. 1. [CONTINUE] In Tab. 1, we show the distances based on the L2-norm between the mean 3-dimensional PCA-vectors of all samples in which a label is true, and those in which the label is false. We do that for all previously mentioned labels. The highest value per label is set bold, while the lowest is set italics. [CONTINUE] most information about the hair is found in the BG+hair subspace and for pale skin it is in the face. [CONTINUE] the Bushy Eyebrows label seems to have the most information in the BG+hair subspace."
    },
    {
        "id": "654",
        "table": {
            "header": [
                "[EMPTY]",
                "Reward"
            ],
            "rows": [
                [
                    "GP-PTR-IRL",
                    "9.51 \u00b1 4.92"
                ],
                [
                    "GP-ME-IRL [levine2011nonlinear]",
                    "9.58 \u00b1 4.90"
                ],
                [
                    "GP-Increasing-IRL [angelov2019composing]",
                    "7.39 \u00b1 5.72"
                ]
            ],
            "title": "TABLE I: Averaged total returns using VI policy trained using inferred reward from optimal demonstrations."
        },
        "insight": "Table [CONTINUE] shows the averaged total returns obtained for trials in environments when rewards are inferred from optimal demonstrations using the probabilistic temporal ranking3 (GPPTR-IRL), a Gaussian process maximum entropy approach  (GP-ME-IRL) and an increasing linear model assumption [CONTINUE] In the optimal demonstration case, policies obtained using both the maximum entropy and probabilistic temporal ranking approach perform equally well,"
    },
    {
        "id": "655",
        "table": {
            "header": [
                "[EMPTY]",
                "Reward"
            ],
            "rows": [
                [
                    "GP-PTR-IRL",
                    "7.42 \u00b1 4.82"
                ],
                [
                    "GP-ME-IRL [levine2011nonlinear]",
                    "3.31 \u00b1 4.24"
                ],
                [
                    "GP-Increasing-IRL [angelov2019composing]",
                    "2.77 \u00b1 4.30"
                ]
            ],
            "title": "TABLE II: Averaged total returns using VI policy trained using inferred reward from sub-optimal demonstrations."
        },
        "insight": "Table II shows the averaged total returns obtained for trials in environments when rewards are inferred from sub-optimal demonstrations using the probabilistic temporal ranking, the Gaussian process maximum entropy approach and the increasing linear model assumption. [CONTINUE] In this sub-optimal demonstration case, policies obtained using the maximum entropy approach regularly fail, while the probabilistic temporal ranking continues to perform relatively well."
    },
    {
        "id": "656",
        "table": {
            "header": [
                "[EMPTY]",
                "Train Category",
                "Train Category",
                "Train Category"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "37.1",
                    "23.7",
                    "8.3"
                ],
                [
                    "[EMPTY]",
                    "32.6",
                    "33.5",
                    "8.8"
                ],
                [
                    "[EMPTY]",
                    "30.9",
                    "18.8",
                    "33.4"
                ]
            ],
            "title": "Table 2: Cross-validation experiments for analyzing how part knowledge transfers across category boundaries."
        },
        "insight": "We analyze the capability of transferring part knowledge across category boundaries under our framework. Table 2 presents experimental results of doing cross-validation using chairs, tables and lamps by training on one category and testing on another."
    },
    {
        "id": "657",
        "table": {
            "header": [
                "Context",
                "[EMPTY]",
                "Seen Category",
                "Seen Category",
                "Seen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category"
            ],
            "rows": [
                [
                    "w/",
                    "L1",
                    "62.7",
                    "68.9",
                    "24.7",
                    "38.1",
                    "14.4",
                    "57.5",
                    "71.3",
                    "57.4",
                    "33.9",
                    "70"
                ],
                [
                    "w/",
                    "L2",
                    "47.6",
                    "57.5",
                    "20.8",
                    "-",
                    "12.3",
                    "-",
                    "-",
                    "-",
                    "27.9",
                    "-"
                ],
                [
                    "w/",
                    "L3",
                    "41.5",
                    "44.5",
                    "19.7",
                    "-",
                    "10.9",
                    "34.6",
                    "-",
                    "25.6",
                    "19",
                    "60.7"
                ],
                [
                    "w/",
                    "Avg",
                    "[BOLD] 50.6",
                    "[BOLD] 57",
                    "[BOLD] 21.7",
                    "38.1",
                    "[BOLD] 12.3",
                    "46.1",
                    "71.3",
                    "41.5",
                    "26.9",
                    "65.4"
                ],
                [
                    "w/o",
                    "L1",
                    "59.8",
                    "67",
                    "24.4",
                    "41.6",
                    "12",
                    "61.9",
                    "72.2",
                    "57.6",
                    "40.8",
                    "71.9"
                ],
                [
                    "w/o",
                    "L2",
                    "44.6",
                    "55.2",
                    "19.2",
                    "-",
                    "10.6",
                    "-",
                    "-",
                    "-",
                    "32.6",
                    "-"
                ],
                [
                    "w/o",
                    "L3",
                    "39.1",
                    "42.9",
                    "18.1",
                    "-",
                    "8.7",
                    "36.5",
                    "-",
                    "27.1",
                    "20.1",
                    "62.1"
                ],
                [
                    "w/o",
                    "Avg",
                    "47.8",
                    "55",
                    "20.6",
                    "[BOLD] 41.6",
                    "10.4",
                    "[BOLD] 49.2",
                    "[BOLD] 72.2",
                    "[BOLD] 42.4",
                    "[BOLD] 31.2",
                    "[BOLD] 67"
                ]
            ],
            "title": "Table 3: Quantitative evaluation of involving more context. w/ and w/o denote making decision with and without involving more context, respectively. Note that we only introduce more context in the late grouping process and the involved context is restricted in a very local region. The number is the mean recall of segmentation results. The L1, L2 and L3 refer to the three levels of segmentation defined in PartNet. Avg is the average among mean recall of three levels segmentation results."
        },
        "insight": "From the results listed in Table 3, we can point out that the involved context helps to consistently improve the performance on seen categories, but has negative effects on most unseen categories."
    },
    {
        "id": "658",
        "table": {
            "header": [
                "[EMPTY]",
                "Seen Category",
                "Seen Category",
                "Seen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category"
            ],
            "rows": [
                [
                    "no purity",
                    "38.6",
                    "36.8",
                    "5.6",
                    "30.3",
                    "7.0",
                    "29.4",
                    "63",
                    "21.1",
                    "10.9",
                    "53.5"
                ],
                [
                    "no rectification",
                    "38.4",
                    "36.4",
                    "5.5",
                    "29.7",
                    "6.9",
                    "27.5",
                    "57.6",
                    "22.1",
                    "10.3",
                    "52.8"
                ],
                [
                    "full-model",
                    "38.8",
                    "37.6",
                    "5.7",
                    "33.1",
                    "7.2",
                    "32.6",
                    "66.5",
                    "23.0",
                    "10.5",
                    "55.2"
                ]
            ],
            "title": "Table 4: Quantitative results of the components analysis. We train the models on the Chair, Lamp, Storage Furniture of level-3 annotations and test on the listed categories. The number is the mean recall of the most fine-grained annotations of each category."
        },
        "insight": "Here, we show the quantitative results and validate the effectiveness of these components. The results are listed in Table 4, where we train the model on the Chair, Lamp, Storage Furniture of level-3 annotations."
    },
    {
        "id": "659",
        "table": {
            "header": [
                "[EMPTY]",
                "Seen Category",
                "Seen Category",
                "Seen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category",
                "Unseen Category"
            ],
            "rows": [
                [
                    "PosAcc",
                    "94.1",
                    "95.8",
                    "91.7",
                    "88.7",
                    "86.7",
                    "97.1",
                    "97.1",
                    "88.2",
                    "87.7",
                    "90.6",
                    "84.3",
                    "88.3"
                ],
                [
                    "NegAcc",
                    "66.5",
                    "61.3",
                    "73.8",
                    "59.9",
                    "64.9",
                    "12.9",
                    "20.2",
                    "20.3",
                    "42",
                    "66",
                    "60.6",
                    "31.9"
                ],
                [
                    "[EMPTY]",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category",
                    "Unseen Category"
                ],
                [
                    "PosAcc",
                    "96.4",
                    "98.1",
                    "98.6",
                    "98.1",
                    "97",
                    "86.2",
                    "96.3",
                    "86.3",
                    "91.7",
                    "95.9",
                    "93.6",
                    "89.1"
                ],
                [
                    "NegAcc",
                    "39.4",
                    "40.7",
                    "3.8",
                    "33.7",
                    "77.6",
                    "42.8",
                    "45.5",
                    "46.9",
                    "62.3",
                    "67.7",
                    "31.1",
                    "36.2"
                ]
            ],
            "title": "Table 5: Quantitative evaluation of the sub-part proposal module. PosAcc and NegAcc refer to positive accuracy and negative accuracy of the binary segmentation."
        },
        "insight": "To validate the transferring performance of this module, we train the module on Chair, Storage Furniture, and Lamp of level-3 annotations and test on all categories with evaluating by the most fine-grained level annotations of each category. The results are listed in Table 5."
    },
    {
        "id": "660",
        "table": {
            "header": [
                "[BOLD] Algorithm",
                "[BOLD] Dataset Movielens",
                "[BOLD] Dataset DBLP",
                "[BOLD] Dataset MIT",
                "[BOLD] Dataset Yelp",
                "[BOLD] Dataset Douban"
            ],
            "rows": [
                [
                    "DeepWalk",
                    "0.847",
                    "0.794",
                    "0.899",
                    "0.842",
                    "0.687"
                ],
                [
                    "HARP(DW)",
                    "0.817",
                    "0.659",
                    "0.902",
                    "0.743",
                    "0.559"
                ],
                [
                    "HSRL(DW)",
                    "[BOLD] 0.879\u2020",
                    "[BOLD] 0.847\u2020",
                    "[BOLD] 0.926\u2020",
                    "[BOLD] 0.901\u2020",
                    "[BOLD] 0.842\u2020"
                ],
                [
                    "Gain of HSRL(%)",
                    "[BOLD] 3.6",
                    "[BOLD] 6.3",
                    "[BOLD] 2.9",
                    "[BOLD] 6.5",
                    "[BOLD] 18.4"
                ],
                [
                    "node2vec",
                    "0.843",
                    "0.673",
                    "0.843",
                    "0.742",
                    "0.569"
                ],
                [
                    "HARP(N2V)",
                    "0.828",
                    "0.647",
                    "0.879",
                    "0.708",
                    "0.552"
                ],
                [
                    "HSRL(N2V)",
                    "[BOLD] 0.865\u2020",
                    "[BOLD] 0.840\u2020",
                    "[BOLD] 0.921\u2020",
                    "[BOLD] 0.892\u2020",
                    "[BOLD] 0.819\u2020"
                ],
                [
                    "Gain of HSRL(%)",
                    "[BOLD] 2.5",
                    "[BOLD] 19.9",
                    "[BOLD] 8.5",
                    "[BOLD] 16.8",
                    "[BOLD] 30.5"
                ],
                [
                    "LINE",
                    "0.613",
                    "0.641",
                    "0.814",
                    "0.752",
                    "0.624"
                ],
                [
                    "HARP(LINE)",
                    "0.220",
                    "0.387",
                    "0.702",
                    "0.306",
                    "0.399"
                ],
                [
                    "HSRL(LINE)",
                    "[BOLD] 0.735\u2020",
                    "[BOLD] 0.664\u2020",
                    "[BOLD] 0.819",
                    "[BOLD] 0.799\u2020",
                    "[BOLD] 0.756\u2020"
                ],
                [
                    "Gain of HSRL(%)",
                    "[BOLD] 16.6",
                    "[BOLD] 3.5",
                    "[BOLD] 0.5",
                    "[BOLD] 5.9",
                    "[BOLD] 17.5"
                ]
            ],
            "title": "TABLE II: AUC of link prediction."
        },
        "insight": "Each experiment is independently implemented for 20 times and the average performances on the testing set are reported in Table II. [CONTINUE] HSRL significantly outperforms all baselines on all datasets. For the small and sparse network movielens, the improvements of HSRL(DW), HSRL(N2V), and HSRL(LINE) are 3.6%, 2.5%, and 16.6% respectively. For the dense network MIT, HSRL(DW), HSRL(N2V), and HSRL(LINE) outperform the baselines by 2.9%, 8.5%, and 0.5%. For three large networks, DBLP, Yelp, and Douban, the improvement of HSRL is striking: the improvements of HSRL(DW), HSRL(N2V), and HSRL(LINE) are 6.3%, 19.9%, 3.5% for DBLP, 6.5%, 16.8%, 5.9% for Yelp, and 18.4%, 30.5%, 17.5% for Douban. [CONTINUE] The results of HARP on movielens, DBLP, Yelp, and Douban are worse than the original NRL methods. Moreover, the performance of HARP(LINE) is drastically worse than LINE. It only works better than DeepWalk, node2vec on MIT which is a small and dense network. [CONTINUE] The compressed networks generated by HARP on a network could not reveal its global topologies. Hence, using node embeddings of compressed networks as initialization could mislead the NRL methods to a bad local minimum. Such an issue could occur especially when the input network is large-scale and the objective function of the NRL method is highly non-convex, e.g., LINE. [CONTINUE] The improvements of HSRL on DBLP, Yelp, and Duban are larger than that on Movielens and MIT. It demonstrates that HSRL works much better than baselines on large-scale networks."
    },
    {
        "id": "661",
        "table": {
            "header": [
                "Learning algorithm",
                "Maximal disturbance in  [ITALIC] Ns Sagittal",
                "Maximal disturbance in  [ITALIC] Ns Lateral"
            ],
            "rows": [
                [
                    "TRPO",
                    "240",
                    "78"
                ],
                [
                    "DDPG",
                    "75",
                    "160"
                ],
                [
                    "PPO",
                    "192",
                    "36"
                ],
                [
                    "Baseline from ( 6 )",
                    "53",
                    "78"
                ]
            ],
            "title": "TABLE I: Maximal rejectable impulses for the various learning algorithms without taking steps."
        },
        "insight": "Due to the structure and choice of our framework, the learned policy is independent of the type of learning algorithm. We trained a policy for maintaining balance via TRPO, PPO, and DDPG, and found similar resulting behavior (cf. Table I). All four balancing strategies (Fig. 1) emerges regardless of the DRL algorithm used. However, from our simulations, TRPO is able to achieve higher rewards and is able to withstand higher impulses. Figure 5 shows the learning curves for the policies learned in Table I."
    },
    {
        "id": "662",
        "table": {
            "header": [
                "[EMPTY]",
                "A: Wieber 2006a [cite:wieber2006TrajectoryFree]",
                "B: Wieber 2006b [WIEBER2006559]",
                "C: Stephens 2010 [cite:stephens2010PushRecovery]",
                "D: Urata 2011 [cite:urata2011OnlineDecision]",
                "E: Sagittal push w/o foot stepping",
                "F: Sagittal push w/ foot stepping",
                "G: Lateral push w/o foot stepping"
            ],
            "rows": [
                [
                    "Robot",
                    "HRP-2",
                    "Biped model",
                    "Sarcos Primus",
                    "HRP3L-JSK",
                    "Valkyrie",
                    "Valkyrie",
                    "Valkyrie"
                ],
                [
                    "Robot height [ [ITALIC] m]",
                    "1.539",
                    "1.425",
                    "1.575",
                    "[EMPTY]",
                    "1.8",
                    "1.8",
                    "1.8"
                ],
                [
                    "CoM height [ [ITALIC] m]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "0.803",
                    "1.1",
                    "1.1",
                    "1.1"
                ],
                [
                    "Mass [ [ITALIC] kg]",
                    "58",
                    "40",
                    "92",
                    "53",
                    "137",
                    "137",
                    "137"
                ],
                [
                    "Force [ [ITALIC] N]",
                    "1500",
                    "750.0",
                    "[EMPTY]",
                    "597",
                    "600",
                    "2000",
                    "650"
                ],
                [
                    "Interval [ [ITALIC] s]",
                    "0.025",
                    "0.025",
                    "[EMPTY]",
                    "0.05",
                    "0.12",
                    "0.12",
                    "0.12"
                ],
                [
                    "Impulse [ [ITALIC] Ns]",
                    "37.5",
                    "18.8",
                    "42.0",
                    "29.9",
                    "72.0",
                    "240.0",
                    "78.0"
                ],
                [
                    "Normalized impulse [ [ITALIC] Nskg]",
                    "0.6",
                    "0.52",
                    "0.52",
                    "0.6",
                    "0.57",
                    "1.73",
                    "0.56"
                ],
                [
                    "Stepping",
                    "No",
                    "No",
                    "Yes",
                    "Yes",
                    "No",
                    "Yes",
                    "No"
                ],
                [
                    "Simulated",
                    "Yes",
                    "Yes",
                    "No",
                    "No",
                    "Yes",
                    "Yes",
                    "Yes"
                ]
            ],
            "title": "TABLE IV: Push disturbance from various push recovery studies"
        },
        "insight": "The normalized impulse is used for comparison between the controllers of other works , , ,  and the learned policy (Table IV). We compared sagittal and lateral pushes. For the sagittal push, two impulses are chosen such that foot stepping occurs for the larger impulse (1.73N s/kg), while the smaller impulse (0.57N s/kg) will result in a strategy without stepping. By comparing the rejectable normalized impulse of the strategies not taking a step (A, B, E, G), it can be seen that our policy performs similar (E: 0.57N s/kg, G: 0.56N s/kg) to the other controllers (A: 0.6N s/kg, B: 0.52N s/kg). For the strategies taking a step (C, D, F), our policy is able to perform better (F: 1.73N s/kg) than the stepping controllers [CONTINUE] (0.52N s/kg) and D (0.6N s/kg). Albeit our results are obtained from simulation whereas [CONTINUE] and D are obtained from real experiments, we show in the next section that the generated motions are realistic and within the real physical constraints."
    },
    {
        "id": "663",
        "table": {
            "header": [
                "[EMPTY]",
                "Peak joint torque [ [ITALIC] N] Torso pitch",
                "Peak joint torque [ [ITALIC] N] Hip pitch",
                "Peak joint torque [ [ITALIC] N] Hip Roll",
                "Peak joint torque [ [ITALIC] N] Knee pitch",
                "Peak joint torque [ [ITALIC] N] Ankle Pitch",
                "Peak joint torque [ [ITALIC] N] Ankle Roll",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Torso pitch",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Hip pitch",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Hip Roll",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Knee pitch",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Ankle Pitch",
                "Peak joint velocity [ [ITALIC] rad/ [ITALIC] s] Ankle Roll"
            ],
            "rows": [
                [
                    "Joint limit",
                    "150",
                    "350",
                    "350",
                    "350",
                    "205",
                    "205",
                    "9.00",
                    "6.11",
                    "6.11",
                    "11.00",
                    "11.00",
                    "11.00"
                ],
                [
                    "Nominal standing",
                    "68.4",
                    "39.5",
                    "57.1",
                    "122",
                    "44.9",
                    "46.2",
                    "0.0",
                    "0.0",
                    "0.0",
                    "0.0",
                    "0.0",
                    "0.0"
                ],
                [
                    "0.55m Drop",
                    "150",
                    "221",
                    "350",
                    "350",
                    "205",
                    "205",
                    "4.69",
                    "2.01",
                    "6.11",
                    "9.95",
                    "11.0",
                    "11.0"
                ],
                [
                    "78 [ITALIC] Ns Pelvis (lateral)",
                    "104",
                    "147",
                    "103",
                    "264",
                    "117",
                    "106",
                    "0.15",
                    "0.58",
                    "0.46",
                    "1.09",
                    "0.96",
                    "11.0"
                ],
                [
                    "240 [ITALIC] Ns Pelvis (sagittal)",
                    "150",
                    "187",
                    "350",
                    "350",
                    "205",
                    "205",
                    "6.49",
                    "1.47",
                    "6.11",
                    "5.69",
                    "11.0",
                    "11.0"
                ]
            ],
            "title": "TABLE V: Peak torques and velocities for different scenarios."
        },
        "insight": "Table V compares the peak torques and velocities for different scenarios. The chosen scenarios require the largest joint torque for dropping and the largest joint velocities for taking multiple steps due to large pushes at the pelvis. All other presented test cases required less joint torque and velocity than the the ones presented in Table V."
    },
    {
        "id": "664",
        "table": {
            "header": [
                "[ITALIC] Tpred=3.2 [ITALIC] s",
                "Model \\Dataset",
                "ETH Hotel",
                "ETH Univ",
                "UCY Zara01",
                "UCY Zara02",
                "Average"
            ],
            "rows": [
                [
                    "Average",
                    "LSTM",
                    "0.039\u00b1 0.001",
                    "0.146\u00b1 0.006",
                    "0.020\u00b1 0.000",
                    "[BOLD] 0.024\u00b1 0.000",
                    "0.057 \u00b1 0.002"
                ],
                [
                    "Displacement",
                    "S-LSTM",
                    "0.041\u00b1 0.002",
                    "0.133\u00b1 0.007",
                    "0.020\u00b1 0.000",
                    "0.026\u00b1 0.000",
                    "0.055\u00b1 0.002"
                ],
                [
                    "Error",
                    "[BOLD] RDB (ours)",
                    "[BOLD] 0.037\u00b1 0.001",
                    "[BOLD] 0.113\u00b1 0.002",
                    "[BOLD] 0.017\u00b1 0.000",
                    "[BOLD] 0.024\u00b1 0.000",
                    "[BOLD] 0.048\u00b1 0.001"
                ],
                [
                    "Final",
                    "LSTM",
                    "0.093\u00b1 0.003",
                    "0.041\u00b1 0.002",
                    "0.056\u00b1 0.001",
                    "0.066\u00b1 0.001",
                    "0.064 \u00b1 0.002"
                ],
                [
                    "Displacement",
                    "S-LSTM",
                    "0.101\u00b1 0.003",
                    "0.036\u00b1 0.002",
                    "0.062\u00b1 0.002",
                    "0.071\u00b1 0.002",
                    "0.068\u00b1 0.004"
                ],
                [
                    "Error",
                    "[BOLD] RDB (ours)",
                    "[BOLD] 0.086\u00b1 0.003",
                    "[BOLD] 0.029\u00b1 0.001",
                    "[BOLD] 0.049\u00b1 0.001",
                    "0.068 \u00b1 0.003",
                    "[BOLD] 0.058\u00b1 0.003"
                ],
                [
                    "[ITALIC] Tpred=4.8 [ITALIC] s",
                    "Model \\Dataset",
                    "ETH Hotel",
                    "ETH Univ",
                    "UCY Zara01",
                    "UCY Zara02",
                    "Average"
                ],
                [
                    "Average",
                    "LSTM",
                    "0.082\u00b1 0.003",
                    "0.245\u00b1 0.021",
                    "0.053\u00b1 0.000",
                    "0.055\u00b1 0.002",
                    "0.109\u00b1 0.007"
                ],
                [
                    "Displacement",
                    "Social LSTM",
                    "0.080\u00b1 0.004",
                    "0.235\u00b1 0.019",
                    "0.049\u00b1 0.001",
                    "0.066\u00b1 0.003",
                    "0.108\u00b1 0.007"
                ],
                [
                    "Error",
                    "[BOLD] RDB (ours)",
                    "[BOLD] 0.073\u00b1 0.003",
                    "[BOLD] 0.182\u00b1 0.009",
                    "[BOLD] 0.046\u00b1 0.000",
                    "0.063\u00b1 0.003",
                    "[BOLD] 0.091\u00b1 0.004"
                ],
                [
                    "Final",
                    "LSTM",
                    "0.207\u00b1 0.009",
                    "0.051\u00b1 0.004",
                    "0.160\u00b1 0.003",
                    "0.163\u00b1 0.005",
                    "0.145\u00b1 0.005"
                ],
                [
                    "Displacement",
                    "Social LSTM",
                    "0.184\u00b1 0.011",
                    "0.049\u00b1 0.003",
                    "0.196\u00b1 0.004",
                    "0.197\u00b1 0.008",
                    "0.157\u00b1 0.007"
                ],
                [
                    "Error",
                    "[BOLD] RDB (ours)",
                    "[BOLD] 0.164\u00b1 0.009",
                    "[BOLD] 0.033\u00b1 0.003",
                    "[BOLD] 0.136\u00b1 0.005",
                    "0.190\u00b1 0.007",
                    "[BOLD] 0.131\u00b1 0.006"
                ]
            ],
            "title": "Table 1: Reported results assume observation length of Tobs=1.6sec and prediction lengths of Tpred=3.2sec and Tpred=4.8sec. First 4 rows correspond to the Average Displacement Error and last 4 to the final displacement error. Each result represents an average obtained from 5 independent runs with different random seeds."
        },
        "insight": "The proposed solution outperforms the alternatives. [CONTINUE] Table 1 shows the results obtained on this task. [CONTINUE] it can be observed that the variability of the predicted trajectories is affected by the uncertainty of the global knowledge of the environment. [CONTINUE] a well optimised vanilla LSTM with stochastic output often outperforms alternative approaches. [CONTINUE] we observed results proportional to the quality of the learned global model of the environment. Building an environmental model for the UCY Zara01 and Zara02 datasets is particularly challenging and thus the results are not substantially better than alternatives. [CONTINUE] importance of environmental or spatial information when making predictions."
    },
    {
        "id": "665",
        "table": {
            "header": [
                "Dataset \\Model",
                "B",
                "R+B",
                "D+B",
                "R+D+B"
            ],
            "rows": [
                [
                    "ETH Hotel",
                    "0.039\u00b1 0.002",
                    "0.045\u00b1 0.002",
                    "0.042\u00b1 0.001",
                    "[BOLD] 0.037\u00b1 0.001"
                ],
                [
                    "ETH Univ",
                    "0.146\u00b1 0.006",
                    "0.136\u00b1 0.002",
                    "0.133\u00b1 0.002",
                    "[BOLD] 0.113\u00b1 0.007"
                ],
                [
                    "UCY Zara01",
                    "0.020\u00b1 0.000",
                    "0.031\u00b1 0.000",
                    "0.021\u00b1 0.000",
                    "[BOLD] 0.017\u00b1 0.000"
                ],
                [
                    "UCY Zara02",
                    "[BOLD] 0.024\u00b1 0.000",
                    "0.029\u00b1 0.000",
                    "[BOLD] 0.024\u00b1 0.000",
                    "[BOLD] 0.024\u00b1 0.000"
                ],
                [
                    "Average",
                    "0.057\u00b1 0.002",
                    "0.060\u00b1 0.001",
                    "0.055\u00b1 0.001",
                    "[BOLD] 0.048\u00b1 0.001"
                ]
            ],
            "title": "Table 2: Ablation Table"
        },
        "insight": "Table 2 shows that a combination of both global dynamics and static representation are useful. [CONTINUE] considered individually, less accurate predictions are made, but combining these outperforms alternatives. [CONTINUE] contains longer, more complex trajectorThe less accurate model of the world in this scenario has not resulted in improved performance."
    },
    {
        "id": "666",
        "table": {
            "header": [
                "Dataset",
                "BP",
                "DNI (3)",
                "Critic (3)",
                "LC (1)",
                "LC (3)",
                "LC (5)"
            ],
            "rows": [
                [
                    "CIFAR-10",
                    "93.93 \u00b10.20",
                    "64.86 \u00b10.42",
                    "91.92 \u00b10.30",
                    "92.06 \u00b10.20",
                    "92.39 \u00b10.09",
                    "91.38 \u00b10.20"
                ],
                [
                    "CIFAR-100",
                    "75.14 \u00b10.18",
                    "36.53 \u00b10.64",
                    "69.07 \u00b10.25",
                    "73.61 \u00b10.31",
                    "69.91 \u00b10.50",
                    "63.53 \u00b10.24"
                ]
            ],
            "title": "TABLE I: Average test accuracy (%) of backpropagation (BP), DNI [8], critic training [3], and proposed local critic training (LC). The numbers of local networks used are shown in the parentheses. The standard deviation values are also shown."
        },
        "insight": "When more local critic networks are used, the accuracy tends to decrease more due to higher reliance on predicted gradients rather than true gradients, while more layer groups can be trained independently. [CONTINUE] The classification performance of the proposed local critic training approach is evaluated in Table I. For comparison, the performance of the regular backpropagation, DNI , and critic training  is also evaluated. [CONTINUE] When compared to the result of backpropagation, the proposed approach successfully decouples training of the layer groups at a small expense of accuracy decrease [CONTINUE] The degradation of the accuracy of our method is larger for CIFAR-100, [CONTINUE] The DNI method shows poor performance as in . The proposed method shows similar (or even slightly better) performance to the critic training method, which shows the efficacy of the cascaded learning scheme of the local networks in our method."
    },
    {
        "id": "667",
        "table": {
            "header": [
                "Dataset",
                "[1,1,1] (default)",
                "[3,3,3]",
                "[5,5,5]",
                "[3,2,1]",
                "[1,2,3]",
                "[5,4,3]",
                "[3,4,5]"
            ],
            "rows": [
                [
                    "CIFAR-10",
                    "92.39 \u00b10.09",
                    "92.36 \u00b10.22",
                    "91.72 \u00b10.19",
                    "92.07 \u00b10.21",
                    "92.20 \u00b10.12",
                    "92.10 \u00b10.16",
                    "91.90 \u00b10.16"
                ],
                [
                    "CIFAR-100",
                    "69.91 \u00b10.50",
                    "70.02 \u00b10.29",
                    "70.34 \u00b10.16",
                    "70.06 \u00b10.64",
                    "69.81 \u00b10.33",
                    "70.87 \u00b10.40",
                    "69.93 \u00b10.56"
                ]
            ],
            "title": "TABLE II: Average test accuracy (%) with respect to the number of layers in the local critic networks. [a,b,c] means that the numbers of convolutional layers in LC1, LC2, and LC3 are a, b, and c, respectively."
        },
        "insight": "The results for various structure combinations of the three local critic networks are shown in Table II. [CONTINUE] As the number of convolutional layers increase for all local networks (the first three cases in the table), the accuracy for CIFAR-100 slightly increases from 69.91% (with one convolutional layer) to 70.02% (three convolutional layers) and 70.34% (five convolutional layers), whereas for CIFAR-10 the accuracy slightly decreases when five convolutional layers are used. [CONTINUE] The results are shown in the last four columns of Table II."
    },
    {
        "id": "668",
        "table": {
            "header": [
                "model",
                "FLOP",
                "# of parameters"
            ],
            "rows": [
                [
                    "Sub-model 1",
                    "2.85M",
                    "1.42M"
                ],
                [
                    "Sub-model 2",
                    "1.76M",
                    "0.88M"
                ],
                [
                    "Sub-model 3",
                    "4.52M",
                    "2.26M"
                ],
                [
                    "Main model",
                    "15.72M",
                    "7.87M"
                ]
            ],
            "title": "TABLE V: FLOPs required for a feedforward pass and numbers of model parameters in the sub-models and main model for CIFAR-10. Note that sub-model 2 has less FLOPs and parameters than sub-model 1 due to the pooling operation in sub-model 2."
        },
        "insight": "Table V shows the complexities of the sub-models in terms of the amount of computation for a feedforward pass and the number of weight parameters. [CONTINUE] For CIFAR-10, the computational complexity in terms of the number of floating-point operations (FLOPs) and the memory complexity are reduced to only about 30% (15.72 to 4.52 million FLOPs, and 7.87 to 2.26 million parameters), as shown in Table V."
    },
    {
        "id": "669",
        "table": {
            "header": [
                "Dataset",
                "1/1",
                "1/2",
                "1/3",
                "1/4",
                "1/5"
            ],
            "rows": [
                [
                    "CIFAR-10",
                    "92.39 \u00b10.09",
                    "91.91 \u00b10.19",
                    "91.78 \u00b10.18",
                    "91.57 \u00b10.12",
                    "91.35 \u00b10.17"
                ],
                [
                    "CIFAR-100",
                    "69.91 \u00b10.50",
                    "67.99 \u00b10.49",
                    "67.76\u00b10.19",
                    "66.74 \u00b10.41",
                    "66.39 \u00b10.39"
                ]
            ],
            "title": "TABLE III: Average test accuracy (%) with respect to the update frequency of local critic networks."
        },
        "insight": "we compare different update frequency in Table III. [CONTINUE] When the update frequency is a half of that for the main network (i.e., 1/2), the accuracy drops by 0.48% and 1.92% for the two datasets, respectively. Then, the decrease of the accuracy is only 0.56% for CIFAR-10 and 1.60% for CIFAR100 when the update frequency decreases from 1/2 to 1/5."
    },
    {
        "id": "670",
        "table": {
            "header": [
                "Dataset",
                "BP sub 1",
                "LC sub 1",
                "BP sub 2",
                "LC sub 2",
                "BP sub 3",
                "LC sub 3"
            ],
            "rows": [
                [
                    "CIFAR-10",
                    "74.46 \u00b10.91",
                    "85.24 \u00b10.49",
                    "88.03 \u00b10.87",
                    "90.53 \u00b10.15",
                    "92.05 \u00b10.24",
                    "92.29 \u00b10.09"
                ],
                [
                    "CIFAR-100",
                    "47.58 \u00b11.10",
                    "55.39 \u00b10.57",
                    "61.79 \u00b10.92",
                    "63.62 \u00b10.31",
                    "67.81 \u00b10.22",
                    "67.54 \u00b10.70"
                ]
            ],
            "title": "TABLE IV: Average test accuracy (%) of the sub-models produced by local critic training and the networks trained by regular backpropagation."
        },
        "insight": "Table IV compares the performance of the sub-models, [CONTINUE] The largest sub-model (sub-model 3) shows similar accuracy to the main model (92.29% vs. 92.39% for CIFAR-10 and 67.54% vs. 69.91% for CIFAR-100), [CONTINUE] the table also shows the accuracy of the networks that have the same structures with the sub-models but are trained using regular backpropagation."
    },
    {
        "id": "671",
        "table": {
            "header": [
                "[EMPTY]",
                "FLOP",
                "Accuracy (%)"
            ],
            "rows": [
                [
                    "Progressive inference (0.9)",
                    "2.90M",
                    "91.18 \u00b10.10"
                ],
                [
                    "Progressive inference (0.95)",
                    "3.05M",
                    "91.75 \u00b10.16"
                ],
                [
                    "Main model",
                    "15.72M",
                    "92.39 \u00b10.09"
                ]
            ],
            "title": "TABLE VI: Average FLOPs and accuracy of progressive inference for test data of CIFAR-10 when the threshold is set to 0.9 or 0.95."
        },
        "insight": "The results are shown in Table VI. [CONTINUE] When the threshold is 0.9, with only a slight loss of accuracy (92.39% to 91.18%), the computational complexity is reduced significantly, which is only 18.45% of that of the main model. When the threshold increases to 0.95, the accuracy loss becomes smaller (only 0.64%), while the complexity reduction remains almost the same (19.40% of the main model's complexity)."
    },
    {
        "id": "672",
        "table": {
            "header": [
                "Subsample size",
                "200",
                "400",
                "800",
                "1200",
                "1500",
                "2000"
            ],
            "rows": [
                [
                    "CF",
                    "0.035",
                    "0.021",
                    "0.015",
                    "0.014",
                    "0.011",
                    "0.007"
                ],
                [
                    "LLCF",
                    "0.027",
                    "0.017",
                    "0.013",
                    "0.013",
                    "0.011",
                    "0.006"
                ]
            ],
            "title": "Table 1: Estimated in-sample MSE (13) of estimating the treatment effect on subsampled welfare data, averaged over 200 runs at each subsample size. We show estimated error from local linear causal forests (LLCF) and standard causal forests (CF). Tuning parameters were selected via cross-validation using the R-learner objective."
        },
        "insight": "Table 1 has error estimates for both types of forests using (13), and verifies that using the local linear correction improves empirical performance across different subsample sizes."
    },
    {
        "id": "673",
        "table": {
            "header": [
                "[ITALIC] d",
                "[ITALIC] n",
                "[ITALIC] \u03c3",
                "RF",
                "Lasso- RF",
                "LLF",
                "BART",
                "XGBoost"
            ],
            "rows": [
                [
                    "5",
                    "1000",
                    "0.1",
                    "0.10",
                    "0.06",
                    "[BOLD] 0.02",
                    "0.27",
                    "0.07"
                ],
                [
                    "5",
                    "5000",
                    "0.1",
                    "0.06",
                    "[BOLD] 0.02",
                    "[BOLD] 0.02",
                    "0.22",
                    "0.06"
                ],
                [
                    "50",
                    "1000",
                    "0.1",
                    "0.29",
                    "0.18",
                    "0.11",
                    "0.52",
                    "[BOLD] 0.07"
                ],
                [
                    "50",
                    "5000",
                    "0.1",
                    "0.18",
                    "0.10",
                    "0.07",
                    "0.62",
                    "[BOLD] 0.06"
                ],
                [
                    "5",
                    "1000",
                    "1",
                    "0.21",
                    "0.24",
                    "[BOLD] 0.14",
                    "0.47",
                    "0.56"
                ],
                [
                    "5",
                    "5000",
                    "1",
                    "0.15",
                    "0.11",
                    "[BOLD] 0.09",
                    "0.26",
                    "0.52"
                ],
                [
                    "50",
                    "1000",
                    "1",
                    "0.41",
                    "0.39",
                    "[BOLD] 0.20",
                    "0.82",
                    "0.53"
                ],
                [
                    "50",
                    "5000",
                    "1",
                    "0.23",
                    "0.21",
                    "[BOLD] 0.10",
                    "0.57",
                    "0.52"
                ],
                [
                    "5",
                    "1000",
                    "2",
                    "0.31",
                    "0.55",
                    "[BOLD] 0.26",
                    "0.69",
                    "1.21"
                ],
                [
                    "5",
                    "5000",
                    "2",
                    "0.25",
                    "0.28",
                    "[BOLD] 0.21",
                    "0.40",
                    "1.18"
                ],
                [
                    "50",
                    "1000",
                    "2",
                    "0.47",
                    "0.27",
                    "[BOLD] 0.24",
                    "0.89",
                    "1.22"
                ],
                [
                    "50",
                    "5000",
                    "2",
                    "0.33",
                    "0.27",
                    "[BOLD] 0.15",
                    "0.70",
                    "0.96"
                ]
            ],
            "title": "Table 5: RMSE from simulations on equation 1 on random forests, lasso-random forest, local linear forests, BART, and boosting. We vary sample size n, error variance \u03c3, and ambient dimension d, and report test error on 1000 test points. We estimate Var[E[Y\u2223X]] as 3.52 over 10,000 Monte Carlo repetitions, so that signal-to-noise ratio ranges from 352 at \u03c3=0.1 to 0.88 at \u03c3=2. All errors are averaged over 50 runs, and minimizing errors are in bold."
        },
        "insight": "Local linear forests do quite well here; they detect the strong linear signal in the tail"
    },
    {
        "id": "674",
        "table": {
            "header": [
                "[BOLD] Global Graph Recovery",
                "[BOLD] ML",
                "[BOLD] precision 86.98%",
                "[BOLD] recall 90.79%",
                "[BOLD] F-score  [BOLD] 88.84%",
                "[BOLD] MSE  [BOLD] 1.6E-03"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[BOLD] GL-informed",
                    "81.26%",
                    "88.91%",
                    "84.48%",
                    "2.6E-03"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] GL-conv",
                    "63.82%",
                    "100%",
                    "77.41%",
                    "2.1E-03"
                ],
                [
                    "[BOLD] Mask Recovery",
                    "[BOLD] ML",
                    "92.57%",
                    "94.88%",
                    "93.68%",
                    "-"
                ]
            ],
            "title": "TABLE I: Global Graph Recovery and Mask Recovery Performances"
        },
        "insight": "The findings are summarized in Table I. [CONTINUE] GL-conv yields a high difference between the recall and the precision rate [CONTINUE] which leads to poor F-score compared to other methods. [CONTINUE] The MSE score of ML and GL-conv is better than the one of GL-informed. [CONTINUE] ML achieves good rates on the"
    },
    {
        "id": "675",
        "table": {
            "header": [
                "Measurement",
                "GPS",
                "Altitude"
            ],
            "rows": [
                [
                    "Temperature",
                    "36%",
                    "64%"
                ],
                [
                    "Snowfall (cm)",
                    "37%",
                    "63%"
                ],
                [
                    "Humidity",
                    "51%",
                    "49%"
                ],
                [
                    "Precipitation (mm)",
                    "52%",
                    "48%"
                ],
                [
                    "Cloudy days",
                    "65%",
                    "35%"
                ],
                [
                    "Sunshine (h)",
                    "54%",
                    "46%"
                ]
            ],
            "title": "TABLE II: Contribution of layers on the structure of different measurements"
        },
        "insight": "In Table II, the percentage of the connections that ML draws from the GPS and the altitude layer is given for different types of measurements that are used as signals. To begin with temperature, its structure seems to be highly coherent with the altitude similarity considering the percentage contribution of each layer."
    },
    {
        "id": "676",
        "table": {
            "header": [
                "Dataset",
                "proposed",
                "edit distance"
            ],
            "rows": [
                [
                    "Strings",
                    "[BOLD] 1.29 \u00b1 0.098 sec",
                    "23.7 \u00b1 0.09 sec"
                ],
                [
                    "Glycan_SP",
                    "[BOLD] 0.665 \u00b1 0.059 sec",
                    "28.4 \u00b1 2.21 sec"
                ],
                [
                    "Glycan_EL",
                    "[BOLD] 0.249 \u00b1 0.121 sec",
                    "182.9 \u00b1 8.86 sec"
                ],
                [
                    "Words",
                    "[BOLD] 41.3 \u00b1 0.31 sec",
                    "283.9 \u00b1 5.18 sec"
                ],
                [
                    "Sentiment",
                    "[BOLD] 1.79 \u00b1 0.105 sec",
                    "9013 \u00b1 770 sec"
                ]
            ],
            "title": "Table 3: Mean inference time of the 3-nearest neighbor classifier in each dataset. The first column specifies the dataset. The second column shows the inference time of the 3-nearest neighbor classifier using the weighted pq-gram distance. The third column shows that the inference time of the 3-nearest neighbor classifier using the tree edit distance."
        },
        "insight": "Table 3 shows the mean running time of 3-nearest neighbor inference for 5-fold cross-validation. [CONTINUE] In all datasets, the weighted pqgram distance yields much shorter inference times than the tree edit distance [CONTINUE] Especially in the Sentiment dataset, our proposed method yields over 5000 times shorter inference time than that using the tree edit distance."
    },
    {
        "id": "677",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] LSTM",
                "[BOLD] LSTMD",
                "[BOLD] PG",
                "[BOLD] PGIS",
                "[BOLD] AC",
                "[BOLD] PGU",
                "[BOLD] ACU",
                "[BOLD] IRecGAN"
            ],
            "rows": [
                [
                    "[BOLD] P@10\u00a0(%)",
                    "32.89\u00b10.50",
                    "33.42\u00b10.40",
                    "33.28\u00b10.71",
                    "28.13\u00b10.45",
                    "31.93\u00b10.17",
                    "34.12\u00b10.52",
                    "32.43\u00b10.22",
                    "[BOLD] 35.06\u00b10.48"
                ],
                [
                    "[BOLD] P@1\u00a0(%)",
                    "8.20\u00b10.65",
                    "[BOLD] 8.55\u00b10.63",
                    "6.25\u00b10.14",
                    "4.61\u00b10.73",
                    "6.54\u00b10.19",
                    "6.44\u00b10.56",
                    "6.63\u00b1 0.29",
                    "6.79\u00b10.44"
                ]
            ],
            "title": "Table 1: Rerank evaluation on real-world dataset with random splitting."
        },
        "insight": "IRecGAN achieved encouraging P@10 improvement against all baselines. [CONTINUE] PGIS did not perform as well as PG [CONTINUE] PGU was able to fit the given data more accurately than PG [CONTINUE] PGU performed worse than IRecGAN [CONTINUE] LSTMD outperformed LSTM,"
    },
    {
        "id": "678",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] LSTM",
                "[BOLD] LSTMD",
                "[BOLD] PG",
                "[BOLD] PGIS",
                "[BOLD] AC",
                "[BOLD] PGU",
                "[BOLD] ACU",
                "[BOLD] IRecGAN"
            ],
            "rows": [
                [
                    "[BOLD] P@10\u00a0(%)",
                    "28.79\u00b10.44",
                    "31.98\u00b10.64",
                    "32.44\u00b11.16",
                    "30.72\u00b10.37",
                    "29.26\u00b10.79",
                    "30.33\u00b10.47",
                    "28.53\u00b10.35",
                    "[BOLD] 33.45\u00b10.71"
                ],
                [
                    "[BOLD] P@1\u00a0(%, all)",
                    "9.64\u00b10.38",
                    "[BOLD] 11.26\u00b10.34",
                    "8.40\u00b10.18",
                    "7.67\u00b10.31",
                    "7.33\u00b10.41",
                    "8.27\u00b10.44",
                    "7.08\u00b10.32",
                    "9.78\u00b10.37"
                ],
                [
                    "[BOLD] P@1\u00a0(%)",
                    "9.68\u00b10.29",
                    "[BOLD] 11.06\u00b10.23",
                    "6.83\u00b10.38",
                    "6.09\u00b10.19",
                    "6.11\u00b10.18",
                    "6.67\u00b10.51",
                    "5.86\u00b10.26",
                    "7.84\u00b10.25"
                ]
            ],
            "title": "Table 2: Rerank evaluation on real-world recommendation dataset when split by session ID."
        },
        "insight": "To compare results under different data separation strategies, we evaluated models when splitting the dataset in the order of session ID or time, as shown in Table 2 and 3, respectively. [CONTINUE] LSTM and LSTMD performed better than the RL agents in P@1. And the RL agents (IRecGAN and other RL baselines) had advantages in capturing users' overall interests, which led to better P@10 results. [CONTINUE] LSTMD outperformed LSTM and IRecGAN outperformed all other RL-based methods in both P@1 and P@10. [CONTINUE] By comparing the P@1 and P@1 (all), we observed that the differences between these two metrics in user behavior models (LSTM and LSTMD) were smaller than those between most RL agents. More specifically, most of RL agents performed better under the P@1 (all) metric than P@1, where the former included evaluations with less than 10 ranking candidates."
    },
    {
        "id": "679",
        "table": {
            "header": [
                "Observation rate  [ITALIC] \u03c1 Treatment effect  [ITALIC] \u03bc",
                "Observation rate  [ITALIC] \u03c1 Treatment effect  [ITALIC] \u03bc",
                "0.1 1",
                "0.1 2",
                "0.1 5",
                "0.3 1",
                "0.3 2",
                "0.3 5",
                "0.5 1",
                "0.5 2",
                "0.5 5"
            ],
            "rows": [
                [
                    "MSE( [ITALIC] Y)",
                    "fPCA",
                    "0.521",
                    "2.172",
                    "5.455",
                    "0.525",
                    "2.039",
                    "5.170",
                    "0.525",
                    "2.036",
                    "5.166"
                ],
                [
                    "MSE( [ITALIC] Y)",
                    "SLI",
                    "0.430",
                    "1.162",
                    "2.561",
                    "0.379",
                    "0.658",
                    "1.203",
                    "0.341",
                    "0.543",
                    "0.893"
                ],
                [
                    "MSE( [ITALIC] Y)",
                    "CSI",
                    "0.311",
                    "0.306",
                    "0.318",
                    "0.314",
                    "0.297",
                    "0.320",
                    "0.294",
                    "0.299",
                    "0.295"
                ]
            ],
            "title": "Table 1: Comparisons between fPCA, SLI and CSI under different values of \u03c1 and \u03bc."
        },
        "insight": "The results are presented in Table 1 and Figure 1. From Table 1 and the left plot of Figure 1, we have the following findings: [CONTINUE] . CSI achieves better performance than SLI and fPCA, regardless of the treatment effect \u00b5 and observation rate \u03c1. Meanwhile SLI performs better than fPCA. 2. All three methods give comparable errors for smaller values of \u00b5. In particular, our introduc tion of treatment effect \u00b5 does not over-fit the model in the case of \u00b5 = 0. 3. As the treatment effect \u00b5 increases, the performance of CSI remains the same whereas the performances of SLI and fPCA deteriorate rapidly. As a result, CSI outperforms SLI and fPCA by a significant margin for large values of \u00b5. For example, when \u03c1 = 0.1, the MSE( \u02c6Y ) of CSI decreases from 72.3% of SLI and 59.6% of fPCA at \u00b5 = 1 to 12.4% of SLI and 5.8% of fPCA at \u00b5 = 5. 4. All three algorithms suffer a higher MSE( \u02c6Y ) with smaller observation rate \u03c1. The biggest decay comes from SLI with an average 118% increase in test error from \u03c1 = 0.5 to \u03c1 = 0.1. The performances of fPCA and CSI remains comparatively stable among different observation rate with a 6% and 12% increase respectively. This implies that our algorithm is tolerant to low observation rate."
    },
    {
        "id": "680",
        "table": {
            "header": [
                "Type",
                "Cardinality",
                "Reward",
                "BLER",
                "SE",
                "BER"
            ],
            "rows": [
                [
                    "QL-AMC",
                    "10",
                    "BLER",
                    "0.0320",
                    "3.6700",
                    "0.0088"
                ],
                [
                    "QL-AMC",
                    "15",
                    "BLER",
                    "0.0306",
                    "3.3238",
                    "0.0087"
                ],
                [
                    "QL-AMC",
                    "30",
                    "BLER",
                    "0.0302",
                    "3.5594",
                    "0.0087"
                ],
                [
                    "QL-AMC",
                    "60",
                    "BLER",
                    "0.0306",
                    "3.8783",
                    "0.0087"
                ],
                [
                    "QL-AMC",
                    "10",
                    "SE",
                    "0.0306",
                    "3.9187",
                    "0.0086"
                ],
                [
                    "QL-AMC",
                    "15",
                    "SE",
                    "0.0301",
                    "3.8207",
                    "0.0085"
                ],
                [
                    "QL-AMC",
                    "30",
                    "SE",
                    "0.0310",
                    "3.9922",
                    "0.0086"
                ],
                [
                    "QL-AMC",
                    "60",
                    "SE",
                    "0.0311",
                    "4.1553",
                    "0.0086"
                ],
                [
                    "Table",
                    "-",
                    "-",
                    "0.0311",
                    "3.8704",
                    "0.0088"
                ],
                [
                    "OLLA 1",
                    "-",
                    "-",
                    "0.0309",
                    "3.6700",
                    "0.0088"
                ],
                [
                    "OLLA 2",
                    "-",
                    "-",
                    "0.0330",
                    "1.8511",
                    "0.0090"
                ],
                [
                    "OLLA 3",
                    "-",
                    "-",
                    "0.0343",
                    "0.9999",
                    "0.0092"
                ]
            ],
            "title": "TABLE III: Deployment Phase Results (Average over 200 runs)"
        },
        "insight": "Table III summarizes the results in the deployment phase in terms of average values for each configuration of the QL-AMC and baseline solution. [CONTINUE] we see that the two QL-AMC configurations presenting the best results in terms of spectral efficiency are those with cardinality 30 and 60,"
    },
    {
        "id": "681",
        "table": {
            "header": [
                "[EMPTY]",
                "mean",
                "scaled mean",
                "sd"
            ],
            "rows": [
                [
                    "CSI",
                    "74.28",
                    "0.62",
                    "8.90"
                ],
                [
                    "SLI",
                    "79.92",
                    "0.66",
                    "9.22"
                ],
                [
                    "fPCA",
                    "127.73",
                    "1.06",
                    "13.54"
                ],
                [
                    "rMean",
                    "87.26",
                    "0.73",
                    "8.96"
                ],
                [
                    "pMean",
                    "119.80",
                    "1.00",
                    "12.84"
                ]
            ],
            "title": "Figure 2: Test error on GDI dataset"
        },
        "insight": "We run all five methods on the same training/validation/test set for 40 times and compare the mean and sd of test-errors. The results are presented in Table 2 and Figure 2. Compared with the null model pMean (Column 2 of Table 2), fPCA gives roughly the same order of error; CSI, SLI and rowMean provide better predictions, achieving 62%, 66% and 73% of the test errors respectively. In particular, our algorithm CSI improves the result of vanilla model SLI by 7%, it also provide a stable estimation with the smallest sd across multiple selections of test sets."
    },
    {
        "id": "682",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] N=2,  [ITALIC] D=2 point",
                "[ITALIC] N=2,  [ITALIC] D=2 MoG-1",
                "[ITALIC] N=2,  [ITALIC] D=2 MoG-2",
                "[ITALIC] N=2,  [ITALIC] D=3 point",
                "[ITALIC] N=2,  [ITALIC] D=3 MoG-1",
                "[ITALIC] N=2,  [ITALIC] D=3 MoG-2",
                "[ITALIC] N=3,  [ITALIC] D=2 point",
                "[ITALIC] N=3,  [ITALIC] D=2 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=2 MoG-2",
                "[ITALIC] N=3,  [ITALIC] D=3 point",
                "[ITALIC] N=3,  [ITALIC] D=3 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=3 MoG-2"
            ],
            "rows": [
                [
                    "[BOLD] Verification AP",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.987",
                    "0.989",
                    "0.990",
                    "0.996",
                    "0.996",
                    "0.996",
                    "0.978",
                    "0.981",
                    "0.976",
                    "0.987",
                    "0.989",
                    "0.991"
                ],
                [
                    "corrupt",
                    "0.880",
                    "0.907",
                    "0.912",
                    "0.913",
                    "0.926",
                    "0.932",
                    "0.886",
                    "0.899",
                    "0.904",
                    "0.901",
                    "0.922",
                    "0.925"
                ],
                [
                    "[BOLD] KNN Accuracy",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.871",
                    "0.879",
                    "0.888",
                    "0.942",
                    "0.953",
                    "0.939",
                    "0.554",
                    "0.591",
                    "0.540",
                    "0.795",
                    "0.770",
                    "0.766"
                ],
                [
                    "corrupt",
                    "0.583",
                    "0.760",
                    "0.757",
                    "0.874",
                    "0.909",
                    "0.885",
                    "0.274",
                    "0.350",
                    "0.351",
                    "0.522",
                    "0.555",
                    "0.598"
                ]
            ],
            "title": "Table 1: Accuracy of pairwise verification and KNN identification tasks for point embeddings, and our hedged embeddings with a single Gaussian component (MoG-1) and two components (MoG-2). We report results for images with N digits and using D embedding dimensions."
        },
        "insight": "The average | precision (AP) of this task is reported in the top half of Table 1. HIB shows improved performance, especially for corrupted test images. For example, in the [CONTINUE] = 2 digit case, when using D = 2 dimensions, point embeddings achieve 88.0% AP on corrupt test images, while hedged instance embeddings improves to 90.7% with [CONTINUE] = 1 Gaussian, and 91.2% with [CONTINUE] = 2 Gaussians. [CONTINUE] We next measure performance on the KNN identification task. The bottom half of Table 1 reports the results. In the KNN experiment, each clean image is identified by comparing to the test gallery which is either comprised of clean or corrupted images based on its K = 5 nearest neighbors. The identification is considered correct only if a majority of the neighbors have the correct class. Again, the proposed stochastic embeddings generally outperform point embeddings, with the greatest advantage for the corrupted input samples. For example, in the [CONTINUE] = 2 digit case, when using D = 2 dimensions, point embeddings achieve 58.3% AP on corrupt test images, while HIB improves to [CONTINUE] 6.0% with [CONTINUE] = 1 Gaussian, and 75.7% with [CONTINUE] = 2 Gaussians."
    },
    {
        "id": "683",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] N=2,  [ITALIC] D=2 MoG-1",
                "[ITALIC] N=2,  [ITALIC] D=2 MoG-2",
                "[ITALIC] N=2,  [ITALIC] D=3 MoG-1",
                "[ITALIC] N=2,  [ITALIC] D=3 MoG-2",
                "[ITALIC] N=3,  [ITALIC] D=2 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=2 MoG-2",
                "[ITALIC] N=3,  [ITALIC] D=3 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=3 MoG-2"
            ],
            "rows": [
                [
                    "[BOLD] AP Correlation",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.74",
                    "0.43",
                    "0.68",
                    "0.48",
                    "0.63",
                    "0.28",
                    "0.51",
                    "0.39"
                ],
                [
                    "corrupt",
                    "0.81",
                    "0.79",
                    "0.86",
                    "0.87",
                    "0.82",
                    "0.76",
                    "0.85",
                    "0.79"
                ],
                [
                    "[BOLD] KNN Correlation",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.71",
                    "0.57",
                    "0.72",
                    "0.47",
                    "0.76",
                    "0.29",
                    "0.74",
                    "0.54"
                ],
                [
                    "corrupt",
                    "0.47",
                    "0.43",
                    "0.55",
                    "0.52",
                    "0.49",
                    "0.50",
                    "0.67",
                    "0.34"
                ]
            ],
            "title": "Table 1: Accuracy of pairwise verification and KNN identification tasks for point embeddings, and our hedged embeddings with a single Gaussian component (MoG-1) and two components (MoG-2). We report results for images with N digits and using D embedding dimensions."
        },
        "insight": "The average precision (AP) of this task is reported in the top half of Table 1. HIB shows improved performance, especially for corrupted test images. For example, in the [CONTINUE] = 2 digit case, when using D = 2 dimensions, point embeddings achieve 88.0% AP on corrupt test images, while hedged instance embeddings improves to 90.7% with [CONTINUE] = 1 Gaussian, and 91.2% with [CONTINUE] = 2 Gaussians. We next measure performance on the KNN identification task. The bottom half of Table 1 reports the results. In the KNN experiment, each clean image is identified by comparing to the test gallery which is either comprised of clean or corrupted images based on its K = 5 nearest neighbors. The identification is considered correct only if a majority of the neighbors have the correct class. Again, the proposed stochastic embeddings generally outperform point embeddings, with the greatest advantage for the corrupted input samples. For example, in the [CONTINUE] = 2 digit case, when using D = 2 dimensions, point embeddings achieve 58.3% AP on corrupt test images, while HIB improves to 76.0% with [CONTINUE] = 1 Gaussian, and 75.7% with [CONTINUE] = 2 Gaussians."
    },
    {
        "id": "684",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] N=3,  [ITALIC] D=4 point",
                "[ITALIC] N=3,  [ITALIC] D=4 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=4 MoG-2"
            ],
            "rows": [
                [
                    "[BOLD] Verification AP",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.996",
                    "0.996",
                    "0.995"
                ],
                [
                    "corrupt",
                    "0.942",
                    "0.944",
                    "0.948"
                ],
                [
                    "[BOLD] KNN Accuracy",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.914",
                    "0.917",
                    "0.922"
                ],
                [
                    "corrupt",
                    "0.803",
                    "0.816",
                    "0.809"
                ]
            ],
            "title": "Table 4: Task performance and uncertainty measure correlations with task performance for 4D embeddings with 3-digit MNIST. (a) Task accuracies for pairwise verification and KNN identification."
        },
        "insight": "In Table 4, we report the results of 4D embeddings with 3-digit MNIST. Here, the task performance begins to saturate, with verification accuracy on clean input images above 0.99. However, we again observe that HIB provides a slight performance improvement over point embeddings for corrupt images, and task performance correlates with uncertainty."
    },
    {
        "id": "685",
        "table": {
            "header": [
                "[EMPTY]",
                "[ITALIC] N=3,  [ITALIC] D=4 MoG-1",
                "[ITALIC] N=3,  [ITALIC] D=4 MoG-2"
            ],
            "rows": [
                [
                    "[BOLD] AP Correlation",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.35",
                    "0.33"
                ],
                [
                    "corrupt",
                    "0.79",
                    "0.68"
                ],
                [
                    "[BOLD] KNN Correlation",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.31",
                    "0.32"
                ],
                [
                    "corrupt",
                    "0.42",
                    "0.35"
                ]
            ],
            "title": "Table 4: Task performance and uncertainty measure correlations with task performance for 4D embeddings with 3-digit MNIST. (b) Correlations between uncertainty, \u03b7(x), and task performances."
        },
        "insight": "In Table 4, we report the results of 4D embeddings with 3-digit MNIST. Here, the task performance begins to saturate, with verification accuracy on clean input images above 0.99. However, we again observe that HIB provides a slight performance improvement over point embeddings for corrupt images, and task performance correlates with uncertainty."
    },
    {
        "id": "686",
        "table": {
            "header": [
                "[ITALIC] \u03b2",
                "[ITALIC] N=2,  [ITALIC] D=2 0",
                "[ITALIC] N=2,  [ITALIC] D=2 10\u22124",
                "[ITALIC] N=3,  [ITALIC] D=3 0",
                "[ITALIC] N=3,  [ITALIC] D=3 10\u22124"
            ],
            "rows": [
                [
                    "[BOLD] Verification AP",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.986",
                    "0.988",
                    "0.993",
                    "0.992"
                ],
                [
                    "corrupt",
                    "0.900",
                    "0.906",
                    "0.932",
                    "0.932"
                ],
                [
                    "[BOLD] KNN Accuracy",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.867",
                    "0.891",
                    "0.858",
                    "0.861"
                ],
                [
                    "corrupt",
                    "0.729",
                    "0.781",
                    "0.685",
                    "0.730"
                ]
            ],
            "title": "Table 5: Results for single Gaussian embeddings with and without KL divergence term. We report results for images with N digits and using D embedding dimensions. (a) Task performances."
        },
        "insight": "For quantitative analysis on the impact of \u03b2 on main task performance and uncertainty quality, see Table 5 where we compare \u03b2 = 0 and \u03b2 = 10\u22124cases. We observe mild improvements in main task performances when the KL divergence regularization is used. For example, KNN accuracy improves from 0.685 to 0.730 by including the KL term for [CONTINUE] = 3, D = 3 case. Improvement in uncertainty quality, measured in terms of Kendall\u2019s tau correlation, is more pronounced. For example, KNN accuracy and uncertainty are nearly uncorrelated when \u03b2 = 0 under the [CONTINUE] = 3, D = 3 setting (-0.080 for clean and 0.183 for corrupt inputs), while they are well-correlated when \u03b2 = 10\u22124 (0.685 for clean and 0.549 for corrupt inputs)."
    },
    {
        "id": "687",
        "table": {
            "header": [
                "[ITALIC] \u03b2",
                "[ITALIC] N=2,  [ITALIC] D=2 0",
                "[ITALIC] N=2,  [ITALIC] D=2 10\u22124",
                "[ITALIC] N=3,  [ITALIC] D=3 0",
                "[ITALIC] N=3,  [ITALIC] D=3 10\u22124"
            ],
            "rows": [
                [
                    "[BOLD] AP Correlation",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.680",
                    "0.766",
                    "0.425",
                    "0.630"
                ],
                [
                    "corrupt",
                    "0.864",
                    "0.836",
                    "0.677",
                    "0.764"
                ],
                [
                    "[BOLD] KNN Correlation",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "clean",
                    "0.748",
                    "0.800",
                    "-0.080",
                    "0.685"
                ],
                [
                    "corrupt",
                    "0.636",
                    "0.609",
                    "0.183",
                    "0.549"
                ]
            ],
            "title": "Table 5: Results for single Gaussian embeddings with and without KL divergence term. We report results for images with N digits and using D embedding dimensions. (b) Uncertainty correlations."
        },
        "insight": "For quantitative analysis on the impact of \u03b2 on main task performance and uncertainty quality, see Table 5 where we compare \u03b2 = 0 and \u03b2 = 10\u22124cases. We observe mild improvements in main task performances when the KL divergence regularization is used. For example, KNN accuracy improves from 0.685 to 0.730 by including the KL term for [CONTINUE] = 3, D = 3 case. Improvement in uncertainty quality, measured in terms of Kendall\u2019s tau correlation, is more pronounced. For example, KNN accuracy and uncertainty are nearly uncorrelated when \u03b2 = 0 under the [CONTINUE] = 3, D = 3 setting (-0.080 for clean and 0.183 for corrupt inputs), while they are well-correlated when \u03b2 = 10\u22124 (0.685 for clean and 0.549 for corrupt inputs)."
    },
    {
        "id": "688",
        "table": {
            "header": [
                "[BOLD] Algorithms",
                "[BOLD] AUC"
            ],
            "rows": [
                [
                    "Binary Classifier",
                    "50.0"
                ],
                [
                    "M. Hasan  [ITALIC] et al. ",
                    "50.6"
                ],
                [
                    "C. Lu  [ITALIC] et al. ",
                    "65.51"
                ],
                [
                    "W. Sultani  [ITALIC] et al. ",
                    "75.41"
                ],
                [
                    "Proposed Method (3D ResNet + constr. + loss)",
                    "[BOLD] 75.62"
                ],
                [
                    "Proposed Method (3D ResNet + constr. + new rank. loss)",
                    "[BOLD] 76.67"
                ]
            ],
            "title": "TABLE I: Comparison of all the methods on the UCF-Crime Dataset."
        },
        "insight": "As shown in Figures 2, 3 and in Table I, ROC-Curve and UC methods show the quantitative comparisons of our algorithm with other state-of-art methods. [CONTINUE] from the Table [CONTINUE] and Fig. 3, we can say that our proposed method gives the highest AUC score in all. Additionally, the effectiveness of our proposed ranking loss function is also shown. From these comparisons, it is shown that our proposed algorithm along with the proposed ranking loss shows the best performance in other state-of-art methods."
    },
    {
        "id": "689",
        "table": {
            "header": [
                "Dataset",
                "Model",
                "FPR-95%-TPR",
                "AUPR-Error",
                "AUPR-Success",
                "AUC"
            ],
            "rows": [
                [
                    "[BOLD] MNIST MLP",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "14.87",
                    "37.70",
                    "99.94",
                    "97.13"
                ],
                [
                    "[BOLD] MNIST MLP",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "15.15",
                    "38.22",
                    "99.94",
                    "97.15"
                ],
                [
                    "[BOLD] MNIST MLP",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "12.31",
                    "52.18",
                    "99.95",
                    "97.52"
                ],
                [
                    "[BOLD] MNIST MLP",
                    "ConfidNet (Ours)",
                    "[BOLD] 11.79",
                    "[BOLD] 57.37",
                    "[BOLD] 99.95",
                    "[BOLD] 97.83"
                ],
                [
                    "[BOLD] MNIST Small ConvNet",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "5.56",
                    "35.05",
                    "99.99",
                    "98.63"
                ],
                [
                    "[BOLD] MNIST Small ConvNet",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "5.26",
                    "38.50",
                    "99.99",
                    "98.65"
                ],
                [
                    "[BOLD] MNIST Small ConvNet",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "10.00",
                    "35.88",
                    "99.98",
                    "98.20"
                ],
                [
                    "[BOLD] MNIST Small ConvNet",
                    "ConfidNet (Ours)",
                    "[BOLD] 3.33",
                    "[BOLD] 45.89",
                    "[BOLD] 99.99",
                    "[BOLD] 98.82"
                ],
                [
                    "[BOLD] SVHN Small ConvNet",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "31.28",
                    "48.18",
                    "99.54",
                    "93.20"
                ],
                [
                    "[BOLD] SVHN Small ConvNet",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "36.60",
                    "43.87",
                    "99.52",
                    "92.85"
                ],
                [
                    "[BOLD] SVHN Small ConvNet",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "34.74",
                    "43.32",
                    "99.48",
                    "92.16"
                ],
                [
                    "[BOLD] SVHN Small ConvNet",
                    "ConfidNet (Ours)",
                    "[BOLD] 28.58",
                    "[BOLD] 50.72",
                    "[BOLD] 99.55",
                    "[BOLD] 93.44"
                ],
                [
                    "[BOLD] CIFAR-10 VGG16",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "47.50",
                    "45.36",
                    "99.19",
                    "91.53"
                ],
                [
                    "[BOLD] CIFAR-10 VGG16",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "49.02",
                    "46.40",
                    "[BOLD] 99.27",
                    "92.08"
                ],
                [
                    "[BOLD] CIFAR-10 VGG16",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "55.70",
                    "38.10",
                    "98.76",
                    "88.47"
                ],
                [
                    "[BOLD] CIFAR-10 VGG16",
                    "ConfidNet (Ours)",
                    "[BOLD] 44.94",
                    "[BOLD] 49.94",
                    "99.24",
                    "[BOLD] 92.12"
                ],
                [
                    "[BOLD] CIFAR-100 VGG16",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "67.86",
                    "71.99",
                    "92.49",
                    "85.67"
                ],
                [
                    "[BOLD] CIFAR-100 VGG16",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "64.68",
                    "72.59",
                    "[BOLD] 92.96",
                    "86.09"
                ],
                [
                    "[BOLD] CIFAR-100 VGG16",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "71.74",
                    "66.82",
                    "91.58",
                    "84.17"
                ],
                [
                    "[BOLD] CIFAR-100 VGG16",
                    "ConfidNet (Ours)",
                    "[BOLD] 62.96",
                    "[BOLD] 73.68",
                    "92.68",
                    "[BOLD] 86.28"
                ],
                [
                    "[BOLD] CamVid SegNet",
                    "Baseline\u00a0(MCP)\u00a0 hendrycks17baseline ",
                    "63.87",
                    "48.53",
                    "96.37",
                    "84.42"
                ],
                [
                    "[BOLD] CamVid SegNet",
                    "MCDropout\u00a0 Gal:2016:DBA:3045390.3045502 ",
                    "62.95",
                    "49.35",
                    "96.40",
                    "84.58"
                ],
                [
                    "[BOLD] CamVid SegNet",
                    "TrustScore\u00a0 NIPS2018_7798 ",
                    "[EMPTY]",
                    "20.42",
                    "92.72",
                    "68.33"
                ],
                [
                    "[BOLD] CamVid SegNet",
                    "ConfidNet (Ours)",
                    "[BOLD] 61.52",
                    "[BOLD] 50.51",
                    "[BOLD] 96.58",
                    "[BOLD] 85.02"
                ]
            ],
            "title": "Table 1: Comparison of failure prediction methods on various datasets. All methods share the same classification network. Note that for MCDropout, test accuracy is averaged over random sampling. All values are percentages."
        },
        "insight": "Table 1: Comparison of failure prediction methods on various datasets. All methods share the same classification network. Note that for MCDropout, test accuracy is averaged over random sampling. All values are percentages."
    },
    {
        "id": "690",
        "table": {
            "header": [
                "[EMPTY]",
                "Cao [cao2017paf] MC\u2217",
                "NADS-Net (Ours) MC",
                "NADS-Net (Ours) ND\u2217\u2217",
                "NADS-Net (Ours) All\u2217\u2217\u2217"
            ],
            "rows": [
                [
                    "Drivers",
                    "80%",
                    "81%",
                    "75%",
                    "88%"
                ],
                [
                    "Front Passengers",
                    "85%",
                    "89%",
                    "78%",
                    "90%"
                ],
                [
                    "Men",
                    "84%",
                    "87%",
                    "79%",
                    "90%"
                ],
                [
                    "Women",
                    "79%",
                    "81%",
                    "73%",
                    "88%"
                ],
                [
                    "White",
                    "83%",
                    "86%",
                    "77%",
                    "89%"
                ],
                [
                    "Black",
                    "75%",
                    "77%",
                    "70%",
                    "87%"
                ],
                [
                    "Asian",
                    "85%",
                    "87%",
                    "80%",
                    "91%"
                ],
                [
                    "Glasses",
                    "83%",
                    "83%",
                    "75%",
                    "89%"
                ],
                [
                    "Sunglasses",
                    "78%",
                    "82%",
                    "72%",
                    "85%"
                ],
                [
                    "Short Sleeves",
                    "84%",
                    "85%",
                    "77%",
                    "89%"
                ],
                [
                    "Long Sleeves",
                    "85%",
                    "87%",
                    "78%",
                    "90%"
                ],
                [
                    "Jacket/Coat",
                    "79%",
                    "81%",
                    "75%",
                    "88%"
                ],
                [
                    "Scarf",
                    "82%",
                    "84%",
                    "78%",
                    "91%"
                ],
                [
                    "Hat",
                    "82%",
                    "84%",
                    "77%",
                    "90%"
                ],
                [
                    "Beard",
                    "82%",
                    "87%",
                    "81%",
                    "90%"
                ],
                [
                    "Daytime",
                    "85%",
                    "86%",
                    "77%",
                    "89%"
                ],
                [
                    "Nighttime",
                    "74%",
                    "77%",
                    "75%",
                    "88%"
                ],
                [
                    "[BOLD] Overall",
                    "[BOLD] 82%",
                    "[BOLD] 84%",
                    "[BOLD] 77%",
                    "[BOLD] 89%"
                ],
                [
                    "\u2217 Trained with MS-COCO data set only.",
                    "\u2217 Trained with MS-COCO data set only.",
                    "\u2217 Trained with MS-COCO data set only.",
                    "\u2217 Trained with MS-COCO data set only.",
                    "\u2217 Trained with MS-COCO data set only."
                ],
                [
                    "\u2217\u2217 Trained with new driving data set only.",
                    "\u2217\u2217 Trained with new driving data set only.",
                    "\u2217\u2217 Trained with new driving data set only.",
                    "\u2217\u2217 Trained with new driving data set only.",
                    "\u2217\u2217 Trained with new driving data set only."
                ],
                [
                    "\u2217\u2217\u2217 Trained with both data sets combined.",
                    "\u2217\u2217\u2217 Trained with both data sets combined.",
                    "\u2217\u2217\u2217 Trained with both data sets combined.",
                    "\u2217\u2217\u2217 Trained with both data sets combined.",
                    "\u2217\u2217\u2217 Trained with both data sets combined."
                ]
            ],
            "title": "Table 2: Accuracy evaluation with mPCKh@0.5."
        },
        "insight": "We compared mPCKh scores of NADS-Net model and the baseline model for each individual key point location. As reported at the bottom of Table 2, the baseline model scored the average mPCKh of 82% over all key points while NADS-Net model scored 84%. Unlike the baseline model, NADS-Net does not have refining stages and have fewer parameters, but PCK-wise, shows a similar or slightly better performance. [CONTINUE] Race-wise, the model performance was slightly better on Asian populations followed by white populations. Per- formance on people with darker skin tone was noticeably lower,"
    },
    {
        "id": "691",
        "table": {
            "header": [
                "Model",
                "QM9  [ITALIC] U0",
                "Mat.Proj.",
                "OQMD"
            ],
            "rows": [
                [
                    "V-RF",
                    "-",
                    "76.8 (79.8)",
                    "74.5 (75.1)"
                ],
                [
                    "SchNet",
                    "13.6 (14.2)",
                    "31.8 (33.3)",
                    "27.5 (27.9)"
                ],
                [
                    "Proposed Model",
                    "[BOLD] 10.5 (11.1)",
                    "[BOLD] 22.7 (24.0)",
                    "[BOLD] 14.9 (15.2)"
                ]
            ],
            "title": "Table 2: Mean absolute error of formation energy predictions for V-RF, SchNet and the proposed model. For QM9 the error is in meV and for the Materials Project and OQMD the numbers are in meV/atom. The lowest error is highlighted in bold. We have obtained the V-RF results by running the implementation provided by the authors (Ward et\u00a0al., 2017), while SchNet results have been obtained by running our own SchNet implementation. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1\u00d7106 times."
        },
        "insight": "Table 2: Mean absolute error of formation energy predictions for V-RF, SchNet and the proposed model. For QM9 the error is in meV and for the Materials Project and OQMD the numbers are in meV/atom. The lowest error is highlighted in bold. We have obtained the V-RF results by running the implementation provided by the authors (Ward et al., 2017), while SchNet results have been obtained by running our own SchNet implementation. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1 \u00d7 106 times. [CONTINUE] The mean absolute error for the test set predictions are shown in Table 2. In all three benchmarks we observe a big improvement in prediction accuracy when using the edge update network."
    },
    {
        "id": "692",
        "table": {
            "header": [
                "Target",
                "Unit",
                "SchNet",
                "enn-s2s",
                "Proposed",
                "(95th)"
            ],
            "rows": [
                [
                    "[ITALIC] \u03b5HOMO",
                    "meV",
                    "41",
                    "43",
                    "[BOLD] 36.7",
                    "(37.3)"
                ],
                [
                    "[ITALIC] \u03b5LUMO",
                    "meV",
                    "34",
                    "37",
                    "[BOLD] 30.8",
                    "(31.3)"
                ],
                [
                    "\u0394 [ITALIC] \u03b5",
                    "meV",
                    "63",
                    "69",
                    "[BOLD] 58.0",
                    "(58.9)"
                ],
                [
                    "ZPVE",
                    "meV",
                    "1.7",
                    "[BOLD] 1.5",
                    "[BOLD] 1.49",
                    "(1.52)"
                ],
                [
                    "[ITALIC] \u03bc",
                    "Debye",
                    "0.033",
                    "0.030",
                    "[BOLD] 0.029",
                    "(0.029)"
                ],
                [
                    "[ITALIC] \u03b1",
                    "Bohr3",
                    "0.235",
                    "0.092",
                    "[BOLD] 0.077",
                    "(0.082)"
                ],
                [
                    "\u27e8 [ITALIC] R2\u27e9",
                    "Bohr2",
                    "[BOLD] 0.073",
                    "0.180",
                    "[BOLD] 0.072",
                    "(0.075)"
                ],
                [
                    "[ITALIC] U0",
                    "meV",
                    "14",
                    "19",
                    "[BOLD] 10.5",
                    "(11.1)"
                ],
                [
                    "[ITALIC] U",
                    "meV",
                    "19",
                    "19",
                    "[BOLD] 10.6",
                    "(11.2)"
                ],
                [
                    "[ITALIC] H",
                    "meV",
                    "14",
                    "17",
                    "[BOLD] 11.3",
                    "(11.9)"
                ],
                [
                    "[ITALIC] G",
                    "meV",
                    "14",
                    "19",
                    "[BOLD] 12.2",
                    "(12.7)"
                ],
                [
                    "[ITALIC] Cv",
                    "cal/molK",
                    "[BOLD] 0.033",
                    "0.040",
                    "[BOLD] 0.032",
                    "(0.033)"
                ]
            ],
            "title": "Table 3: Mean absolute error of predictions for different target properties of the QM9 dataset using 110k training examples. The lowest error is highlighted in bold. SchNet and enn-s2s results are from (Sch\u00fctt et\u00a0al., 2017c) and (Gilmer et\u00a0al., 2017) respectively. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1\u00d7106 times."
        },
        "insight": "Mean absolute error of predictions for different target properties of the QM9 dataset using 110k training examples. The lowest error is highlighted in bold. SchNet and enn-s2s results are from (Sch\u00fctt et al., 2017c) and (Gilmer et al., 2017) respectively. The numbers in parenthesis are the estimated 95th percentile, which have been obtained by sampling the test set (with replacement) 1 \u00d7 106 times."
    },
    {
        "id": "693",
        "table": {
            "header": [
                "Choice of Interest",
                "WaveNet",
                "LSTM"
            ],
            "rows": [
                [
                    "Number of Parameters",
                    "32",
                    "2726"
                ],
                [
                    "Optimization Objective",
                    "[ITALIC] MAE",
                    "[ITALIC] MAE"
                ],
                [
                    "Optimization Algorithm",
                    "[ITALIC] Adam",
                    "[ITALIC] Adam"
                ],
                [
                    "Learning Rate",
                    "10\u22123",
                    "10\u22123"
                ],
                [
                    "Initialization",
                    "[ITALIC] He",
                    "[ITALIC] Xavier"
                ],
                [
                    "L2 regularization",
                    "10\u22123",
                    "\u2212"
                ],
                [
                    "Mini batch size",
                    "32",
                    "32"
                ],
                [
                    "Number epochs",
                    "100",
                    "30"
                ],
                [
                    "Training Set Size",
                    "1000",
                    "1000"
                ]
            ],
            "title": "Table 1: Training Choices: Unconditional Case"
        },
        "insight": "It was found that the SGD works better and faster than the full gradient descent optimization with 20000 iterations from In the . Brief hyperparameter tuning was performed, notably for learning rate, mini batch size and number of epochs selection in the absence of a validation set. [CONTINUE] Table 1 presents the training choices for each architecture. 6 [CONTINUE] The difference in number of parameters between a WaveNet and an LSTM architecture is noticeable. [CONTINUE] The loss function is the median absolute error producing the one step ahead median estimate J(\u03b8) = |\u02c6xt+1 \u2212 xt+1| (12) 1 [CONTINUE] T \u22121 t=0 where [CONTINUE] is the size of the training set. In the convolutional neural network case the objective includes the term corresponding to the l2 penalty on the weights as well. The Xavier initialization  gives uni). Conversely, the He initialization form initial weights in the range [\u2212c, c], where c = ( 6 (nin+nout adjusts to the use of rectified linear units by using a scheme of the form [CONTINUE] orm(0, ( 2 ni ))."
    },
    {
        "id": "694",
        "table": {
            "header": [
                "Model",
                "x mean(std)",
                "y mean(std)",
                "z mean(std)"
            ],
            "rows": [
                [
                    "Unconditional WaveNet",
                    "0.00480(0.00008)",
                    "\u2212",
                    "\u2212"
                ],
                [
                    "Conditional WaveNet",
                    "0.00047(0.00020)",
                    "0.00750(\u2212)",
                    "0.00520(\u2212)"
                ],
                [
                    "Multitask Conditional WaveNet",
                    "\u2212",
                    "\u2212",
                    "0.00200(\u2212)"
                ],
                [
                    "Conditional WaveNet - Lorenz parameters ( [ITALIC] \u03c3=10, r=28, b=8/3)",
                    "0.00110(0.00140)",
                    "0.01200(0.00120)",
                    "0.00950(0.00035)"
                ],
                [
                    "Unconditional LSTM random sampling",
                    "0.00300(0.00008)",
                    "0.00629(0.00043)",
                    "0.00253(0.001)"
                ],
                [
                    "Unconditional LSTM random sampling: adjacent samples",
                    "0.01200(0.0001)",
                    "0.00700(\u2212)",
                    "0.00380(\u2212)"
                ],
                [
                    "Conditional LSTM random sampling",
                    "0.00313(0.00068)",
                    "0.00176(0.00017)",
                    "0.00250(0.00014)"
                ],
                [
                    "Conditional LSTM - Lorenz parameters ( [ITALIC] \u03c3=10, r=28, b=8/3)",
                    "0.00457(0.00045)",
                    "0.005233(0.00130)",
                    "0.01000(0.00082)"
                ]
            ],
            "title": "Table 2: Test set RMSE for one-step ahead forecast for x, y and z"
        },
        "insight": "The following table presents the RMSE for a range of scenarios calculated on the test set of size 500. Several random seeds were used, for example 1234, 1235 and 42. From  the benchmarks (average over the three series) for gradient descent with 20000 iterations unconditional WaveNet architecture is , with conditional WaveNet is, with LSTM is 0.00675, and from  with feed forward networks from 1992 the benchmark was 0.065. [CONTINUE] Calculated over two sets of starting values and parameters for the Lorenz map, the Conditional WaveNet produces and average ranging between 0.004 and 0.007. The LSTM architecture ranges around 0.0063. The gains in speed versus full gradient descent are at least ten fold. Even if the time series are of short history, there are gains to using a stochastic gradient descent optimization routine. In Table 2 several itemized results means and standard deviations are reported. [CONTINUE] The suspicion that rigorous tuning can lead to significant improvement is the 10 fold improvement in x prediction RMSE over best benchmark achieved uniformly over all runs with the conditional WaveNet and different Lorenz data distributions (different \u03c3, r and b parameters)."
    },
    {
        "id": "695",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Year",
                "[BOLD] Sce- nes",
                "[BOLD] Size (hr)",
                "[BOLD] RGB imgs",
                "[BOLD] PCs lidar\u2020\u2020",
                "[BOLD] PCs radar",
                "[BOLD] Ann. frames",
                "[BOLD] 3D boxes",
                "[BOLD] Night / Rain",
                "[BOLD] Map layers",
                "[BOLD] Clas- ses",
                "[BOLD] Locations"
            ],
            "rows": [
                [
                    "CamVid\u00a0",
                    "2008",
                    "4",
                    "0.4",
                    "18k",
                    "0",
                    "0",
                    "700",
                    "0",
                    "No/No",
                    "0",
                    "32",
                    "Cambridge"
                ],
                [
                    "Cityscapes\u00a0",
                    "2016",
                    "[EMPTY]",
                    "-",
                    "25k",
                    "0",
                    "0",
                    "25k",
                    "0",
                    "No/No",
                    "0",
                    "30",
                    "50 cities"
                ],
                [
                    "Vistas\u00a0",
                    "2017",
                    "[EMPTY]",
                    "-",
                    "25k",
                    "0",
                    "0",
                    "25k",
                    "0",
                    "Yes/Yes",
                    "0",
                    "152",
                    "Global"
                ],
                [
                    "BDD100K\u00a0",
                    "2017",
                    "100k",
                    "1k",
                    "100M",
                    "0",
                    "0",
                    "100k",
                    "0",
                    "Yes/Yes",
                    "0",
                    "10",
                    "NY, SF"
                ],
                [
                    "ApolloScape\u00a0",
                    "2018",
                    "-",
                    "100",
                    "144k",
                    "0\u2217\u2217",
                    "0",
                    "144k",
                    "70k",
                    "Yes/No",
                    "0",
                    "8-35",
                    "4x China"
                ],
                [
                    "[ITALIC] D2-City\u00a0",
                    "2019",
                    "1k\u2020",
                    "-",
                    "700k\u2020",
                    "0",
                    "0",
                    "700k\u2020",
                    "0",
                    "No/Yes",
                    "0",
                    "12",
                    "5x China"
                ],
                [
                    "KITTI\u00a0",
                    "2012",
                    "22",
                    "1.5",
                    "15k",
                    "15k",
                    "0",
                    "15k",
                    "200k",
                    "No/No",
                    "0",
                    "8",
                    "Karlsruhe"
                ],
                [
                    "AS lidar\u00a0",
                    "2018",
                    "-",
                    "2",
                    "0",
                    "20k",
                    "0",
                    "20k",
                    "475k",
                    "-/-",
                    "0",
                    "6",
                    "China"
                ],
                [
                    "KAIST\u00a0",
                    "2018",
                    "-",
                    "-",
                    "8.9k",
                    "8.9k",
                    "0",
                    "8.9k",
                    "0",
                    "[BOLD] Yes/No",
                    "0",
                    "3",
                    "Seoul"
                ],
                [
                    "H3D\u00a0",
                    "2019",
                    "160",
                    "0.77",
                    "83k",
                    "27k",
                    "0",
                    "27k",
                    "1.1M",
                    "No/No",
                    "0",
                    "8",
                    "SF"
                ],
                [
                    "nuScenes",
                    "2019",
                    "[BOLD] 1k",
                    "5.5",
                    "[BOLD] 1.4M",
                    "[BOLD] 400k",
                    "[BOLD] 1.3M",
                    "[BOLD] 40k",
                    "1.4M",
                    "[BOLD] Yes/Yes",
                    "[BOLD] 11",
                    "[BOLD] 23",
                    "Boston, SG"
                ],
                [
                    "Argoverse\u00a0",
                    "2019",
                    "113\u2020",
                    "0.6\u2020",
                    "490k\u2020",
                    "44k",
                    "0",
                    "22k\u2020",
                    "993k\u2020",
                    "[BOLD] Yes/Yes",
                    "2",
                    "15",
                    "Miami, PT"
                ],
                [
                    "Lyft L5\u00a0",
                    "2019",
                    "366",
                    "2.5",
                    "323k",
                    "46k",
                    "0",
                    "[BOLD] 46k",
                    "1.3M",
                    "No/No",
                    "7",
                    "9",
                    "Palo Alto"
                ],
                [
                    "Waymo Open\u00a0",
                    "2019",
                    "[BOLD] 1k",
                    "5.5",
                    "1M",
                    "200k",
                    "0",
                    "[BOLD] 200k\u2021",
                    "[BOLD] 12M\u2021",
                    "[BOLD] Yes/Yes",
                    "0",
                    "4",
                    "3x USA"
                ],
                [
                    "A\u22173D\u00a0",
                    "2019",
                    "[EMPTY]",
                    "[BOLD] 55",
                    "39k",
                    "39k",
                    "0",
                    "[BOLD] 39k",
                    "230k",
                    "[BOLD] Yes/Yes",
                    "0",
                    "7",
                    "SG"
                ],
                [
                    "A2D2\u00a0",
                    "2019",
                    "[EMPTY]",
                    "-",
                    "-",
                    "-",
                    "0",
                    "12k",
                    "-",
                    "-/-",
                    "0",
                    "14",
                    "3x Germany"
                ]
            ],
            "title": "Table 1: AV dataset comparison. The top part of the table indicates datasets without range data. The middle and lower parts indicate datasets (not publications) with range data released until and after the initial release of this dataset. We use bold highlights to indicate the best entries in every column among the datasets with range data. Only datasets which provide annotations for at least car, pedestrian and bicycle are included in this comparison. (\u2020) We report numbers only for scenes annotated with cuboids. (\u2021) The current Waymo Open dataset size is comparable to nuScenes, but at a 5x higher annotation frequency. (\u2020\u2020) Lidar pointcloud count collected from each lidar. (**)\u00a0[41] provides static depth maps. (-) indicates that no information is provided. SG: Singapore, NY: New York, SF: San Francisco, PT: Pittsburgh, AS: ApolloScape."
        },
        "insight": "nuScenes represents a large leap forward in terms of data volumes and complexities (Table 1), [CONTINUE] After the initial nuScenes release, [71, 10, 60, 33, 43] followed to release their own large-scale AV datasets (Table 1). [CONTINUE] Only the Waymo Open dataset provides a similarly high capture frequency of 10Hz (Table 1). [CONTINUE] One of the contributions of nuScenes is the dataset size, and in particular the increase compared to KITTI (Table 1)."
    },
    {
        "id": "696",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] NDS",
                "[BOLD] mAP",
                "[BOLD] mATE",
                "[BOLD] mASE",
                "[BOLD] mAOE",
                "[BOLD] mAVE",
                "[BOLD] mAAE"
            ],
            "rows": [
                [
                    "(%)",
                    "(%)",
                    "(%)",
                    "(m)",
                    "(1-iou)",
                    "(rad)",
                    "(m/s)",
                    "(1-acc)"
                ],
                [
                    "OFT\u00a0\u2020",
                    "21.2",
                    "12.6",
                    "0.82",
                    "0.36",
                    "0.85",
                    "1.73",
                    "0.48"
                ],
                [
                    "SSD+3D\u2020",
                    "26.8",
                    "16.4",
                    "0.90",
                    "0.33",
                    "0.62",
                    "1.31",
                    "0.29"
                ],
                [
                    "MDIS\u00a0\u2020",
                    "38.4",
                    "30.4",
                    "0.74",
                    "0.26",
                    "0.55",
                    "1.55",
                    "0.13"
                ],
                [
                    "PP\u00a0",
                    "45.3",
                    "30.5",
                    "0.52",
                    "0.29",
                    "0.50",
                    "0.32",
                    "0.37"
                ],
                [
                    "Megvii ",
                    "[BOLD] 63.3",
                    "[BOLD] 52.8",
                    "[BOLD] 0.30",
                    "[BOLD] 0.25",
                    "[BOLD] 0.38",
                    "[BOLD] 0.25",
                    "[BOLD] 0.14"
                ]
            ],
            "title": "Table 4: Object detection results on the test set of nuScenes. PointPillars, OFT and SSD+3D are baselines provided in this paper, other methods are the top submissions to the nuScenes detection challenge leaderboard. (\u2020) use only monocular camera images as input. All other methods use lidar. PP: PointPillars\u00a0[51], MDIS: MonoDIS\u00a0[70]."
        },
        "insight": "The detection challenge enabled lidar-based and camera-based detection works such as [84, 66], that improved over the state-of-the-art at the time of initial release [49, 65] by 40% and 81% (Table 4). [CONTINUE] We compare performance of published methods (Table 4) when using our proposed 2m center-distance matching versus the IOU matching used in KITTI. [CONTINUE] We compare PointPillars, which is a fast and light lidar detector with MonoDIS, a top image detector (Table 4). The two methods achieve similar mAP (30.5% vs. 30.4%), but PointPillars has higher NDS (45.3% vs. 38.4%). [CONTINUE] Compared to the mAP and NDS detection results in Table 4, the ranking is similar."
    },
    {
        "id": "697",
        "table": {
            "header": [
                "[BOLD] Lidar sweeps",
                "[BOLD] Pretraining",
                "[BOLD] NDS (%)",
                "[BOLD] mAP (%)",
                "[BOLD] mAVE (m/s)"
            ],
            "rows": [
                [
                    "1",
                    "KITTI",
                    "31.8",
                    "21.9",
                    "1.21"
                ],
                [
                    "5",
                    "KITTI",
                    "42.9",
                    "27.7",
                    "0.34"
                ],
                [
                    "10",
                    "KITTI",
                    "[BOLD] 44.8",
                    "28.8",
                    "0.30"
                ],
                [
                    "10",
                    "ImageNet",
                    "[BOLD] 44.9",
                    "28.9",
                    "0.31"
                ],
                [
                    "10",
                    "None",
                    "44.2",
                    "27.6",
                    "0.33"
                ]
            ],
            "title": "Table 3: PointPillars\u00a0[51] detection performance on the val set. We can see that more lidar sweeps lead to a significant performance increase and that pretraining with ImageNet is on par with KITTI."
        },
        "insight": "The results show that both detection and velocity estimates improve with an increasing number of lidar sweeps but with diminishing rate of return (Table 3). [CONTINUE] the final performance of the network only marginally varied between different pretrainings (Table 3)."
    },
    {
        "id": "698",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Singapore",
                "[BOLD] Rain",
                "[BOLD] Night"
            ],
            "rows": [
                [
                    "OFT\u00a0\u2020",
                    "6%",
                    "10%",
                    "[BOLD] 55%"
                ],
                [
                    "MDIS\u00a0\u2020",
                    "8%",
                    "-3%",
                    "[BOLD] 58%"
                ],
                [
                    "PP\u00a0",
                    "1%",
                    "6%",
                    "[BOLD] 36%"
                ]
            ],
            "title": "Table 6: Object detection performance drop evaluated on subsets of the nuScenes val set. Performance is reported as the relative drop in mAP compared to evaluating on the entire val set. We evaluate the performance on Singapore data, rain data and night data for three object detection methods. Note that the MDIS results are not directly comparable to other sections of this work, as a ResNet34\u00a0[39] backbone and a different training protocol are used. (\u2020) use only monocular camera images as input. PP uses only lidar."
        },
        "insight": "Class specifc performance is in Table 6-SM. PointPillars was stronger for the two most common classes: cars (68.4% vs. 47.8% AP), and pedestrians (59.7% vs. 37.0% AP). MonoDIS, on the other hand, was stronger for the smaller classes bicycles (24.5% vs. 1.1% AP) and cones (48.7% vs. 30.8% AP). [CONTINUE] Pillars  is shown in Table 6-SM (top) The per class performance of Point [CONTINUE] In Section 4.2 and Table 6-SM we show that the PointPillars baseline achieves only an AP of 1% on the bicycle class."
    },
    {
        "id": "699",
        "table": {
            "header": [
                "Device",
                "CPU (FP32)",
                "GPU (FP16)"
            ],
            "rows": [
                [
                    "Samsung Galaxy S5",
                    "79",
                    "300"
                ],
                [
                    "Samsung Galaxy S7",
                    "124",
                    "730"
                ],
                [
                    "Samsung Galaxy S9",
                    "270",
                    "730"
                ]
            ],
            "title": "Table 1: Example of available compute power on mobile in gigaflops (billion floating point instructions per second). FP16 and FP32 refer to 16- and 32-bit floating point arithmetic, respectively."
        },
        "insight": "Table 1 demonstrates that GPU has significantly more compute power than CPU."
    },
    {
        "id": "700",
        "table": {
            "header": [
                "Adreno GPU Model",
                "conv_2d",
                "depthwise_conv"
            ],
            "rows": [
                [
                    "630",
                    "(4,8,4)",
                    "(4,4,8)"
                ],
                [
                    "540",
                    "(8,2,2)",
                    "(8,8,2)"
                ],
                [
                    "510",
                    "(8,4,4)",
                    "(8,4,4)"
                ],
                [
                    "509",
                    "(8,4,8)",
                    "(8,4,2)"
                ],
                [
                    "50X/4XX",
                    "(8,4,8)",
                    "(8,4,8)"
                ]
            ],
            "title": "Table 2: Optimal work group sizes for Adreno GPUs."
        },
        "insight": "Work groups from the Table 2 are currently used in TFLite GPU and their stability is statistically proven. While they do not necessarily result in peak optimal time across all parameters, they are reliable in giving top 10% performance regardless of the convolution parameters."
    },
    {
        "id": "701",
        "table": {
            "header": [
                "Strategy",
                "MobileNet",
                "MobileNetV2",
                "DeeplabV3"
            ],
            "rows": [
                [
                    "Na\u00efve",
                    "9.6",
                    "13.2",
                    "24.3"
                ],
                [
                    "Greedy",
                    "[BOLD] 2.3",
                    "4.0",
                    "[BOLD] 3.6"
                ],
                [
                    "MCFP",
                    "2.7",
                    "[BOLD] 3.8",
                    "4.2"
                ]
            ],
            "title": "Table 3: Total memory allocated (in MB) for all intermediate tensors. Na\u00efve means no memory manager and serves as baseline. Bold number means the smallest memory footprint for each model."
        },
        "insight": "There is no clear winner between these two memory management algorithms in terms of the minimal memory footprint, and it depends on the network (Table 3). TFLite GPU is using the greedy algorithm by default with the developer being able to choose the MCFP algorithm if desired."
    },
    {
        "id": "702",
        "table": {
            "header": [
                "Android Device",
                "TFLite GPU",
                "MACE",
                "SNPE"
            ],
            "rows": [
                [
                    "Samsung S9",
                    "13",
                    "12",
                    "6.9"
                ],
                [
                    "(Adreno 630)",
                    "13",
                    "12",
                    "6.9"
                ],
                [
                    "Xiaomi Mi8 SE",
                    "35.9",
                    "29.6",
                    "20"
                ],
                [
                    "(Adreno 616)",
                    "35.9",
                    "29.6",
                    "20"
                ],
                [
                    "Huawei P20 Pro",
                    "13.5",
                    "45",
                    "N/A1"
                ],
                [
                    "(Mali G72-MP12)",
                    "13.5",
                    "45",
                    "N/A1"
                ],
                [
                    "Google Pixel 2",
                    "18",
                    "N/A2",
                    "N/A2"
                ],
                [
                    "(Adreno 540)",
                    "18",
                    "N/A2",
                    "N/A2"
                ],
                [
                    "Google Pixel 3",
                    "12.5",
                    "N/A2",
                    "N/A2"
                ],
                [
                    "12.5",
                    "N/A2",
                    "N/A2",
                    "[EMPTY]"
                ],
                [
                    "(Adreno 630)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 5: Average inference latency (in milliseconds) of Android-compatible ML frameworks on MobileNet v1. Note that TFLite GPU employs OpenGL and thus has the widest coverage with reasonable performance. MACE and SNPE employ OpenCL and may run faster on devices shipped with OpenCL, but may not run on all devices. 1 Arm Mali GPUs are not compatible with SNPE. 2 Google Pixel devices do not support OpenCL."
        },
        "insight": "Android-compatible ML frameworks on MobileNet v1, respectively. Note that TFLite GPU employs OpenGL for the widest coverage with reasonable performance. MACE and SNPE employ OpenCL and may outperform TFLite GPU on some mobile devices shipped with OpenCL. As OpenCL is not a part of the standard Android distribution, apps using those frameworks may not be able to guarantee their inference performance e.g. on Google Pixel devices. Also note that SNPE does not run on devices with Arm Mali GPUs."
    },
    {
        "id": "703",
        "table": {
            "header": [
                "[BOLD] Algorithm/Algorithm",
                "[BOLD] O_SVFDT-I",
                "[BOLD] O_SVFDT-II",
                "[BOLD] O_VFDT",
                "[BOLD] SVFDT-I",
                "[BOLD] SVFDT-II",
                "[BOLD] VFDT"
            ],
            "rows": [
                [
                    "[BOLD] O_SVFDT-I",
                    "\u2013",
                    "1",
                    "1",
                    "9",
                    "7",
                    "7"
                ],
                [
                    "[BOLD] O_SVFDT-II",
                    "6",
                    "\u2013",
                    "1",
                    "8",
                    "9",
                    "7"
                ],
                [
                    "[BOLD] O_VFDT",
                    "9",
                    "8",
                    "\u2013",
                    "9",
                    "8",
                    "8"
                ],
                [
                    "[BOLD] SVFDT-I",
                    "0",
                    "0",
                    "0",
                    "\u2013",
                    "1",
                    "1"
                ],
                [
                    "[BOLD] SVFDT-II",
                    "0",
                    "0",
                    "0",
                    "5",
                    "0",
                    "0"
                ],
                [
                    "[BOLD] VFDT",
                    "3",
                    "2",
                    "1",
                    "7",
                    "6",
                    "\u2013"
                ]
            ],
            "title": "(a) Mean Accuracy"
        },
        "insight": "We present the observed results in Tables Ia, Ib, and Ic. [CONTINUE] Considering accuracy (Table Ia), the [CONTINUE] VFDT was statistically better than the other algorithms more times than any [CONTINUE] other algorithm. Moreover, we can also observe that by adding OLBoost to the SVFDT-I and SVFDT-II, both algorithms were able to surpass the accuracy achieved by even the best performing technique (VFDT)."
    },
    {
        "id": "704",
        "table": {
            "header": [
                "Biometric",
                "Linear SVM Normal",
                "Linear SVM Normal",
                "Linear SVM Mitigation",
                "Linear SVM Mitigation",
                "Radial Svm Normal",
                "Radial Svm Normal",
                "Radial Svm Mitigation",
                "Radial Svm Mitigation",
                "Random Forest Normal",
                "Random Forest Normal",
                "Random Forest Mitigation",
                "Random Forest Mitigation",
                "Deep Neural Network Normal",
                "Deep Neural Network Normal",
                "Deep Neural Network Mitigation",
                "Deep Neural Network Mitigation"
            ],
            "rows": [
                [
                    "Modality",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR",
                    "FPR",
                    "AR"
                ],
                [
                    "Gait",
                    "0.160",
                    "0.24",
                    "0.160",
                    "0.04",
                    "0.140",
                    "0.18",
                    "0.140",
                    "0.04",
                    "0.09",
                    "0.03",
                    "0.09",
                    "0.00",
                    "0.215",
                    "0.20",
                    "0.170",
                    "0.00"
                ],
                [
                    "Touch",
                    "0.325",
                    "0.49",
                    "0.340",
                    "0.01",
                    "0.265",
                    "0.41",
                    "0.265",
                    "0.03",
                    "0.21",
                    "0.23",
                    "0.21",
                    "0.00",
                    "0.325",
                    "0.30",
                    "0.375",
                    "0.00"
                ],
                [
                    "Face",
                    "0.050",
                    "0.15",
                    "0.065",
                    "0.11",
                    "0.040",
                    "0.01",
                    "0.040",
                    "0.01",
                    "0.03",
                    "0.78",
                    "0.03",
                    "0.00",
                    "0.095",
                    "0.10",
                    "0.065",
                    "0.04"
                ],
                [
                    "Voice",
                    "0.030",
                    "0.08",
                    "0.030",
                    "0.06",
                    "0.020",
                    "0.00",
                    "0.020",
                    "0.00",
                    "0.04",
                    "0.01",
                    "0.04",
                    "0.00",
                    "0.115",
                    "0.08",
                    "0.090",
                    "0.02"
                ]
            ],
            "title": "TABLE I: Equal Error Rate and AR with and without the mitigation strategy. Green (resp., red) shades highlight improvement (resp., deterioration) in FPR and AR. Color intensity is proportional to degree of performance change."
        },
        "insight": "In Table I, we show the resulting FPR and AR after the addition of beta noise at the equal error rate. [CONTINUE] In every configuration (classifier-dataset pairs), we see a significant decrease in AR. The AR is now lower than FPR in every configuration. In 14 out of 16 cases, the AR is \u2264 0.04. The two exceptions are LinSVM (with face and voice datasets). We further see that in 13 out of 16 instances the FPR either remains unchanged or improves! The 3 instances where the FPR degrades are LinSVM with face and face datasets both by +0.015, and DNN with Touch where the difference is +0.05. [CONTINUE] Thus, adding beta distributed noise does indeed decrease the AR with minimal impact on FPR. [CONTINUE] The resulting FPR is marginally higher than the FPR from only beta-distributed noise in some cases (Table I)."
    },
    {
        "id": "705",
        "table": {
            "header": [
                "Biometric",
                "Linear SVM Normal",
                "Linear SVM Normal",
                "Linear SVM Mitigation",
                "Linear SVM Mitigation",
                "Linear SVM Mitigation",
                "Radial Svm Normal",
                "Radial Svm Normal",
                "Radial Svm Mitigation",
                "Radial Svm Mitigation",
                "Radial Svm Mitigation",
                "Random Forest Normal",
                "Random Forest Normal",
                "Random Forest Mitigation",
                "Random Forest Mitigation",
                "Random Forest Mitigation",
                "Deep Neural Network Normal",
                "Deep Neural Network Normal",
                "Deep Neural Network Mitigation",
                "Deep Neural Network Mitigation",
                "Deep Neural Network Mitigation"
            ],
            "rows": [
                [
                    "Modality",
                    "FPR",
                    "RAR",
                    "FPR",
                    "[ITALIC] \u03b2-RAR",
                    "RAR",
                    "FPR",
                    "RAR",
                    "FPR",
                    "[ITALIC] \u03b2-RAR",
                    "RAR",
                    "FPR",
                    "RAR",
                    "FPR",
                    "[ITALIC] \u03b2-RAR",
                    "RAR",
                    "FPR",
                    "RAR",
                    "FPR",
                    "[ITALIC] \u03b2-RAR",
                    "RAR"
                ],
                [
                    "Touch Raw",
                    "0.325",
                    "0.45",
                    "0.345",
                    "0.44",
                    "0.00",
                    "0.265",
                    "0.40",
                    "0.265",
                    "0.36",
                    "0.01",
                    "0.21",
                    "0.18",
                    "0.215",
                    "0.05",
                    "0.00",
                    "0.325",
                    "0.32",
                    "0.38",
                    "0.26",
                    "0.00"
                ],
                [
                    "Face Raw",
                    "0.050",
                    "0.12",
                    "0.075",
                    "0.14",
                    "0.00",
                    "0.040",
                    "0.09",
                    "0.040",
                    "0.09",
                    "0.00",
                    "0.03",
                    "0.02",
                    "0.030",
                    "0.01",
                    "0.00",
                    "0.095",
                    "0.10",
                    "0.07",
                    "0.06",
                    "0.03"
                ]
            ],
            "title": "TABLE II: Equal Error Rate and RAR with and without the mitigation strategy. The AR values remain the same as in Table\u00a0I. \u03b2-RAR indicates RAR treated with only \u03b2 noise. RAR indicates the inclusion of both \u03b2 noise and raw random input samples."
        },
        "insight": "Interestingly, beta distributed noise only marginally reduces the raw acceptance rate as can be seen in Table II (columns labeled \u03b2-RAR). [CONTINUE] Table II shows that the mitigation strategy reduces the RAR to less than or equal to 0.03 in all instances (columns labeled RAR). [CONTINUE] Thus, the inclusion of betadistributed noise in conjunction with subset of raw inputs in the training data reduces both AR and RAR with minimal impact on FPR and FRR."
    },
    {
        "id": "706",
        "table": {
            "header": [
                "[BOLD] Approaches",
                "[BOLD] Accuracy (%)"
            ],
            "rows": [
                [
                    "CNN using RGB data",
                    "80.8\u00b11.4"
                ],
                [
                    "CNN using OC-LBCP",
                    "66.6\u00b12.2"
                ],
                [
                    "Dual-stream CNN (without shared weights)",
                    "82.1\u00b11.6"
                ],
                [
                    "[BOLD] Proposed network",
                    "[BOLD] 85.0\u00b11.9"
                ]
            ],
            "title": "Table 2: Performance analysis on rank-1 recognition accuracy. The highest accuracy is written in bold."
        },
        "insight": "Table 2 presents the performance analysis on the capability of a single-stream CNN with respective features, followed by dual-stream CNN without sharing the weights in conv and fc layers, and our proposed network. Note that, the dual-stream CNN without sharing [CONTINUE] the weights also implemented the late-fusion and score fusion. [CONTINUE] Table 2 shows that our proposed network achieved the highest rank-1 recognition accuracy with 85.0\u00b11.9%. However, single-stream CNN only achieved 80.8\u00b11.4% and 66.6\u00b12.2% with RGB data and OC-LBCP, respectively. As compared to single-stream CNN, the results indicate that the late-fusion layers are significant to correlate the RGB data and OC-LBCP in order to achieve better recognition performance. [CONTINUE] Compared with dual-stream CNN without shared weights, the experimental results in Table 2 show that our proposed network is well-performed than dual-stream CNN without shared weights at least 2.9% improvement. This is because our proposed network utilised the shared conv and the fusion fc layers to aggregate the RGB data and OCLBCP."
    },
    {
        "id": "707",
        "table": {
            "header": [
                "[BOLD] Approach",
                "[BOLD] Ethnic-ocular (%)  [ITALIC] Rank-1",
                "[BOLD] Ethnic-ocular (%)  [ITALIC] Rank-5",
                "[BOLD] UBIPr (%)  [ITALIC] Rank-1",
                "[BOLD] UBIPr (%)  [ITALIC] Rank-5"
            ],
            "rows": [
                [
                    "AlexNet",
                    "64.72\u00b13.28",
                    "82.98\u00b12.52",
                    "84.88\u00b12.50",
                    "96.01\u00b11.77"
                ],
                [
                    "FaceNet",
                    "78.71\u00b13.66",
                    "92.19\u00b11.59",
                    "90.24\u00b11.43",
                    "97.36\u00b10.44"
                ],
                [
                    "LCNN-29",
                    "79.35\u00b12.64",
                    "92.17\u00b11.80",
                    "90.28\u00b11.71",
                    "97.18\u00b10.67"
                ],
                [
                    "VGG-16",
                    "76.43\u00b12.16",
                    "91.29\u00b11.54",
                    "90.24\u00b11.38",
                    "97.09\u00b11.14"
                ],
                [
                    "DeepIrisNet-A",
                    "79.54\u00b13.12",
                    "90.43\u00b12.44",
                    "90.30\u00b11.16",
                    "97.41\u00b11.07"
                ],
                [
                    "DeepIrisNet-B",
                    "81.13\u00b13.08",
                    "92.37\u00b11.20",
                    "90.20\u00b11.66",
                    "97.43\u00b10.54"
                ],
                [
                    "Multi-abstract fusion CNN",
                    "81.79\u00b13.54",
                    "93.03\u00b11.33",
                    "90.75\u00b11.01",
                    "97.44\u00b10.34"
                ],
                [
                    "[BOLD] Proposed network",
                    "[BOLD] 85.03\u00b11.88",
                    "[BOLD] 94.23\u00b11.26",
                    "[BOLD] 91.28\u00b11.18",
                    "[BOLD] 98.59\u00b10.44"
                ]
            ],
            "title": "Table 3: Evaluation of recognition performance on the Ethnic-ocular dataset and UBIPr dataset. The highest accuracy is written in bold."
        },
        "insight": "Table 3 presents that dual-stream CNN achieves the highest average rank-1 and rank-5 recognition accuracies with 91.28\u00b11.18% and 98.59\u00b10.44%, respectively. The second best is achieved by multi-abstract fusion CNN with 90.75\u00b11.01% and 97.44\u00b10.34% as rank-1 and rank-5 accuracies. [CONTINUE] We presented the experimental results in Table 3 by following the recognition protocol as mentioned in Section 4.2. To evaluate the performance of [CONTINUE] the proposed network, we compared our results with seven benchmark approaches (see Table 3). For the results of recognition, our proposed network achieved 85.03\u00b11.88% and 94.23\u00b11.26% as rank-1 and rank-5 accuracies, respectively. [CONTINUE] Besides, the results proved that our network can learn new features from the late-fusion layers for better recognition."
    },
    {
        "id": "708",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Params",
                "[BOLD] Valid. Set",
                "[BOLD] Test Set"
            ],
            "rows": [
                [
                    "[BOLD] Single Models",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Large Regularized LSTM Zaremba et\u00a0al. ( 2015 )",
                    "66M",
                    "82.2",
                    "78.4"
                ],
                [
                    "Large + BD + WT Press and Wolf ( 2016 )",
                    "51M",
                    "75.8",
                    "73.2"
                ],
                [
                    "Neural cache model (size = 500) Grave et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "72.1"
                ],
                [
                    "Medium Pointer Sentinel-LSTM Merity et\u00a0al. ( 2017 )",
                    "21M",
                    "72.4",
                    "70.9"
                ],
                [
                    "Attentive LM w/  [ITALIC] combined score function Salton et\u00a0al. ( 2017 )",
                    "14.5M",
                    "72.6",
                    "70.7"
                ],
                [
                    "Attentive LM w/  [ITALIC] single score function Salton et\u00a0al. ( 2017 )",
                    "14.5M",
                    "71.7",
                    "70.1"
                ],
                [
                    "Averaging RNN-LM",
                    "14.1M",
                    "71.6",
                    "[BOLD] 69.9"
                ],
                [
                    "[BOLD] Model Averaging",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "2 Medium regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "40M",
                    "80.6",
                    "77.0"
                ],
                [
                    "5 Medium regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "100M",
                    "76.7",
                    "73.3"
                ],
                [
                    "10 Medium regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "200M",
                    "75.2",
                    "72.0"
                ],
                [
                    "2 Large regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "122M",
                    "76.9",
                    "73.6"
                ],
                [
                    "10 Large regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "660M",
                    "72.8",
                    "69.5"
                ],
                [
                    "38 Large regularized LSTMs Zaremba et\u00a0al. ( 2015 )",
                    "2508M",
                    "71.9",
                    "[BOLD] 68.7"
                ]
            ],
            "title": "Table 1: Perplexity results over the PTB. Symbols: WT = weight tying Press and Wolf (2016); BD = Bayesian Dropout Gal and Ghahramani (2015). Please note that we could not calculate the number of parameters for some models given missing information in the original publications."
        },
        "insight": "Table 1 presents the results in terms of perplexity of the models trained over the PTB dataset. [CONTINUE] The Averaging LSTM-LM achieves the lowest perplexity for a single model on the PTB [CONTINUE] Given the similarity of the results between the Attentive RNN-LMs [CONTINUE] and the Averaging LSTM-LM it would appear that our hypothesis that the Attentive RNN-LMs was (indirectly) learning to use the dynamics of the LSTM gating mechanism is correct. [CONTINUE] The Averaging LSTM-LM is also very competitive on the PTB compared to the ensemble methods and in this case the difference in total number of parameters between the Averaging LSTM-LM and these ensemble models is significant."
    },
    {
        "id": "709",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Params",
                "[BOLD] Valid. Set",
                "[BOLD] Test Set"
            ],
            "rows": [
                [
                    "Zoneout + Variational LSTM Merity et\u00a0al. ( 2017 )",
                    "20M",
                    "108.7",
                    "100.9"
                ],
                [
                    "LSTM-LM Grave et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "99.3"
                ],
                [
                    "Variational LSTM Merity et\u00a0al. ( 2017 )",
                    "20M",
                    "101.7",
                    "96.3"
                ],
                [
                    "Neural cache model (size = 100) Grave et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "81.6"
                ],
                [
                    "Pointer LSTM (window = 100) Merity et\u00a0al. ( 2017 )",
                    "21M",
                    "84.8",
                    "80.8"
                ],
                [
                    "Averaging RNN-LM",
                    "50M",
                    "74.6",
                    "71.3"
                ],
                [
                    "Attentive LM w/  [ITALIC] combined score function Salton et\u00a0al. ( 2017 )",
                    "51M",
                    "74.3",
                    "70.8"
                ],
                [
                    "Attentive LM w/  [ITALIC] single score function Salton et\u00a0al. ( 2017 )",
                    "51M",
                    "73.7",
                    "69.7"
                ],
                [
                    "Neural cache model (size = 2000) Grave et\u00a0al. ( 2017 )",
                    "-",
                    "-",
                    "[BOLD] 68.9"
                ]
            ],
            "title": "Table 2: Perplexity results over the wikitext2. Please note that we could not calculate the number of parameters for some models given missing information in the original publications."
        },
        "insight": "Table 2 presents the results in terms of perplexity of the models trained over the wikitext2 dataset. [CONTINUE] Focusing on the results for the wikitext2 dataset (see Table 2) the Neural cache model is still the best performing model on this dataset. [CONTINUE] In discussing the wikietext2 results it is worth noting that the Attentive RNN-LMs and the Averaging LSTM-LM are the only models in Table 2 that reset their memory at each sentence boundary whereas the memory buffers of other models were allowed to span sentence boundaries. [CONTINUE] Given this difference the competitive performance of the Averaging LSTM-LM is encouraging."
    },
    {
        "id": "710",
        "table": {
            "header": [
                "\u2113\u221e pert.",
                "MNIST 0.03",
                "MNIST 0.1",
                "MNIST 0.2",
                "MNIST 0.3",
                "MNIST 0.4",
                "CIFAR-10 0.03",
                "CIFAR-10 0.1",
                "CIFAR-10 0.2",
                "CIFAR-10 0.3",
                "CIFAR-10 0.4"
            ],
            "rows": [
                [
                    "FGSM",
                    "97.8%",
                    "97.6%",
                    "97.1%",
                    "95.4%",
                    "91%",
                    "48.2%",
                    "47.9%",
                    "45.3%",
                    "39.7%",
                    "31.1"
                ],
                [
                    "BIM",
                    "97.8%",
                    "97.6%",
                    "96.7%",
                    "95.2%",
                    "91%",
                    "48.2%",
                    "47.7%",
                    "45.2%",
                    "39%",
                    "30.1%"
                ],
                [
                    "PGD",
                    "97.8%",
                    "97.7%",
                    "97%",
                    "95.2%",
                    "91.3%",
                    "48.3%",
                    "47.6%",
                    "45.4%",
                    "41.6%",
                    "37.1%"
                ],
                [
                    "MIM",
                    "97.8%",
                    "97.6%",
                    "[BOLD] 95.8%",
                    "[BOLD] 87.4%",
                    "[BOLD] 67.5%",
                    "48.2%",
                    "47.5%",
                    "40.4%",
                    "[BOLD] 26.1%",
                    "[BOLD] 15.6%"
                ],
                [
                    "Blackbox",
                    "[BOLD] 97.6 %",
                    "[BOLD] 97.4 %",
                    "95.9%",
                    "92.1%",
                    "84.1%",
                    "[BOLD] 47.6%",
                    "[BOLD] 45.1%",
                    "[BOLD] 36.7%",
                    "26.4%",
                    "18.5%"
                ],
                [
                    "\u21132 pert.",
                    "0.1",
                    "0.7",
                    "1.1",
                    "3.2",
                    "4",
                    "0.3",
                    "2",
                    "4",
                    "6.5",
                    "10.5"
                ],
                [
                    "FGM",
                    "97.8%",
                    "97.8%",
                    "97.8%",
                    "97.3%",
                    "97.1%",
                    "48.1%",
                    "48.1%",
                    "48%",
                    "47.3%",
                    "45.4%"
                ],
                [
                    "PGD",
                    "97.8%",
                    "97.8%",
                    "97.8%",
                    "[BOLD] 97.3%",
                    "[BOLD] 96.7%",
                    "48.3%",
                    "48.3%",
                    "[BOLD] 47.5%",
                    "[BOLD] 45.7%",
                    "[BOLD] 39.3%"
                ],
                [
                    "CW",
                    "[BOLD] 97.7%",
                    "[BOLD] 97.4%",
                    "[BOLD] 97.7%",
                    "97.7%",
                    "97.8%",
                    "[BOLD] 48%",
                    "[BOLD] 48%",
                    "47.9%",
                    "46.5%",
                    "39.5%"
                ]
            ],
            "title": "TABLE I: Accuracy of ensemble of 50 PPD models on \u2113\u221e and \u21132 attacks for MNIST and CIFAR-10. The lowest accuracy for each perturbation strength is in bold."
        },
        "insight": "achieving 88.8% accuracy on (cid:96)\u221e attack of strength 0.3 for MNIST 4 and 44.7% on (cid:96)\u221e attack of strength 0.03 on CIFAR10 5. With the same attack budget, our worst accuracy on MNIST and CIFAR-10 are 87.4% and 47.6%, respectively (Table I). [CONTINUE] Increasing (cid:96)\u221e attack budget to 0.4 for MNIST and 0.1 for CIFAR-10 brings Madry's accuracy down to 0% and 10%, respectively (Figure 6 of ), while PPD still gives accuracy of 67.5% and 45.1%, respectively. [CONTINUE] This is while PPD retains significant resistance to (cid:96)2 perturbations (Table I)."
    },
    {
        "id": "711",
        "table": {
            "header": [
                "Time Gap",
                "[BOLD] AUC LSTM+SA",
                "[BOLD] AUC LSTM"
            ],
            "rows": [
                [
                    "60",
                    "0.83",
                    "0.72"
                ],
                [
                    "120",
                    "0.82",
                    "0.72"
                ],
                [
                    "180",
                    "0.81",
                    "0.72"
                ],
                [
                    "240",
                    "0.81",
                    "0.71"
                ]
            ],
            "title": "Table 3: Mean AUC results of cross-validation procedure."
        },
        "insight": "As baseline, we compared with a LSTM network without self-attention, but with the same pre-trained input embeddings. Table 3 report the average area under ROC curve (AUC) over folds with the self-attentive model (LSTM+SA) and with standard LSTM. [CONTINUE] Comparing self-attentive model with standard LSTM we can see a better performance. This is due to the ability to deal with longer inputs with the self-attention (SA) mechanism. Figure 1 shows a part of neural network attention mapping scores for a particular individual. SA allowing us to see which parts of the input are attended to predict diabetes disease evolution."
    },
    {
        "id": "712",
        "table": {
            "header": [
                "[BOLD] Layer",
                "[BOLD] Output size",
                "[BOLD] Stride",
                "[BOLD] Repeats"
            ],
            "rows": [
                [
                    "Raster image",
                    "300\u00d7300\u00d73",
                    "\u2212",
                    "\u2212"
                ],
                [
                    "Conv 3\u00d73",
                    "150\u00d7150\u00d724",
                    "2",
                    "1"
                ],
                [
                    "DwConv 3\u00d73",
                    "75\u00d775\u00d724",
                    "2",
                    "1"
                ],
                [
                    "FMNet block 1",
                    "75\u00d775\u00d712",
                    "1",
                    "2"
                ],
                [
                    "FMNet block 2",
                    "38\u00d738\u00d716",
                    "2",
                    "3"
                ],
                [
                    "FMNet block 3",
                    "19\u00d719\u00d732",
                    "2",
                    "4"
                ],
                [
                    "FMNet block 4",
                    "19\u00d719\u00d748",
                    "1",
                    "3"
                ],
                [
                    "FMNet block 5",
                    "10\u00d710\u00d780",
                    "2",
                    "3"
                ],
                [
                    "FMNet block 6",
                    "10\u00d710\u00d7160",
                    "1",
                    "1"
                ],
                [
                    "Conv 1\u00d71",
                    "10\u00d710\u00d7640",
                    "1",
                    "1"
                ],
                [
                    "Global average pooling",
                    "1\u00d71\u00d7640",
                    "1",
                    "1"
                ]
            ],
            "title": "TABLE I: Architecture of FastMobileNet (upsample factor for all FMNet blocks is set to k=6)"
        },
        "insight": "the FMNet architecture corresponding to the CNN part is shown in Table I, where the layer sizes and block repeats of the model are based on MNv2-0.5  (i.e., MNv2 with halved channel sizes in all layers)."
    },
    {
        "id": "713",
        "table": {
            "header": [
                "[BOLD] Architecture",
                "[BOLD] ADE [m]",
                "[BOLD] Latency [ms]",
                "[BOLD] FLOPS",
                "[BOLD] Num. parameters",
                "[BOLD] MAC",
                "[BOLD] Num. ops"
            ],
            "rows": [
                [
                    "AlexNet",
                    "1.36",
                    "15.8",
                    "2.63G",
                    "70.3M",
                    "364 MB",
                    "[BOLD] 131"
                ],
                [
                    "ResNet18",
                    "1.29",
                    "36.2",
                    "6.26G",
                    "11.7M",
                    "163 MB",
                    "641"
                ],
                [
                    "MNv2-0.5",
                    "1.27",
                    "21.3",
                    "308M",
                    "598K",
                    "146 MB",
                    "1542"
                ],
                [
                    "MnasNet-0.5",
                    "1.28",
                    "18.3",
                    "323M",
                    "844K",
                    "113 MB",
                    "1490"
                ],
                [
                    "FMNet",
                    "1.28",
                    "12.1",
                    "340M",
                    "565K",
                    "55 MB",
                    "336"
                ],
                [
                    "FMNet with spatial fusion",
                    "[BOLD] 1.24",
                    "[BOLD] 10.4",
                    "[BOLD] 285M",
                    "[BOLD] 558K",
                    "[BOLD] 47 MB",
                    "370"
                ]
            ],
            "title": "TABLE II: Comparison of various CNN architectures (all models except the last one use the concatenation feature fusion)"
        },
        "insight": "First, we compared the prediction accuracy and inference latency on several base CNN architectures. We found that the proposed FMNet gives similar prediction accuracy as other modern architectures such as ResNet, MNv2, and MnasNet, while being much faster during inference. In terms of the number of FLOPS and parameters, FMNet is similar to MnasNet-0.5 and MNv2-0.5 (which it is based on), while AlexNet and ResNet18 have many more FLOPS and parameters. Fast inference of FMNet can be explained by low MAC and operation counts, which we specifically optimized for during the model design phase. It is interesting to note that AlexNet is the second fastest CNN while having the second largest FLOPS. This is possibly due to it having the smallest number of layers, as evidenced by its lowest operation counts, although its accuracy is not on par with the competing networks. [CONTINUE] In the first set of experiments we compared a number of CNN architectures, summarizing results in Table II. To ensure fair comparison and avoid potential issues with small data sets, we trained all models on vehicle actors where we set the prediction horizon to 6s. Average prediction error and latency are reported in the table. We skip the feature fusion layers when computing the number of parameters, as the concatenation feature fusion adds a large amount of parameters which complicates the comparison (an 1024 \u00d7 4096 fully-connected layer for feature fusion adds 4M extra parameters). MAC is approximated by the sum of tensor sizes of all graph nodes. Column \"Num. ops\" refers to the total number of operations in the TensorFlow graph of each model. The inference latency is measured at a batch of 32 actors on a GTX 1080Ti GPU. [CONTINUE] For simplicity and to facilitate fair comparison, we implemented all models using TensorFlow built-in operations without additional optimization. [CONTINUE] Secondly, we found that FMNet with spatial feature fusion further improves the accuracy and inference time when compared to the model with feature fusion through concatenation. [CONTINUE] This resulted in lower FLOPS, as well as lower number of parameters and MAC. Following these results we use the best performing FMNet with spatial fusion as the model architecture in the following ablation studies on rasterization choices."
    },
    {
        "id": "714",
        "table": {
            "header": [
                "[BOLD] Approach",
                "[BOLD] Resolution",
                "[BOLD] Bicyclists  [BOLD] Average",
                "[BOLD] Bicyclists  [BOLD] @1s",
                "[BOLD] Bicyclists  [BOLD] @5s",
                "[BOLD] Pedestrians  [BOLD] Average",
                "[BOLD] Pedestrians  [BOLD] @1s",
                "[BOLD] Pedestrians  [BOLD] @5s"
            ],
            "rows": [
                [
                    "UKF",
                    "\u2212",
                    "2.89",
                    "0.80",
                    "6.60",
                    "0.67",
                    "0.22",
                    "1.22"
                ],
                [
                    "Social-LSTM",
                    "\u2212",
                    "3.79",
                    "1.85",
                    "6.61",
                    "0.53",
                    "0.29",
                    "0.95"
                ],
                [
                    "RasterNet",
                    "0.1 [ITALIC] m",
                    "1.07",
                    "0.43",
                    "2.73",
                    "[BOLD] 0.51",
                    "[BOLD] 0.17",
                    "[BOLD] 0.90"
                ],
                [
                    "RasterNet",
                    "0.2 [ITALIC] m",
                    "1.07",
                    "0.44",
                    "2.72",
                    "0.52",
                    "0.18",
                    "0.93"
                ],
                [
                    "RasterNet",
                    "0.3 [ITALIC] m",
                    "1.09",
                    "0.45",
                    "2.80",
                    "0.53",
                    "0.18",
                    "0.95"
                ],
                [
                    "RasterNet w/o rotation",
                    "0.2 [ITALIC] m",
                    "1.29",
                    "0.49",
                    "3.30",
                    "0.58",
                    "0.20",
                    "1.02"
                ],
                [
                    "RasterNet w/o traffic lights",
                    "0.2 [ITALIC] m",
                    "1.11",
                    "0.44",
                    "2.86",
                    "0.55",
                    "0.20",
                    "0.96"
                ],
                [
                    "RasterNet w/o lane headings",
                    "0.2 [ITALIC] m",
                    "1.07",
                    "0.43",
                    "2.72",
                    "0.52",
                    "0.18",
                    "0.93"
                ],
                [
                    "RasterNet with learned colors",
                    "0.2 [ITALIC] m",
                    "[BOLD] 1.05",
                    "[BOLD] 0.42",
                    "[BOLD] 2.70",
                    "0.53",
                    "0.18",
                    "0.93"
                ],
                [
                    "RasterNet vehicle model",
                    "0.2 [ITALIC] m",
                    "3.11",
                    "0.89",
                    "8.47",
                    "1.96",
                    "0.40",
                    "3.82"
                ],
                [
                    "RasterNet vehicle fine-tuned",
                    "0.2 [ITALIC] m",
                    "[BOLD] 1.05",
                    "[BOLD] 0.42",
                    "[BOLD] 2.70",
                    "0.59",
                    "0.20",
                    "1.05"
                ]
            ],
            "title": "TABLE III: Comparison of prediction displacement errors (in meters) for different experimental settings"
        },
        "insight": "We conducted ablation study regarding the rasterization setup, modifying various parameters of the rasterization configuration of the base setup. Empirical results in terms of average prediction error, as well as short- and long-term errors are given in Table III. We can see that the all variants of the proposed CNN approach (referred to as RasterNet) significantly outperform UKF, especially at longer horizons. [CONTINUE] As can be seen, for pedestrians 0.1m-resolution indeed resulted in lower error, while setting 0.3m gave the worst performance. We observed that resolution of 0.1m showed no significant difference for bicyclists, while 0.3m resulted in slightly higher error. [CONTINUE] Next, we evaluated the impact of not rotating raster such that actor heading points up, as discussed in Section III-B, [CONTINUE] which resulted in a significant drop of accuracy for both actor types. [CONTINUE] We observed error increase without traffic light rasterization for both actor types, matching this intuition. [CONTINUE] Furthermore, we removed lane heading information provided in raster images, encoded by using different colors to indicate different directions. Without lane heading bicyclist model degraded slightly in performance, while pedestrian model was unaffected. [CONTINUE] Finally, we tried learning raster colors instead of setting them manually. The results indicate that learned colors slightly improved accuracy for bicyclists, whereas the pedestrian model slightly degraded compared to the baseline. [CONTINUE] Lastly, due to significantly larger amount of vehicle data as compared to VRUs, instead of training from scratch we finetuned VRU models using preloaded weights from a pretrained vehicle model. As observed in the second-to-last row, directly applying the vehicle model to bicyclists and pedestrians without fine-tuning results in poor performance across the board, even worse than UKF, due to a different nature of these actor types as compared to vehicles. On the other hand, with additional fine-tuning using training data of corresponding actor types the bicyclist performance improved over the baseline, indicating that bicyclists may exhibit similar behavior to vehicles. We can also see that the pedestrian model regressed, explained by the fact that pedestrian motion is very different from vehicle motion, thus making vehicle pretraining ineffective."
    },
    {
        "id": "715",
        "table": {
            "header": [
                "Model",
                "Generated Images CIFAR-10",
                "Generated Images CelebA",
                "Generated Images Flickr-Face",
                "Reconstructed Images CIFAR-10",
                "Reconstructed Images CelebA",
                "Reconstructed Images Flickr-Face"
            ],
            "rows": [
                [
                    "VAE",
                    "130.62 \u00b1 0.37",
                    "57.19 \u00b1 0.09",
                    "79.35 \u00b1 0.32",
                    "122.54 \u00b1 0.09",
                    "48.57 \u00b1 0.05",
                    "73.56 \u00b1 0.24"
                ],
                [
                    "VAE-c",
                    "104.35 \u00b1 0.31",
                    "53.87 \u00b1 0.16",
                    "80.52 \u00b1 0.14",
                    "92.23 \u00b1 0.21",
                    "40.78 \u00b1 0.03",
                    "70.96 \u00b1 0.10"
                ],
                [
                    "Wavelet-VAE",
                    "112.45 \u00b1 0.17",
                    "49.96 \u00b1 0.17",
                    "[BOLD] 71.38 \u00b1 0.14",
                    "92.40 \u00b1 0.12",
                    "43.94 \u00b1 0.08",
                    "[BOLD] 65.72 \u00b1 0.07"
                ],
                [
                    "Wavelet-VAE-MR",
                    "[BOLD] 101.86 \u00b1 0.28",
                    "[BOLD] 47.92 \u00b1 0.08",
                    "84.90 \u00b1 0.28",
                    "[BOLD] 80.65 \u00b1 0.13",
                    "[BOLD] 37.89 \u00b1 0.07",
                    "71.21 \u00b1 0.14"
                ]
            ],
            "title": "Table 1: FID score comparison for generated and reconstructed images of the presented Wavelet-VAE and Wavelet-VAE-MR models against standard VAE trained with MSE loss and VAE trained with cross-entropy loss for the likelihood term. For each dataset, we performed 5 independent trials and report the mean and standard deviation of the FID scores."
        },
        "insight": "In Table 1, we present the FID score for the presented Wavelet-VAE compared against the two standard VAE models for both generated and reconstructed image samples. [CONTINUE] For all three natural image datasets, the Wavelet-VAE is consistently better than both the baseline VAE models. Among the two out of three datasets, we achieved better performance using the multi-resolution version of the proposed Wavelet-VAE."
    },
    {
        "id": "716",
        "table": {
            "header": [
                "URL",
                "First Time Visit",
                "Last Time Visit",
                "URL Counts",
                "Frecency"
            ],
            "rows": [
                [
                    "https://web.facebook.com/",
                    "1521241972",
                    "1522351859",
                    "177",
                    "56640"
                ],
                [
                    "http://localhost/phpmyadmin/",
                    "1518413861",
                    "1522075694",
                    "24",
                    "39312"
                ],
                [
                    "https://mail.google.com/mail/u/",
                    "1516596003",
                    "1522352010",
                    "36",
                    "33264"
                ],
                [
                    "https://github.com/",
                    "1517215489",
                    "1522352266",
                    "37",
                    "27528"
                ],
                [
                    "https://www.youtube.com/",
                    "1517229227",
                    "1521978502",
                    "24",
                    "14792"
                ]
            ],
            "title": "TABLE I: Sample Browser history dataset"
        },
        "insight": "Thus after preprocessing, sample dataset looks like Table I:"
    },
    {
        "id": "717",
        "table": {
            "header": [
                "Mean Square Error",
                "Root Mean Square Error",
                "Score"
            ],
            "rows": [
                [
                    "586430.6574",
                    "765.7876",
                    "87.62%"
                ]
            ],
            "title": "TABLE III: Experimental results of frecency prediction."
        },
        "insight": "Experimental results of frecency prediction are shown in Table III."
    },
    {
        "id": "718",
        "table": {
            "header": [
                "URL",
                "URL Counts",
                "Frecency"
            ],
            "rows": [
                [
                    "http://localhost/phpmyadmin/",
                    "16",
                    "2906.7627"
                ],
                [
                    "http://localhost:8888/tree",
                    "15",
                    "2717.497"
                ],
                [
                    "http://localhost:8000/home",
                    "13",
                    "2274.1109"
                ]
            ],
            "title": "TABLE IV: Predicted Frecency."
        },
        "insight": "The link prediction for the search term \"loc\" will predict Table IV URLs."
    },
    {
        "id": "719",
        "table": {
            "header": [
                "mean",
                "standard-deviation",
                "n-gram-range",
                "tfidf-use-idf",
                "clf-alpha"
            ],
            "rows": [
                [
                    "0.69971",
                    "0.00035",
                    "(1,2)",
                    "True",
                    "0.01"
                ]
            ],
            "title": "TABLE V: Best Parameters"
        },
        "insight": "Table V and all parameters list is shown in Table VI. The best parameters are found by random search analyzing 8 iterations. Here clf-alpha is a learning rate. Table VII shows the Precision, Recall, F1 score and"
    },
    {
        "id": "720",
        "table": {
            "header": [
                "mean",
                "standard-deviation",
                "n-gram-range",
                "tfidf-use-idf",
                "clf-alpha"
            ],
            "rows": [
                [
                    "0.69245",
                    "0.00036",
                    "(1,1)",
                    "True",
                    "0.01"
                ],
                [
                    "0.69971",
                    "0.00035",
                    "(1,2)",
                    "True",
                    "0.01"
                ],
                [
                    "0.69460",
                    "0.00053",
                    "(1,1)",
                    "False",
                    "0.01"
                ],
                [
                    "0.69702",
                    "0.00047",
                    "(1,2)",
                    "False",
                    "0.01"
                ],
                [
                    "0.69153",
                    "0.00028",
                    "(1,1)",
                    "True",
                    "0.001"
                ],
                [
                    "0.69804",
                    "0.00034",
                    "(1,2)",
                    "True",
                    "0.001"
                ],
                [
                    "0.69348",
                    "0.00062",
                    "(1,1)",
                    "True",
                    "0.001"
                ],
                [
                    "0.69614",
                    "0.00047",
                    "(1,2)",
                    "True",
                    "0.001"
                ]
            ],
            "title": "TABLE VI: All Parameters list by random search"
        },
        "insight": "Table V and all parameters list is shown in Table VI. The best parameters are found by random search analyzing 8 iterations. Here clf-alpha is a learning rate. Table VII shows the Precision, Recall, F1 score and"
    },
    {
        "id": "721",
        "table": {
            "header": [
                "Class",
                "Precision",
                "Recall",
                "F1 score",
                "Accuracy"
            ],
            "rows": [
                [
                    "2",
                    "84.35%",
                    "83.33%",
                    "83.22%",
                    "83.33%"
                ],
                [
                    "4",
                    "73.45%",
                    "68.33%",
                    "66.29%",
                    "68.33%"
                ],
                [
                    "15",
                    "61.66%",
                    "44.01%",
                    "39.74%",
                    "44.01%"
                ]
            ],
            "title": "TABLE VII: Classification Accuracy Without Random Search (Previous Method)"
        },
        "insight": "Table V and all parameters list is shown in Table VI. The best parameters are found by random search analyzing 8 iterations. Here clf-alpha is a learning rate. Table VII shows the Precision, Recall, F1 score and"
    },
    {
        "id": "722",
        "table": {
            "header": [
                "Class",
                "Precision",
                "Recall",
                "F1 score",
                "Accuracy"
            ],
            "rows": [
                [
                    "2",
                    "85.70%",
                    "84.34%",
                    "84.20%",
                    "84.34%"
                ],
                [
                    "4",
                    "73.74%",
                    "71.23%",
                    "70.54%",
                    "71.23%"
                ],
                [
                    "15",
                    "56.34%",
                    "49.87%",
                    "48.66%",
                    "49.87%"
                ]
            ],
            "title": "TABLE VIII: Classification Accuracy With Random Search (Proposed Method)"
        },
        "insight": "Comparing Table VII and Table VIII, it is clear that our approach gives a better result than previous."
    },
    {
        "id": "723",
        "table": {
            "header": [
                "URL",
                "URL Counts",
                "Frecency",
                "Category"
            ],
            "rows": [
                [
                    "https://web.facebook.com/",
                    "543",
                    "102108.26",
                    "Computers"
                ],
                [
                    "https://drive.google.com/drive/my-drive",
                    "28",
                    "4873.655",
                    "Computers"
                ],
                [
                    "http://codeforces.com/contests",
                    "21",
                    "3650.896",
                    "Arts"
                ],
                [
                    "https://www.floydhub.com/jobs",
                    "4",
                    "665.371",
                    "Business"
                ],
                [
                    "http://www.cricbuzz.com/live-cricket-scores",
                    "4",
                    "579.825",
                    "Games"
                ],
                [
                    "http://localhost/map/googlemap.php",
                    "9",
                    "528.395",
                    "Computers"
                ],
                [
                    "https://www.kaggle.com/competitions",
                    "2",
                    "309.769",
                    "Arts"
                ],
                [
                    "https://freebitco.in/",
                    "1",
                    "111.909",
                    "Business"
                ]
            ],
            "title": "TABLE IX: Predicted sample result for frecency and category"
        },
        "insight": "Table IX and the total visit of individual category."
    },
    {
        "id": "724",
        "table": {
            "header": [
                "Method",
                "MN \u2192 US (p)",
                "MN \u2192 US (f)",
                "US \u2192 MN",
                "SV \u2192 MN"
            ],
            "rows": [
                [
                    "Source only",
                    "76.0\u00b11.8",
                    "79.3\u00b10.7",
                    "59.5\u00b11.9",
                    "62.1\u00b11.2"
                ],
                [
                    "MMD Long2015",
                    "[EMPTY]",
                    "81.1\u00b10.3",
                    "[EMPTY]",
                    "71.1\u00b10.5"
                ],
                [
                    "RevGrad gradient_reverse",
                    "77.1\u00b11.8",
                    "85.1\u00b10.8",
                    "73.0\u00b12.0",
                    "73.9\u00b11.2"
                ],
                [
                    "CoGAN co-gan",
                    "91.2\u00b10.8",
                    "[EMPTY]",
                    "89.1\u00b11.0",
                    "[EMPTY]"
                ],
                [
                    "DRCN deep_reconstruction",
                    "91.8\u00b10.1",
                    "[EMPTY]",
                    "73.7\u00b10.0",
                    "82.0\u00b10.1"
                ],
                [
                    "ADDA adda",
                    "89.4\u00b10.2",
                    "[EMPTY]",
                    "90.1\u00b10.8",
                    "76.0\u00b11.8"
                ],
                [
                    "PixelDA pixel_level",
                    "[EMPTY]",
                    "95.9\u00b10.7",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "MSTN semantic",
                    "92.9\u00b11.1",
                    "[EMPTY]",
                    "[EMPTY]",
                    "91.7\u00b11.5"
                ],
                [
                    "GTA generate_to_adapt",
                    "92.8\u00b10.9",
                    "95.3\u00b10.7",
                    "90.8\u00b11.3",
                    "92.4\u00b10.9"
                ],
                [
                    "ADR adversarial_dropout",
                    "93.2\u2013\u2013\u2013\u2013\u2013\u00b12.5",
                    "96.1\u2013\u2013\u2013\u2013\u2013\u00b10.3",
                    "93.1\u2013\u2013\u2013\u2013\u2013\u00b11.3",
                    "95.0\u2013\u2013\u2013\u2013\u2013\u00b11.9"
                ],
                [
                    "DM-ADA (ours)",
                    "[BOLD] 94.8\u00b10.7",
                    "[BOLD] 96.7\u00b10.5",
                    "[BOLD] 94.2\u00b10.9",
                    "[BOLD] 95.5\u00b11.1"
                ]
            ],
            "title": "Table 1: Classification accuracy (mean \u00b1 std %) values of target domain over five independent runs on the digits datasets. The best performance is indicated in bold and the second best one is underlined."
        },
        "insight": "Table 1 presents the results of our approach in [CONTINUE] comparison with other adaptation approaches on the digits datasets. [CONTINUE] Our approach achieves the stateof-the-art performance on all four settings. Especially, it outperforms former GAN-based approaches (Bousmalis et al. 2017; Ghifary et al. 2016; Sankaranarayanan et al. 2018),"
    },
    {
        "id": "725",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Uncalibrated Freq.",
                "Uncalibrated MC Dropout",
                "Uncalibrated MC Dropout",
                "TS Calibrated Freq.",
                "TS Calibrated MC Dropout",
                "TS Calibrated MC Dropout"
            ],
            "rows": [
                [
                    "Data Set",
                    "Model",
                    "ECE",
                    "ECE",
                    "UCE",
                    "ECE",
                    "ECE",
                    "UCE"
                ],
                [
                    "CIFAR-10",
                    "ResNet-18",
                    "8.95",
                    "8.41",
                    "7.60",
                    "1.40",
                    "[BOLD] 0.47",
                    "[BOLD] 5.27"
                ],
                [
                    "CIFAR-100",
                    "ResNet-101",
                    "29.63",
                    "24.62",
                    "30.33",
                    "3.50",
                    "[BOLD] 1.92",
                    "[BOLD] 2.41"
                ],
                [
                    "CIFAR-100",
                    "DenseNet-169",
                    "30.62",
                    "23.98",
                    "29.62",
                    "6.10",
                    "[BOLD] 2.89",
                    "[BOLD] 2.69"
                ]
            ],
            "title": "Table 1: ECE and UCE test set results in % (M=15 bins). 0\u2009% means perfect calibration. In TS calibration with MC dropout the same value of T was used to report both ECE and UCE."
        },
        "insight": "Tab. 1 reports test set results for different networks [17, 18] and data sets used to evaluate the performance of temperature scaling for dropout variational inference. The proposed UCE metric is used to quantify calibration of uncertainty. [CONTINUE] Uncalibrated ECE shows, that MC dropout already reduces miscalibration of model likelihood by up to 6.6 percentage points. With TS calibration, MC dropout reduces ECE by 45\u201366 % and UCE drops drastically (especially for larger networks)."
    },
    {
        "id": "726",
        "table": {
            "header": [
                "[BOLD] H3.6M",
                "Protocol #1",
                "Protocol #2",
                "Protocol #3"
            ],
            "rows": [
                [
                    "[BOLD] H3.6M",
                    "MPJPE",
                    "MPJPE",
                    "MPJPE"
                ],
                [
                    "Martinez (ICCV\u201917)\u00a0",
                    "62.9",
                    "47.7",
                    "84.8"
                ],
                [
                    "Fang (AAAI\u201918)\u00a0",
                    "60.3",
                    "45.7",
                    "[BOLD] 72.8"
                ],
                [
                    "Rhodin (CVPR\u201918)\u00a0",
                    "66.8",
                    "-",
                    "-"
                ],
                [
                    "Yang (CVPR\u201918)\u00a0",
                    "58.6",
                    "[BOLD] 37.7",
                    "-"
                ],
                [
                    "Hossain (ECCV\u201918)\u00a0",
                    "[BOLD] 51.9",
                    "42.0",
                    "-"
                ],
                [
                    "Lassner (CVPR\u201917)\u00a0",
                    "80.7",
                    "-",
                    "-"
                ],
                [
                    "HMR (CVPR\u201918)\u00a0",
                    "88.0",
                    "56.8",
                    "77.3"
                ],
                [
                    "Pavlakos (CVPR\u201918)\u00a0",
                    "-",
                    "75.9",
                    "-"
                ],
                [
                    "NBF (3DV\u201918)\u00a0",
                    "-",
                    "59.9",
                    "-"
                ],
                [
                    "DenseRaC baseline",
                    "82.4",
                    "53.9",
                    "77.0"
                ],
                [
                    "+ render-and-compare",
                    "79.5",
                    "51.4",
                    "75.9"
                ],
                [
                    "+ synthetic data",
                    "76.8",
                    "48.0",
                    "74.1"
                ]
            ],
            "title": "Table 1: Quantitative comparisons of mean per joint position error (MPJPE), PCK and AUC between the estimated 3D pose and ground truth on H3.6M under Protocol #1, #2, #3 and MPI-INF-3DHP under Protocol #1, #2. - indicates results not reported. Lower MPJPE, higher PCK and AUC indicate better performance. Best scores are marked in bold."
        },
        "insight": "As reported in Table 1, we can observe each component in DenseRaC contributes to the final performance and leads DenseRaC to outperform state-of-the-art parametric body model estimators by a large margin."
    },
    {
        "id": "727",
        "table": {
            "header": [
                "[BOLD] MPI-INF-3DHP",
                "Protocol #1",
                "Protocol #1",
                "Protocol #1",
                "Protocol #2",
                "Protocol #2",
                "Protocol #2"
            ],
            "rows": [
                [
                    "[BOLD] MPI-INF-3DHP",
                    "PCK",
                    "AUC",
                    "MPJPE",
                    "PCK",
                    "AUC",
                    "MPJPE"
                ],
                [
                    "Mehta (3DV\u201917)\u00a0",
                    "75.7",
                    "39.3",
                    "117.6",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Mehta (TOG\u201917)\u00a0",
                    "76.6",
                    "40.4",
                    "124.7",
                    "83.9",
                    "47.3",
                    "98.0"
                ],
                [
                    "HMR (CVPR\u201918)\u00a0",
                    "72.9",
                    "36.5",
                    "124.2",
                    "86.3",
                    "47.8",
                    "89.8"
                ],
                [
                    "DenseRaC baseline",
                    "73.1",
                    "36.7",
                    "123.1",
                    "86.8",
                    "47.8",
                    "88.7"
                ],
                [
                    "+ render-and-compare",
                    "74.7",
                    "38.6",
                    "124.9",
                    "87.5",
                    "48.3",
                    "86.7"
                ],
                [
                    "+ synthetic data",
                    "[BOLD] 76.9",
                    "[BOLD] 41.1",
                    "[BOLD] 114.2",
                    "[BOLD] 89.0",
                    "[BOLD] 49.1",
                    "[BOLD] 83.5"
                ]
            ],
            "title": "Table 1: Quantitative comparisons of mean per joint position error (MPJPE), PCK and AUC between the estimated 3D pose and ground truth on H3.6M under Protocol #1, #2, #3 and MPI-INF-3DHP under Protocol #1, #2. - indicates results not reported. Lower MPJPE, higher PCK and AUC indicate better performance. Best scores are marked in bold."
        },
        "insight": "As reported in Table 1, we can observe each component in DenseRaC contributes to the final performance and leads DenseRaC to outperform state-of-the-art parametric bodymodel estimators by a large margin."
    },
    {
        "id": "728",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Pearson"
            ],
            "rows": [
                [
                    "Boston",
                    "0.76"
                ],
                [
                    "Diabetes",
                    "0.50"
                ],
                [
                    "Abalone",
                    "0.76"
                ],
                [
                    "Wine",
                    "0.87"
                ]
            ],
            "title": "Table 1: Correlation of extrapolation scores and ensemble std. deviations on 4 datasets."
        },
        "insight": "On each dataset, we found a similar, significantly linear relationship (see Table 1 and Appendix A). We note the relationship is weaker with the Diabetes dataset; the standard deviations of these ensembles are an order of magnitude higher than the other datasets, indicating much noisier data."
    },
    {
        "id": "729",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] M/E",
                "[BOLD] M/H",
                "[BOLD] A/E",
                "[BOLD] A/H"
            ],
            "rows": [
                [
                    "MaxProb",
                    "0.738",
                    "[BOLD] 0.677",
                    "0.461",
                    "0.433"
                ],
                [
                    "NN (Pixels)",
                    "0.561",
                    "0.550",
                    "[BOLD] 0.521",
                    "0.547"
                ],
                [
                    "NN (Reprs)",
                    "0.584",
                    "0.578",
                    "[BOLD] 0.503",
                    "0.533"
                ],
                [
                    "NN (Final Layer)",
                    "0.589",
                    "0.517",
                    "0.480",
                    "0.497"
                ],
                [
                    "LE (Loss)",
                    "[BOLD] 0.770",
                    "[BOLD] 0.684",
                    "0.454",
                    "0.456"
                ],
                [
                    "LE (Predictions)",
                    "0.364",
                    "0.544",
                    "[BOLD] 0.519",
                    "[BOLD] 0.582"
                ]
            ],
            "title": "Table 2: AUC for Latent Factors OOD detection task. Column heading denotes in-distribution definitions: labels are M (Male) and A (Attractive); spurious correlates are E (Eyeglasses) and H (Wearing Hat). Image is in-distribution iff label = spurious correlate. LE stands for local ensembles. Each Lanczos iteration uses 3000 eigenvectors. 500 examples from each test set are used. 95% CI is bolded."
        },
        "insight": "In Table 2, we present results for each of the four L, [CONTINUE] settings, showing both the loss gradient and the prediction gradient variant of local ensembles. Note that the loss gradient cannot be calculated at test time since we do not have labels available \u2014 instead, we calculate a separate extrapolation score using the gradient for the loss with respect to each possible label, and take the minimum. Our method achieves the best performance on most settings, and is competitive with the best baseline on each. However, the variation between the tasks is quite noteworthy. We note two patterns in particular. Firstly, we note that the performance of MaxProb and the loss gradient variant of our method are quite correlated, and we hypothesize this correlation is related to \u2207 \u02c6Y (cid:96). Additionally, observe the effect of increasing m is inconsistent between experiments:"
    },
    {
        "id": "730",
        "table": {
            "header": [
                "[EMPTY]",
                "2D Supervision DRC (Mask)\u00a0",
                "2D Supervision SoftRas\u00a0",
                "2D Supervision Ours (LRGB)",
                "2.5D Supervision DRC (Depth)\u00a0",
                "2.5D Supervision Ours (LDepth)",
                "3D Supervision 3D R2N2\u00a0",
                "3D Supervision ONet\u00a0",
                "3D Supervision Pixel2Mesh\u00a0"
            ],
            "rows": [
                [
                    "category",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "airplane",
                    "0.659",
                    "[BOLD] 0.149",
                    "0.190",
                    "0.377",
                    "[BOLD] 0.143",
                    "0.215",
                    "[BOLD] 0.151",
                    "0.183"
                ],
                [
                    "bench",
                    "-",
                    "0.241",
                    "[BOLD] 0.210",
                    "-",
                    "[BOLD] 0.165",
                    "0.210",
                    "[BOLD] 0.171",
                    "0.191"
                ],
                [
                    "cabinet",
                    "-",
                    "0.231",
                    "[BOLD] 0.220",
                    "-",
                    "[BOLD] 0.183",
                    "0.246",
                    "[BOLD] 0.189",
                    "0.194"
                ],
                [
                    "car",
                    "0.340",
                    "0.221",
                    "[BOLD] 0.196",
                    "0.316",
                    "[BOLD] 0.179",
                    "0.250",
                    "0.181",
                    "[BOLD] 0.154"
                ],
                [
                    "chair",
                    "0.660",
                    "0.338",
                    "[BOLD] 0.264",
                    "0.510",
                    "[BOLD] 0.226",
                    "0.282",
                    "[BOLD] 0.224",
                    "0.259"
                ],
                [
                    "display",
                    "-",
                    "0.284",
                    "[BOLD] 0.255",
                    "-",
                    "[BOLD] 0.246",
                    "0.323",
                    "0.275",
                    "[BOLD] 0.231"
                ],
                [
                    "lamp",
                    "-",
                    "[BOLD] 0.381",
                    "0.413",
                    "-",
                    "[BOLD] 0.362",
                    "0.566",
                    "0.380",
                    "[BOLD] 0.309"
                ],
                [
                    "loudspeaker",
                    "-",
                    "0.320",
                    "[BOLD] 0.289",
                    "-",
                    "[BOLD] 0.295",
                    "0.333",
                    "0.290",
                    "[BOLD] 0.284"
                ],
                [
                    "rifle",
                    "-",
                    "[BOLD] 0.155",
                    "0.175",
                    "-",
                    "[BOLD] 0.143",
                    "0.199",
                    "0.160",
                    "[BOLD] 0.151"
                ],
                [
                    "sofa",
                    "-",
                    "0.407",
                    "[BOLD] 0.224",
                    "-",
                    "[BOLD] 0.221",
                    "0.264",
                    "0.217",
                    "[BOLD] 0.211"
                ],
                [
                    "table",
                    "-",
                    "0.374",
                    "[BOLD] 0.280",
                    "-",
                    "[BOLD] 0.180",
                    "0.247",
                    "[BOLD] 0.185",
                    "0.215"
                ],
                [
                    "telephone",
                    "-",
                    "[BOLD] 0.131",
                    "0.148",
                    "-",
                    "[BOLD] 0.130",
                    "0.221",
                    "0.155",
                    "[BOLD] 0.145"
                ],
                [
                    "vessel",
                    "-",
                    "[BOLD] 0.233",
                    "0.245",
                    "-",
                    "[BOLD] 0.206",
                    "0.248",
                    "0.220",
                    "[BOLD] 0.201"
                ],
                [
                    "mean",
                    "0.553",
                    "0.266",
                    "[BOLD] 0.239",
                    "0.401",
                    "[BOLD] 0.206",
                    "0.277",
                    "0.215",
                    "[BOLD] 0.210"
                ]
            ],
            "title": "Table 1: Single-View Reconstruction. We report Chamfer-L1 distances wrt. the ground truth meshes for the single-view experiment. We compare against Differentiable Ray Consistency (DRC) [79] (2D and 2.5D supervision), Soft Rasterizer [44] (2D supervision), 3D-R2N2 [13], Occupancy Networks (ONet) [48], and Pixel2Mesh [80] (all 3D supervision)."
        },
        "insight": "In Table 1 and Fig. 4 we show quantitative and qualitative results for our method and various baselines. We can see that our method is able to infer accurate 3D shape and texture representations from single-view images when only trained on multi-view images and object masks as supervision signal. Quantitatively (Table 1), our method performs best among the approaches with 2D supervision and rivals the quality of methods with full 3D supervision. When trained with depth, our method performs comparably to the best methods which use full 3D information. Qualita"
    },
    {
        "id": "731",
        "table": {
            "header": [
                "[EMPTY]",
                "Trim Param.",
                "Accuracy",
                "Completeness",
                "Chamfer- [ITALIC] L1"
            ],
            "rows": [
                [
                    "Tola\u00a0 + sPSR",
                    "0",
                    "2.409",
                    "1.242",
                    "1.826"
                ],
                [
                    "Furu\u00a0 + sPSR",
                    "0",
                    "2.146",
                    "0.888",
                    "1.517"
                ],
                [
                    "Colmap\u00a0 + sPSR",
                    "0",
                    "[BOLD] 1.881",
                    "0.726",
                    "[BOLD] 1.303"
                ],
                [
                    "Camp\u00a0 + sPSR",
                    "0",
                    "2.213",
                    "[BOLD] 0.670",
                    "1.441"
                ],
                [
                    "Tola\u00a0 + sPSR",
                    "5",
                    "1.531",
                    "1.267",
                    "1.399"
                ],
                [
                    "Furu\u00a0 + sPSR",
                    "5",
                    "1.733",
                    "0.888",
                    "1.311"
                ],
                [
                    "Colmap\u00a0 + sPSR",
                    "5",
                    "[BOLD] 1.400",
                    "0.782",
                    "[BOLD] 1.091"
                ],
                [
                    "Camp\u00a0 + sPSR",
                    "5",
                    "1.991",
                    "[BOLD] 0.670",
                    "1.331"
                ],
                [
                    "Tola\u00a0 + sPSR",
                    "7",
                    "[BOLD] 0.396",
                    "1.424",
                    "0.910"
                ],
                [
                    "Furu\u00a0 + sPSR",
                    "7",
                    "0.723",
                    "0.955",
                    "0.839"
                ],
                [
                    "Colmap\u00a0 + sPSR",
                    "7",
                    "0.446",
                    "1.020",
                    "[BOLD] 0.733"
                ],
                [
                    "Camp\u00a0 + sPSR",
                    "7",
                    "1.466",
                    "[BOLD] 0.719",
                    "1.092"
                ],
                [
                    "Ours (LRGB)",
                    "-",
                    "1.054",
                    "[BOLD] 0.760",
                    "0.907"
                ],
                [
                    "Ours (LRGB + LDepth)",
                    "-",
                    "[BOLD] 0.789",
                    "0.775",
                    "[BOLD] 0.782"
                ]
            ],
            "title": "Table 2: Multi-View Stereo. We show quantitative results for scans 65, 106, and 118 on the DTU\u00a0dataset. For the baselines, we perform screened Poisson surface reconstruction (sPSR)\u00a0[34] with trim parameters 0, 5, and 7 to obtain the final output. It shows that our generic method achieves results comparable to the highly optimized MVS methods."
        },
        "insight": "Results: We show qualitative and quantitative results in Fig. 6 and Table 2. Qualitatively, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes. The ability to accurately model cavities of the objects shows that our model uses texture information to improve over the visual hull (Fig. 7). Quantitatively, Table 2 shows that our approach rivals the results from highly tuned MVS algorithms. We note that the DTU ground truth is itself sparse (Fig. 7c) and methods are therefore rewarded for trading off completeness for accuracy, which explains the better quantitative performance of the baselines for higher trim parameters (Fig. 8)."
    },
    {
        "id": "732",
        "table": {
            "header": [
                "Target",
                "Unit",
                "PPGN",
                "SchNet",
                "PhysNet",
                "MEGNet-s",
                "Cormorant",
                "[BOLD] DimeNet"
            ],
            "rows": [
                [
                    "[ITALIC] \u03bc",
                    "D",
                    "0.047",
                    "0.033",
                    "0.0529",
                    "0.05",
                    "0.13",
                    "0.0286"
                ],
                [
                    "[ITALIC] \u03b1",
                    "[ITALIC] a0\u00a03",
                    "0.131",
                    "0.235",
                    "0.0615",
                    "0.081",
                    "0.092",
                    "0.0469"
                ],
                [
                    "[ITALIC] \u03f5HOMO",
                    "meV",
                    "40.3",
                    "41",
                    "32.9",
                    "43",
                    "36",
                    "27.8"
                ],
                [
                    "[ITALIC] \u03f5LUMO",
                    "meV",
                    "32.7",
                    "34",
                    "24.7",
                    "44",
                    "36",
                    "19.7"
                ],
                [
                    "\u0394 [ITALIC] \u03f5",
                    "meV",
                    "60.0",
                    "63",
                    "42.5",
                    "66",
                    "60",
                    "34.8"
                ],
                [
                    "\u27e8 [ITALIC] R2\u27e9",
                    "[ITALIC] a0\u00a02",
                    "0.592",
                    "0.073",
                    "0.765",
                    "0.302",
                    "0.673",
                    "0.331"
                ],
                [
                    "ZPVE",
                    "meV",
                    "3.12",
                    "1.7",
                    "1.39",
                    "1.43",
                    "1.98",
                    "1.29"
                ],
                [
                    "[ITALIC] U0",
                    "meV",
                    "36.8",
                    "14",
                    "8.15",
                    "12",
                    "28",
                    "8.02"
                ],
                [
                    "[ITALIC] U",
                    "meV",
                    "36.8",
                    "19",
                    "8.34",
                    "13",
                    "-",
                    "7.89"
                ],
                [
                    "[ITALIC] H",
                    "meV",
                    "36.3",
                    "14",
                    "8.42",
                    "12",
                    "-",
                    "8.11"
                ],
                [
                    "[ITALIC] G",
                    "meV",
                    "36.4",
                    "14",
                    "9.40",
                    "12",
                    "-",
                    "8.98"
                ],
                [
                    "[ITALIC] cv",
                    "calmolK",
                    "0.055",
                    "0.033",
                    "0.0280",
                    "0.029",
                    "0.031",
                    "0.0249"
                ],
                [
                    "std. MAE",
                    "\\char 37",
                    "1.84",
                    "1.76",
                    "1.37",
                    "1.80",
                    "2.14",
                    "1.05"
                ],
                [
                    "logMAE",
                    "-",
                    "\u22124.64",
                    "\u22125.17",
                    "\u22125.35",
                    "\u22125.17",
                    "\u22124.75",
                    "\u22125.57"
                ]
            ],
            "title": "Table 1: MAE on QM9. DimeNet sets the state of the art on 11 targets, outperforming the second-best model on average by 31\\char37 (mean std. MAE)."
        },
        "insight": "In Table 1 we report the mean absolute error (MAE) of each target and the overall mean standardized MAE (std. MAE) and mean standardized logMAE [CONTINUE] DimeNet sets the new state of the art on 11 out of 12 targets and decreases mean std. MAE by 31 % and mean logMAE by 0.22 compared to the second-best model."
    },
    {
        "id": "733",
        "table": {
            "header": [
                "Method",
                "A \u2192 W",
                "D \u2192 W",
                "W \u2192 D",
                "A \u2192 D",
                "D \u2192 A",
                "W \u2192 A",
                "Average"
            ],
            "rows": [
                [
                    "AlexNet (source only) alexnet",
                    "60.6\u00b10.4",
                    "95.4\u00b10.2",
                    "99.0\u00b10.1",
                    "64.2\u00b10.3",
                    "45.5\u00b10.5",
                    "48.3\u00b10.5",
                    "68.8"
                ],
                [
                    "TCA tca",
                    "59.0\u00b10.0",
                    "90.2\u00b10.0",
                    "88.2\u00b10.0",
                    "57.8\u00b10.0",
                    "51.6\u00b10.0",
                    "47.9\u00b10.0",
                    "65.8"
                ],
                [
                    "DDC ddc",
                    "61.0\u00b10.5",
                    "95.0\u00b10.3",
                    "98.5\u00b10.3",
                    "64.9\u00b10.4",
                    "47.2\u00b10.5",
                    "49.4\u00b10.4",
                    "69.3"
                ],
                [
                    "DAN Long2015",
                    "68.5\u00b10.3",
                    "96.0\u00b10.1",
                    "99.0\u00b10.1",
                    "66.8\u00b10.2",
                    "50.0\u00b10.4",
                    "49.8\u00b10.3",
                    "71.7"
                ],
                [
                    "RevGrad gradient_reverse",
                    "73.0\u00b10.5",
                    "96.4\u00b10.3",
                    "99.2\u00b10.3",
                    "72.3\u00b10.3",
                    "52.4\u00b10.4",
                    "50.4\u00b10.5",
                    "74.1"
                ],
                [
                    "DRCN deep_reconstruction",
                    "68.7\u00b10.3",
                    "96.4\u00b10.3",
                    "99.0\u00b10.2",
                    "66.8\u00b10.5",
                    "56.0\u00b10.5",
                    "54.9\u00b10.5",
                    "73.6"
                ],
                [
                    "MADA multi-adversarial",
                    "78.5\u00b10.2",
                    "[BOLD] 99.8\u00b10.1",
                    "[BOLD] 100\u00b10.0",
                    "74.1\u00b10.1",
                    "56.0\u00b10.2",
                    "54.5\u00b10.3",
                    "77.1"
                ],
                [
                    "MSTN semantic",
                    "80.5\u00b10.4",
                    "96.9\u00b10.1",
                    "99.9\u2013\u2013\u2013\u2013\u2013\u00b10.1",
                    "74.5\u00b10.4",
                    "62.5\u00b10.4",
                    "60.0\u00b10.6",
                    "79.1"
                ],
                [
                    "GCAN gcan",
                    "82.7\u2013\u2013\u2013\u2013\u2013\u00b10.1",
                    "97.1\u2013\u2013\u2013\u2013\u2013\u00b10.1",
                    "99.8\u00b10.1",
                    "76.4\u2013\u2013\u2013\u2013\u2013\u00b10.5",
                    "[BOLD] 64.9\u00b10.1",
                    "62.6\u2013\u2013\u2013\u2013\u2013\u00b10.3",
                    "80.6"
                ],
                [
                    "DM-ADA (ours)",
                    "[BOLD] 83.9\u00b10.4",
                    "[BOLD] 99.8\u00b10.1",
                    "99.9\u2013\u2013\u2013\u2013\u2013\u00b10.1",
                    "[BOLD] 77.5\u00b10.2",
                    "64.6\u2013\u2013\u2013\u2013\u2013\u00b10.4",
                    "[BOLD] 64.0\u00b10.5",
                    "[BOLD] 81.6"
                ]
            ],
            "title": "Table 2: Classification accuracy (mean \u00b1 std %) values of target domain over five independent runs on the Office-31 dataset. The best performance is indicated in bold and the second best one is underlined."
        },
        "insight": "Table 2 reports the performance of our method compared with other works. [CONTINUE] Our approach obtains the best performance in three of four hard cases: A \u2192 W, W \u2192 A and A \u2192 D. For two easier cases: W \u2192 D and D \u2192 W, our approach achieves accuracy higher than 99.5% and ranks the first two places."
    },
    {
        "id": "734",
        "table": {
            "header": [
                "[EMPTY]",
                "prec.",
                "rec.",
                "F1"
            ],
            "rows": [
                [
                    "ARE",
                    "0.82",
                    "0.84",
                    "0.83"
                ],
                [
                    "Acord\u00e3o",
                    "0.71",
                    "0.89",
                    "0.79"
                ],
                [
                    "Desp.",
                    "0.74",
                    "0.82",
                    "0.78"
                ],
                [
                    "Outro",
                    "0.91",
                    "0.82",
                    "0.87"
                ],
                [
                    "RE",
                    "0.77",
                    "0.70",
                    "0.73"
                ],
                [
                    "Sent.",
                    "0.92",
                    "0.95",
                    "0.93"
                ],
                [
                    "average",
                    "0.85",
                    "0.84",
                    "0.84"
                ]
            ],
            "title": "Figure 2: Bi-LSTM results per class."
        },
        "insight": "Figures 2 and 3 detail our results. Our BI-LSTM model archives a mean precision of 85% and f1-Score 84% without requiring any preprocessing heuristics and regular expressions. This contrasts with the CNN model of , which uses a set of hand crafted rules in the tokenization process as well as regular expressions to remove noisy text."
    },
    {
        "id": "735",
        "table": {
            "header": [
                "[BOLD] Epochs (For 1500 data-set)",
                "[BOLD] Precision",
                "[BOLD] Recall",
                "[BOLD] Test-accuracy",
                "[ITALIC]  [BOLD] F [BOLD] 1-measure"
            ],
            "rows": [
                [
                    "30",
                    "0.8990",
                    "[BOLD] 0.8732",
                    "0.8340",
                    "0.9346"
                ],
                [
                    "40",
                    "0.9334",
                    "0.8662",
                    "0.8551",
                    "[BOLD] 0.9495"
                ],
                [
                    "80",
                    "0.9754",
                    "0.7246",
                    "0.9470",
                    "0.9493"
                ],
                [
                    "115",
                    "[BOLD] 0.9889",
                    "0.4042",
                    "[BOLD] 0.9610",
                    "0.9445"
                ]
            ],
            "title": "TABLE II: Comparison of metrics across different epochs"
        },
        "insight": "After training the model with the available data-set of 1500 distinct images over GPU, the weights of the trained model are saved and the model is evaluated to get the confusion matrix metrics as shown in Table.II and test-accuracy using the loss function (mean-squared error) as given in (3). [CONTINUE] As depicted in (Table II), we notice that recall descends with the data-set unlike precision. This is because the classification performed by the model is a pixel-wise classification of occluded images, which is of very high order per image and the reason the saturated recall value for the available dataset is 0.4042 which is not the best for the available dataset due to surplus hardware."
    },
    {
        "id": "736",
        "table": {
            "header": [
                "[BOLD] Training data-set",
                "[BOLD] Epochs",
                "[BOLD] Testing Data-set",
                "[BOLD] Test-accuracy"
            ],
            "rows": [
                [
                    "300",
                    "30",
                    "100",
                    "83.4 %"
                ],
                [
                    "500",
                    "40",
                    "300",
                    "85.5 %"
                ],
                [
                    "1500",
                    "80",
                    "500",
                    "94.7 %"
                ],
                [
                    "1500",
                    "115",
                    "500",
                    "96.1 %"
                ]
            ],
            "title": "TABLE I: Performance of the current Segnet model"
        },
        "insight": "With this support, the model has been trained on data-sets of 500 and 1500 images with their labelled ground-truth data, followed by validation on 200 and 300 images respectively (Table I), and the test-accuracy is obtained using the rest of data-set of 300 and 500 images respectively. [CONTINUE] Similarly, certain distinctive test data-sets have been evaluated over different hyper-parameters to obtain the over-all accuracy of the model and the results had been tabulated in Table.I. We observed a significant improvement in the results in comparison to the existing methods like SVM, MLPC, as shown in Fig.6 and a few other adaptive algorithms, as in , . [CONTINUE] To start with, we used smaller data-set of 100 images tested on the model trained with 300 images (Table I), on a system with specifications of I7 processor, 8GB RAM and 4GB Nvidia G-force GPU. But, the time-complexity hasn't reduced when compared to the previous models. Hence, the model has been implemented on a 8-core cluster based online (cloud) GPU with low time-complexity [CONTINUE] As tabulated in [Table I], effective training with larger data-sets and better efficiency ( 97%) is made possible with CNN's. The images obtained through a set of Google APIs, near the surroundings of our institute (Fig.7) have been tested with the correlation coefficient approximated to value 1."
    },
    {
        "id": "737",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "[ITALIC] \u03b8init=0.5 Deterministic",
                "[ITALIC] \u03b8init=0.5 Stochastic",
                "[ITALIC] \u03b8init=0.969 Deterministic",
                "[ITALIC] \u03b8init=0.969 Stochastic"
            ],
            "rows": [
                [
                    "AdaptiveLayer (a)",
                    "( [ITALIC] \u03bb=2)",
                    "[BOLD] 2.200 (0.125)",
                    "2.218 (0.135)",
                    "[BOLD] 2.366 (0.901)",
                    "2.375 (0.889)"
                ],
                [
                    "AdaptiveLayer (b)",
                    "( [ITALIC] \u03bb=2)",
                    "15.69 (29.9)",
                    "15.67 (29.9)",
                    "35.26 (41.6)",
                    "35.27 (41.7)"
                ],
                [
                    "[EMPTY]",
                    "( [ITALIC] \u03bb=8)",
                    "2.406 (0.189)",
                    "2.423 (0.189)",
                    "65.74 (38.8)",
                    "65.74 (38.8)"
                ],
                [
                    "[EMPTY]",
                    "( [ITALIC] \u03bb=32)",
                    "2.439 (0.224)",
                    "2.453 (0.228)",
                    "80.59 (24.6)",
                    "80.60 (24.6)"
                ],
                [
                    "[EMPTY]",
                    "( [ITALIC] \u03bb=128)",
                    "2.394 (0.163)",
                    "2.405 (0.173)",
                    "80.58 (24.8)",
                    "80.58 (24.8)"
                ],
                [
                    "StochasticLayer",
                    "[EMPTY]",
                    "4.704 (0.752)",
                    "4.704 (0.752)",
                    "4.704 (0.752)",
                    "4.704 (0.752)"
                ]
            ],
            "title": "Table 1: Mean test errors (%) over 30 trials at the final iteration in the experiment of selection of layers. The values in parentheses denote the standard deviation."
        },
        "insight": "Table 1 shows the test error of each method at the final iteration. [CONTINUE] We observe that AdaptiveLayer [CONTINUE] (a) shows the best performance among the proposed methods, and the performances of AdaptiveLayer (b) become significantly worse when the bad initialization (\u03b8init \u2248 0.968) is used. [CONTINUE] Comparing the deterministic and stochastic predictions, the performance differences are not significant [CONTINUE] The test error of StochasticLayer is inferior to the most proposed methods;"
    },
    {
        "id": "738",
        "table": {
            "header": [
                "[EMPTY]",
                "Test error (Deterministic)",
                "Test error (Stochastic)",
                "Training time (min.)"
            ],
            "rows": [
                [
                    "AdaptiveActivation",
                    "1.414 (0.054)",
                    "[BOLD] 1.407 (0.036)",
                    "255"
                ],
                [
                    "StochasticActivation",
                    "\u2013",
                    "1.452 (0.025)",
                    "204"
                ],
                [
                    "ReLU",
                    "1.609 (0.044)",
                    "\u2013",
                    "120"
                ],
                [
                    "tanh",
                    "1.592 (0.069)",
                    "\u2013",
                    "120"
                ]
            ],
            "title": "Table 2: Mean test errors (%) over 30 trials at the final iteration in the experiment of selection of activation functions. The values in parentheses denote the standard deviation. The training time of a typical single run is reported."
        },
        "insight": "Table 2 shows the test error and training time of each algorithm. [CONTINUE] We observe that AdaptiveActivation (stochastic) outperforms StochasticActivation in which the Bernoulli parameters stay constant, [CONTINUE] The predictive performance of AdaptiveActivation (deterministic) is competitive with StochasticActivation, [CONTINUE] the obtained networks by AdaptiveActivation have a better classification performance compared to both uniform activations: ReLU and hyperbolic tangent. [CONTINUE] Comparing the training time, we observe that the proposed method needs about twice the computational time for training compared to the fixed structured neural networks."
    },
    {
        "id": "739",
        "table": {
            "header": [
                "[EMPTY]",
                "Test error (%)",
                "Time (hour)"
            ],
            "rows": [
                [
                    "AdaptiveNet",
                    "1.645 (0.072)",
                    "1.01"
                ],
                [
                    "BO (budget=10)",
                    "1.780",
                    "9.59"
                ],
                [
                    "BO (budget=20)",
                    "1.490",
                    "18.29"
                ]
            ],
            "title": "Table 3: Test errors (%) and computational time of the proposed method (AdaptiveNet) and the Bayesian optimization (BO) with different budgets in the experiment of adaptation of stochastic network. The mean values over 30 trials are reported in the proposed method, and the value in parentheses denotes the standard deviation. For the Bayesian optimization, the result of a single run is reported."
        },
        "insight": "Table 3 shows that the test errors of the stochastic networks obtained by the proposed method and the Bayesian optimization with different budgets, where budget indicates the number of hyper-parameters to be evaluated. [CONTINUE] we observe that the computational time of the Bayesian optimization proportionally increases for the number of budgets while our method is more computationally efficient. [CONTINUE] The proposed method can find a competitive stochastic network with reasonable computational time. [CONTINUE] the Bayesian optimization could find a better configuration in this case within several ten budgets,"
    },
    {
        "id": "740",
        "table": {
            "header": [
                "[EMPTY]",
                "CIFAR-10 Deterministic",
                "CIFAR-10 Stochastic",
                "CIFAR-100 Deterministic",
                "CIFAR-100 Stochastic"
            ],
            "rows": [
                [
                    "AdaptiveConnection",
                    "5.427 (0.167)",
                    "5.399 (0.153)",
                    "25.461 (0.408)",
                    "[BOLD] 25.315 (0.409)"
                ],
                [
                    "Normal DenseNet (40 depth,  [ITALIC] k=12)",
                    "[BOLD] 5.050 (0.147)",
                    "\u2013",
                    "25.518 (0.380)",
                    "\u2013"
                ]
            ],
            "title": "Table 4: Test errors (%) at the final iteration in the experiment of connection selection for DenseNets. The values in parentheses denote the standard deviation."
        },
        "insight": "Table 4 shows the test errors of AdaptiveConnection and the normal DenseNet at the final iteration. [CONTINUE] the stochastic prediction is slightly better than the deterministic one, but the difference is not significant. [CONTINUE] The difference of the predictive performances between AdaptiveConnection and the normal DenseNet is not significant for the CIFAR-100 datasets, whereas AdaptiveConnection is inferior for the CIFAR-10 dataset."
    },
    {
        "id": "741",
        "table": {
            "header": [
                "Method",
                "Accuracy (%)"
            ],
            "rows": [
                [
                    "ResNet-101 (source only) resnet",
                    "52.4"
                ],
                [
                    "RevGrad gradient_reverse",
                    "57.4"
                ],
                [
                    "DAN Long2015",
                    "62.8"
                ],
                [
                    "JAN Long2017",
                    "65.7"
                ],
                [
                    "GTA generate_to_adapt",
                    "69.5"
                ],
                [
                    "MCD-DA max_discrepancy",
                    "71.9"
                ],
                [
                    "ADR adversarial_dropout",
                    "73.5"
                ],
                [
                    "DM-ADA (ours)",
                    "[BOLD] 75.6"
                ]
            ],
            "title": "Table 3: Classification accuracy on the validation set of VisDA-2017 challenge."
        },
        "insight": "Table 3 reports the results on the VisDA-2017 cross-domain classification dataset. [CONTINUE] Our approach achieves the highest accuracy among all adaptation approaches, and exceeds the baseline with a great margin."
    },
    {
        "id": "742",
        "table": {
            "header": [
                "Model",
                "Cardiac",
                "Coronary",
                "Medical",
                "Surgical",
                "Avg"
            ],
            "rows": [
                [
                    "SVM",
                    "0.627",
                    "0.572",
                    "0.503",
                    "0.532",
                    "0.558"
                ],
                [
                    "LR",
                    "0.629",
                    "0.601",
                    "0.510",
                    "0.517",
                    "0.564"
                ],
                [
                    "RF",
                    "0.610",
                    "0.578",
                    "0.587",
                    "0.623",
                    "0.599"
                ],
                [
                    "TT",
                    "0.821",
                    "0.769",
                    "0.722",
                    "0.727",
                    "0.759"
                ],
                [
                    "LSTM",
                    "0.812",
                    "0.807",
                    "0.742",
                    "0.769",
                    "0.782"
                ],
                [
                    "CNN",
                    "0.866",
                    "0.802",
                    "0.747",
                    "0.812",
                    "0.807"
                ],
                [
                    "NT\u2212",
                    "0.876",
                    "0.833",
                    "0.737",
                    "0.801",
                    "0.812"
                ],
                [
                    "NT",
                    "0.876",
                    "0.837",
                    "0.757",
                    "0.812",
                    "0.820"
                ],
                [
                    "[Che  [ITALIC] et al., 2015]",
                    "0.853",
                    "0.802",
                    "0.760",
                    "0.785",
                    "0.800"
                ],
                [
                    "[Che  [ITALIC] et al., 2018]",
                    "0.868",
                    "0.824",
                    "0.775",
                    "[BOLD] 0.823",
                    "0.823"
                ],
                [
                    "CNN\u2212LSTM",
                    "[BOLD] 0.885",
                    "[BOLD] 0.848",
                    "[BOLD] 0.782",
                    "[BOLD] 0.827",
                    "[BOLD] 0.836"
                ]
            ],
            "title": "TABLE II AUC NUMBERS FOR SHALLOW AND DEEP MODELS. NUMBERS IN BOLD INDICATE THE BEST MODELS FOR EACH ICU DOMAIN."
        },
        "insight": "AUC NUMBERS FOR SHALLOW AND DEEP MODELS. NUMBERS IN BOLD INDICATE THE BEST MODELS FOR EACH ICU DOMAIN. TABLE II [CONTINUE] Table II shows AUC numbers for each model. We report numbers for each ICU domain, and also the macro-averaged result. Clearly, [CONTINUE] \u2212 CNN LSTM consistently outperforms all shallow baselines, and also . Employing CNN and LSTM components together is beneficial, since NT is consistently superior than CNN and LSTM. Domain adaptation is beneficial for most of the domains. The only exception occurs with the Coronary domain for which performance remains statistically the same when compared with NT. Overall, CNN LSTM shows a macro-averaged AUC of 0.832."
    },
    {
        "id": "743",
        "table": {
            "header": [
                "Target",
                "A1",
                "A2",
                "A3",
                "A4",
                "A5"
            ],
            "rows": [
                [
                    "Cardiac",
                    "0.852",
                    "[BOLD] 0.885",
                    "0.829",
                    "0.849",
                    "0.858"
                ],
                [
                    "Coronary",
                    "[BOLD] 0.848",
                    "0.812",
                    "0.807",
                    "0.793",
                    "0.784"
                ],
                [
                    "Medical",
                    "0.754",
                    "0.763",
                    "[BOLD] 0.782",
                    "0.759",
                    "0.736"
                ],
                [
                    "Surgical",
                    "0.822",
                    "[BOLD] 0.827",
                    "0.808",
                    "0.818",
                    "0.788"
                ],
                [
                    "Overall",
                    "0.819",
                    "[BOLD] 0.822",
                    "0.806",
                    "0.804",
                    "0.791"
                ]
            ],
            "title": "TABLE III: AUC numbers for different feature transference approaches. Numbers in bold indicate the best transference approach for each target ICU domain."
        },
        "insight": "TABLE III AUC NUMBERS FOR DIFFERENT FEATURE TRANSFERENCE APPROACHES. NUMBERS IN BOLD INDICATE THE BEST TRANSFERENCE APPROACH FOR EACH TARGET ICU DOMAIN. [CONTINUE] \u2212 Table III shows AUC numbers for CNN LSTM models learned following the different feature transference approaches. The best transference approach varies depending on the target ICU domain. Randomly initializing the weights for fine-tuning does not show to be the best approach, as A4 and A5 were not the best performers for any target domain. It seems that specific temporal patterns play an important role for mortality prediction in the Surgical domain, as A1 and A2 were the best approaches for this domain. For the Medical domain, A3 was the best approach, suggesting that features learned from other domains are effective. For the Cardiac and Coronary domains, A2 was the best transference approach, which indicates that specific features are important in this domain."
    },
    {
        "id": "744",
        "table": {
            "header": [
                "[EMPTY]",
                "GEM CGR-AP@10-S1",
                "BCGD [ITALIC] l CGR-AP@10-S1",
                "BCGD [ITALIC] g CGR-AP@10-S1",
                "Triad CGR-AP@10-S1",
                "Ours0.2,0.5 CGR-AP@10-S1",
                "GEM CGR-AP@10-S2",
                "BCGD [ITALIC] l CGR-AP@10-S2",
                "BCGD [ITALIC] g CGR-AP@10-S2",
                "Triad CGR-AP@10-S2",
                "Ours0.2,0.5 CGR-AP@10-S2"
            ],
            "rows": [
                [
                    "AS733",
                    "06.38",
                    "53.90",
                    "13.87",
                    "56.39",
                    "[BOLD] 79.19",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chess",
                    "12.10",
                    "46.99",
                    "15.85",
                    "50.27",
                    "[BOLD] 80.27",
                    "08.56",
                    "44.81",
                    "11.68",
                    "52.57",
                    "[BOLD] 81.40"
                ],
                [
                    "DNC",
                    "25.55",
                    "57.50",
                    "49.02",
                    "[BOLD] 67.42",
                    "65.85",
                    "[EMPTY]",
                    "38.03",
                    "73.67",
                    "[BOLD] 83.27",
                    "81.88"
                ],
                [
                    "Elec",
                    "[EMPTY]",
                    "38.57",
                    "29.89",
                    "46.07",
                    "[BOLD] 65.11",
                    "12.19",
                    "26.45",
                    "19.94",
                    "49.39",
                    "[BOLD] 58.57"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "06.66",
                    "00.22",
                    "51.60",
                    "[BOLD] 85.08",
                    "[EMPTY]",
                    "07.16",
                    "00.22",
                    "62.97",
                    "[BOLD] 86.79"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "74.31",
                    "56.62",
                    "[EMPTY]",
                    "[BOLD] 84.81",
                    "[EMPTY]",
                    "67.20",
                    "46.90",
                    "[EMPTY]",
                    "[BOLD] 83.50"
                ],
                [
                    "[EMPTY]",
                    "GR-AP@10-S1",
                    "GR-AP@10-S1",
                    "GR-AP@10-S1",
                    "GR-AP@10-S1",
                    "GR-AP@10-S1",
                    "GR-AP@10-S2",
                    "GR-AP@10-S2",
                    "GR-AP@10-S2",
                    "GR-AP@10-S2",
                    "GR-AP@10-S2"
                ],
                [
                    "AS733",
                    "00.60",
                    "48.83",
                    "02.48",
                    "63.31",
                    "[BOLD] 81.12",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chess",
                    "04.41",
                    "43.84",
                    "04.41",
                    "54.61",
                    "[BOLD] 85.80",
                    "04.94",
                    "49.74",
                    "04.74",
                    "57.72",
                    "[BOLD] 87.05"
                ],
                [
                    "DNC",
                    "03.33",
                    "34.96",
                    "22.14",
                    "[BOLD] 76.93",
                    "76.02",
                    "[EMPTY]",
                    "12.09",
                    "75.92",
                    "[BOLD] 81.72",
                    "51.26"
                ],
                [
                    "Elec",
                    "[EMPTY]",
                    "17.89",
                    "09.13",
                    "57.71",
                    "[BOLD] 81.62",
                    "03.85",
                    "16.31",
                    "08.94",
                    "59.65",
                    "[BOLD] 78.27"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "03.94",
                    "00.11",
                    "58.94",
                    "[BOLD] 90.20",
                    "[EMPTY]",
                    "04.02",
                    "00.14",
                    "71.29",
                    "[BOLD] 91.42"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "61.34",
                    "31.40",
                    "[EMPTY]",
                    "[BOLD] 81.27",
                    "[EMPTY]",
                    "55.55",
                    "27.72",
                    "[EMPTY]",
                    "[BOLD] 80.61"
                ],
                [
                    "[EMPTY]",
                    "CGR-AP@100-S1",
                    "CGR-AP@100-S1",
                    "CGR-AP@100-S1",
                    "CGR-AP@100-S1",
                    "CGR-AP@100-S1",
                    "CGR-AP@100-S2",
                    "CGR-AP@100-S2",
                    "CGR-AP@100-S2",
                    "CGR-AP@100-S2",
                    "CGR-AP@100-S2"
                ],
                [
                    "AS733",
                    "06.67",
                    "80.42",
                    "80.35",
                    "73.34",
                    "[BOLD] 91.30",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chess",
                    "13.06",
                    "64.26",
                    "40.82",
                    "61.12",
                    "[BOLD] 84.68",
                    "11.89",
                    "71.83",
                    "51.93",
                    "67.12",
                    "[BOLD] 90.66"
                ],
                [
                    "DNC",
                    "29.24",
                    "82.25",
                    "75.28",
                    "83.48",
                    "[BOLD] 87.14",
                    "[EMPTY]",
                    "73.01",
                    "79.92",
                    "95.18",
                    "[BOLD] 99.31"
                ],
                [
                    "Elec",
                    "[EMPTY]",
                    "48.45",
                    "38.93",
                    "56.04",
                    "[BOLD] 69.40",
                    "12.47",
                    "39.33",
                    "33.54",
                    "61.68",
                    "[BOLD] 66.88"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "06.91",
                    "00.23",
                    "63.00",
                    "[BOLD] 97.00",
                    "[EMPTY]",
                    "11.45",
                    "02.50",
                    "76.47",
                    "[BOLD] 98.68"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "68.70",
                    "53.88",
                    "[EMPTY]",
                    "[BOLD] 85.51",
                    "[EMPTY]",
                    "62.21",
                    "45.87",
                    "[EMPTY]",
                    "[BOLD] 84.51"
                ],
                [
                    "[EMPTY]",
                    "GR-AP@100-S1",
                    "GR-AP@100-S1",
                    "GR-AP@100-S1",
                    "GR-AP@100-S1",
                    "GR-AP@100-S1",
                    "GR-AP@100-S2",
                    "GR-AP@100-S2",
                    "GR-AP@100-S2",
                    "GR-AP@100-S2",
                    "GR-AP@100-S2"
                ],
                [
                    "AS733",
                    "01.31",
                    "88.87",
                    "95.39",
                    "83.15",
                    "[BOLD] 97.15",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chess",
                    "06.55",
                    "74.81",
                    "45.51",
                    "71.83",
                    "[BOLD] 94.90",
                    "08.91",
                    "82.25",
                    "59.78",
                    "74.99",
                    "[BOLD] 96.79"
                ],
                [
                    "DNC",
                    "05.77",
                    "81.45",
                    "82.84",
                    "94.38",
                    "[BOLD] 97.11",
                    "[EMPTY]",
                    "82.51",
                    "88.54",
                    "98.42",
                    "[BOLD] 99.85"
                ],
                [
                    "Elec",
                    "[EMPTY]",
                    "42.29",
                    "36.75",
                    "74.07",
                    "[BOLD] 86.74",
                    "04.55",
                    "40.10",
                    "41.57",
                    "76.17",
                    "[BOLD] 86.05"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "04.88",
                    "00.17",
                    "71.42",
                    "[BOLD] 98.59",
                    "[EMPTY]",
                    "08.60",
                    "02.69",
                    "83.52",
                    "[BOLD] 99.39"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "58.28",
                    "30.17",
                    "[EMPTY]",
                    "[BOLD] 84.91",
                    "[EMPTY]",
                    "55.84",
                    "30.96",
                    "[EMPTY]",
                    "[BOLD] 85.83"
                ],
                [
                    "[EMPTY]",
                    "LP-AUC-S1",
                    "LP-AUC-S1",
                    "LP-AUC-S1",
                    "LP-AUC-S1",
                    "LP-AUC-S1",
                    "LP-AUC-S2",
                    "LP-AUC-S2",
                    "LP-AUC-S2",
                    "LP-AUC-S2",
                    "LP-AUC-S2"
                ],
                [
                    "AS733",
                    "60.18",
                    "61.37",
                    "70.15",
                    "65.54",
                    "[BOLD] 85.56",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chess",
                    "64.23",
                    "[BOLD] 86.66",
                    "85.77",
                    "79.32",
                    "77.24",
                    "68.79",
                    "[BOLD] 88.62",
                    "79.94",
                    "85.83",
                    "73.12"
                ],
                [
                    "DNC",
                    "75.90",
                    "84.18",
                    "89.34",
                    "[BOLD] 90.30",
                    "78.76",
                    "[EMPTY]",
                    "76.52",
                    "[BOLD] 94.21",
                    "92.81",
                    "89.82"
                ],
                [
                    "Elec",
                    "66.32",
                    "91.16",
                    "82.83",
                    "[BOLD] 97.31",
                    "90.31",
                    "67.69",
                    "82.13",
                    "82.47",
                    "[BOLD] 90.34",
                    "78.38"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "82.83",
                    "82.88",
                    "81.76",
                    "[BOLD] 88.00",
                    "[EMPTY]",
                    "84.51",
                    "85.02",
                    "83.93",
                    "[BOLD] 89.69"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "88.39",
                    "82.37",
                    "[EMPTY]",
                    "[BOLD] 90.25",
                    "[EMPTY]",
                    "[BOLD] 89.99",
                    "81.17",
                    "[EMPTY]",
                    "89.58"
                ]
            ],
            "title": "Table 2: CGR, GR and LP tasks under slicing ways S1 (left) and S2 (right): each entry (in %) is calculated by the mean over 20 time steps and over 10 runs; the best result of each half row is in bold."
        },
        "insight": "Both tasks are then evaluated by Average Precision@k (AP@k) score . [CONTINUE] LP task is then evaluated by Area under the ROC Curve (AUC) score [CONTINUE] As shown in Table 2, for all CGR and GR tasks, our method significantly outperforms other methods in most cases. One exception is the GR-AP@10-S2-DNC case where BCGD-local and our method both obtain bad results, which is due to that S2 makes the snapshots of DNC snapshots dramatically sparse as shown in Table 1 [CONTINUE] For all LP tasks, all methods except DynGEM are comparable, since no method can always outperform others."
    },
    {
        "id": "745",
        "table": {
            "header": [
                "[EMPTY]",
                "GEM",
                "BCGD [ITALIC] l",
                "BCGD [ITALIC] g",
                "Triad",
                "Ours0.2,0.5",
                "Ours0.2,0.5 [ITALIC] t=0",
                "Ours0.2,0.5 [ITALIC] t\u22651, [ITALIC] avg",
                "Ours0.4,0.5 [ITALIC] t\u22651, [ITALIC] avg",
                "Ours0.6,0.5 [ITALIC] t\u22651, [ITALIC] avg"
            ],
            "rows": [
                [
                    "DNC",
                    "696",
                    "241",
                    "939",
                    "129",
                    "[BOLD] 91",
                    "11.31",
                    "3.89",
                    "6.90",
                    "9.98"
                ],
                [
                    "AS733",
                    "1289",
                    "563",
                    "1800",
                    "[BOLD] 111",
                    "152",
                    "16.31",
                    "6.68",
                    "11.40",
                    "16.05"
                ],
                [
                    "Chess",
                    "5495",
                    "1505",
                    "4108",
                    "776",
                    "[BOLD] 333",
                    "47.85",
                    "14.08",
                    "25.48",
                    "36.79"
                ],
                [
                    "Elec",
                    "9677",
                    "1902",
                    "4829",
                    "2729",
                    "[BOLD] 495",
                    "88.57",
                    "20.07",
                    "37.80",
                    "55.54"
                ],
                [
                    "HepPh",
                    "[EMPTY]",
                    "11878",
                    "16834",
                    "[EMPTY]",
                    "[BOLD] 2128",
                    "319.02",
                    "88.71",
                    "152.12",
                    "223.54"
                ],
                [
                    "FBW",
                    "[EMPTY]",
                    "10042",
                    "23139",
                    "4896",
                    "[BOLD] 2506",
                    "473.79",
                    "100.83",
                    "192.38",
                    "285.69"
                ]
            ],
            "title": "Table 3: Wall-clock time under slicing way S1: the left part shows the total time over 21 time steps including I/O; the right part shows the detailed time of the offline or online stage excluding I/O."
        },
        "insight": "To access wall-clock time, all the results as shown in Table 3 are produced [CONTINUE] It is obvious that our method is much faster than other methods [CONTINUE] Considering online stage Ours0.2,0.5 t\u22651,avg, the superiority will become more obvious if lasting for more times steps."
    },
    {
        "id": "746",
        "table": {
            "header": [
                "Method",
                "VGG-13",
                "VGG-16",
                "VGG-19",
                "WRN 52-1",
                "WRN 16-4",
                "WRN 28-10"
            ],
            "rows": [
                [
                    "SGD",
                    "5.88",
                    "6.32",
                    "6.49",
                    "6.23",
                    "4.96",
                    "3.89"
                ],
                [
                    "ADAM",
                    "6.43",
                    "6.61",
                    "6.92",
                    "6.77",
                    "5.32",
                    "3.86"
                ],
                [
                    "Cayley SGD",
                    "5.90",
                    "[BOLD] 5.77",
                    "5.85",
                    "6.35",
                    "5.15",
                    "3.66"
                ],
                [
                    "Cayley ADAM",
                    "5.93",
                    "5.88",
                    "6.03",
                    "6.44",
                    "5.22",
                    "[BOLD] 3.57"
                ]
            ],
            "title": "Table 1: Classification errors(%) on CIFAR10."
        },
        "insight": "Table 1 and Table 2 show classification errors on CIFAR10 and CIFAR100 respectively using different optimization algorithms. As shown in the tables, the proposed two algorithms achieve competitive performance, and for certain deep architectures, the best performance. Specifically, the network WRN-28-10 trained with Cayley ADAM achieves the best error rate of 3.57% and 18.10% on CIFAR10 and CIFAR100 respectively."
    },
    {
        "id": "747",
        "table": {
            "header": [
                "Method",
                "VGG-13",
                "VGG-16",
                "VGG-19",
                "WRN 52-1",
                "WRN 16-4",
                "WRN 28-10"
            ],
            "rows": [
                [
                    "SGD",
                    "26.17",
                    "26.84",
                    "27.62",
                    "27.44",
                    "23.41",
                    "18.66"
                ],
                [
                    "ADAM",
                    "26.58",
                    "27.10",
                    "27.88",
                    "27.89",
                    "24.45",
                    "18.45"
                ],
                [
                    "Cayley SGD",
                    "[BOLD] 24.86",
                    "25.48",
                    "25.68",
                    "27.64",
                    "23.71",
                    "18.26"
                ],
                [
                    "Cayley ADAM",
                    "25.10",
                    "25.61",
                    "25.70",
                    "27.91",
                    "24.18",
                    "[BOLD] 18.10"
                ]
            ],
            "title": "Table 2: Classification errors(%) on CIFAR100."
        },
        "insight": "Table 1 and Table 2 show classification errors on CIFAR10 and CIFAR100 respectively using different optimization algorithms. As shown in the tables, the proposed two algorithms achieve competitive performance, and for certain deep architectures, the best performance. Specifically, the network WRN-28-10 trained with Cayley ADAM achieves the best error rate of 3.57% and 18.10% on CIFAR10 and CIFAR100 respectively."
    },
    {
        "id": "748",
        "table": {
            "header": [
                "PM",
                "FM",
                "Tri",
                "A-distance",
                "Accuracy (%)"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "1.528",
                    "76.7"
                ],
                [
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "1.519",
                    "78.1"
                ],
                [
                    "\u2713",
                    "[EMPTY]",
                    "\u2713",
                    "1.508",
                    "79.4"
                ],
                [
                    "[EMPTY]",
                    "\u2713",
                    "[EMPTY]",
                    "1.497",
                    "82.1"
                ],
                [
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "1.492",
                    "83.2"
                ],
                [
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "1.489",
                    "83.9"
                ]
            ],
            "title": "Table 4: Effectiveness of pixel-level mixup (PM), feature-level mixup (FM) and triplet loss (Tri)."
        },
        "insight": "Table 4 examines the effectiveness of pixel-level mixup (PM) and feature-level mixup (FM). [CONTINUE] In the fourth row, featurelevel mixup achieves notable improvement compared with baseline, [CONTINUE] In the fifth row, pixel-level mixup further enhance model's performance [CONTINUE] In Table 4, we evaluate another key component, i.e., triplet loss (Tri). [CONTINUE] In the third and sixth rows, it can also be observed that model's performance is improved after adding the triplet loss to discriminator's training process,"
    },
    {
        "id": "749",
        "table": {
            "header": [
                "[EMPTY]",
                "Method",
                "Error Rate(%) CIFAR10",
                "Error Rate(%) CIFAR100",
                "Training time(s)"
            ],
            "rows": [
                [
                    "Baselines",
                    "SGD",
                    "3.89",
                    "18.66",
                    "102.5"
                ],
                [
                    "Baselines",
                    "ADAM",
                    "3.85",
                    "18.52",
                    "115.2"
                ],
                [
                    "Soft orthonormality",
                    "SO (Bansal et\u00a0al.,  2018 )",
                    "3.76",
                    "18.56",
                    "297.3"
                ],
                [
                    "Soft orthonormality",
                    "DSO (Bansal et\u00a0al.,  2018 )",
                    "3.86",
                    "18.21",
                    "311.0"
                ],
                [
                    "Soft orthonormality",
                    "SRIP (Bansal et\u00a0al.,  2018 )",
                    "3.60",
                    "18.19",
                    "321.8"
                ],
                [
                    "Hard orthonormality",
                    "OMDSM (Huang et\u00a0al.,  2018a )",
                    "3.73",
                    "18.61",
                    "943.6"
                ],
                [
                    "Hard orthonormality",
                    "DBN (Huang et\u00a0al.,  2018b )",
                    "3.79",
                    "18.36",
                    "889.4"
                ],
                [
                    "Hard orthonormality",
                    "Polar (Absil et\u00a0al.,  2009 )",
                    "3.75",
                    "18.50",
                    "976.5"
                ],
                [
                    "Hard orthonormality",
                    "QR (Absil et\u00a0al.,  2009 )",
                    "3.75",
                    "18.65",
                    "469.3"
                ],
                [
                    "Hard orthonormality",
                    "Wen&Yin (Wen & Yin,  2013 )",
                    "3.82",
                    "18.70",
                    "305.8"
                ],
                [
                    "Hard orthonormality",
                    "Cayley closed form w/o momentum",
                    "3.80",
                    "18.68",
                    "1071.5"
                ],
                [
                    "Hard orthonormality",
                    "Cayley SGD\u00a0( [BOLD] Ours)",
                    "3.66",
                    "18.26",
                    "218.7"
                ],
                [
                    "Hard orthonormality",
                    "Cayley ADAM\u00a0( [BOLD] Ours)",
                    "3.57",
                    "18.10",
                    "224.4"
                ]
            ],
            "title": "Table 3: Error rate and training time per epoch comparison to baselines with WRN-28-10 on CIFAR10 and CIFAR100. All experiments are performed on one TITAN Xp GPU."
        },
        "insight": "We compare the proposed algorithms with two sets of state of the art. The first set of approaches are soft orthonormality regularization approaches (Bansal et al., 2018). [CONTINUE] The second set of approaches includes the following hard orthonormality methods: Polar decomposition(Absil et al., 2009), QR decomposition(Absil et al., 2009), closed-form Cayley transform, Wen&Yin (Wen & Yin, 2013), OMDSM(Huang et al., 2018a), DBN(Huang et al., 2018b). [CONTINUE] Also, we use the closed-form Cayley transform without momentum as an ablation study of the momentum effect. All experiments are evaluated on the benchmark network WRN28-10. Table 3 shows that our algorithms achieve comparable error rates with state of the art. All algorithms are run on one TITAN Xp GPU, and their average training time are compared per epoch. Table 3 shows that our algorithms run much faster than existing algorithms, except for the baseline SGD and ADAM which do not impose orthonormality constraints."
    },
    {
        "id": "750",
        "table": {
            "header": [
                "Model",
                "Hidden Size",
                "Closed-Form Acc(%)",
                "Closed-Form Time(s)",
                "Cayley SGD Acc(%)",
                "Cayley SGD Time(s)",
                "Cayley ADAM Acc(%)",
                "Cayley ADAM Time(s)"
            ],
            "rows": [
                [
                    "Full-uRNN",
                    "116",
                    "92.8",
                    "2.10",
                    "92.6",
                    "1.42",
                    "92.7",
                    "1.50"
                ],
                [
                    "Full-uRNN",
                    "512",
                    "96.9",
                    "2.44",
                    "96.7",
                    "1.67",
                    "[BOLD] 96.9",
                    "1.74"
                ]
            ],
            "title": "Table 4: Pixel-by-pixel MNIST accuracy and training time per iteration of the closed-form Cayley Transform, Cayley SGD, and Cayley ADAM for Full-uRNNs (Wisdom et\u00a0al., 2016). All experiments are performed on one TITAN Xp GPU."
        },
        "insight": "Wisdom et al. (2016) restricted the transition unitary matrix on the Stiefel manifold via a closed-form Cayley transform. On the contrary, we use Cayley SGD with momentum and Cayley ADAM to reduce the training time. Table 4 shows that the proposed algorithms reduce the training [CONTINUE] time by about 35% for all settings of the network, while maintaining the same level of accuracy. All experiments are performed on one TITAN Xp GPU."
    },
    {
        "id": "751",
        "table": {
            "header": [
                "Hidden Size",
                "s=0",
                "s=1",
                "s=2",
                "s=3",
                "s=4",
                "Closed-form"
            ],
            "rows": [
                [
                    "n=116",
                    "3.231e-3",
                    "2.852e-4",
                    "7.384e-6",
                    "7.353e-6",
                    "7.338e-6",
                    "8.273e-5"
                ],
                [
                    "n=512",
                    "6.787e-3",
                    "5.557e-4",
                    "2.562e-5",
                    "2.547e-5",
                    "2.544e-5",
                    "3.845e-5"
                ]
            ],
            "title": "Table 5: Checking unitariness by computing the error ||KHK\u2212I||F for varying numbers of iterations in the iterative Cayley transform and the closed-form Cayley transform."
        },
        "insight": "To show that the proposed algorithms are valid optimization algorithms on the Stiefel manifold, we check the unitariness of the hidden-to-hidden matrix K by computing the error term ||K H K \u2212 I||F during training. Table 5 compares average errors for varying numbers of iterations s. As can be seen, the iterative Cayley transform can approximate the unitary matrix when s = 2."
    },
    {
        "id": "752",
        "table": {
            "header": [
                "[EMPTY]",
                "\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00afSINR",
                "SINR5",
                "SINR50",
                "SINR95"
            ],
            "rows": [
                [
                    "Baseline",
                    "20.52",
                    "4.91",
                    "20.26",
                    "36.99"
                ],
                [
                    "Opt. Simulator",
                    "23.24",
                    "7.25",
                    "23.18",
                    "39.27"
                ],
                [
                    "Opt. Emulator",
                    "23.49",
                    "7.47",
                    "23.45",
                    "39.64"
                ]
            ],
            "title": "TABLE I: Numerical results shown in Fig. 5."
        },
        "insight": "The optimization results obtained within the scenario described in the previous section are presented in Table [CONTINUE] [CONTINUE] Results show that in the proposed scenario, a baseline setup [CONTINUE] performs significantly worse than the optimized ones. [CONTINUE] The other two configurations represent the optimum obtained over the collected dataset (Opt. Simulator, [CONTINUE] and the global optimum obtained using our framework (Opt. Emulator). [CONTINUE] Results show a \u223c 3 dB improvement over the trivial baseline. Although in this case the results are really close"
    },
    {
        "id": "753",
        "table": {
            "header": [
                "[BOLD] Data set",
                "[BOLD] instances",
                "[BOLD] features",
                "[BOLD] classes",
                "[BOLD] #neurons",
                "[BOLD] batch size",
                "[BOLD] lr",
                "[BOLD] epochs",
                "[BOLD] min samples leaf",
                "[BOLD] pruning"
            ],
            "rows": [
                [
                    "2-D parabola",
                    "500",
                    "2",
                    "2",
                    "100,\u2009100,\u200910",
                    "100",
                    "0.001",
                    "1000",
                    "-",
                    "N"
                ],
                [
                    "Iris\u00a0",
                    "150",
                    "4",
                    "3",
                    "8",
                    "10",
                    "0.01",
                    "50",
                    "5",
                    "Y"
                ],
                [
                    "Breast Cancer Wisconsin\u00a0",
                    "569",
                    "30",
                    "2",
                    "64, 32",
                    "10",
                    "0.001",
                    "10",
                    "15",
                    "Y"
                ],
                [
                    "Pima Indians Diabetes\u00a0",
                    "768",
                    "8",
                    "2",
                    "24",
                    "128",
                    "0.01",
                    "10",
                    "30",
                    "Y"
                ],
                [
                    "Titanic\u00a0",
                    "891",
                    "11",
                    "2",
                    "100,\u200950,\u200925",
                    "16",
                    "0.005",
                    "10",
                    "35",
                    "Y"
                ],
                [
                    "Mushroom\u00a0",
                    "8,124",
                    "22",
                    "2",
                    "16",
                    "10",
                    "0.005",
                    "25",
                    "45",
                    "Y"
                ],
                [
                    "Adult\u00a0",
                    "48,842",
                    "14",
                    "2",
                    "32, 16",
                    "32",
                    "0.005",
                    "10",
                    "75",
                    "Y"
                ],
                [
                    "Diabetes\u00a0",
                    "100,000",
                    "50",
                    "2",
                    "32, 16, 8",
                    "512",
                    "0.01",
                    "50",
                    "250",
                    "Y"
                ]
            ],
            "title": "TABLE I: Data sets and training parameters used for evaluation. #neurons refers to number of neurons per hidden layer."
        },
        "insight": "Table [CONTINUE] summarizes the data sets and sizes of the hidden layers of the MLPs used for the experiments."
    },
    {
        "id": "754",
        "table": {
            "header": [
                "Data set",
                "L1-O  [ITALIC] \u03bb1",
                "L1-O  [ITALIC] \u03bborth",
                "L1-O  [BOLD] APL",
                "L1-O  [BOLD] Consistency",
                "L1-O  [BOLD] Fidelity",
                "FN  [ITALIC] \u03bb1",
                "FN  [ITALIC] \u03bborth",
                "FN  [BOLD] APL",
                "FN  [BOLD] Consistency",
                "FN  [BOLD] Fidelity"
            ],
            "rows": [
                [
                    "Titanic",
                    "0.006",
                    "0.025",
                    "2.4",
                    "0.70",
                    "0.98\u00b1\u20090.03",
                    "0.02",
                    "0.75",
                    "1.7",
                    "1.00",
                    "0.99\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.001",
                    "0.01",
                    "4.7",
                    "1.00",
                    "0.93\u00b1\u20090.00",
                    "0.003",
                    "0.75",
                    "4.2",
                    "0.70",
                    "0.97\u00b1\u20090.01"
                ],
                [
                    "[EMPTY]",
                    "0.001",
                    "0.001",
                    "7.9",
                    "1.00",
                    "0.91\u2009\u00b1\u20090.01",
                    "0.002",
                    "0.001",
                    "8.0",
                    "0.94",
                    "0.93\u00b1\u20090.01"
                ],
                [
                    "Mushroom",
                    "0.1",
                    "0.75",
                    "1.4",
                    "0.90",
                    "1.00\u00b1\u20090.00",
                    "0.1",
                    "0.75",
                    "3.8",
                    "0.53",
                    "0.99\u00b1\u20090.0"
                ],
                [
                    "[EMPTY]",
                    "0.08",
                    "1.1",
                    "6.4",
                    "0.93",
                    "0.99\u00b1\u20090.00",
                    "0.08",
                    "1.1",
                    "6.4",
                    "0.52",
                    "1.00\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.003",
                    "0.001",
                    "14",
                    "1.0",
                    "0.98\u00b1\u20090.00",
                    "0.003",
                    "0.001",
                    "14",
                    "1.0",
                    "0.98\u00b1\u20090.00"
                ],
                [
                    "Adult",
                    "0.09",
                    "0.5",
                    "1.7",
                    "0.82",
                    "1.00\u2009\u00b1\u20090.00",
                    "0.08",
                    "1.1",
                    "1.7",
                    "0.86",
                    "0.99\u2009\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.02",
                    "0.25",
                    "16",
                    "0.88",
                    "0.98\u2009\u00b1\u20090.00",
                    "0.04",
                    "0.75",
                    "13.6",
                    "0.97",
                    "0.99\u2009\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.0025",
                    "0.1",
                    "39",
                    "0.92",
                    "0.95\u2009\u00b1\u20090.00",
                    "0.003",
                    "0.25",
                    "37.2",
                    "0.86",
                    "0.97\u2009\u00b1\u20090.01"
                ],
                [
                    "Diabetes",
                    "0.04",
                    "0.75",
                    "3.9",
                    "0.84",
                    "1.00\u2009\u00b1\u20090.00",
                    "0.03",
                    "1.0",
                    "3.4",
                    "0.79",
                    "0.99\u2009\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.02",
                    "0.25",
                    "16",
                    "0.59",
                    "0.98\u2009\u00b1\u20090.01",
                    "0.02",
                    "1.1",
                    "14.1",
                    "0.94",
                    "0.99\u2009\u00b1\u20090.00"
                ],
                [
                    "[EMPTY]",
                    "0.0075",
                    "0.075",
                    "57",
                    "0.87",
                    "0.96\u2009\u00b1\u20090.01",
                    "0.004",
                    "0.01",
                    "57.9",
                    "0.83",
                    "0.94\u2009\u00b1\u20090.00"
                ]
            ],
            "title": "TABLE III: Consistency of DT predictions."
        },
        "insight": "The results for four data sets are listed in Table III in ascending order of the complexity of the generated DTs (APL)."
    },
    {
        "id": "755",
        "table": {
            "header": [
                "Regularization",
                "2D-parabola",
                "Iris",
                "Breast Cancer"
            ],
            "rows": [
                [
                    "L1-O",
                    "[BOLD] 42.72",
                    "[BOLD] 0.59",
                    "[BOLD] 6.44"
                ],
                [
                    "Tree",
                    "2,841.56",
                    "17.78",
                    "471.49"
                ]
            ],
            "title": "TABLE IV: Computation time in seconds."
        },
        "insight": "As Table IV indicates, the run times of treeregularized models are 30\u201370 times higher than those of an L1-O-regularized model."
    },
    {
        "id": "756",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3"
            ],
            "rows": [
                [
                    "m=0",
                    "0.91",
                    "[BOLD] 0.08",
                    "0.98"
                ],
                [
                    "m=1",
                    "0.16",
                    "[BOLD] 0.07",
                    "0.38"
                ],
                [
                    "m=2",
                    "0.13",
                    "0.82",
                    "0.11"
                ]
            ],
            "title": "TABLE I: P-value of RCMSE features of ECG signals (Left:Arousal, Right:Valence)"
        },
        "insight": "The p-value of RCMSE and MMSE of ECG and GSR are shown in Table [CONTINUE] and II. [CONTINUE] RCMSE of ECG has the best p-value when setting scaling factor to 2 for both arousal and valence."
    },
    {
        "id": "757",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3"
            ],
            "rows": [
                [
                    "m=0",
                    "0.91",
                    "[ITALIC]  [BOLD] 0.04",
                    "[BOLD] 0.08"
                ],
                [
                    "m=1",
                    "0.46",
                    "[BOLD] 0.06",
                    "0.14"
                ],
                [
                    "m=2",
                    "0.76",
                    "[ITALIC]  [BOLD] 0.01",
                    "0.91"
                ]
            ],
            "title": "TABLE I: P-value of RCMSE features of ECG signals (Left:Arousal, Right:Valence)"
        },
        "insight": "For RCMSE of GSR, we can observe that for arousal, the p-value becomes significant (0.01) when scaling factor is greater than 5. In these settings, the positive class would always have higher RCMSE, implying that the arousal of a subject is proportional to the complexity of its physiological signals. Note that the GSR would respond relatively slow according to the affect, thus the p-values become significant by increasing the scale factor."
    },
    {
        "id": "758",
        "table": {
            "header": [
                "[ITALIC] Dcls",
                "pseudo",
                "A-distance",
                "Accuracy (%)"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "1.503",
                    "80.6"
                ],
                [
                    "\u2713",
                    "[EMPTY]",
                    "1.496",
                    "82.3"
                ],
                [
                    "\u2713",
                    "\u2713",
                    "1.489",
                    "83.9"
                ]
            ],
            "title": "Table 5: Effectiveness of Dcls and pseudo target labels."
        },
        "insight": "the effectiveness of them is examined in Table 5. [CONTINUE] After appending Dcls, classification accuracy increases by 1.7%, [CONTINUE] On such basis, pseudo target labels introduce the discriminative information of target domain to the adaptation process and make model's performance state-of-the-art."
    },
    {
        "id": "759",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10"
            ],
            "rows": [
                [
                    "A ( [ITALIC] m2)",
                    "0.23",
                    "0.20",
                    "0.18",
                    "0.11",
                    "0.10",
                    "[BOLD] 0.07",
                    "[BOLD] 0.05",
                    "[ITALIC]  [BOLD] 0.04",
                    "[ITALIC]  [BOLD] 0.03",
                    "[ITALIC]  [BOLD] 0.03"
                ],
                [
                    "V ( [ITALIC] m2)",
                    "0.25",
                    "0.33",
                    "0.28",
                    "0.34",
                    "0.29",
                    "0.31",
                    "0.33",
                    "0.35",
                    "0.33",
                    "0.32"
                ],
                [
                    "scale",
                    "11",
                    "12",
                    "13",
                    "14",
                    "15",
                    "16",
                    "17",
                    "18",
                    "19",
                    "20"
                ],
                [
                    "A ( [ITALIC] m2)",
                    "[ITALIC]  [BOLD] 0.02",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01"
                ],
                [
                    "V ( [ITALIC] m2)",
                    "0.31",
                    "0.29",
                    "0.29",
                    "0.29",
                    "0.28",
                    "0.27",
                    "0.27",
                    "0.27",
                    "0.27",
                    "0.26"
                ]
            ],
            "title": "TABLE II: P-value of RCMSE features of GSR signals (A: arousal, V: valence) (m2 means pattern length = 2)"
        },
        "insight": "The p-value of RCMPE and MMPE of ECG, GSR and EEG are shown in Table III, IV and V. [CONTINUE] RCMPE of ECG performs better in valence than arousal with the significantly low p-values (0.01) in all scale."
    },
    {
        "id": "760",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3"
            ],
            "rows": [
                [
                    "A ( [ITALIC] m3)",
                    "[ITALIC]  [BOLD] 0.03",
                    "[ITALIC]  [BOLD] 0.03",
                    "[BOLD] 0.1"
                ],
                [
                    "V ( [ITALIC] m6)",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01"
                ]
            ],
            "title": "TABLE III: P-value of RCMPE features of ECG signals (A: arousal, V: valence) (m3 means embedding dimension = 3)"
        },
        "insight": "RCMPE of GSR gets greater performance in arousal since most of the features are lower than 0.05 in arousal. The positive class will have higher RCMPE which is congruent with the case [CONTINUE] in RCMSE of GSR."
    },
    {
        "id": "761",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10"
            ],
            "rows": [
                [
                    "A ( [ITALIC] m2)",
                    "[ITALIC]  [BOLD] 0.01",
                    "[BOLD] 0.08",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01"
                ],
                [
                    "V ( [ITALIC] m5)",
                    "[BOLD] 0.06",
                    "0.13",
                    "0.18",
                    "0.22",
                    "0.25",
                    "0.28",
                    "0.31",
                    "0.32",
                    "0.34",
                    "0.36"
                ],
                [
                    "scale",
                    "11",
                    "12",
                    "13",
                    "14",
                    "15",
                    "16",
                    "17",
                    "18",
                    "19",
                    "20"
                ],
                [
                    "A ( [ITALIC] m2)",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01"
                ],
                [
                    "V ( [ITALIC] m5)",
                    "0.37",
                    "0.38",
                    "0.39",
                    "0.39",
                    "0.40",
                    "0.40",
                    "0.41",
                    "0.41",
                    "0.41",
                    "0.41"
                ]
            ],
            "title": "TABLE IV: P-value of RCMPE features of GSR signals (A: arousal, V: valence) (m2 means embedding dimension = 2)"
        },
        "insight": "MMPE of EEG performs much better in valence when the scale factor goes up, which indicates the importance of the coarse-graining step."
    },
    {
        "id": "762",
        "table": {
            "header": [
                "scale",
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10"
            ],
            "rows": [
                [
                    "A ( [ITALIC] m4)",
                    "0.43",
                    "0.46",
                    "0.99",
                    "0.82",
                    "0.66",
                    "0.45",
                    "0.54",
                    "0.52",
                    "0.33",
                    "0.47"
                ],
                [
                    "V ( [ITALIC] m6)",
                    "0.30",
                    "0.37",
                    "0.38",
                    "0.46",
                    "0.48",
                    "0.46",
                    "0.38",
                    "0.30",
                    "0.25",
                    "0.15"
                ],
                [
                    "scale",
                    "11",
                    "12",
                    "13",
                    "14",
                    "15",
                    "16",
                    "17",
                    "18",
                    "19",
                    "20"
                ],
                [
                    "A ( [ITALIC] m4)",
                    "0.26",
                    "0.22",
                    "0.27",
                    "0.12",
                    "0.13",
                    "[BOLD] 0.1",
                    "0.14",
                    "0.17",
                    "[ITALIC]  [BOLD] 0.04",
                    "0.17"
                ],
                [
                    "V ( [ITALIC] m4)",
                    "[BOLD] 0.08",
                    "[ITALIC]  [BOLD] 0.03",
                    "[ITALIC]  [BOLD] 0.02",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01",
                    "[ITALIC]  [BOLD] 0.01"
                ]
            ],
            "title": "TABLE V: P-value of MMPE features of EEG signals (AF3, AF4) (A: arousal, V: valence) (m4 means embedding dimension = 4)"
        },
        "insight": "The emotion recognition performance is shown in Table VI. [CONTINUE] Scheme [CONTINUE] was the one reported in , where 213 traditional features and Gaussian Naive Bayes were employed. Scheme II was also fed with traditional features but had XGBoost as the classification model. Scheme III was the one we proposed, which utilized entropy-domain features and XGBoost. We [CONTINUE] concatenated old traditional features with new entropy-domain features which were statistically significant (p-value < 0.05). There were 41 and 101 new entropy-domain features for arousal and valence respectively. The result shows that the Entropy-assisted model (III) we propose has the best performance in most of the situations. For single modality, all three of them in valence raise the F1score by over or around 10%. Huge improvements compared to previous methods (Scheme I), +12.1% and +25.3% for arousal and valence respectively, are found in Fusion modalities. +17.8% can be found in EEG for valence between Scheme II and Scheme III."
    },
    {
        "id": "763",
        "table": {
            "header": [
                "Scheme (A)",
                "EEG",
                "ECG",
                "GSR",
                "Fusion"
            ],
            "rows": [
                [
                    "I ",
                    "[BOLD] 0.592",
                    "0.550",
                    "0.548",
                    "0.585"
                ],
                [
                    "II",
                    "0.568",
                    "0.556",
                    "0.665",
                    "0.687"
                ],
                [
                    "III",
                    "0.568",
                    "[BOLD] 0.561",
                    "[BOLD] 0.692",
                    "[BOLD] 0.706"
                ]
            ],
            "title": "TABLE VI: Mean F1-score of emotion recognition on AMIGOS (up: arousal, down: valence)"
        },
        "insight": "Dominant features in terms of the feature importance of Fusion modalities in Scheme III are shown in Table VII. entropy-domain features are highlighted in boldface. The selection of entropy-domain features (MMPE of EEG and RCMPE of ECG) vindicates the improvements in the performance of Fusion between Scheme II and III where +1.9% and +2.9% are found for arousal and valence."
    },
    {
        "id": "764",
        "table": {
            "header": [
                "Dataset n/k",
                "Dataset n",
                "Spiral M1",
                "Spiral M2",
                "Spiral M3",
                "Swiss roll M1",
                "Swiss roll M2",
                "Swiss roll M3"
            ],
            "rows": [
                [
                    "55",
                    "1500",
                    "[BOLD] 6.86E-5 \u00b1 2E-6",
                    "6.88E-5 \u00b1 2E-6",
                    "8.25E-5 \u00b1 2E-5",
                    "3.71E-1 \u00b1 9E-3",
                    "[BOLD] 3.46E-1 \u00b1 1E-2",
                    "3.77E-1 \u00b1 2E-2"
                ],
                [
                    "55",
                    "1700",
                    "6.56E-5 \u00b1 2E-6",
                    "[BOLD] 6.53E-5 \u00b1 3E-6",
                    "7.40E-5 \u00b1 1E-5",
                    "3.53E-1 \u00b1 1E-2",
                    "[BOLD] 3.42E-1 \u00b1 5E-3",
                    "3.65E-1 \u00b1 1E-2"
                ],
                [
                    "55",
                    "1900",
                    "[BOLD] 6.31E-5 \u00b1 2E-6",
                    "6.45E-5 \u00b1 2E-6",
                    "6.95E-5 \u00b1 3E-6",
                    "3.50E-1 \u00b1 1E-2",
                    "[BOLD] 3.35E-1 \u00b1 8E-3",
                    "3.65E-1 \u00b1 2E-2"
                ],
                [
                    "55",
                    "2100",
                    "[BOLD] 6.20E-5 \u00b1 2E-6",
                    "6.35E-5 \u00b1 2E-6",
                    "7.55E-5 \u00b1 2E-5",
                    "3.55E-1 \u00b1 1E-2",
                    "[BOLD] 3.34E-1 \u00b1 8E-3",
                    "3.60E-1 \u00b1 2E-2"
                ],
                [
                    "55",
                    "2300",
                    "[BOLD] 6.12E-5 \u00b1 1E-6",
                    "6.21E-5 \u00b1 2E-6",
                    "7.09E-5 \u00b1 1E-5",
                    "3.47E-1 \u00b1 9E-3",
                    "[BOLD] 3.34E-1 \u00b1 9E-3",
                    "3.51E-1 \u00b1 1E-2"
                ],
                [
                    "65",
                    "1500",
                    "7.52E-5 \u00b1 3E-6",
                    "[BOLD] 7.44E-5 \u00b1 3E-6",
                    "9.13E-5 \u00b1 1E-5",
                    "[BOLD] 3.54E-1 \u00b1 1E-2",
                    "3.62E-1 \u00b1 2E-2",
                    "3.81E-1 \u00b1 2E-2"
                ],
                [
                    "65",
                    "1700",
                    "[BOLD] 6.79E-5 \u00b1 2E-6",
                    "6.82E-5 \u00b1 3E-6",
                    "7.65E-5 \u00b1 8E-6",
                    "[BOLD] 3.52E-1 \u00b1 8E-3",
                    "3.53E-1 \u00b1 2E-2",
                    "3.95E-1 \u00b1 4E-2"
                ],
                [
                    "65",
                    "1900",
                    "[BOLD] 6.43E-5 \u00b1 1E-6",
                    "6.64E-5 \u00b1 2E-6",
                    "6.96E-5 \u00b1 3E-6",
                    "3.49E-1 \u00b1 1E-2",
                    "[BOLD] 3.42E-1 \u00b1 1E-2",
                    "3.67E-1 \u00b1 2E-2"
                ],
                [
                    "65",
                    "2100",
                    "6.42E-5 \u00b1 3E-6",
                    "[BOLD] 6.35E-5 \u00b1 1E-6",
                    "6.93E-5 \u00b1 4E-6",
                    "3.54E-1 \u00b1 1E-2",
                    "[BOLD] 3.35E-1 \u00b1 1E-2",
                    "3.59E-1 \u00b1 2E-2"
                ],
                [
                    "65",
                    "2300",
                    "[BOLD] 6.10E-5 \u00b1 2E-6",
                    "6.32E-5 \u00b1 2E-6",
                    "6.94E-5 \u00b1 8E-6",
                    "3.47E-1 \u00b1 1E-2",
                    "[BOLD] 3.28E-1 \u00b1 9E-3",
                    "3.57E-1 \u00b1 3E-2"
                ],
                [
                    "75",
                    "1500",
                    "[BOLD] 8.02E-5 \u00b1 3E-6",
                    "8.21E-5 \u00b1 6E-6",
                    "9.75E-5 \u00b1 1E-5",
                    "[BOLD] 3.60E-1 \u00b1 1E-2",
                    "3.92E-1 \u00b1 8E-2",
                    "4.11E-1 \u00b1 3E-2"
                ],
                [
                    "75",
                    "1700",
                    "[BOLD] 7.46E-5 \u00b1 2E-6",
                    "7.53E-5 \u00b1 3E-6",
                    "8.50E-5 \u00b1 8E-6",
                    "3.55E-1 \u00b1 1E-2",
                    "[BOLD] 3.50E-1 \u00b1 6E-3",
                    "3.90E-1 \u00b1 6E-2"
                ],
                [
                    "75",
                    "1900",
                    "6.99E-5 \u00b1 3E-6",
                    "[BOLD] 6.88E-5 \u00b1 3E-6",
                    "7.82E-5 \u00b1 8E-6",
                    "3.50E-1 \u00b1 1E-2",
                    "[BOLD] 3.49E-1 \u00b1 2E-2",
                    "3.92E-1 \u00b1 4E-2"
                ],
                [
                    "75",
                    "2100",
                    "[BOLD] 6.48E-5 \u00b1 3E-6",
                    "6.59E-5 \u00b1 2E-6",
                    "7.54E-5 \u00b1 7E-6",
                    "3.50E-1 \u00b1 1E-2",
                    "[BOLD] 3.38E-1 \u00b1 1E-2",
                    "3.63E-1 \u00b1 2E-2"
                ],
                [
                    "75",
                    "2300",
                    "[BOLD] 6.40E-5 \u00b1 2E-6",
                    "6.47E-5 \u00b1 2E-6",
                    "6.72E-5 \u00b1 3E-6",
                    "[BOLD] 3.38E-1 \u00b1 1E-2",
                    "3.46E-1 \u00b1 2E-2",
                    "3.83E-1 \u00b1 5E-2"
                ],
                [
                    "85",
                    "1500",
                    "[BOLD] 9.55E-5 \u00b1 6E-6",
                    "1.01E-4 \u00b1 8E-6",
                    "1.34E-4 \u00b1 3E-5",
                    "4.33E-1 \u00b1 1E-1",
                    "[BOLD] 4.22E-1 \u00b1 1E-1",
                    "4.24E-1 \u00b1 5E-2"
                ],
                [
                    "85",
                    "1700",
                    "8.10E-5 \u00b1 5E-6",
                    "[BOLD] 7.89E-5 \u00b1 4E-6",
                    "1.06E-4 \u00b1 2E-5",
                    "3.81E-1 \u00b1 9E-2",
                    "[BOLD] 3.65E-1 \u00b1 1E-2",
                    "3.94E-1 \u00b1 3E-2"
                ],
                [
                    "85",
                    "1900",
                    "[BOLD] 7.37E-5 \u00b1 3E-6",
                    "7.43E-5 \u00b1 4E-6",
                    "8.76E-5 \u00b1 1E-5",
                    "3.50E-1 \u00b1 9E-3",
                    "[BOLD] 3.49E-1 \u00b1 1E-2",
                    "3.91E-1 \u00b1 5E-2"
                ],
                [
                    "85",
                    "2100",
                    "7.13E-5 \u00b1 2E-6",
                    "[BOLD] 6.94E-5 \u00b1 1E-6",
                    "7.71E-5 \u00b1 4E-6",
                    "3.49E-1 \u00b1 9E-3",
                    "[BOLD] 3.49E-1 \u00b1 8E-3",
                    "3.71E-1 \u00b1 2E-2"
                ],
                [
                    "85",
                    "2300",
                    "6.72E-5 \u00b1 2E-6",
                    "[BOLD] 6.62E-5 \u00b1 2E-6",
                    "6.92E-5 \u00b1 4E-6",
                    "3.44E-1 \u00b1 1E-2",
                    "[BOLD] 3.33E-1 \u00b1 5E-3",
                    "3.65E-1 \u00b1 1E-2"
                ],
                [
                    "95",
                    "1500",
                    "[BOLD] 1.13E-4 \u00b1 1E-5",
                    "1.21E-4 \u00b1 1E-5",
                    "1.52E-4 \u00b1 2E-5",
                    "5.45E-1 \u00b1 2E-1",
                    "[BOLD] 4.74E-1 \u00b1 1E-1",
                    "4.91E-1 \u00b1 1E-1"
                ],
                [
                    "95",
                    "1700",
                    "[BOLD] 8.98E-5 \u00b1 4E-6",
                    "9.62E-5 \u00b1 7E-6",
                    "1.33E-4 \u00b1 4E-5",
                    "4.97E-1 \u00b1 2E-1",
                    "[BOLD] 4.33E-1 \u00b1 1E-1",
                    "4.90E-1 \u00b1 1E-1"
                ],
                [
                    "95",
                    "1900",
                    "7.97E-5 \u00b1 5E-6",
                    "[BOLD] 7.75E-5 \u00b1 3E-6",
                    "9.70E-5 \u00b1 1E-5",
                    "3.59E-1 \u00b1 9E-3",
                    "[BOLD] 3.55E-1 \u00b1 1E-2",
                    "3.92E-1 \u00b1 3E-2"
                ],
                [
                    "95",
                    "2100",
                    "7.58E-5 \u00b1 4E-6",
                    "[BOLD] 7.16E-5 \u00b1 2E-6",
                    "9.23E-5 \u00b1 1E-5",
                    "3.48E-1 \u00b1 9E-3",
                    "[BOLD] 3.46E-1 \u00b1 9E-3",
                    "3.80E-1 \u00b1 2E-2"
                ],
                [
                    "95",
                    "2300",
                    "7.03E-5 \u00b1 2E-6",
                    "[BOLD] 6.87E-5 \u00b1 1E-6",
                    "7.50E-5 \u00b1 5E-6",
                    "[BOLD] 3.43E-1 \u00b1 6E-3",
                    "3.49E-1 \u00b1 1E-2",
                    "4.00E-1 \u00b1 3E-2"
                ]
            ],
            "title": "Table 1: Mean +- 1 standard deviations in expected reconstruction error for the Spiral and Swiss roll datasets, as a function of n and n/k."
        },
        "insight": "Thirdly, as can be seen in Tables 1 and 2, M1 tends to perform better on the 2D datasets when compared to the 3D datasets. [CONTINUE] Upon varying n/k and n, we would expect a general trend of estimation quality to improve as n/k decreases (the number of tangent spaces k increases) and as the training set size n increases."
    },
    {
        "id": "765",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Model",
                "[BOLD] Batch Size",
                "[BOLD] Dropout",
                "[ITALIC] \u03b7UCB",
                "[ITALIC] \u03b7CB",
                "[ITALIC] \u03b7S"
            ],
            "rows": [
                [
                    "MNIST",
                    "LR",
                    "128",
                    "NO",
                    "0.01",
                    "0.001",
                    "0.0001"
                ],
                [
                    "MNIST",
                    "MLP",
                    "128",
                    "NO",
                    "0.1",
                    "0.001",
                    "0.005"
                ],
                [
                    "MNIST",
                    "MLP",
                    "128",
                    "YES",
                    "0.1",
                    "0.0005",
                    "0.005"
                ],
                [
                    "MNIST",
                    "MLP",
                    "16",
                    "NO",
                    "0.3",
                    "0.0001",
                    "0.05"
                ],
                [
                    "CIFAR-10",
                    "CNN",
                    "128",
                    "NO",
                    "0.01",
                    "5\u00d710\u22125",
                    "0.0001"
                ],
                [
                    "CIFAR-10",
                    "CNN",
                    "128",
                    "YES",
                    "0.05",
                    "0.0001",
                    "0.0001"
                ],
                [
                    "CIFAR-10",
                    "CNN",
                    "16",
                    "NO",
                    "0.3",
                    "1\u00d710\u22125",
                    "0.005"
                ]
            ],
            "title": "Table 1: Best hyperparameters \u03b7 obtained for AdamUCB, AdamCB and AdamS algorithms when training MNIST and CIFAR-10 on various architectures."
        },
        "insight": "We report the mean and the standard deviation for various performance metrics such as loss and accuracy across three random runs of our experiments. The hyperparameter \u03b7 is tuned for each variant of the optimizer, and the best values found are listed in Table 1."
    },
    {
        "id": "766",
        "table": {
            "header": [
                "[EMPTY]",
                "#quasi-adv / #tested",
                "NNC,M",
                "NNC,C",
                "NNG,M",
                "NNG,C",
                "NNG,CWG"
            ],
            "rows": [
                [
                    "MNIST",
                    "18 / 32",
                    "2",
                    "3",
                    "1",
                    "3",
                    "7"
                ],
                [
                    "CIFAR10",
                    "26 / 32",
                    "16",
                    "12",
                    "7",
                    "6",
                    "25"
                ]
            ],
            "title": "Table 1: Number of Successful Adversarial Attacks for Different Implementations"
        },
        "insight": "We apply random perturbations outlined in Algorithm 1 to these quasi-adversarial inputs (x1 from Section 3.2 Step 2) to obtain adversarial inputs for the verified robust input image (x0 = \u03b1xseed from Section 3.2 Step 1). All the implementations that we have considered are successfully attacked. We present the detailed numbers in Table 1."
    },
    {
        "id": "767",
        "table": {
            "header": [
                "MNIST",
                "# attack",
                "NNC,M 4",
                "NNC,C 5",
                "NNG,M 2",
                "NNG,C 3",
                "NNG,CWG 17"
            ],
            "rows": [
                [
                    "MNIST",
                    "min test acc",
                    "98.40%",
                    "97.83%",
                    "98.57%",
                    "97.83%",
                    "97.83%"
                ],
                [
                    "CIFAR10",
                    "# attack",
                    "20",
                    "16",
                    "14",
                    "8",
                    "26"
                ],
                [
                    "CIFAR10",
                    "min test acc",
                    "58.74%",
                    "58.74%",
                    "58.74%",
                    "58.74%",
                    "58.74%"
                ]
            ],
            "title": "Table 2: Adversarial Attacks for Different Implementations Allowing Modifying Softmax Bias"
        },
        "insight": "Furthermore, since a large \u03c40 in the quasi-adversarial inputs makes our method less likely to succeed and it can result from a large value of \u03b4 in step 1 due to the solver exceeding the time limit, we also evaluate the performance of our attack algorithm when modifying the bias of the softmax layer is allowed. We decrease the bias of the target class by \u03c40 to obtain a model that is still verifiably robust but easier to be attacked. Since modifying the bias would affect test accuracy, we also evaluate the minimal test accuracy of modified models of the successful attacks for each implementation. The results are presented in Table 2, showing that attack success rate is improved."
    },
    {
        "id": "768",
        "table": {
            "header": [
                "Model  [ITALIC] f( [ITALIC] X) Noise",
                "Model  [ITALIC] f( [ITALIC] X) Noise",
                "[ITALIC] X2  [ITALIC] \u03b5\u223c [ITALIC] Exp(1)",
                "sin( [ITALIC] \u03c0X)  [ITALIC] \u03b5\u223c [ITALIC] ChiSqr(3)",
                "[ITALIC] e2 [ITALIC] X  [ITALIC] \u03b5\u223c [ITALIC] Rayl(4)",
                "[ITALIC] sigmoid(5 [ITALIC] X)  [ITALIC] \u03b5\u223c [ITALIC] BioNom(20,0.3)"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "SVR",
                    "8.320e-01",
                    "5.651e+00",
                    "6.320e+00",
                    "3.849e+00"
                ],
                [
                    "[EMPTY]",
                    "HSIC-reg",
                    "8.419e-01",
                    "5.688e+00",
                    "6.386e+00",
                    "3.878e+00"
                ],
                [
                    "[EMPTY]",
                    "NN-MSE",
                    "8.373e-01",
                    "5.548e+00",
                    "6.226e+00",
                    "3.707e+00"
                ],
                [
                    "MSE",
                    "GP",
                    "8.262e-01",
                    "5.586e+00",
                    "6.228e+00",
                    "3.846e+00"
                ],
                [
                    "MSE",
                    "AdOSE",
                    "8.301e-01",
                    "7.658e+00",
                    "9.708e+00",
                    "5.112e+00"
                ],
                [
                    "MSE",
                    "AdOR",
                    "9.299e-01",
                    "9.740e+00",
                    "1.209e+01",
                    "4.073e+00"
                ],
                [
                    "MSE",
                    "SVR",
                    "6.945e-01",
                    "1.926e+00",
                    "2.031e+00",
                    "1.555e+00"
                ],
                [
                    "MSE",
                    "HSIC-reg",
                    "7.049e-01",
                    "1.933e+00",
                    "2.051e+00",
                    "1.561e+00"
                ],
                [
                    "MSE",
                    "NN-MSE",
                    "7.061e-01",
                    "1.909e+00",
                    "2.037e+00",
                    "1.531e+00"
                ],
                [
                    "MAE",
                    "GP",
                    "6.975e-01",
                    "1.918e+00",
                    "2.035e+00",
                    "1.559e+00"
                ],
                [
                    "MAE",
                    "AdOSE",
                    "7.008e-01",
                    "2.071e+00",
                    "2.406e+00",
                    "1.527e+00"
                ],
                [
                    "MAE",
                    "AdOR",
                    "7.426e-01",
                    "2.492e+00",
                    "2.768e+00",
                    "1.626e+00"
                ],
                [
                    "MAE",
                    "SVR",
                    "1.160e-02",
                    "1.363e-01",
                    "2.197e-01",
                    "1.669e-02"
                ],
                [
                    "MAE",
                    "HSIC-reg",
                    "2.666e-02",
                    "1.985e-01",
                    "2.551e-01",
                    "1.209e-01"
                ],
                [
                    "MAE",
                    "NN-MSE",
                    "8.430e-03",
                    "2.634e-01",
                    "1.078e-01",
                    "2.866e-01"
                ],
                [
                    "ISE",
                    "GP",
                    "5.247e-03",
                    "1.422e-01",
                    "3.491e-02",
                    "2.059e-02"
                ],
                [
                    "ISE",
                    "AdOSE",
                    "5.109e-03",
                    "6.633e-02",
                    "8.289e-02",
                    "1.554e-02"
                ],
                [
                    "ISE",
                    "AdOR",
                    "[BOLD] 1.734e-03",
                    "[BOLD] 4.974e-02",
                    "[BOLD] 1.908e-02",
                    "[BOLD] 2.232e-03"
                ]
            ],
            "title": "Table 1: Comparison of Different methods."
        },
        "insight": "In this part, AdOR and AdOSE are compared with four regression methods: Support Vector Regression (Smola and Sch\u00a8olkopf 2004), neural network with same structure as AdOR with MSE loss minimization, HSIC regression proposed by Mooij et al.(2009), and Gaussian Process regression (Williams and Rasmussen 1996) with RBF kernel. [CONTINUE] \u03b5. In each test, 300 samples are drawn from uniform distribution X \u223c [CONTINUE] (\u22121, 1). The function f (.) is nonlinear and \u03b5 is generated from different non-Gaussian distributions. Note that for AdOSE, the averaged E\u03b5 [Y |X = x] is plotted by feeding 5000 samples of nG at each X = x. [CONTINUE] Comparison between methods is shown in Table 1 for different performance measures of Mean Squared Error (MSE), Mean Absolute Error (MAE) between predictions and responses, and Integral Squared Error (ISE) between estimated function and f (x). [CONTINUE] As can be seen, in each case, AdOR has the worst MSE and MAE among the others; in contrast, its performance is much better in terms of ISE measure."
    },
    {
        "id": "769",
        "table": {
            "header": [
                "[EMPTY]",
                "FALL",
                "Network Lasso",
                "KNN",
                "KRR",
                "Ridge",
                "Lasso"
            ],
            "rows": [
                [
                    "[BOLD] Concrete data",
                    "42.16 (\u00b1 19.38)",
                    "49.85 (\u00b1 22.12)",
                    "58.14 (\u00b1 26.85)",
                    "110.80 (\u00b1 33.57)",
                    "111.05 (\u00b1 33.92)",
                    "111.10 (\u00b1 33.88)"
                ],
                [
                    "[BOLD] Superconduct data",
                    "129.66 (\u00b1 28.33)",
                    "[EMPTY]",
                    "117.48 (\u00b1 18.77)",
                    "312.11 (\u00b1 25.82)",
                    "308.52 (\u00b1 22.56)",
                    "345.25 (\u00b1 99.80)"
                ],
                [
                    "[BOLD] Aquatic toxicity data",
                    "1.46 (\u00b1 0.74)",
                    "1.72 (\u00b1 1.01)",
                    "1.81 (\u00b1 0.74)",
                    "1.92 (\u00b1 0.75)",
                    "1.58 (\u00b1 0.72)",
                    "1.58 (\u00b1 0.71)"
                ],
                [
                    "[BOLD] Fish toxicity data",
                    "0.92 (\u00b1 0.48)",
                    "0.98 (\u00b1 0.53)",
                    "0.91 (\u00b1 0.48)",
                    "1.14 (\u00b1 0.57)",
                    "0.99 (\u00b1 0.44)",
                    "0.99 (\u00b1 0.44)"
                ],
                [
                    "[BOLD] P.T. motor UPDRS data",
                    "20.76 (\u00b1 4.51)",
                    "24.63 (\u00b1 6.89)*",
                    "20.88 (\u00b1 4.29)",
                    "58.30 (\u00b1 5.50)",
                    "56.69 (\u00b1 5.26)",
                    "56.81 (\u00b1 5.20)"
                ]
            ],
            "title": "Table 1: Averaged mean squared error on the real-world datasets over 50 runs. For each method, we select the best parameters by using 3-fold cross-validation. N/A means that a memory problem occurred when computing the model on the dataset. The asterisk * means that the results were computed without cross-validation but with fixed parameters (because of a too long runtime)."
        },
        "insight": "Averaged mean squared error on the real-world datasets over 50 runs. For each method, we select the best parameters by using 3-fold cross-validation. N/A means that a memory problem occurred when computing the model on the dataset. The asterisk * means that the results were computed without cross-validation but with fixed parameters (because of a too long runtime). [CONTINUE] We compared our method on several datasets with other methods. We computed the average mean squared error with 50 runs and the training and test time. We compared the proposed FALL model with KNN, kernel ridge regression (KRR), ridge and Lasso , and network Lasso . It should be noted that network Lasso is a state-of-the-art locally linear model. To compare the methods fairly, we selected the best hyperparameters by cross-validation with optuna  with a timeout of 60 s. [CONTINUE] Through experiments, the proposed FALL method was found to compare favorably with network Lasso and other existing baselines. In particular, the concrete data, the FALL method significantly outperforms baseline methods. Overall, because the FALL method is a locally linear model, the performance of the FALL method is similar to the one with KNN."
    },
    {
        "id": "770",
        "table": {
            "header": [
                "[BOLD] Concrete data",
                "Train",
                "FALL 0.06 (\u00b1 0.05)",
                "Network Lasso 16.58 (\u00b1 3.41)",
                "KNN 0.00 (\u00b1 0.00)",
                "KRR 0.09 (\u00b1 0.08)",
                "Ridge 0.00 (\u00b1 0.00)",
                "Lasso 0.01 (\u00b1 0.00)"
            ],
            "rows": [
                [
                    "[BOLD] Concrete data",
                    "Test",
                    "0.01 (\u00b1 0.00)",
                    "0.19 (\u00b1 0.03)",
                    "0.00 (\u00b1 0.00)",
                    "0.01 (\u00b1 0.02)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.00)"
                ],
                [
                    "[BOLD] Superconduct data",
                    "Train",
                    "1.24 (\u00b1 1.33)",
                    "[EMPTY]",
                    "0.11 (\u00b1 0.02)",
                    "124.61 (\u00b1 109.20)",
                    "0.02 (\u00b1 0.01)",
                    "121.04 (\u00b1 125.75)"
                ],
                [
                    "[BOLD] Superconduct data",
                    "Test",
                    "5.90 (\u00b1 0.71)",
                    "[EMPTY]",
                    "0.22 (\u00b1 0.08)",
                    "0.23 (\u00b1 0.13)",
                    "0.00 (\u00b1 0.01)",
                    "0.01 (\u00b1 0.01)"
                ],
                [
                    "[BOLD] Aquatic toxicity data",
                    "Train",
                    "0.02 (\u00b1 0.05)",
                    "5.31 (\u00b1 1.00)",
                    "0.00 (\u00b1 0.00)",
                    "0.01 (\u00b1 0.01)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.00)"
                ],
                [
                    "[BOLD] Aquatic toxicity data",
                    "Test",
                    "0.00 (\u00b1 0.00)",
                    "0.07 (\u00b1 0.01)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.01)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.00)"
                ],
                [
                    "[BOLD] Fish toxicity data",
                    "Train",
                    "0.05 (\u00b1 0.05)",
                    "13.23 (\u00b1 1.78)",
                    "0.00 (\u00b1 0.00)",
                    "0.03 (\u00b1 0.04)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.00)"
                ],
                [
                    "[BOLD] Fish toxicity data",
                    "Test",
                    "0.01 (\u00b1 0.00)",
                    "0.14 (\u00b1 0.02)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.01)",
                    "0.00 (\u00b1 0.00)",
                    "0.00 (\u00b1 0.00)"
                ],
                [
                    "[BOLD] P.T. motor UPDRS data",
                    "Train",
                    "0.25 (\u00b1 0.99)",
                    "\u223c 30 min*",
                    "0.00 (\u00b1 0.00)",
                    "1.39 (\u00b1 0.51)",
                    "0.00 (\u00b1 0.00)",
                    "0.01 (\u00b1 0.01)"
                ],
                [
                    "[BOLD] P.T. motor UPDRS data",
                    "Test",
                    "0.27 (\u00b1 0.02)",
                    "\u223c 4 seconds*",
                    "0.01 (\u00b1 0.00)",
                    "0.02 (\u00b1 0.01)",
                    "0.00 (\u00b1 0.01)",
                    "0.01 (\u00b1 0.01)"
                ]
            ],
            "title": "Table 2: Training time and testing time (in seconds) on real-world datasets over 50 runs. For FALL, the training time is the sum of the creation of the anchor models and the final model. N/A means that a memory problem occurred when computing the model on the dataset. The asterisk * means that the training time and testing time were averaged with few samples (because of a too long runtime)."
        },
        "insight": "Training time and testing time (in seconds) on real-world datasets over 50 runs. For FALL, the training time is the sum of the creation of the anchor models and the final model. N/A means that a memory problem occurred when computing the model on the dataset. The asterisk * means that the training time and testing time were averaged with few samples (because of a too long runtime). [CONTINUE] The important point here is that the computation time of the FALL method is two orders of magnitude smaller than that of network Lasso."
    },
    {
        "id": "771",
        "table": {
            "header": [
                "Target",
                "(LOO)",
                "TrgOnly",
                "[BOLD] Prop",
                "SrcOnly",
                "S&TV",
                "TrAda",
                "GDM",
                "Copula",
                "IW(.0)",
                "IW(.5)",
                "IW(.95)"
            ],
            "rows": [
                [
                    "AUT",
                    "1",
                    "5.88 (1.60)",
                    "[BOLD] 5.39 (1.86)",
                    "9.67 (0.57)",
                    "9.84 (0.62)",
                    "5.78 (2.15)",
                    "31.56 (1.39)",
                    "27.33 (0.77)",
                    "39.72 (0.74)",
                    "39.45 (0.72)",
                    "39.18 (0.76)"
                ],
                [
                    "BEL",
                    "1",
                    "10.70 (7.50)",
                    "[BOLD] 7.94 (2.19)",
                    "8.19 (0.68)",
                    "9.48 (0.91)",
                    "8.10 (1.88)",
                    "89.10 (4.12)",
                    "119.86 (2.64)",
                    "105.15 (2.96)",
                    "105.28 (2.95)",
                    "104.30 (2.95)"
                ],
                [
                    "CAN",
                    "1",
                    "5.16 (1.36)",
                    "[BOLD] 3.84 (0.98)",
                    "157.74 (8.83)",
                    "156.65 (10.69)",
                    "51.94 (30.06)",
                    "516.90 (4.45)",
                    "406.91 (1.59)",
                    "592.21 (1.87)",
                    "591.21 (1.84)",
                    "589.87 (1.91)"
                ],
                [
                    "DNK",
                    "1",
                    "3.26 (0.61)",
                    "[BOLD] 3.23 (0.63)",
                    "30.79 (0.93)",
                    "28.12 (1.67)",
                    "25.60 (13.11)",
                    "16.84 (0.85)",
                    "14.46 (0.79)",
                    "22.15 (1.10)",
                    "22.11 (1.10)",
                    "21.72 (1.07)"
                ],
                [
                    "FRA",
                    "1",
                    "2.79 (1.10)",
                    "[BOLD] 1.92 (0.66)",
                    "4.67 (0.41)",
                    "3.05 (0.11)",
                    "52.65 (25.83)",
                    "91.69 (1.34)",
                    "156.29 (1.96)",
                    "116.32 (1.27)",
                    "116.54 (1.25)",
                    "115.29 (1.28)"
                ],
                [
                    "DEU",
                    "1",
                    "16.99 (8.04)",
                    "[BOLD] 6.71 (1.23)",
                    "229.65 (9.13)",
                    "210.59 (14.99)",
                    "341.03 (157.80)",
                    "739.29 (11.81)",
                    "929.03 (4.85)",
                    "817.50 (4.60)",
                    "818.13 (4.55)",
                    "812.60 (4.57)"
                ],
                [
                    "GRC",
                    "1",
                    "3.80 (2.21)",
                    "[BOLD] 3.55 (1.79)",
                    "5.30 (0.90)",
                    "5.75 (0.68)",
                    "11.78 (2.36)",
                    "26.90 (1.89)",
                    "23.05 (0.53)",
                    "47.07 (1.92)",
                    "45.50 (1.82)",
                    "45.72 (2.00)"
                ],
                [
                    "IRL",
                    "1",
                    "[BOLD] 3.05 (0.34)",
                    "4.35 (1.25)",
                    "135.57 (5.64)",
                    "12.34 (0.58)",
                    "23.40 (17.50)",
                    "3.84 (0.22)",
                    "26.60 (0.59)",
                    "6.38 (0.13)",
                    "6.31 (0.14)",
                    "6.16 (0.13)"
                ],
                [
                    "ITA",
                    "1",
                    "[BOLD] 13.00 (4.15)",
                    "14.05 (4.81)",
                    "35.29 (1.83)",
                    "39.27 (2.52)",
                    "87.34 (24.05)",
                    "226.95 (11.14)",
                    "343.10 (10.04)",
                    "244.25 (8.50)",
                    "244.84 (8.58)",
                    "242.60 (8.46)"
                ],
                [
                    "JPN",
                    "1",
                    "10.55 (4.67)",
                    "12.32 (4.95)",
                    "[BOLD] 8.10 (1.05)",
                    "8.38 (1.07)",
                    "18.81 (4.59)",
                    "95.58 (7.89)",
                    "71.02 (5.08)",
                    "135.24 (13.57)",
                    "134.89 (13.50)",
                    "134.16 (13.43)"
                ],
                [
                    "NLD",
                    "1",
                    "3.75 (0.80)",
                    "3.87 (0.79)",
                    "[BOLD] 0.99 (0.06)",
                    "0.99 (0.05)",
                    "9.45 (1.43)",
                    "28.35 (1.62)",
                    "29.53 (1.58)",
                    "33.28 (1.78)",
                    "33.23 (1.77)",
                    "33.14 (1.77)"
                ],
                [
                    "NOR",
                    "1",
                    "2.70 (0.51)",
                    "2.82 (0.73)",
                    "1.86 (0.29)",
                    "[BOLD] 1.63 (0.11)",
                    "24.25 (12.50)",
                    "23.36 (0.88)",
                    "31.37 (1.17)",
                    "27.86 (0.94)",
                    "27.86 (0.93)",
                    "27.52 (0.91)"
                ],
                [
                    "ESP",
                    "1",
                    "5.18 (1.05)",
                    "6.09 (1.53)",
                    "5.17 (1.14)",
                    "[BOLD] 4.29 (0.72)",
                    "14.85 (4.20)",
                    "33.16 (6.99)",
                    "152.59 (6.19)",
                    "53.53 (2.47)",
                    "52.56 (2.42)",
                    "52.06 (2.40)"
                ],
                [
                    "SWE",
                    "1",
                    "6.44 (2.66)",
                    "5.47 (2.63)",
                    "2.48 (0.23)",
                    "[BOLD] 2.02 (0.21)",
                    "2.18 (0.25)",
                    "15.53 (2.59)",
                    "2706.85 (17.91)",
                    "118.46 (1.64)",
                    "118.23 (1.64)",
                    "118.27 (1.64)"
                ],
                [
                    "CHE",
                    "1",
                    "3.51 (0.46)",
                    "[BOLD] 2.90 (0.37)",
                    "43.59 (1.77)",
                    "7.48 (0.49)",
                    "38.32 (9.03)",
                    "8.43 (0.24)",
                    "29.71 (0.53)",
                    "9.72 (0.29)",
                    "9.71 (0.29)",
                    "9.79 (0.28)"
                ],
                [
                    "TUR",
                    "1",
                    "1.65 (0.47)",
                    "1.06 (0.15)",
                    "1.22 (0.18)",
                    "[BOLD] 0.91 (0.09)",
                    "2.19 (0.34)",
                    "64.26 (5.71)",
                    "142.84 (2.04)",
                    "159.79 (2.63)",
                    "157.89 (2.63)",
                    "157.13 (2.69)"
                ],
                [
                    "GBR",
                    "1",
                    "5.95 (1.86)",
                    "[BOLD] 2.66 (0.57)",
                    "15.92 (1.02)",
                    "10.05 (1.47)",
                    "7.57 (5.10)",
                    "50.04 (1.75)",
                    "68.70 (1.25)",
                    "70.98 (1.01)",
                    "70.87 (0.99)",
                    "69.72 (1.01)"
                ],
                [
                    "USA",
                    "1",
                    "4.98 (1.96)",
                    "[BOLD] 1.60 (0.42)",
                    "21.53 (3.30)",
                    "12.28 (2.52)",
                    "2.06 (0.47)",
                    "308.69 (5.20)",
                    "244.90 (1.82)",
                    "462.51 (2.14)",
                    "464.75 (2.08)",
                    "465.88 (2.16)"
                ],
                [
                    "#Best",
                    "-",
                    "2",
                    "10",
                    "2",
                    "4",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ]
            ],
            "title": "Table 2: Results of the real-world data experiments for different choices of the target domain. The evaluation score is MSE normalized by that of LOO (the lower the better). All experiments were repeated 10 times with different train-test splits of target domain data, and the average performance is reported with the standard errors in the brackets. The target column indicates abbreviated country names. Bold-face indicates the best score (Prop: proposed method, TrAda: TrAdaBoost, the numbers in the brackets of IW indicate the value of \u03b1). The proposed method often improves upon the baseline TrgOnly or is relatively more resistant to negative transfer, with notable improvements in DEU, GBR, and USA."
        },
        "insight": "The results are reported in Table 2. We report the MSE scores normalized by that of LOO [CONTINUE] In many of the target domain choices, the naive baselines (SrcOnly and S&TV) suffer from negative transfer, i.e., higher average MSE than TarOnly (in 12 out of 18 domains). On the other hand, the proposed method successfully performs better than TarOnly or is more resistant to negative transfer than the other compared methods. [CONTINUE] The performances of GDM, Copula, and IW are often inferior even compared to the baseline performance of SrcAndTarValid. [CONTINUE] TrAdaBoost works reasonably well for many but not all domains. For some domains, it suffered from negative transfer similarly to others,"
    },
    {
        "id": "772",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Embedding",
                "[BOLD] 5-Way Accuracy(%) 1-shot",
                "[BOLD] 5-Way Accuracy(%) 5-shot"
            ],
            "rows": [
                [
                    "[BOLD] Proto Net",
                    "[ITALIC] Resnet",
                    "51.15\u00b10.85",
                    "69.02\u00b10.75"
                ],
                [
                    "[BOLD] Proto Net",
                    "[ITALIC] Conv",
                    "49.42\u00b10.78",
                    "68.20\u00b10.66"
                ],
                [
                    "[BOLD] Relation Net",
                    "[ITALIC] Resnet",
                    "52.13\u00b10.82",
                    "64.72\u00b10.72"
                ],
                [
                    "[BOLD] Relation Net",
                    "[ITALIC] Conv",
                    "50.44\u00b10.82",
                    "65.32\u00b10.70"
                ],
                [
                    "[BOLD] R2D2",
                    "[ITALIC] Resnet",
                    "51.80\u00b10.20",
                    "68.70\u00b10.20"
                ],
                [
                    "[BOLD] R2D2",
                    "[ITALIC] Conv",
                    "49.50\u00b10.20",
                    "65.40\u00b10.20"
                ],
                [
                    "[BOLD] DN4",
                    "[ITALIC] Resnet",
                    "54.37\u00b10.36",
                    "74.44\u00b10.29"
                ],
                [
                    "[BOLD] DN4",
                    "[ITALIC] Conv",
                    "51.24\u00b10.74",
                    "71.02\u00b10.64"
                ],
                [
                    "[BOLD] Dynamic-Net",
                    "[ITALIC] Resnet",
                    "55.45\u00b10.89",
                    "70.13\u00b10.68"
                ],
                [
                    "[BOLD] Dynamic-Net",
                    "[ITALIC] Conv",
                    "[BOLD] 56.20\u00b10.86",
                    "72.81\u00b10.62"
                ],
                [
                    "[BOLD] Ours",
                    "[ITALIC] Resnet",
                    "[BOLD] 58.12\u00b10.92",
                    "[BOLD] 78.39\u00b10.58"
                ],
                [
                    "[BOLD] Ours",
                    "[ITALIC] Conv",
                    "[BOLD] 53.30\u00b10.32",
                    "[BOLD] 73.22\u00b10.45"
                ],
                [
                    "[BOLD] Methods with Conv Embedding",
                    "[BOLD] Methods with Conv Embedding",
                    "[BOLD] Methods with Conv Embedding",
                    "[BOLD] Methods with Conv Embedding"
                ],
                [
                    "[BOLD] Matching Nets",
                    "[ITALIC] Conv",
                    "43.56\u00b10.84",
                    "55.31\u00b10.73"
                ],
                [
                    "[BOLD] Meta-Learn LSTM",
                    "[ITALIC] Conv",
                    "43.44\u00b10.77",
                    "60.60\u00b10.71"
                ],
                [
                    "[BOLD] MAML",
                    "[ITALIC] Conv",
                    "48.70\u00b11.84",
                    "63.11\u00b10.92"
                ],
                [
                    "[BOLD] CovaMNet",
                    "[ITALIC] Conv",
                    "51.19\u00b10.76",
                    "67.65\u00b10.63"
                ],
                [
                    "[BOLD] SalNet",
                    "[ITALIC] Conv",
                    "[BOLD] 57.45\u00b10.88",
                    "72.01\u00b10.67"
                ]
            ],
            "title": "Table 1: The mean accuracies of the 5-way 1-shot and 5-shot tasks on the miniImageNet dataset, with 95% confidence intervals."
        },
        "insight": "We report the experiment results in table 1. [CONTINUE] When adopting the Resnet as the embedding module, our model can achieve state-ofthe-art results both in the 5-way 1-shot and 5-shot task, especially in the 5-shot task (3.95 % higher than the 74.44% reported by DN4 [Li et al., 2019a]). Besides, when using the Conv as embedding module, our model also achieves the highest accuracy on the 5-way 5-shot task, gaining the 5.57%, 2.2%, and 0.41% over CovaMNet [Li et al., 2019b], DN4, and Dynamic-Net [Gidaris and Komodakis, 2018]. We also obtain very competitive accuracy on 5-way 1-shot task with Conv embedding module, gaining 3.8%, 2.11%, 1.76% improvement over R2D2 [Bertinetto et al., 2019], CovaMNet, and DN4. As for Dynamic-Net and SalNet on the 5-way 1shot task, they perform very complicated training steps to obtain state-of-the-art results."
    },
    {
        "id": "773",
        "table": {
            "header": [
                "Dataset Stanford Dog",
                "Dataset 1shot",
                "Proto Net 31.54\u00b10.41",
                "Relation Net 31.24\u00b10.61",
                "K-tuplet loss 37.33\u00b10.65",
                "[BOLD] ours  [BOLD] 43.69\u00b10.72"
            ],
            "rows": [
                [
                    "Stanford Dog",
                    "5shot",
                    "47.84\u00b10.48",
                    "42.47\u00b10.68",
                    "49.97\u00b10.66",
                    "[BOLD] 61.50\u00b10.73"
                ],
                [
                    "Stanford Car",
                    "1shot",
                    "29.19\u00b10.40",
                    "28.83\u00b10.55",
                    "31.20\u00b10.58",
                    "[BOLD] 32.87\u00b10.57"
                ],
                [
                    "Stanford Car",
                    "5shot",
                    "38.00\u00b10.42",
                    "35.43\u00b10.58",
                    "47.10\u00b10.62",
                    "[BOLD] 50.10\u00b10.69"
                ],
                [
                    "CUB200",
                    "1shot",
                    "37.55\u00b10.51",
                    "38.30\u00b10.71",
                    "40.16\u00b10.68",
                    "[BOLD] 43.30\u00b10.75"
                ],
                [
                    "CUB200",
                    "5shot",
                    "55.03\u00b10.49",
                    "50.89\u00b10.69",
                    "56.96\u00b10.65",
                    "[BOLD] 62.21\u00b10.73"
                ]
            ],
            "title": "Table 2: The mean accuracies of the 5-way 1-shot and 5-shot accuracies (%) on three fined-grained datasets using the model trained on miniImageNet, with 95% confidence intervals. All the experiments are conducted with the same network for fair comparison."
        },
        "insight": "As table 2 shown, compared with those methods, our method achieves the best performance on the Stanford Dogs and CUB-200 under both the 5-way 1-shot and 5-shot task. Besides, when evaluating on Stanford Cars, the Resnet embedding module gains 26.25% and 11.66% improvement over Conv embedding module, which is higher than the other two datasets (gaining 1.95% and 6.26% improvement on Stanford Dogs, 4.90% and 3.67% improvement on CUB-200). Such uncompetitive performance of Conv embedding is caused by two reasons: (i) the Resnet module can obtain more detail and more discriminate feature with the very limited samples than the Conv module which is a shallow network. (ii) the GradCAM can not generate very well initial object seeds over Stanford Cars."
    },
    {
        "id": "774",
        "table": {
            "header": [
                "[BOLD] Algorithms",
                "[BOLD] False Alarm Rate (Normal Videos)"
            ],
            "rows": [
                [
                    "M. Hasan  [ITALIC] et al. ",
                    "27.2"
                ],
                [
                    "C. Lu  [ITALIC] et al. ",
                    "3.1"
                ],
                [
                    "W. Sultani  [ITALIC] et al. ",
                    "1.9"
                ],
                [
                    "Proposed Method(3D ResNet+constr.+loss)",
                    "0.83"
                ],
                [
                    "Proposed Method(3D ResNet+constr.+new rank.loss)",
                    "[BOLD] 0.80"
                ]
            ],
            "title": "TABLE II: Comparison of false alarm rate on normal videos of UCF-Crime testing dataset."
        },
        "insight": "Table II shows the comparison of false alarm rate at the threshold of 50% of different algorithms. These false alarm rates have been calculated on the normal videos of the UCF-Crime  testing dataset. [CONTINUE] As shown in Table II, our proposed method gives the lowest false alarm rate on normal videos as compare to other algorithms. Additionally, the effectiveness of using new proposed ranking loss function is also shown in the Table II. [CONTINUE] This comparison shows that using both abnormal and normal videos in training the network improves the abnormal activity detection task and we can generalize our method for other real world related abnormal activity detection. [CONTINUE] Therefore, both the tables II and III, indicate that we have reduced both cases of false alarm with the help of our proposed method."
    },
    {
        "id": "775",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Embedding",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] Stanford Dogs",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] Stanford Dogs",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] Stanford Cars",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] Stanford Cars",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] CUB-200",
                "[BOLD] 5-Way accuracy(%)  [ITALIC] CUB-200"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "1-shot",
                    "5-shot",
                    "1-shot",
                    "5-shot",
                    "1-shot",
                    "5-shot"
                ],
                [
                    "[BOLD] Baseline K-NN",
                    "[ITALIC] Conv",
                    "26.14\u00b10.91",
                    "43.14\u00b11.02",
                    "23.50\u00b10.88",
                    "34.45\u00b10.98",
                    "25.81\u00b10.90",
                    "45.34\u00b11.03"
                ],
                [
                    "[BOLD] Proto Net",
                    "[ITALIC] Conv",
                    "37.59\u00b11.00",
                    "48.19\u00b11.03",
                    "40.90\u00b11.01",
                    "52.93\u00b11.03",
                    "37.36\u00b11.00",
                    "45.28\u00b11.03"
                ],
                [
                    "[BOLD] GNN",
                    "[ITALIC] Conv",
                    "46.98\u00b10.98",
                    "62.27\u00b10.95",
                    "55.85\u00b10.97",
                    "71.25\u00b10.89",
                    "51.83\u00b10.98",
                    "63.69\u00b10.94"
                ],
                [
                    "[BOLD] CovaMNet",
                    "[ITALIC] Conv",
                    "49.10\u00b10.76",
                    "63.04\u00b10.65",
                    "56.65\u00b10.86",
                    "71.33\u00b10.62",
                    "52.42\u00b10.76",
                    "63.76\u00b10.64"
                ],
                [
                    "[BOLD] DN4",
                    "[ITALIC] Conv",
                    "45.41\u00b10.76",
                    "63.51\u00b10.62",
                    "[BOLD] 59.84\u00b10.80",
                    "[BOLD] 88.65\u00b10.44",
                    "46.84\u00b10.81",
                    "74.92\u00b10.64"
                ],
                [
                    "[BOLD] Ours",
                    "[ITALIC] Conv",
                    "[BOLD] 52.50\u00b10.90",
                    "[BOLD] 67.37\u00b10.76",
                    "[BOLD] 46.46\u00b10.89",
                    "[BOLD] 82.28\u00b11.02",
                    "[BOLD] 55.31\u00b10.78",
                    "[BOLD] 75.03\u00b10.64"
                ],
                [
                    "[BOLD] Ours",
                    "[ITALIC] Resnet",
                    "[BOLD] 54.45\u00b10.95",
                    "[BOLD] 73.63\u00b10.92",
                    "[BOLD] 72.71\u00b10.43",
                    "[BOLD] 93.94\u00b10.33",
                    "[BOLD] 60.21\u00b10.87",
                    "[BOLD] 78.70\u00b10.62"
                ]
            ],
            "title": "Table 3: The mean accuracies of the 5-way 1-shot and 5-shot tasks on three fine-grained datasets, with 95% confidence intervals."
        },
        "insight": "The experiment results show that our model outperforms previous work(Proto Net, Relation Net, and K-tuplet loss [Li et al., 2019c]) on the three novel datasets, which demonstrates the superior generalization capacity of our approach."
    },
    {
        "id": "776",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Without training  [BOLD] RBM-NQS-CS",
                "[BOLD] Without training  [BOLD] RBM-NQS-I",
                "[BOLD] With training  [BOLD] RBM-NQS-CS",
                "[BOLD] With training  [BOLD] RBM-NQS-I",
                "[BOLD] Tensor network"
            ],
            "rows": [
                [
                    "[BOLD] Energy",
                    "-127.9799 (0.0029)",
                    "-127.9799 (0.0029)",
                    "-127.9799 (0.0029)",
                    "-127.9799 (0.0029)",
                    "-128.00000"
                ],
                [
                    "[ITALIC] M2 [ITALIC] F",
                    "0.0079 (0.0001)",
                    "0.0079 (0.0001)",
                    "0.0079 (0.0001)",
                    "0.0079 (0.0001)",
                    "0.00781"
                ],
                [
                    "[ITALIC] M2 [ITALIC] A",
                    "0.0078 (0.0001)",
                    "0.0078 (0.0001)",
                    "0.0078 (0.0001)",
                    "0.0078 (0.0001)",
                    "0.00781"
                ],
                [
                    "[ITALIC] CF, [ITALIC] i, [ITALIC] d",
                    "0.0003 (0.0007)",
                    "0.0003 (0.0007)",
                    "0.0003 (0.0007)",
                    "0.0003 (0.0007)",
                    "0.00000"
                ],
                [
                    "[ITALIC] CA, [ITALIC] i, [ITALIC] d",
                    "-0.0003 (0.0007)",
                    "-0.0003 (0.0007)",
                    "-0.0003 (0.0007)",
                    "-0.0003 (0.0007)",
                    "0.00000"
                ],
                [
                    "[BOLD] Iterations",
                    "-",
                    "-",
                    "0",
                    "0",
                    "-"
                ]
            ],
            "title": "Table 1: The performance evaluation of the RBM-NQS-CS and RBM-NQS-I for one-dimensional system in Ising model where the system size is 128 and parameter of the system J/|h|=0.0. The reported value is average value over 20 realisations. The value inside the parentheses is the standard deviation."
        },
        "insight": "Table 1, Table 2 and Table 3 show the evaluation of the RBM-NQS-CS and RBM-NQS-I where the size of the system is 128 and parameter of the system J/|h| = 0.0, J/|h| = 3.0 and J/|h| = \u22123.0, respectively. [CONTINUE] In the case of a deep paramagnetic phase (J/|h| = 0.0 in Table 1), we observe that both the energy and the order parameter for both the RBM-NQS-CS and RBM-NQS-I without training are very close to the result of the tensor network method."
    },
    {
        "id": "777",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Without training  [BOLD] RBM-NQS-CS",
                "[BOLD] Without training  [BOLD] RBM-NQS-I",
                "[BOLD] With training  [BOLD] RBM-NQS-CS",
                "[BOLD] With training  [BOLD] RBM-NQS-I",
                "[BOLD] Tensor network"
            ],
            "rows": [
                [
                    "[BOLD] Energy",
                    "-127.9061 (0.2577)",
                    "-217.5726 (0.3152)",
                    "-372.2911 (4.7748)",
                    "-391.7046 (0.0182)",
                    "-391.91198"
                ],
                [
                    "[ITALIC] M2 [ITALIC] F",
                    "0.0078 (0.0001)",
                    "0.2934 (0.0009)",
                    "0.0981 (0.1041)",
                    "0.9658 (0.0005)",
                    "0.96980"
                ],
                [
                    "[ITALIC] M2 [ITALIC] A",
                    "0.0078 (0.0001)",
                    "0.0056 (0.0001)",
                    "0.0006 (0.0001)",
                    "0.0003 (0.0000)",
                    "0.00023"
                ],
                [
                    "[ITALIC] CF, [ITALIC] i, [ITALIC] d",
                    "-0.0002 (0.0011)",
                    "0.2897 (0.0059)",
                    "0.0360 (0.2928)",
                    "0.9362 (0.0051)",
                    "0.92871"
                ],
                [
                    "[ITALIC] CA, [ITALIC] i, [ITALIC] d",
                    "0.0001 (0.0010)",
                    "-0.0020 (0.0009)",
                    "-0.0025 (0.0032)",
                    "-0.0072 (0.0002)",
                    "-0.00704"
                ],
                [
                    "[BOLD] Iterations",
                    "-",
                    "-",
                    "1621.7250 (1271.7007)",
                    "20.8500 (0.4770)",
                    "-"
                ]
            ],
            "title": "Table 2: The performance evaluation of RBM-NQS-CS and RBM-NQS-I for one-dimensional system in Ising model where the system size is 128 and parameter of the system J/|h|=3.0. The reported value is average value over 20 realisations. The value inside the parentheses is the standard deviation."
        },
        "insight": "In the case of a deep ferromagnetic phase (J/|h| = 3.0 in Table 2) and antiferromagnetic phase (J/|h| = \u22123.0 in Table 3), we observe that the results of the RBM-NQS-I are closer to the result of the tensor network method and need less iterations to converge to the stopping criterion than RBM-NQS-CS."
    },
    {
        "id": "778",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Without training  [BOLD] RBM-NQS-CS",
                "[BOLD] Without training  [BOLD] RBM-NQS-I",
                "[BOLD] With training  [BOLD] RBM-NQS-CS",
                "[BOLD] With training  [BOLD] RBM-NQS-I",
                "[BOLD] Exact Diagonalization"
            ],
            "rows": [
                [
                    "[BOLD] Energy",
                    "-16.0000 (0.0001)",
                    "-16.0000 (0.0001)",
                    "-16.0000 (0.0001)",
                    "-16.0000 (0.0001)",
                    "-16.0000"
                ],
                [
                    "[ITALIC] M2 [ITALIC] F",
                    "0.0624 (0.0007)",
                    "0.0624 (0.0007)",
                    "0.0624 (0.0007)",
                    "0.0624 (0.0007)",
                    "0.0625"
                ],
                [
                    "[ITALIC] M2 [ITALIC] A",
                    "0.0624 (0.0009)",
                    "0.0624 (0.0009)",
                    "0.0624 (0.0009)",
                    "0.0624 (0.0009)",
                    "0.0625"
                ],
                [
                    "[ITALIC] CF, [ITALIC] i, [ITALIC] d",
                    "0.2503 (0.0041)",
                    "0.2503 (0.0041)",
                    "0.2503 (0.0041)",
                    "0.2503 (0.0041)",
                    "0.2500"
                ],
                [
                    "[ITALIC] CA, [ITALIC] i, [ITALIC] d",
                    "0.2490 (0.0037)",
                    "0.2490 (0.0037)",
                    "0.2490 (0.0037)",
                    "0.2490 (0.0037)",
                    "0.2500"
                ],
                [
                    "[BOLD] Iterations",
                    "-",
                    "-",
                    "0",
                    "0",
                    "-"
                ]
            ],
            "title": "Table 6: The performance evaluation of RBM-NQS-CS and RBM-NQS-I for two-dimensional system in Ising model where the system size is 4\u00d74 and parameter of the system J/|h|=0.0. The reported value is average value over 20 realisations. The value inside the parentheses is the standard deviation."
        },
        "insight": "Table 6, Table 7 and Table 8 in Appendix A show the evaluation of the RBM-NQS-CS and RBM-NQS-I for two-dimensional system where the size of the system is 4 \u00d7 4 and parameter of the system J/|h| = 0.0, J/|h| = 3.0 and J/|h| = \u22123.0, respectively. [CONTINUE] In the case of a deep paramagnetic phase (J/|h| = 0.0 in Table 6 in Appendix A and Table 9 in Appendix B ), the results of the RBMNQS-CS and RBM-NQS-I are very close to the result of the exact diagonalization method. [CONTINUE] Table 6, Table 7 and Table 8 shows the evaluation of the RBM-NQS-CS and RBM-NQS-I for two-dimensional system where the size of the system is 4 \u00d7 4 and parameter of the system J/|h| = 0.0, J/|h| = 3.0 and J/|h| = \u22123.0, respectively."
    },
    {
        "id": "779",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Without training  [BOLD] RBM-NQS-CS",
                "[BOLD] Without training  [BOLD] RBM-NQS-I",
                "[BOLD] With training  [BOLD] RBM-NQS-CS",
                "[BOLD] With training  [BOLD] RBM-NQS-I",
                "[BOLD] Exact Diagonalization"
            ],
            "rows": [
                [
                    "[BOLD] Energy",
                    "-8.0000 (0.0000)",
                    "-8.0000 (0.0000)",
                    "-8.0000 (0.0000)",
                    "-8.0000 (0.0000)",
                    "-8"
                ],
                [
                    "[ITALIC] M2 [ITALIC] F",
                    "0.1244 (0.0022)",
                    "0.1244 (0.0022)",
                    "0.1244 (0.0022)",
                    "0.1244 (0.0022)",
                    "0.125"
                ],
                [
                    "[ITALIC] M2 [ITALIC] A",
                    "0.1250 (0.0028)",
                    "0.1250 (0.0028)",
                    "0.1250 (0.0028)",
                    "0.1250 (0.0028)",
                    "0.125"
                ],
                [
                    "[ITALIC] CF, [ITALIC] i, [ITALIC] d",
                    "0.4999 (0.0059)",
                    "0.4999 (0.0059)",
                    "0.4999 (0.0059)",
                    "0.4999 (0.0059)",
                    "0.5"
                ],
                [
                    "[ITALIC] CA, [ITALIC] i, [ITALIC] d",
                    "0.5000 (0.0059)",
                    "0.5000 (0.0059)",
                    "0.5000 (0.0059)",
                    "0.5000 (0.0059)",
                    "0.5"
                ],
                [
                    "[BOLD] Iterations",
                    "-",
                    "-",
                    "0.0000 (0.0000)",
                    "0.0000 (0.0000)",
                    "-"
                ]
            ],
            "title": "Table 9: The performance evaluation of RBM-NQS-CS and RBM-NQS-I for three-dimensional system in Ising model where the system size is 2\u00d72\u00d72 and parameter of the system J/|h|=0.0. The reported value is average value over 20 realisations. The value inside the parentheses is the standard deviation."
        },
        "insight": "Ta [CONTINUE] Table 9, Table 10 and Table 11 shows the evaluation of the RBM-NQS-CS and RBM-NQS-I for three-dimensional system where the size of the system is 2 \u00d7 2 \u00d7 2 and parameter of the system J/|h| = 0.0, J/|h| = 3.0 and J/|h| = \u22123.0, respectively."
    },
    {
        "id": "780",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Without training  [BOLD] RBM-NQS-CS",
                "[BOLD] Without training  [BOLD] RBM-NQS-I",
                "[BOLD] With training  [BOLD] RBM-NQS-CS",
                "[BOLD] With training  [BOLD] RBM-NQS-I",
                "[BOLD] Exact Diagonalization"
            ],
            "rows": [
                [
                    "[BOLD] Energy",
                    "-15.9808 (0.1652)",
                    "-52.1501 (0.1441)",
                    "-72.9401 (0.0035)",
                    "-72.9397 (0.0039)",
                    "-72.9455"
                ],
                [
                    "[ITALIC] M2 [ITALIC] F",
                    "0.0625 (0.0008)",
                    "0.6064 (0.0023)",
                    "0.9856 (0.0005)",
                    "0.9861 (0.0005)",
                    "0.9860"
                ],
                [
                    "[ITALIC] M2 [ITALIC] A",
                    "0.0624 (0.0007)",
                    "0.0263 (0.0003)",
                    "0.0010 (0.0000)",
                    "0.0009 (0.0000)",
                    "0.0009"
                ],
                [
                    "[ITALIC] CF, [ITALIC] i, [ITALIC] d",
                    "0.2483 (0.0042)",
                    "0.6863 (0.0056)",
                    "0.9924 (0.0009)",
                    "0.9924 (0.0009)",
                    "0.9934"
                ],
                [
                    "[ITALIC] CA, [ITALIC] i, [ITALIC] d",
                    "0.2520 (0.0045)",
                    "0.1042 (0.0036)",
                    "0.0022 (0.0004)",
                    "0.0024 (0.0006)",
                    "0.0017"
                ],
                [
                    "[BOLD] Iterations",
                    "-",
                    "-",
                    "180.4000 (4.8311)",
                    "126.0500 (1.7741)",
                    "-"
                ]
            ],
            "title": "Table 7: The performance evaluation of RBM-NQS-CS and RBM-NQS-I for two-dimensional system in Ising model where the system size is 4\u00d74 and parameter of the system J/|h|=3.0. The reported value is average value over 20 realisations. The value inside the parentheses is the standard deviation."
        },
        "insight": "In the case of a deep ferromagnetic phase (J/|h| = 3.0 in Table 7 in Appendix A and Table 10 in Appendix B) and antiferromagnetic phase (J/|h| = \u22123.0 in Table 8 in Appendix A and Table 11 in Appendix B), we see that the results of RBM-NQS-I are closer to those of the exact diagonalization calculations than RBM-NQS-CS before training."
    },
    {
        "id": "781",
        "table": {
            "header": [
                "[BOLD] System size",
                "[BOLD] RBM-NQS-CS",
                "[BOLD] RBM-NQS-IT",
                "[BOLD] Tensor network",
                "[BOLD] Exact diag."
            ],
            "rows": [
                [
                    "8",
                    "1.114 (0.009)",
                    "1.105 (0.006)",
                    "1.11",
                    "1.109"
                ],
                [
                    "16",
                    "1.007 (0.008)",
                    "1.040 (0.005)",
                    "1.08",
                    "1.090"
                ],
                [
                    "32",
                    "1.011 (0.009)",
                    "1.013 (0.001)",
                    "1.05",
                    "-"
                ],
                [
                    "64",
                    "1.004 (0.009)",
                    "1 (0.001)",
                    "1.02",
                    "-"
                ],
                [
                    "128",
                    "0.646 (0.38)",
                    "1 (0.001)",
                    "1.01",
                    "-"
                ],
                [
                    "2\u00d72",
                    "0.662 (0.04)",
                    "0.673 (0.05)",
                    "-",
                    "0.69"
                ],
                [
                    "4\u00d74",
                    "0.5 (0.0)",
                    "0.501 (0.003)",
                    "-",
                    "0.51"
                ],
                [
                    "2\u00d72\u00d72",
                    "0.505 (0.012)",
                    "0.502 (0.004)",
                    "-",
                    "0.527"
                ]
            ],
            "title": "Table 4: The value of the inflection point for one-, two- and three-dimensional systems of given sizes with RBM-NQS-CS, RBM-NQS-IT, tensor network and exact diagonalization method with ferromagnetic magnetisation M2F order parameter. The value inside the parentheses is the standard deviation."
        },
        "insight": "Table 4 and Table 5 show the value of the inflection point for different sizes of the system of one-dimensional, two-dimensional and three-dimensional systems with RBM-NQS-CS, RBM-NQS-IT and tensor network method with ferromagnetic magnetisation M 2 F and antiferromagnetic magnetisation M 2 A order parameter, respectively. [CONTINUE] In Table 4 and Table 5, we observe that RBM-NQS-CS performs the worst overall since the value of the inflection point is far from both the tensor network and exact diagonalization methods, especially in system of large size. It is particularly unstable in a onedimensional system with 64 and 128 particles, as shown by a very large standard deviation."
    },
    {
        "id": "782",
        "table": {
            "header": [
                "net",
                "weights",
                "mse",
                "time [min]"
            ],
            "rows": [
                [
                    "gru-64-fft",
                    "46k",
                    "7.4\u22c510\u22124",
                    "172"
                ],
                [
                    "cgRNN-32-fft",
                    "23k",
                    "9.3\u22c510\u22124",
                    "275"
                ],
                [
                    "cgRNN-54-fft",
                    "46k",
                    "5.2\u22c510\u22124",
                    "230"
                ],
                [
                    "cgRNN-64-fft",
                    "58k",
                    "3.4\u22c510\u22124",
                    "272"
                ]
            ],
            "title": "Table 1: Real and complex valued architecture comparison on the mackey-glass data."
        },
        "insight": "We explore the effect of a complex valued cell in table 1. We apply the complex gru proposed in , and compare to a real valued gru. We speculate that complex cells are better suited to process fourier representations due to their complex nature. Our observations show that both cells solve the task, while the complex cell does so with fewer parameters at a higher cost."
    },
    {
        "id": "783",
        "table": {
            "header": [
                "Network",
                "mse [MW]",
                "weights",
                "run [min]"
            ],
            "rows": [
                [
                    "time-RNN",
                    "1.3\u22c5107",
                    "13k",
                    "772"
                ],
                [
                    "time-RNN-windowed",
                    "8.8\u22c5105",
                    "28k",
                    "12"
                ],
                [
                    "fRNN",
                    "8.3\u22c5105",
                    "44k",
                    "13"
                ],
                [
                    "fRNN-lowpass-1/4",
                    "7.6\u22c5105",
                    "20k",
                    "13"
                ],
                [
                    "fRNN-lowpass-1/8",
                    "1.3\u22c5106",
                    "16k",
                    "13"
                ]
            ],
            "title": "Table 2: 60 day ahead power load prediction and ground truth. Power prediction results using various different architectures"
        },
        "insight": "five days; all other parameters are left the same as the day-ahead prediction setting. Table 2 shows that the windowed methods are able to extract patterns, but the scalar-time domain approach failed. Additionally we observe that we do not require the full set of Fourier coefficients to construct useful predictions on the power-data set. The results are tabulated in table 2. It turns out that the lower quarter of Fourier coefficients is enough, which allowed us to reduce the number of parameters considerably."
    },
    {
        "id": "784",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "Cora",
                "Cite.",
                "Pubm.",
                "Cham.",
                "Squi.",
                "Actor",
                "Corn.",
                "Texa.",
                "Wisc."
            ],
            "rows": [
                [
                    "GCN",
                    "85.77",
                    "73.68",
                    "88.13",
                    "28.18",
                    "23.96",
                    "26.86",
                    "52.70",
                    "52.16",
                    "45.88"
                ],
                [
                    "GAT",
                    "[BOLD] 86.37",
                    "74.32",
                    "87.62",
                    "42.93",
                    "30.03",
                    "28.45",
                    "54.32",
                    "58.38",
                    "49.41"
                ],
                [
                    "Geom-GCN-I",
                    "85.19",
                    "[BOLD] 77.99",
                    "[BOLD] 90.05",
                    "60.31",
                    "33.32",
                    "29.09",
                    "56.76",
                    "57.58",
                    "58.24"
                ],
                [
                    "Geom-GCN-P",
                    "84.93",
                    "75.14",
                    "88.09",
                    "[BOLD] 60.90",
                    "[BOLD] 38.14",
                    "[BOLD] 31.63",
                    "[BOLD] 60.81",
                    "[BOLD] 67.57",
                    "[BOLD] 64.12"
                ],
                [
                    "Geom-GCN-S",
                    "85.27",
                    "74.71",
                    "84.75",
                    "59.96",
                    "36.24",
                    "30.30",
                    "55.68",
                    "59.73",
                    "56.67"
                ]
            ],
            "title": "Table 3: Mean Classification Accuracy (Percent)"
        },
        "insight": "Results are summarized in Table 3. The reported numbers denote the mean classification accuracy in percent. In general, Geom-GCN achieves state-of-the-art performance. The best performing method is highlighted."
    },
    {
        "id": "785",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "Cora",
                "Cite.",
                "Pumb.",
                "Cham.",
                "Squi.",
                "Actor",
                "Corn.",
                "Texa.",
                "Wisc."
            ],
            "rows": [
                [
                    "[ITALIC] \u03b2",
                    "0.83",
                    "0.71",
                    "0.79",
                    "0.25",
                    "0.22",
                    "0.24",
                    "0.11",
                    "0.06",
                    "0.16"
                ],
                [
                    "Geom-GCN-I-g",
                    "86.26",
                    "[BOLD] 80.64",
                    "[BOLD] 90.72",
                    "[BOLD] 68.00",
                    "[BOLD] 46.01",
                    "31.96",
                    "65.40",
                    "72.51",
                    "68.23"
                ],
                [
                    "Geom-GCN-I-g",
                    "\u21910.48",
                    "\u21916.96",
                    "\u21912.59",
                    "\u219139.82",
                    "\u219122.05",
                    "\u21914.04",
                    "\u219112.70",
                    "\u219121.35",
                    "\u219122.35"
                ],
                [
                    "Geom-GCN-I-s",
                    "77.34",
                    "72.22",
                    "85.02",
                    "61.64",
                    "37.98",
                    "30.59",
                    "62.16",
                    "60.54",
                    "64.90"
                ],
                [
                    "Geom-GCN-I-s",
                    "\u21938.34",
                    "\u21931.46",
                    "\u21933.11",
                    "\u219133.46",
                    "\u219114.02",
                    "\u21912.67",
                    "\u21919.46",
                    "\u21918.38",
                    "\u219119.01"
                ],
                [
                    "Geom-GCN-P-g",
                    "86.30",
                    "75.45",
                    "88.40",
                    "63.07",
                    "38.41",
                    "31.55",
                    "64.05",
                    "73.05",
                    "69.41"
                ],
                [
                    "Geom-GCN-P-g",
                    "\u21910.52",
                    "\u21911.76",
                    "\u21910.27",
                    "\u219134.89",
                    "\u219114.45",
                    "\u21913.63",
                    "\u219111.35",
                    "\u219121.89",
                    "\u219123.53"
                ],
                [
                    "Geom-GCN-P-s",
                    "73.14",
                    "71.65",
                    "86.95",
                    "43.20",
                    "30.47",
                    "[BOLD] 34.59",
                    "[BOLD] 75.40",
                    "[BOLD] 73.51",
                    "[BOLD] 80.39"
                ],
                [
                    "Geom-GCN-P-s",
                    "\u219312.63",
                    "\u21932.04",
                    "\u21931.18",
                    "\u219115.02",
                    "\u21916.51",
                    "\u21916.67",
                    "\u219122.70",
                    "\u219121.35",
                    "\u219134.51"
                ],
                [
                    "Geom-GCN-S-g",
                    "[BOLD] 87.00",
                    "75.73",
                    "88.44",
                    "67.04",
                    "44.92",
                    "31.27",
                    "67.02",
                    "71.62",
                    "69.41"
                ],
                [
                    "Geom-GCN-S-g",
                    "\u21911.23",
                    "\u21912.04",
                    "\u21910.31",
                    "\u219138.86",
                    "\u219120.96",
                    "\u21913.35",
                    "\u219114.32",
                    "\u219119.46",
                    "\u219123.52"
                ],
                [
                    "Geom-GCN-S-s",
                    "66.92",
                    "66.03",
                    "79.41",
                    "49.21",
                    "31.27",
                    "30.32",
                    "62.43",
                    "63.24",
                    "64.51"
                ],
                [
                    "Geom-GCN-S-s",
                    "\u219318.85",
                    "\u21937.65",
                    "\u21938.72",
                    "\u219121.03",
                    "\u21917.31",
                    "\u21912.40",
                    "\u21919.73",
                    "\u219111.08",
                    "\u219118.63"
                ]
            ],
            "title": "Table 4: Mean Classification Accuracy (Percent)"
        },
        "insight": "The results are summarized in Table 4, where positive improvement is denoted by an up arrow \u2191 and negative improvement by a down arrow \u2193. The best performing method is highlighted. [CONTINUE] nodes tend to connect together. From Table 4, one can see that assortative graphs (e.g., citation networks) have a much larger \u03b2 than disassortative graphs (e.g., WebKB networks). [CONTINUE] Table 4 exhibits three interesting patterns: i) Neighborhoods in graph and latent space both benefit the aggregation in most cases; ii) Neighborhoods in latent space have larger contributions in disassortative graphs (with a small \u03b2) than assortative ones, which implies relevant information from disconnected nodes is captured effectively by the neighborhoods in latent space; iii) To our surprise, several variants with only one neighborhood (in Table 4) achieve better performances than the variants with two neighborhoods (in Tabel 3)."
    },
    {
        "id": "786",
        "table": {
            "header": [
                "[ITALIC] tvar",
                "[BOLD] Correctly classified  [BOLD] without a8",
                "[BOLD] Correctly classified  [BOLD] with a8"
            ],
            "rows": [
                [
                    "0.05",
                    "72.0%",
                    "63.3%"
                ],
                [
                    "0.10",
                    "77.4%",
                    "63.8%"
                ],
                [
                    "0.15",
                    "74.4%",
                    "67.9%"
                ],
                [
                    "0.20",
                    "76.2%",
                    "66.8%"
                ]
            ],
            "title": "Table 2: Percentage of correctly classified gestures for different variance thresholds tvar"
        },
        "insight": "From the 168 sequences which were classified, 130 were classified correctly. This equates to approximately 77.4%. [CONTINUE] 25 of 196 sequences were [CONTINUE] classified correctly (63.8%). [CONTINUE] Table 2 shows the classification performance for different values of tvar."
    },
    {
        "id": "787",
        "table": {
            "header": [
                "[EMPTY]",
                "Velocity control",
                "Position control"
            ],
            "rows": [
                [
                    "Controllability",
                    "Better",
                    "Worse"
                ],
                [
                    "Guidance reactivity",
                    "Faster",
                    "Slower"
                ],
                [
                    "Complexity",
                    "2",
                    "1"
                ]
            ],
            "title": "TABLE I: Pros and cons of the Offboard velocity and position control"
        },
        "insight": "Hence, the Offboard mode enables to bypass the autopilot for either the position or velocity setpoint, a trade-off (Table I) had to be done considering our application. \u2022 For the bypass of the velocity, the UAV is more controllable and the guidance is faster since the inner part of the controller is modified. However, more parameters need to be selected: the velocity and a shut-off parameter. \u2022 For the bypass of the position, the UAV is less controllable and the guidance is slower (outer part of the controller), but it is less complex since only one parameter needs to be controlled and then the autopilot does the rest. [CONTINUE] A standard velocity is used when performing the Offboard mode position control. Since robust and fast guidance is needed for sense and avoid purposes, the velocity control is chosen in the Offboard mode in this paper."
    },
    {
        "id": "788",
        "table": {
            "header": [
                "[BOLD] PASCAL3D+ Classification under Occlusion Occ. Area",
                "[BOLD] PASCAL3D+ Classification under Occlusion  [BOLD] Level-3: 60-80%",
                "[BOLD] PASCAL3D+ Classification under Occlusion  [BOLD] Level-3: 60-80%",
                "[BOLD] PASCAL3D+ Classification under Occlusion  [BOLD] Level-3: 60-80%",
                "[BOLD] PASCAL3D+ Classification under Occlusion  [BOLD] Level-3: 60-80%",
                "[BOLD] PASCAL3D+ Classification under Occlusion Mean"
            ],
            "rows": [
                [
                    "Occ. Type",
                    "w",
                    "n",
                    "t",
                    "o",
                    "-"
                ],
                [
                    "1 prototype, 1 recurrence",
                    "79.1",
                    "84.8",
                    "77.9",
                    "75.1",
                    "90.9"
                ],
                [
                    "1 prototype, 2 recurrence",
                    "78.9",
                    "85.2",
                    "78.2",
                    "75.5",
                    "91.0"
                ],
                [
                    "1 prototype, 3 recurrence",
                    "79.7",
                    "85.2",
                    "79",
                    "76.9",
                    "91.2"
                ],
                [
                    "4 prototype, 1 recurrence",
                    "81.1",
                    "87.6",
                    "81.2",
                    "76.8",
                    "92.3"
                ],
                [
                    "4 prototype, 2 recurrence",
                    "81.5",
                    "87.7",
                    "81.9",
                    "77.1",
                    "92.4"
                ],
                [
                    "4 prototype, 3 recurrence",
                    "82.1",
                    "88.1",
                    "82.7",
                    "[BOLD] 79.8",
                    "92.8"
                ],
                [
                    "8 prototype, 1 recurrence",
                    "81.1",
                    "87.6",
                    "80.9",
                    "74.7",
                    "92.1"
                ],
                [
                    "8 prototype, 2 recurrence",
                    "[BOLD] 82.5",
                    "[BOLD] 88.7",
                    "82.5",
                    "78.6",
                    "92.8"
                ],
                [
                    "8 prototype, 3 recurrence",
                    "82.4",
                    "88.2",
                    "[BOLD] 83",
                    "78.6",
                    "[BOLD] 92.9"
                ]
            ],
            "title": "Table 2: Comparison for different prototype numbers of TDAPNet on PASCAL3D+. We just list Level-3 and Mean here, the complete results are in the Supplementary Material."
        },
        "insight": "Multiple prototypes improve the performance. As shown in Table 2, 4 prototypes outperform 1 prototype, while 8 prototypes lead to about the same performance as 4 prototypes."
    },
    {
        "id": "789",
        "table": {
            "header": [
                "Method",
                "PSNR",
                "SSIM"
            ],
            "rows": [
                [
                    "LIME",
                    "12.38",
                    "0.611"
                ],
                [
                    "SRIE",
                    "14.09",
                    "0.659"
                ],
                [
                    "DeepFlash",
                    "15.39",
                    "0.671"
                ],
                [
                    "Ours",
                    "[BOLD] 15.67",
                    "[BOLD] 0.684"
                ]
            ],
            "title": "Table 1: Reporting the mean PSNR and the mean SSIM with SRIE\u00a0[7], LIME\u00a0[9], and DeepFlash\u00a0[3]."
        },
        "insight": "We use the PSNR (Peak Signal-to-Noise Ratio) and the SSIM (Structural Similarity) to measure the performance of our quantitative results. Table 1 reports the mean PSNR and the mean SSIM on the test set, for 1000 epochs. All hyperparameters are setting on the same way for (Capece et al., 2019), and the encoderdecoder network was pre-trained on the ImageNet dataset (Deng et al., 2009) instead on a model used for face recognition (Parkhi et al., 2015). Our quantitative results do not significantly outperform the stateof-the-art image enhancement methods, but at least shows improvements on the flash image enhancement task."
    },
    {
        "id": "790",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] diabetes train",
                "[BOLD] diabetes test",
                "[BOLD] boston train",
                "[BOLD] boston test",
                "[BOLD] concrete train",
                "[BOLD] concrete test",
                "[BOLD] airfoil train",
                "[BOLD] airfoil test",
                "[BOLD] wine quality train",
                "[BOLD] wine quality test"
            ],
            "rows": [
                [
                    "[ITALIC] RR",
                    "0.541",
                    "[BOLD] 0.383",
                    "0.736",
                    "0.748",
                    "0.625",
                    "0.564",
                    "0.517",
                    "0.508",
                    "0.293",
                    "0.310"
                ],
                [
                    "[ITALIC] SVR",
                    "0.580",
                    "0.320",
                    "0.959",
                    "[BOLD] 0.882",
                    "0.933",
                    "0.881",
                    "0.884",
                    "0.851",
                    "0.572",
                    "0.411"
                ],
                [
                    "[ITALIC] RF",
                    "[BOLD] 0.598",
                    "0.354",
                    "[BOLD] 0.983",
                    "0.870",
                    "[BOLD] 0.985",
                    "[BOLD] 0.892",
                    "[BOLD] 0.991",
                    "[BOLD] 0.934",
                    "[BOLD] 0.931",
                    "[BOLD] 0.558"
                ],
                [
                    "[ITALIC] AFR1",
                    "0.553",
                    "0.400",
                    "0.825",
                    "0.810",
                    "0.847",
                    "0.818",
                    "0.569",
                    "0.560",
                    "0.320",
                    "0.341"
                ],
                [
                    "[ITALIC] AFR2",
                    "0.591",
                    "0.353",
                    "0.893",
                    "0.791",
                    "0.913",
                    "0.868",
                    "0.863",
                    "0.842",
                    "0.397",
                    "0.384"
                ],
                [
                    "[ITALIC] AFR3",
                    "0.638",
                    "-12.4",
                    "0.932",
                    "0.048",
                    "0.867",
                    "0.824",
                    "0.884",
                    "0.883",
                    "0.350",
                    "0.342"
                ]
            ],
            "title": "Table 2: R2 scores on the training and test folds of different datasets for ridge regression (RR), support vector regression (SVR), random forests (RF), and the autofeat regression model with one, two, or three feature engineering steps (AFR1-3). Best results per column are in boldface (existing methods) and underlined (AFR)."
        },
        "insight": "While on most datasets, the AutoFeatRegressor model does not quite reach the stateof-the-art performance of a random forest regression model (Table 2), it clearly outperforms standard linear ridge regression, while retaining its interpretability."
    },
    {
        "id": "791",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] diabetes eng",
                "[BOLD] diabetes sel",
                "[BOLD] boston eng",
                "[BOLD] boston sel",
                "[BOLD] concrete eng",
                "[BOLD] concrete sel",
                "[BOLD] airfoil eng",
                "[BOLD] airfoil sel",
                "[BOLD] wine quality eng",
                "[BOLD] wine quality sel"
            ],
            "rows": [
                [
                    "[ITALIC] AFR1",
                    "45",
                    "2",
                    "60",
                    "6",
                    "34",
                    "5",
                    "21",
                    "6",
                    "59",
                    "11"
                ],
                [
                    "[ITALIC] AFR2",
                    "5950",
                    "8",
                    "10528",
                    "15",
                    "3456",
                    "40",
                    "530",
                    "42",
                    "9959",
                    "80"
                ],
                [
                    "[ITALIC] AFR3",
                    "32161",
                    "16",
                    "54631",
                    "21",
                    "14485",
                    "16",
                    "2355",
                    "44",
                    "55648",
                    "26"
                ]
            ],
            "title": "Table 3: Number of engineered (eng) and selected (sel) additional features for each dataset from an autofeat regression model with one, two, or three feature engineering steps (AFR1-3)."
        },
        "insight": "Across all datasets, with one feature engineering step, autofeat generated between 2 and 11 additional features, while with two and three steps, it produced on average 31 additional features (Table 3)."
    },
    {
        "id": "792",
        "table": {
            "header": [
                "Models",
                "#Visible Image Gallery/Subject 1/subject",
                "#Visible Image Gallery/Subject 2/subject",
                "#Visible Image Gallery/Subject all/subject"
            ],
            "rows": [
                [
                    "Deep Perceptual Mapping ",
                    "56.33%",
                    "60.08%",
                    "71%"
                ],
                [
                    "Partial Least Squares ",
                    "31.75%",
                    "34.66%",
                    "51.58%"
                ],
                [
                    "Model with Bilinear Upsample",
                    "40.2%",
                    "45.8%",
                    "75.5%"
                ],
                [
                    "Model with Bilinear Upsample (Aligned)",
                    "42%",
                    "48.75%",
                    "77.25%"
                ],
                [
                    "Model with Up Convolution",
                    "41%",
                    "49.75%",
                    "80.2%"
                ],
                [
                    "Model with Up Convolution (Aligned)",
                    "43.75%",
                    "52.5%",
                    "82.5%"
                ],
                [
                    "[BOLD] Model with Up Convolution + DoG Filter",
                    "[BOLD] 46.8%",
                    "[BOLD] 58.5%",
                    "[BOLD] 82.5%"
                ],
                [
                    "[BOLD] Model with Up Convolution + DoG Filter (Aligned)",
                    "[BOLD] 48%",
                    "[BOLD] 60.25%",
                    "[BOLD] 85%"
                ]
            ],
            "title": "TABLE I: Rank-1 recognition accuracies on the Carl dataset"
        },
        "insight": "Table 1, 2, and 3 present the averaged rank-1 recognition scores on the Carl, UND-X1, and EURECOM datasets, respectively. In the tables, we also include the accuracies of the stateof-the-art methods for comparison purposes. For the Carl and UND-X1 datasets, state-of-the-art models are Deep Perceptual Mapping  and Hu et al. . On EURECOM dataset, only  presented a study for the thermal to visible recognition. The authors propose an image synthesis method by using cascaded refinement network. However, their benchmark setup is different to visible face recognition studies. than ours and earlier thermal [CONTINUE] The highest accuracies are reached when preprocessing, up convolution, and alignment are used. As can be seen in Table I, the proposed method is superior to the DPM, when two visible images per subject and all available visible images per subject are used in the gallery. [CONTINUE] From Table I, it can also be observed that alignment improves the performance by around 2%. [CONTINUE] Up convolution is found to be more useful compared to upsampling leading to increased accuracies on all there datasets. Similarly, applying preprocessing has enhanced the performance further on all three datasets, which indicates its importance."
    },
    {
        "id": "793",
        "table": {
            "header": [
                "Models",
                "#Visible Image Gallery/Subject 1/subject",
                "#Visible Image Gallery/Subject 2/subject",
                "#Visible Image Gallery/Subject all/subject"
            ],
            "rows": [
                [
                    "Deep Perceptual Mapping ",
                    "55.36%",
                    "60.83%",
                    "83.73%"
                ],
                [
                    "Partial Least Squares ",
                    "44.75%",
                    "50.89%",
                    "69.86%"
                ],
                [
                    "Model with Bilinear Upsample",
                    "42%",
                    "50.25%",
                    "75.4%"
                ],
                [
                    "Model with Up Convolution",
                    "49.25%",
                    "57.5%",
                    "82%"
                ],
                [
                    "[BOLD] Model with Up Convolution + DoG Filter",
                    "[BOLD] 58.75%",
                    "[BOLD] 65.25%",
                    "[BOLD] 87.2%"
                ]
            ],
            "title": "TABLE II: Rank-1 recognition accuracies on the UND-X1 dataset"
        },
        "insight": "Table II shows that our proposed method outperforms DPM on the UND-X1 dataset in all the gallery settings. We improved the state-of-the-art by an absolute increase of 14% in accuracy on the Carl dataset and by 3.5% on the UND-X1 dataset when all the samples of the subjects are used in the gallery."
    },
    {
        "id": "794",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Mean Runtime for 200 Samples (s) SGAN ",
                "[BOLD] Mean Runtime for 200 Samples (s) Ours (Full)",
                "[BOLD] Mean Runtime for 200 Samples (s) Ours ( [ITALIC] zbest)"
            ],
            "rows": [
                [
                    "ETH",
                    "6.98 (1x)",
                    "[BOLD] 0.13 (54x)",
                    "[BOLD] 0.13 (54x)"
                ],
                [
                    "Hotel",
                    "6.46 (1x)",
                    "[BOLD] 0.08 (81x)",
                    "[BOLD] 0.08 (81x)"
                ],
                [
                    "Univ",
                    "46.71 (1x)",
                    "2.00 (23x)",
                    "[BOLD] 1.96 (24x)"
                ],
                [
                    "Zara 1",
                    "6.47 (1x)",
                    "[BOLD] 0.16 (40x)",
                    "[BOLD] 0.16 (40x)"
                ],
                [
                    "Zara 2",
                    "9.56 (1x)",
                    "0.37 (26x)",
                    "[BOLD] 0.36 (27x)"
                ]
            ],
            "title": "Table 2: Mean time to generate 200 samples in scenes from each dataset, benchmarked on a computer with a 2.7 GHz Intel Core i5 CPU and 8 GB of RAM. Speedup factors are indicated in brackets."
        },
        "insight": "The results are summarized in Table 2. [CONTINUE] Our methods are significantly faster to sample from than [CONTINUE] SGAN [CONTINUE] Further, our hypothesis that the zbest configuration will be slightly faster holds true."
    },
    {
        "id": "795",
        "table": {
            "header": [
                "Condition",
                "PSNR",
                "SSIM"
            ],
            "rows": [
                [
                    "1.\u00a0 [BOLD] Default\u00a0( [BOLD] RM+ [BOLD] AM)",
                    "[BOLD] 15.67",
                    "[BOLD] 0.684"
                ],
                [
                    "2.\u00a0 [BOLD] R+ [BOLD] A",
                    "15.55",
                    "0.676"
                ],
                [
                    "3.\u00a0 [BOLD] R",
                    "15.64",
                    "0.681"
                ],
                [
                    "4.\u00a0U-Net",
                    "14.81",
                    "0.643"
                ]
            ],
            "title": "Table 2: This table reports the mean PSNR and the mean SSIM for some configurations of the loss function, the use of our attention mechanism, and the network architecture.\u00a0Default configuration represents the model that we use to compare with the state-of-the-art literature."
        },
        "insight": "Table 2: This table reports the mean PSNR and the mean SSIM for some configurations of the loss function, the use of our attention mechanism, and the network architecture. Default configuration represents the model that we use to compare with the state-of-the-art literature."
    },
    {
        "id": "796",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "AB Accuracy",
                "AB  [ITALIC] F1",
                "AB ROC AUC",
                "RT Accuracy",
                "RT  [ITALIC] F1",
                "RT ROC AUC",
                "RF-Treant Accuracy",
                "RF-Treant Accuracy",
                "RF-Treant  [ITALIC] F1",
                "RF-Treant  [ITALIC] F1",
                "RF-Treant ROC AUC",
                "RF-Treant ROC AUC"
            ],
            "rows": [
                [
                    "census",
                    "Budget",
                    "30",
                    "[BOLD] 0.850",
                    "[BOLD] 0.783",
                    "[BOLD] 0.902",
                    "0.813",
                    "0.692",
                    "0.883",
                    "[BOLD] 0.850",
                    "+0.0%",
                    "0.773",
                    "-1.3%",
                    "0.897",
                    "-0.6%"
                ],
                [
                    "census",
                    "Budget",
                    "60",
                    "0.783",
                    "0.690",
                    "0.827",
                    "0.810",
                    "0.698",
                    "0.871",
                    "[BOLD] 0.845",
                    "+4.3%",
                    "[BOLD] 0.766",
                    "+9.7%",
                    "[BOLD] 0.894",
                    "+2.6%"
                ],
                [
                    "census",
                    "Budget",
                    "90",
                    "0.798",
                    "0.705",
                    "0.825",
                    "0.775",
                    "0.607",
                    "0.855",
                    "[BOLD] 0.845",
                    "+5.9%",
                    "[BOLD] 0.769",
                    "+9.1%",
                    "[BOLD] 0.893",
                    "+4.4%"
                ],
                [
                    "census",
                    "Budget",
                    "120",
                    "0.788",
                    "0.694",
                    "0.793",
                    "0.744",
                    "0.558",
                    "0.528",
                    "[BOLD] 0.842",
                    "+6.9%",
                    "[BOLD] 0.762",
                    "+9.8%",
                    "[BOLD] 0.887",
                    "+11.9%"
                ],
                [
                    "wine",
                    "Budget",
                    "20",
                    "0.762",
                    "0.737",
                    "[BOLD] 0.824",
                    "0.734",
                    "0.703",
                    "0.795",
                    "[BOLD] 0.764",
                    "+0.3%",
                    "[BOLD] 0.739",
                    "+0.3%",
                    "0.821",
                    "-0.4%"
                ],
                [
                    "wine",
                    "Budget",
                    "40",
                    "0.723",
                    "[BOLD] 0.689",
                    "0.788",
                    "0.623",
                    "0.548",
                    "0.662",
                    "[BOLD] 0.728",
                    "+0.7%",
                    "[BOLD] 0.689",
                    "+0.0%",
                    "[BOLD] 0.802",
                    "+1.8%"
                ],
                [
                    "wine",
                    "Budget",
                    "60",
                    "0.718",
                    "[BOLD] 0.687",
                    "0.788",
                    "0.552",
                    "0.418",
                    "0.522",
                    "[BOLD] 0.720",
                    "+0.3%",
                    "0.680",
                    "-1.0%",
                    "[BOLD] 0.798",
                    "+1.3%"
                ],
                [
                    "wine",
                    "Budget",
                    "80",
                    "0.715",
                    "0.680",
                    "0.773",
                    "0.566",
                    "0.443",
                    "0.561",
                    "[BOLD] 0.728",
                    "+1.8%",
                    "[BOLD] 0.688",
                    "+1.2%",
                    "[BOLD] 0.800",
                    "+3.5%"
                ],
                [
                    "wine",
                    "Budget",
                    "100",
                    "0.702",
                    "0.668",
                    "0.761",
                    "0.559",
                    "0.429",
                    "0.553",
                    "[BOLD] 0.727",
                    "+3.6%",
                    "[BOLD] 0.687",
                    "+2.8%",
                    "[BOLD] 0.796",
                    "+4.6%"
                ],
                [
                    "wine",
                    "Budget",
                    "120",
                    "0.677",
                    "0.636",
                    "0.732",
                    "0.568",
                    "0.431",
                    "0.544",
                    "[BOLD] 0.728",
                    "+7.5%",
                    "[BOLD] 0.688",
                    "+8.2%",
                    "[BOLD] 0.801",
                    "+9.4%"
                ],
                [
                    "credit",
                    "Budget",
                    "10",
                    "0.811",
                    "0.644",
                    "0.749",
                    "0.799",
                    "0.610",
                    "0.748",
                    "[BOLD] 0.816",
                    "+0.6%",
                    "[BOLD] 0.656",
                    "+1.9%",
                    "[BOLD] 0.765",
                    "+2.1%"
                ],
                [
                    "credit",
                    "Budget",
                    "30",
                    "0.786",
                    "0.544",
                    "0.661",
                    "0.763",
                    "0.457",
                    "0.655",
                    "[BOLD] 0.810",
                    "+3.1%",
                    "[BOLD] 0.617",
                    "+13.4%",
                    "[BOLD] 0.745",
                    "+12.7%"
                ],
                [
                    "credit",
                    "Budget",
                    "40",
                    "0.784",
                    "0.554",
                    "0.660",
                    "0.759",
                    "0.438",
                    "0.632",
                    "[BOLD] 0.808",
                    "+3.1%",
                    "[BOLD] 0.618",
                    "+11.6%",
                    "[BOLD] 0.744",
                    "+12.7%"
                ],
                [
                    "credit",
                    "Budget",
                    "60",
                    "0.777",
                    "0.533",
                    "0.622",
                    "0.759",
                    "0.436",
                    "0.613",
                    "[BOLD] 0.809",
                    "+4.1%",
                    "[BOLD] 0.616",
                    "+15.6%",
                    "[BOLD] 0.744",
                    "+19.6%"
                ]
            ],
            "title": "TABLE III: Comparison of adversarial learning techniques trained and attacked under the same budget. The table also shows the performance difference between RF-Treant and the best competitor."
        },
        "insight": "Table III shows the results obtained by the different adversarial learning techniques for the different training/test budgets. It is clear how our method outperforms its competitors, basically for all measures and datasets. Most importantly, the superiority of our approach becomes even more pronounced as the strength of the attacker grows. For example, the percentage improvement in ROC AUC over AB on the credit dataset amounts to 2.1% for budget 10, while this improvement grows to 19.6% for budget 60. It is also worth noticing that the performance of RT is consistently worse than that of AB."
    },
    {
        "id": "797",
        "table": {
            "header": [
                "Problem Class",
                "R",
                "R+Advisor",
                "PPO",
                "PPO+Advisor",
                "MAML"
            ],
            "rows": [
                [
                    "Pole-balance (d)",
                    "20.32\u00b13.15",
                    "28.52\u00b17.6",
                    "27.87\u00b16.17",
                    "[BOLD] 46.29\u00b1 [BOLD] 6.30",
                    "39.29\u00b15.74"
                ],
                [
                    "Animat",
                    "\u2212779.62\u00b1110.28",
                    "[BOLD] \u2212387.27\u00b1 [BOLD] 162.33",
                    "\u2212751.40\u00b168.73",
                    "\u2212631.97\u00b1155.5",
                    "\u2212669.93\u00b192.32"
                ],
                [
                    "Pole-balance (c)",
                    "\u2014",
                    "\u2014",
                    "29.95\u00b17.90",
                    "[BOLD] 438.13\u00b1 [BOLD] 35.54",
                    "267.76\u00b1163.05"
                ],
                [
                    "Hopper",
                    "\u2014",
                    "\u2014",
                    "13.82\u00b110.53",
                    "[BOLD] 164.43\u00b1 [BOLD] 48.54",
                    "39.41\u00b17.95"
                ],
                [
                    "Ant",
                    "\u2014",
                    "\u2014",
                    "\u221242.75\u00b124.35",
                    "83.76\u00b120.41",
                    "[BOLD] 113.33\u00b1 [BOLD] 64.48"
                ]
            ],
            "title": "Table 1: Average performance (and standard deviations) on discrete and continuous control unseen tasks over the last 50 episodes."
        },
        "insight": "Table 1: Average performance (and standard deviations) on discrete and continuous control unseen tasks over the last 50 episodes. [CONTINUE] In the discrete case, MAML showed a clear improvement over starting from a random initial policy. However, using the advisor with PPO resulted in a clear improvement in pole-balancing and, in the case of animat, training the advisor with REINFORCE led to an almost 50% improvement over MAML. In the case of continuous control, the first test corresponds to a continuous version of pole-balancing. The second and third set of tasks correspond to the \"Hopper\" and \"Ant\" problem classes, where the task variations were obtained by modifying the length and size of the limbs and body. In all continuous control tasks, using the advisor and MAML led to an clear improvement in performance in the alloted time. In the case of pole-balancing using the advisor led the agent to accumulate almost twice as much reward as MAML, and in the case of Hopper, the advisor led to accumulating 4 times the reward. On the other had, MAML achieved a higher average return in the Ant problem class, but showing high variance. An important takeaway from these results is that in all cases, using the advisor resulted in a clear improvement in performance over a limited number of episodes. This does not necessarily mean that the agent can reach a better policy over an arbitrarily long period of time, but rather that it is able to reach a certain performance level much quicker."
    },
    {
        "id": "798",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Acc (%)",
                "[BOLD] DR (%)",
                "[BOLD] FAR (%)",
                "[BOLD] F1 Score",
                "[BOLD] Execution Time (S)"
            ],
            "rows": [
                [
                    "KNN ",
                    "97.4",
                    "96.3",
                    "5.3",
                    "0.934",
                    "911.6"
                ],
                [
                    "SVM ",
                    "96.5",
                    "95.7",
                    "4.8",
                    "0.933",
                    "13765.6"
                ],
                [
                    "DT",
                    "99.99",
                    "99.99",
                    "0.006",
                    "0.999",
                    "328"
                ],
                [
                    "RF",
                    "99.99",
                    "99.99",
                    "0.0003",
                    "0.999",
                    "506.8"
                ],
                [
                    "ET",
                    "99.99",
                    "99.99",
                    "0.0005",
                    "0.999",
                    "216.3"
                ],
                [
                    "XGBoost",
                    "99.98",
                    "99.98",
                    "0.012",
                    "0.999",
                    "3499.1"
                ],
                [
                    "Stacking",
                    "100",
                    "100",
                    "0.0",
                    "1.0",
                    "1237.1"
                ],
                [
                    "FS RF",
                    "99.99",
                    "99.99",
                    "0.0013",
                    "0.999",
                    "99.6"
                ],
                [
                    "FS Stacking",
                    "99.99",
                    "99.99",
                    "0.0006",
                    "0.999",
                    "325.6"
                ]
            ],
            "title": "TABLE III: Performance Evaluation of IDS on CAN-intrusion Dataset"
        },
        "insight": "The results of testing different algorithms on CAN-intrusion data set and CICIDS2017 data set are shown in Tables III and IV, respectively. According to Table III, when testing on the CAN-intrusion data set, the tree-based algorithms including DT, RF, ET, and XGBoost used in the proposed system are 2.5% more accurate than KNN and 3.4% more accurate than SVM used in . In addition, multi-threading is enabled in DT, RF, ET, and XGBoost, resulting in a lower execution time compared with KNN and SVM. Since XGBoost has the lowest accuracy and longest execution time among the four tree-based ML models, only the other three algorithms, DT, RF, and ET were selected into the stacking ensemble model, and the single model with best performance, RF, was selected to be the meta-classifier in the second layer. After using stacking to combine the three tree-based models, the accuracy, detection rate and F1 score reaches 100%, which means that all the trained attacks can be detected. [CONTINUE] From Tables III and IV, it can be seen that RF and XGBoost are the singular models with best accuracy on CAN-intrusion data set and CICIDS2017 data set, respectively. As single base models have lower execution time than the ensemble model, these two singular models were also tested after feature selection. For CAN-intrusion data set, the top-4 important features, CAN ID, 'DATA', 'DATA', 'DATA' were selected for the tests. The results of RF and stacking on CANintrusion data set after feature selection, named \"FS RF\" and \"FS Stacking\", can be seen from Table III."
    },
    {
        "id": "799",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Acc (%)",
                "[BOLD] DR (%)",
                "[BOLD] FAR (%)",
                "[BOLD] F1 Score",
                "[BOLD] Execution Time (S)"
            ],
            "rows": [
                [
                    "KNN ",
                    "96.6",
                    "96.4",
                    "5.6",
                    "0.966",
                    "9243.6"
                ],
                [
                    "SVM ",
                    "98.01",
                    "97.58",
                    "1.48",
                    "0.978",
                    "49953.1"
                ],
                [
                    "DT",
                    "99.72",
                    "99.3",
                    "0.029",
                    "0.998",
                    "126.7"
                ],
                [
                    "RF",
                    "98.37",
                    "98.29",
                    "0.039",
                    "0.983",
                    "2421.6"
                ],
                [
                    "ET",
                    "93.43",
                    "93.35",
                    "0.001",
                    "0.934",
                    "2349.6"
                ],
                [
                    "XGBoost",
                    "99.78",
                    "99.76",
                    "0.069",
                    "0.997",
                    "1637.2"
                ],
                [
                    "Stacking",
                    "99.86",
                    "99.8",
                    "0.012",
                    "0.998",
                    "4519.3"
                ],
                [
                    "FS XGBoost",
                    "99.7",
                    "99.55",
                    "0.077",
                    "0.996",
                    "995.9"
                ],
                [
                    "FS Stacking",
                    "99.82",
                    "99.75",
                    "0.011",
                    "0.997",
                    "2774.8"
                ]
            ],
            "title": "TABLE IV: Performance Evaluation of IDS on CICIDS2017 Dataset"
        },
        "insight": "Similarly in CICIDS2017 data set, as shown in Table IV, most of the used tree-based algorithms shows 1.8-3.2% higher accuracy, detection rate and F1 score than KNN  and SVM  except for ET. Hence, only DT, RF, and XGBoost were chosen in the proposed stacking method, and XGBoost was selected to be the meta-classifier of the stacking model. Although the execution time of stacking is longer than any singular tree-based models, it reaches the highest accuracy among the models (99.86%). [CONTINUE] For CICIDS2017 data set, 36 out of 78 features were selected and the accuracy of XGBoost and stacking models only decreased by 0.08% and 0.04% while saving 39.2% and 38.6% of the execution time, respectively, as shown in Table IV."
    },
    {
        "id": "800",
        "table": {
            "header": [
                "[BOLD] Label",
                "[BOLD] Feature",
                "[BOLD] Weight"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Bwd Packet Length Std",
                    "0.1723"
                ],
                [
                    "DoS",
                    "Average Packet Size",
                    "0.1211"
                ],
                [
                    "[EMPTY]",
                    "Destination Port",
                    "0.0785"
                ],
                [
                    "[EMPTY]",
                    "Total Length of Fwd Packets",
                    "0.3020"
                ],
                [
                    "Port-Scan",
                    "Average Packet Size",
                    "0.1045"
                ],
                [
                    "[EMPTY]",
                    "PSH Flag Count",
                    "0.1019"
                ],
                [
                    "[EMPTY]",
                    "Destination Port",
                    "0.3728"
                ],
                [
                    "Brute-Force",
                    "Fwd Packet Length Min",
                    "0.1022"
                ],
                [
                    "[EMPTY]",
                    "Packet Length Variance",
                    "0.0859"
                ],
                [
                    "[EMPTY]",
                    "Init_Win_bytes_backward",
                    "0.2643"
                ],
                [
                    "Web-Attack",
                    "Average Packet Size",
                    "0.1650"
                ],
                [
                    "[EMPTY]",
                    "Destination Port",
                    "0.0616"
                ],
                [
                    "[EMPTY]",
                    "Destination Port",
                    "0.2364"
                ],
                [
                    "Botnet",
                    "Bwd Packet Length Mean",
                    "0.1240"
                ],
                [
                    "[EMPTY]",
                    "Avg Bwd Segment Size",
                    "0.1104"
                ],
                [
                    "[EMPTY]",
                    "Total Length of Fwd Packets",
                    "0.2298"
                ],
                [
                    "Infiltration",
                    "Subflow Fwd Bytes",
                    "0.1345"
                ],
                [
                    "[EMPTY]",
                    "Destination Port",
                    "0.1149"
                ]
            ],
            "title": "TABLE V: Top-3 Feature Importance by Each Attack"
        },
        "insight": "The list of top-3 important features and their corresponding weights of each attack is shown in Table V. As Table V shows, the destination port can reflect a DoS, brute force, web attack, and botnet attack. The size of packets is another important feature. For example, the average packet size indicates a DoS attack, port scan attack, and web attack. The packet length in the forward direction is related to port scan, brute-force, and infiltration attacks, while the packet length in the backward [CONTINUE] direction reflects a DoS, web attack, and botnet. In addition, the variance of the length of the packets in both forward and backward directions reflects the brute force attack, and the count of pushing flags indicates port scan attack. After getting the feature importance list of each attack, the IDSs with other aims like designing a dedicated system for detecting a single type of attack can be developed by selecting the important features based on the list."
    },
    {
        "id": "801",
        "table": {
            "header": [
                "[BOLD] Conditions Map",
                "[BOLD] Conditions Weather",
                "[BOLD] Conditions Agents",
                "[BOLD] Median Uncertainty Value Follow",
                "[BOLD] Median Uncertainty Value Left",
                "[BOLD] Median Uncertainty Value Right",
                "[BOLD] Median Uncertainty Value Straight"
            ],
            "rows": [
                [
                    "Town1",
                    "Seen",
                    "No",
                    "0.694",
                    "0.827",
                    "0.868",
                    "0.667"
                ],
                [
                    "Town1",
                    "Seen",
                    "Yes",
                    "0.739",
                    "0.823",
                    "0.854",
                    "0.717"
                ],
                [
                    "Town1",
                    "Unseen",
                    "Yes",
                    "0.737",
                    "0.870",
                    "0.903",
                    "0.757"
                ],
                [
                    "Town2",
                    "Seen",
                    "Yes",
                    "0.759",
                    "0.852",
                    "0.881",
                    "0.815"
                ],
                [
                    "Town2",
                    "Unseen",
                    "Yes",
                    "0.740",
                    "0.809",
                    "0.952",
                    "0.753"
                ],
                [
                    "Map Town1 Avg.",
                    "Map Town1 Avg.",
                    "Map Town1 Avg.",
                    "0.788",
                    "0.788",
                    "0.788",
                    "0.788"
                ],
                [
                    "Map Town2 Avg.",
                    "Map Town2 Avg.",
                    "Map Town2 Avg.",
                    "0.820",
                    "0.820",
                    "0.820",
                    "0.820"
                ],
                [
                    "Seen Weather Avg.",
                    "Seen Weather Avg.",
                    "Seen Weather Avg.",
                    "0.791",
                    "0.791",
                    "0.791",
                    "0.791"
                ],
                [
                    "Unseen Weather Avg.",
                    "Unseen Weather Avg.",
                    "Unseen Weather Avg.",
                    "0.815",
                    "0.815",
                    "0.815",
                    "0.815"
                ]
            ],
            "title": "TABLE I: Median Uncertainty Value in Different Scenarios"
        },
        "insight": "The median uncertainty measures under different commands are shown in Table I. [CONTINUE] The general trend is that frames taken in novel environment and under unseen weather have a higher average uncertainty value than those from seen weather and environment."
    },
    {
        "id": "802",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Infraction  [BOLD] Rate",
                "[BOLD] Success Rate Town1",
                "[BOLD] Success Rate Town2",
                "[BOLD] Km per Infraction Town1",
                "[BOLD] Km per Infraction Town2"
            ],
            "rows": [
                [
                    "Passive (full)",
                    "-",
                    "0.55",
                    "0.40",
                    "0.69",
                    "0.55"
                ],
                [
                    "Baseline",
                    "-",
                    "0.52",
                    "0.34",
                    "0.90",
                    "0.47"
                ],
                [
                    "Starter Set",
                    "-",
                    "0.41",
                    "0.24",
                    "0.65",
                    "0.56"
                ],
                [
                    "Active Filter",
                    "-",
                    "0.68",
                    "0.51",
                    "0.90",
                    "0.67"
                ],
                [
                    "Stochastic Mix",
                    "20.05 %",
                    "0.58",
                    "0.44",
                    "0.83",
                    "0.47"
                ],
                [
                    "Random Noise",
                    "19.54 %",
                    "0.73",
                    "0.51",
                    "0.75",
                    "0.51"
                ],
                [
                    "UAIL",
                    "[BOLD] 13.83 %",
                    "[BOLD] 0.74",
                    "[BOLD] 0.61",
                    "0.88",
                    "0.63"
                ]
            ],
            "title": "TABLE II: Performance Comparison on Intersections Benchmark. Avg success rate and distance traveled between infractions are reported. Distance traveled between infractions can be higher for a model with lower success rate due to its failure in learning turning behaviors."
        },
        "insight": "The performance8 of the models trained with different datasets is shown in Table II. [CONTINUE] The model trained with the active filter dataset outperformed that with baseline as we hypothesized. [CONTINUE] the model trained with active filter also achieved better performance than the model using all the passive data,"
    },
    {
        "id": "803",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Raster",
                "[BOLD] State",
                "[BOLD] Loss",
                "[BOLD] Displacement",
                "[BOLD] Along-track",
                "[BOLD] Cross-track"
            ],
            "rows": [
                [
                    "UKF",
                    "\u2013",
                    "yes",
                    "\u2013",
                    "1.46",
                    "1.21",
                    "0.57"
                ],
                [
                    "Linear model",
                    "\u2013",
                    "yes",
                    "( 2 )",
                    "1.19",
                    "1.03",
                    "0.43"
                ],
                [
                    "Lane-assoc",
                    "\u2013",
                    "yes",
                    "\u2013",
                    "1.09",
                    "1.09",
                    "0.19"
                ],
                [
                    "AlexNet",
                    "w/o fading",
                    "no",
                    "( 2 )",
                    "3.14",
                    "3.11",
                    "0.35"
                ],
                [
                    "AlexNet",
                    "w/ fading",
                    "no",
                    "( 2 )",
                    "1.24",
                    "1.23",
                    "0.22"
                ],
                [
                    "AlexNet",
                    "w/o fading",
                    "yes",
                    "( 2 )",
                    "0.97",
                    "0.94",
                    "0.21"
                ],
                [
                    "AlexNet",
                    "w/ fading",
                    "yes",
                    "( 2 )",
                    "0.86",
                    "0.83",
                    "0.20"
                ],
                [
                    "VGG-19",
                    "w/ fading",
                    "yes",
                    "( 2 )",
                    "0.77",
                    "0.75",
                    "0.19"
                ],
                [
                    "ResNet-50",
                    "w/ fading",
                    "yes",
                    "( 2 )",
                    "0.76",
                    "0.74",
                    "0.18"
                ],
                [
                    "MobileNet-v2",
                    "w/ fading",
                    "yes",
                    "( 2 )",
                    "0.73",
                    "0.70",
                    "0.18"
                ],
                [
                    "MobileNet-v2",
                    "w/ fading",
                    "yes",
                    "( 5 )",
                    "0.71",
                    "0.68",
                    "0.18"
                ],
                [
                    "MobileNet-v2 LSTM",
                    "w/ fading",
                    "yes",
                    "( 5 )",
                    "[BOLD] 0.62",
                    "[BOLD] 0.60",
                    "[BOLD] 0.14"
                ]
            ],
            "title": "Table 1: Comparison of average prediction errors for competing methods (in meters)"
        },
        "insight": "In Table 1 we report error metrics relevant for motion prediction: displacement errors, as well as along-track and cross-track errors , averaged over the prediction horizon. [CONTINUE] Considering the baselines, we see that the linear model [CONTINUE] easily outperformed the baseline UKF, which simply propagates an initial actor state. Moreover, using the map information through the lane-assoc model we gained significant improvements, especially in the cross-track which is already at the level of the best deep models. [CONTINUE] We then conducted an ablation study using the feedforward architecture from Figure 2 and AlexNet as a base CNN, running experiments with varying input complexity (upper half of Table 1). When we provide neither fading nor state inputs the model performs worse than UKF, as the network does not have enough information to estimate current state of an actor from the raster. Interestingly, when we include fading the model starts to outperform the baseline by a large margin, indicating that actor state can be inferred solely from providing past positions through fading. If instead of fading we directly provide state estimates we get even better performance, as the state info is already distilled and does not need to be estimated from raster. Furthermore, using raster with fading together with state inputs leads to additional performance boost, suggesting that fading carries additional info not available through the state itself, and that the raster and other external inputs can be seamlessly combined through the proposed architecture to improve accuracy. [CONTINUE] Next, we compared popular CNN architectures as base CNNs. As seen in the bottom half of Table 1, we found that VGG and ResNet models provide improvements over the baseline AlexNet, as observed previously . It is interesting to note that only starting with these models did we outperform the baseline lane-assoc model in terms of all the relevant metrics. However, both models are outperformed by the novel MNv2 architecture that combines a number of deep learning ideas under one roof (e.g., bottleneck layers, residual connections, depthwise convolutions). Taking the best performing MNv2 as a base and extending the output layer by adding uncertainty led to further improvements. [CONTINUE] Lastly, using LSTM decoder at the output, as described in Section 3.3, led to the best results. In our task the future states depend on the past ones, which can be captured by the recurrent architecture."
    },
    {
        "id": "804",
        "table": {
            "header": [
                "[BOLD] Farm Name",
                "[BOLD] Access via",
                "[BOLD] #Downloads",
                "[BOLD] Source",
                "[BOLD] Price(USD/10k)",
                "[BOLD] IP Address",
                "[BOLD] Device ID",
                "[BOLD] Duration(hours)",
                "[BOLD] Date"
            ],
            "rows": [
                [
                    "Farm 1",
                    "Website",
                    "10,000",
                    "Portal site",
                    "4",
                    "Distinct",
                    "None",
                    "12",
                    "06/06/2018"
                ],
                [
                    "Farm 2",
                    "Taobao",
                    "15,000",
                    "Update",
                    "6",
                    "Distinct",
                    "Normal",
                    "2",
                    "07/31/2018"
                ],
                [
                    "Farm 3",
                    "QQ",
                    "10,000",
                    "Null",
                    "6",
                    "Distinct",
                    "Abnormal",
                    "0.2",
                    "08/05/2018"
                ],
                [
                    "Farm 4",
                    "Website",
                    "20,000",
                    "Portal site",
                    "3",
                    "Distinct",
                    "Abnormal",
                    "1",
                    "09/15/2018"
                ]
            ],
            "title": "TABLE II: Comparison between purchased fake downloads injection services on our honeypot App. Portal website: download comes from App market portal website. Update: download comes from updating the App. Null: no download source record."
        },
        "insight": "As Table II shows, we finally contact with fraudster agencies via their websites, e-commerce platforms like Taobao, or online chatting services like QQ. [CONTINUE] Table II shows the main attributes of four purchased download injection services. We could see that fake downloads injected by four farms all have distinct IP addresses but vary in downloading source and device ID. Regular downloads usually come from App market clients on smart devices, while Farm 1 and Farm 4 both inject fake downloads from the App market portal website. The mechanism is simply clicking the honeypot App downloading URL listed at the App market portal website. Moreover, to generate valid device IDs. Farm 2 can simulate the device ID information, but it injects the fake downloads via repeatedly updating the honeypot App, which is not an ordinary behavior. they both fail"
    },
    {
        "id": "805",
        "table": {
            "header": [
                "[BOLD] Feature Type",
                "[BOLD] Precision",
                "[BOLD] Recall",
                "[BOLD] F1",
                "[BOLD] AUC",
                "[BOLD] Accuracy"
            ],
            "rows": [
                [
                    "Device",
                    "0.955",
                    "0.988",
                    "0.963",
                    "0.977",
                    "0.992"
                ],
                [
                    "App",
                    "0.978",
                    "0.972",
                    "0.976",
                    "0.965",
                    "0.993"
                ],
                [
                    "New",
                    "0.974",
                    "0.940",
                    "0.951",
                    "0.969",
                    "0.993"
                ],
                [
                    "Previous",
                    "0.974",
                    "0.977",
                    "0.975",
                    "0.996",
                    "0.987"
                ],
                [
                    "All",
                    "[BOLD] 0.994",
                    "[BOLD] 0.992",
                    "[BOLD] 0.993",
                    "[BOLD] 0.998",
                    "[BOLD] 0.997"
                ]
            ],
            "title": "TABLE III: Testing results of different types of features with XGBoost."
        },
        "insight": "Table III shows the testing result. We note that the device features set could identify more fake downloads, but it has a lower precision, which means it classifies more regular downloads as fake. A high false positive rate will make the legitimate app marketers compromise mobile App markets, and may further reduce the markets' revenue. Though the features proposed by us cannot beat the performance of previous features, aggregating all features together could hit 100% among all metrics. It validates the rationale behind our feature selection."
    },
    {
        "id": "806",
        "table": {
            "header": [
                "data set",
                "Size,  [ITALIC] N",
                "Num. orbits",
                "Iso. graphs,  [ITALIC] I",
                "[ITALIC] I%",
                "[ITALIC] IP%",
                "Mismatched %"
            ],
            "rows": [
                [
                    "SYNTHETIC",
                    "300",
                    "2",
                    "300",
                    "100",
                    "100",
                    "100"
                ],
                [
                    "Cuneiform",
                    "267",
                    "8",
                    "267",
                    "100",
                    "20.46",
                    "100"
                ],
                [
                    "Letter-low",
                    "2250",
                    "32",
                    "2245",
                    "99.78",
                    "8.72",
                    "96.22"
                ],
                [
                    "DHFR_MD",
                    "393",
                    "25",
                    "392",
                    "99.75",
                    "6.87",
                    "94.91"
                ],
                [
                    "COIL-RAG",
                    "3900",
                    "20",
                    "3890",
                    "99.74",
                    "25.22",
                    "99.31"
                ],
                [
                    "COX2_MD",
                    "303",
                    "13",
                    "301",
                    "99.34",
                    "11.83",
                    "98.68"
                ],
                [
                    "ER_MD",
                    "446",
                    "31",
                    "442",
                    "99.1",
                    "5.57",
                    "82.74"
                ],
                [
                    "Fingerprint",
                    "2800",
                    "69",
                    "2774",
                    "99.07",
                    "16.86",
                    "89.29"
                ],
                [
                    "BZR_MD",
                    "306",
                    "22",
                    "303",
                    "99.02",
                    "7.16",
                    "95.75"
                ],
                [
                    "Letter-med",
                    "2250",
                    "39",
                    "2226",
                    "98.93",
                    "8.05",
                    "92.93"
                ]
            ],
            "title": "Table 2: Isomorphic metrics for Top-10 data sets based on the proportion of isomorphic graphs I%. IP% is the proportion of isomorphic pairs of graphs, Mismatched % is the proportion of mismatched labels."
        },
        "insight": "The metrics are presented in Table 2 for top-10 data sets [CONTINUE] The graphs in Table 2 are sorted by the proportion of isomorphic graphs I%. The results for the first Top-10 data sets are somewhat surprising: almost all graphs in the selected data sets have other isomorphic graphs. [CONTINUE] iso(i) the instances contain target labels that are different than those that it has seen, as we show in Table 2 the percentage of mismatched labels can be high in some data sets;"
    },
    {
        "id": "807",
        "table": {
            "header": [
                "data set",
                "Size,  [ITALIC] N",
                "Num. orbits",
                "Iso. graphs,  [ITALIC] I",
                "[ITALIC] I%",
                "[ITALIC] IP%",
                "Mismatched %"
            ],
            "rows": [
                [
                    "SYNTHETIC",
                    "300",
                    "2",
                    "300",
                    "100",
                    "100",
                    "100"
                ],
                [
                    "Cuneiform",
                    "267",
                    "8",
                    "267",
                    "100",
                    "20.46",
                    "100"
                ],
                [
                    "DHFR_MD",
                    "393",
                    "25",
                    "392",
                    "99.75",
                    "6.87",
                    "94.91"
                ],
                [
                    "COX2_MD",
                    "303",
                    "13",
                    "301",
                    "99.34",
                    "11.83",
                    "98.68"
                ],
                [
                    "ER_MD",
                    "446",
                    "31",
                    "442",
                    "99.1",
                    "5.57",
                    "82.74"
                ],
                [
                    "BZR_MD",
                    "306",
                    "22",
                    "303",
                    "99.02",
                    "7.16",
                    "95.75"
                ],
                [
                    "MUTAG",
                    "188",
                    "17",
                    "36",
                    "19.15",
                    "0.14",
                    "0"
                ],
                [
                    "PTC_FM",
                    "349",
                    "22",
                    "54",
                    "15.47",
                    "0.08",
                    "10.89"
                ],
                [
                    "PTC_MM",
                    "336",
                    "22",
                    "50",
                    "14.88",
                    "0.07",
                    "7.74"
                ],
                [
                    "DHFR",
                    "756",
                    "39",
                    "98",
                    "12.96",
                    "0.04",
                    "3.97"
                ]
            ],
            "title": "Table 3: Isomorphic metrics with node labels for Top-10 data sets based on the proportion of isomorphic graphs I%. IP% is the proportion of isomorphic pairs of graphs, Mismatched % is the proportion of mismatched labels."
        },
        "insight": "While for the first six data sets the proportion of isomorphic graphs has not changed much, it is clearly the case for the remaining data sets."
    },
    {
        "id": "808",
        "table": {
            "header": [
                "[EMPTY]",
                "MUTAG",
                "IMDB-B",
                "IMDB-M",
                "COX2",
                "AIDS",
                "PROTEINS"
            ],
            "rows": [
                [
                    "NN",
                    "0.829 (0.840)",
                    "0.737 (0.733)",
                    "0.501 (0.488)",
                    "0.82 (0.872)",
                    "0.996 (0.998)",
                    "0.737 (0.834)"
                ],
                [
                    "NN-PH",
                    "0.867 (1.000)",
                    "[BOLD] 0.756 (1.000)",
                    "[BOLD] 0.522 (1.000)",
                    "[BOLD] 0.838 (1.000)",
                    "[BOLD] 0.996 (1.000)",
                    "0.742 (1.000)"
                ],
                [
                    "NN-P",
                    "0.856 (0.847)",
                    "0.737 (0.731)",
                    "0.499 (0.486)",
                    "0.795 (0.83)",
                    "0.996 (0.999)",
                    "0.729 (0.709)"
                ],
                [
                    "WL",
                    "0.862 (0.867)",
                    "0.734 (0.990)",
                    "0.502 (0.953)",
                    "0.800 (0.974)",
                    "0.993 (0.999)",
                    "0.747 (0.950)"
                ],
                [
                    "WL-PH",
                    "[BOLD] 0.907 (1.000)",
                    "0.736 (1.000)",
                    "0.504 (1.000)",
                    "0.810 (1.000)",
                    "0.994 (1.000)",
                    "[BOLD] 0.749 (1.000)"
                ],
                [
                    "WL-P",
                    "0.870 (0.838)",
                    "0.724 (0.715)",
                    "0.495 (0.487)",
                    "0.794 (0.844)",
                    "0.994 (0.999)",
                    "0.740 (0.742)"
                ],
                [
                    "V",
                    "0.836 (0.902)",
                    "0.707 (0.820)",
                    "0.503 (0.732)",
                    "0.781 (0.966)",
                    "0.994 (0.997)",
                    "0.726 (0.946)"
                ],
                [
                    "V-PH",
                    "0.859 (1.000)",
                    "0.750 (1.000)",
                    "0.517 (1.000)",
                    "0.794 (1.000)",
                    "[BOLD] 0.996 (1.000)",
                    "0.729 (1.000)"
                ],
                [
                    "V-P",
                    "0.827 (0.844)",
                    "0.724 (0.728)",
                    "0.496 (0.481)",
                    "0.768 (0.852)",
                    "0.996 (0.999)",
                    "0.719 (0.741)"
                ]
            ],
            "title": "Table 4: Mean classification accuracy for test sets Ytest and Yiso (in brackets) in 10-fold cross-validation. Top-1 result in bold."
        },
        "insight": "In experiments, we compare neural network model (NN)  with graph kernels, Weisfeiler-Lehman (WL)  and vertex histogram (V) . For each model we consider two modifications: one for peering model on homogeneous Yiso (e.g. NN-PH) and one for peering model on all Yiso (e.g. NN-P). We show accuracy on Ytest and on Yiso (in brackets) in Table 4. Experimentation details can be found in Appendix G. From Table 4 we can conclude that peering model on homogeneous data is always the top performer. This is aligned with the result of Theorem 6.1, which guarantees that acc(Yiso) = 1, but it is an interesting observation if we compare it to the peering model on all isomorphic instances Yiso (-P models). Moreover, the latter model often loses even to the original model, where no information from the train set is explicitly taken into the test set."
    },
    {
        "id": "809",
        "table": {
            "header": [
                "Methods",
                "Random Classes ImageNet",
                "Random Classes ImageNet",
                "Random Classes LSUN",
                "Random Classes LSUN",
                "Random Classes Places365",
                "Random Classes Places365",
                "Completely Overlapping Classes ImageNet",
                "Completely Overlapping Classes ImageNet",
                "Completely Overlapping Classes LSUN",
                "Completely Overlapping Classes LSUN",
                "Completely Overlapping Classes Places365",
                "Completely Overlapping Classes Places365"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "VGG16",
                    "ResNet34",
                    "VGG16",
                    "ResNet34",
                    "VGG16",
                    "ResNet34",
                    "VGG16",
                    "ResNet34",
                    "VGG16",
                    "ResNet34",
                    "VGG16",
                    "ResNet34"
                ],
                [
                    "SPV (Benchmark)",
                    ".7212",
                    ".6953",
                    ".6664",
                    ".6760",
                    ".5525",
                    ".5870",
                    ".7345",
                    ".7490",
                    ".6769",
                    ".7017",
                    ".5960",
                    ".6460"
                ],
                [
                    "SD",
                    ".5543",
                    ".5562",
                    ".5310",
                    ".5350",
                    ".4390",
                    ".4564",
                    "[BOLD] .7275",
                    "[BOLD] .7292",
                    ".7004",
                    "[BOLD] .7041",
                    "[BOLD] .6163",
                    "[BOLD] .6402"
                ],
                [
                    "[BOLD] (A)  [ITALIC] Estimate  [ITALIC] q methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "CE-E",
                    "[BOLD] .6911",
                    "[BOLD] .6852",
                    ".6483",
                    "[BOLD] .6445",
                    "[BOLD] .5484",
                    "[BOLD] .5643",
                    "[BOLD] .7276",
                    "[BOLD] .7290",
                    ".7002",
                    ".7036",
                    "[BOLD] .6162",
                    "[BOLD] .6406"
                ],
                [
                    "MF-P-E",
                    ".6819",
                    ".6747",
                    ".6443",
                    ".6406",
                    ".5349",
                    ".5488",
                    "[BOLD] .7280",
                    "[BOLD] .7297",
                    "[BOLD] .7012",
                    "[BOLD] .7052",
                    "[BOLD] .6167",
                    "[BOLD] .6406"
                ],
                [
                    "MF-LV-E",
                    ".6660",
                    ".6609",
                    ".6348",
                    ".6330",
                    ".5199",
                    ".5414",
                    ".7231",
                    ".7242",
                    "[BOLD] .7031",
                    "[BOLD] .7043",
                    ".6129",
                    ".6374"
                ],
                [
                    "MF-LF-E",
                    ".6886",
                    "[BOLD] .6833",
                    ".6490",
                    "[BOLD] .6458",
                    ".5441",
                    ".5609",
                    ".7265",
                    "[BOLD] .7279",
                    "[BOLD] .7015",
                    "[BOLD] .7057",
                    "[BOLD] .6161",
                    "[BOLD] .6397"
                ],
                [
                    "[BOLD] (B)  [ITALIC] Backprop. methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "CE-BP",
                    "[BOLD] .6902",
                    "[BOLD] .6869",
                    "[BOLD] .6520",
                    ".6439",
                    ".5466",
                    "[BOLD] .5669",
                    "[BOLD] .7275",
                    "[BOLD] .7288",
                    ".7003",
                    "[BOLD] .7040",
                    "[BOLD] .6161",
                    "[BOLD] .6400"
                ],
                [
                    "MF-P-BP",
                    "[BOLD] .6945",
                    "[BOLD] .6872",
                    ".6480",
                    "[BOLD] .6417",
                    "[BOLD] .5471",
                    ".5609",
                    "[BOLD] .7277",
                    "[BOLD] .7287",
                    ".6999",
                    "[BOLD] .7019",
                    ".6146",
                    ".6384"
                ],
                [
                    "MF-LV-BP",
                    ".6889",
                    "[BOLD] .6847",
                    "[BOLD] .6495",
                    ".6389",
                    ".5467",
                    "[BOLD] .5681",
                    ".7229",
                    ".7225",
                    ".7001",
                    "[BOLD] .7046",
                    ".6113",
                    ".6369"
                ],
                [
                    "MF-LF-BP",
                    ".6842",
                    "[BOLD] .6840",
                    "[BOLD] .6523",
                    "[BOLD] .6445",
                    ".5383",
                    "[BOLD] .5624",
                    ".7239",
                    ".7252",
                    "[BOLD] .7020",
                    ".7034",
                    ".6104",
                    ".6366"
                ],
                [
                    "[BOLD] (C)  [ITALIC] Balanced soft labels",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SD-BS",
                    ".6629",
                    ".6574",
                    ".6343",
                    ".6345",
                    ".5283",
                    ".5433",
                    ".7217",
                    ".7214",
                    ".6979",
                    ".7017",
                    ".6094",
                    ".6320"
                ],
                [
                    "CE-BS",
                    "[BOLD] .6928",
                    "[BOLD] .6856",
                    ".6513",
                    "[BOLD] .6464",
                    "[BOLD] .5548",
                    "[BOLD] .5687",
                    ".7215",
                    ".7213",
                    ".6979",
                    ".7018",
                    ".6094",
                    ".6323"
                ],
                [
                    "MF-P-BS",
                    ".6851",
                    ".6756",
                    ".6474",
                    "[BOLD] .6450",
                    ".5455",
                    ".5546",
                    ".7243",
                    ".7252",
                    ".6996",
                    ".7041",
                    ".6124",
                    ".6355"
                ],
                [
                    "MF-LV-BS",
                    ".6772",
                    ".6682",
                    ".6388",
                    ".6357",
                    ".5346",
                    ".5497",
                    ".7168",
                    ".7173",
                    ".7014",
                    ".7028",
                    ".6063",
                    ".6301"
                ],
                [
                    "MF-LF-BS",
                    "[BOLD] .6935",
                    "[BOLD] .6865",
                    "[BOLD] .6549",
                    "[BOLD] .6485",
                    "[BOLD] .5544",
                    "[BOLD] .5692",
                    ".7210",
                    ".7215",
                    ".6998",
                    ".7035",
                    ".6101",
                    ".6330"
                ]
            ],
            "title": "Table 2: Average accuracy of UHC methods over different combinations of HC configurations, datasets, and unified classifier models. (Underline bold: Best method. Bold: Methods which are not statistically significantly different from the best method.)"
        },
        "insight": "Table 2 shows the results for this experiment. Each column shows the average accuracy of each method under each experiment setting, where the best performing method is shown in underlined bold. [CONTINUE] All proposed methods perform significantly better than SD. We can see that all methods in (A), (B), and (C) [CONTINUE] of Table 2 outperform SD by a large margin of 9-15%. [CONTINUE] On the other hand, our proposed methods achieve significantly better results and almost reach the same accuracy as SPV with a gap of 1-4%. [CONTINUE] MF-LF-BS performs well in all cases. We can see that different algorithms perform best under different settings, but MF-LF-BS always performs best or has no statistical difference from the best methods. [CONTINUE] Balancing soft labels helps improve accuracy. While the improvement may be marginal (less than 1.5%), we can see that 'BS' methods in (C) consistently outperform their 'E' counterparts in (A). Surprisingly, SD-BS, which is SD with balanced soft labels, also significantly improved over SD by more than 10%. [CONTINUE] All methods perform rather well. We can see that all methods, including SD, achieve about the same accuracy (within ~1% range). [CONTINUE] Not balancing soft labels performs better. We note that balancing soft labels tends to slightly deteriorate the accuracy. This is the opposite result from the random classes case. Here, even SD-BS which receive an accuracy boost in the random classes case also performs worse than its counterpart SD. [CONTINUE] Distillation may outperform its supervised counterparts. For LSUN and Places365 datasets, we see that many times distillation methods performs better than SPV. Especially for the case of VGG16, we see SPV consistently perform worse than other methods by 1 to 3% in most of the trials."
    },
    {
        "id": "810",
        "table": {
            "header": [
                "Data augmentation",
                "mIoU ( [ITALIC] f=200)",
                "mIoU ( [ITALIC] f=250)",
                "mIoU ( [ITALIC] f=300)",
                "mIoU ( [ITALIC] f=350)",
                "mIoU ( [ITALIC] f=400)"
            ],
            "rows": [
                [
                    "Fixed z-aug",
                    "0.5562",
                    "0.5773",
                    "0.5869",
                    "0.5948",
                    "0.5930"
                ],
                [
                    "Random z-aug",
                    "0.5979",
                    "0.6089",
                    "0.6117",
                    "0.6116",
                    "0.6070"
                ],
                [
                    "Six-DoF aug",
                    "0.5938",
                    "0.6082",
                    "[BOLD] 0.6146",
                    "0.6156",
                    "0.6143"
                ],
                [
                    "Seven-DoF aug",
                    "[BOLD] 0.6093",
                    "[BOLD] 0.6150",
                    "0.6134",
                    "[BOLD] 0.6166",
                    "[BOLD] 0.6154"
                ]
            ],
            "title": "TABLE I: performance of different data augmentation"
        },
        "insight": "The performance of different data augmentation methods are shown in Table I. While the fixed z-aug performs worst, the seven-DoF augmentation we proposed almost performs best in all testing data sets. Compared with other augmentation methods, the seven-DoF has significant advantages, especially in testing data sets with larger distortion (smaller f value)."
    },
    {
        "id": "811",
        "table": {
            "header": [
                "Hand Skin Region",
                "First band",
                "Second Band",
                "Third Band"
            ],
            "rows": [
                [
                    "Proximal Interphalangeal Joints",
                    "1270",
                    "1350",
                    "1685"
                ],
                [
                    "Metacarpal",
                    "1270",
                    "1360",
                    "1690"
                ],
                [
                    "Proximal Phalanges",
                    "1270",
                    "1360",
                    "-"
                ]
            ],
            "title": "Table 1: Rounded most important bandwidth means for classification per region in nanometers"
        },
        "insight": "Table 1: Rounded most important bandwidth means for classification per region in nanometers [CONTINUE] Table 1presents the wavelength bands found to be the most important for the classification task. It is worth noting that the importance of the band is directly linked to the number of times it has been counted among the 100 executions of the algorithm."
    },
    {
        "id": "812",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] MNIST\u2217",
                "[BOLD] FEMNIST",
                "[BOLD] P-MNIST",
                "[BOLD] VSN",
                "[BOLD] HAR"
            ],
            "rows": [
                [
                    "[ITALIC] Global\u2217",
                    "[BOLD] 0.9678\u00b10.0007",
                    "0.44\u00b10.02",
                    "0.919\u00b10.004",
                    "0.926\u00b10.005",
                    "0.797\u00b10.006"
                ],
                [
                    "[ITALIC] Local",
                    "0.9511\u00b10.0020",
                    "0.51\u00b10.01",
                    "[BOLD] 0.950\u00b10.003",
                    "[BOLD] 0.960\u00b10.003",
                    "0.940\u00b10.001"
                ],
                [
                    "FedAvg",
                    "[BOLD] 0.9675\u00b10.0004",
                    "0.45\u00b10.05",
                    "0.905\u00b10.002",
                    "0.916\u00b10.007",
                    "[BOLD] 0.944\u00b10.004"
                ],
                [
                    "Virtual",
                    "[BOLD] 0.9666\u00b10.0017",
                    "[BOLD] 0.56\u00b10.01",
                    "[BOLD] 0.949\u00b10.001",
                    "[BOLD] 0.960\u00b10.002",
                    "[BOLD] 0.944\u00b10.001"
                ]
            ],
            "title": "Table 2: Multi-task average test accuracy. Mean and standard deviation over 5 train-test splits."
        },
        "insight": "In Table 2 we show the advantages given by the multi-task learning framework in the federated setting. In the table we measure the average categorical accuracy over all tasks of the respective dataset. Every experiment has been repeated over 5 random 25% train test splits, and we report mean and standard deviation over the runs."
    },
    {
        "id": "813",
        "table": {
            "header": [
                "Unconstrained T (s)",
                "Unconstrained AVG T (s)",
                "Unconstrained AVG # itr",
                "Constrained (ours) T (s)",
                "Constrained (ours) AVG T (s)",
                "Constrained (ours) AVG # itr"
            ],
            "rows": [
                [
                    "207.98",
                    "2.08",
                    "35.61",
                    "37.15",
                    "0.56",
                    "2.52"
                ],
                [
                    "216.00",
                    "2.16",
                    "37.40",
                    "35.36",
                    "0.54",
                    "2.33"
                ],
                [
                    "212.10",
                    "2.12",
                    "36.72",
                    "34.16",
                    "0.52",
                    "2.26"
                ],
                [
                    "210.71",
                    "2.11",
                    "36.39",
                    "37.03",
                    "0.58",
                    "2.52"
                ],
                [
                    "210.72",
                    "2.11",
                    "36.24",
                    "36.05",
                    "0.55",
                    "2.40"
                ]
            ],
            "title": "Table 1: Results of constrained one-pixel adversarial perturbations on 100 images from CIFAR-10 dataset. For both constrained and unconstrained perturbations, the total time in seconds, average time in seconds, and the average number of DE iterations are presented. Five runs for constrained and unconstrained one-pixel adversarial perturbations show that constrained perturbations converge faster."
        },
        "insight": "Table 1: Results of constrained one-pixel adversarial perturbations on 100 images from CIFAR-10 dataset. For both constrained and unconstrained perturbations, the total time in seconds, average time in seconds, and the average number of DE iterations are presented. Five runs for constrained and unconstrained one-pixel adversarial perturbations show that constrained perturbations converge faster. [CONTINUE] The results for the CIFAR-10 and NIPS-W datasets are summarized in Table 1 and Table 2 respectively. [CONTINUE] It can be clearly observed that across different runs and on average, the constrained one-pixel adversarial perturbations converged faster with comparatively less number of iterations when compared to conventional one-pixel adversarial perturbations. [CONTINUE] This effect is more pronounced for CIFAR10, with the spatially constrained one-pixel adversarial perturbations converging 6\u00d7 faster than conventional attacks."
    },
    {
        "id": "814",
        "table": {
            "header": [
                "Constrained (ours) T (s)",
                "Constrained (ours) AVG T (s)",
                "Constrained (ours) AVG # itr",
                "Unconstrained T (s)",
                "Unconstrained AVG T (s)",
                "Unconstrained AVG # itr"
            ],
            "rows": [
                [
                    "355.69",
                    "6.03",
                    "6.44",
                    "445.55",
                    "8.25",
                    "8.80"
                ],
                [
                    "392.21",
                    "6.88",
                    "7.63",
                    "523.31",
                    "8.58",
                    "9.70"
                ],
                [
                    "446.09",
                    "7.31",
                    "8.13",
                    "516.71",
                    "8.62",
                    "9.52"
                ],
                [
                    "344.90",
                    "5.95",
                    "6.45",
                    "459.81",
                    "7.79",
                    "8.75"
                ],
                [
                    "416.18",
                    "7.19",
                    "7.86",
                    "559.48",
                    "9.32",
                    "10.60"
                ]
            ],
            "title": "Table 2: Results of constrained one-pixel adversarial perturbations on the NIPS-W dataset. For both constrained and unconstrained successful attacks, the total time in seconds, average time in seconds, and average number of DE iterations are presented. Five runs for constrained and unconstrained one-pixel adversarial perturbations show that constrained perturbations converge faster."
        },
        "insight": "Table 2: Results of constrained one-pixel adversarial perturbations on the NIPS-W dataset. For both constrained and unconstrained successful attacks, the total time in seconds, average time in seconds, and average number of DE iterations are presented. Five runs for constrained and unconstrained one-pixel adversarial perturbations show that constrained perturbations converge faster."
    },
    {
        "id": "815",
        "table": {
            "header": [
                "[BOLD] \u00a0MNIST",
                "[BOLD] 10 Clients  [BOLD] Clean",
                "[BOLD] 10 Clients  [BOLD] Byzantine",
                "[BOLD] 10 Clients  [BOLD] Flipping",
                "[BOLD] 10 Clients  [BOLD] Noisy",
                "[BOLD] 100 Clients  [BOLD] Clean",
                "[BOLD] 100 Clients  [BOLD] Byzantine",
                "[BOLD] 100 Clients  [BOLD] Flipping",
                "[BOLD] 100 Clients  [BOLD] Noisy"
            ],
            "rows": [
                [
                    "AFA",
                    "2.80\u00b10.12",
                    "[BOLD] 2.99\u00b1 [BOLD] 0.12",
                    "[BOLD] 2.96\u00b1 [BOLD] 0.15",
                    "[BOLD] 3.04\u00b1 [BOLD] 0.14",
                    "[BOLD] 3.82\u00b1 [BOLD] 0.09",
                    "[BOLD] 4.03\u00b1 [BOLD] 0.09",
                    "[BOLD] 3.98\u00b1 [BOLD] 0.08",
                    "[BOLD] 3.97\u00b1 [BOLD] 0.08"
                ],
                [
                    "FA",
                    "[BOLD] 2.56\u00b1 [BOLD] 0.11",
                    "90.03\u00b10.59",
                    "8.48\u00b11.58",
                    "3.79\u00b10.17",
                    "[BOLD] 3.86\u00b1 [BOLD] 0.14",
                    "89.70\u00b10.73",
                    "90.20\u00b10.00",
                    "4.81\u00b10.09"
                ],
                [
                    "MKRUM",
                    "3.96\u00b10.18",
                    "4.01\u00b10.13",
                    "78.98\u00b123.39",
                    "3.94\u00b10.13",
                    "4.68\u00b10.11",
                    "4.63\u00b10.14",
                    "4.62\u00b10.13",
                    "4.61\u00b10.13"
                ],
                [
                    "COMED",
                    "2.82\u00b10.09",
                    "3.17\u00b10.16",
                    "11.55\u00b10.93",
                    "3.56\u00b10.09",
                    "4.08\u00b10.11",
                    "4.21\u00b10.13",
                    "7.32\u00b10.20",
                    "4.51\u00b10.10"
                ]
            ],
            "title": "Table 1: Averaged test error over 10 random training data splits for AFA, FA, MKRUM and COMED in different scenarios: clean datasets for all clients, Byzantine adversaries, label flipping attacks, and noisy clients. For each scenario we considered two settings with 10 and 100 clients with 3 and 30 bad/malicious clients respectively. Best results with statistical significance at the 5% level, according to a Wilcoxon Rank-Sum test, are highlighted in bold."
        },
        "insight": "Thus, for each experiment in Table 1 we highlight those results where one or two algorithms perform significantly better than the others (according to the outcome of the statistical test). From the results we can observe that AFA is robust in all scenarios for the 4 datasets, i.e. the test error of the algorithm is not significantly degraded compared to the performance achieved in the clean scenarios. As expected, FA's performance is significantly affected under the presence of bad clients, especially in byzantine and flipping scenarios. It is also interesting to observe that both MKRUM and COMED are very sensitive to label flipping attacks, especially in MNIST, FMNIST and CIFAR-10 for the case of 10 clients. For example, in MNIST, MKRUM and COMED test errors increases up to 78.98% and 11.55% respectively in the flipping attack scenario with 10 clients, whereas AFA's error is just 2.96%, similar to its performance in the clean scenario. MKRUM also offers a worse performance in the clean scenario compared to the other algorithms, which is especially noticeable in the case of CIFAR-10, where the error rate is 53.02%, i.e. a 20% more than FA's error. This is not the case for COMED, whose performance is similar to AFA and FA when no bad clients are present, except for FMNIST, where COMED exhibits a better performance. However, COMED is not consistently robust to all the scenarios considered in the experiment. [CONTINUE] The results of our experimental evaluation are shown in Table 1. [CONTINUE] as shown Table 1, for CIFAR-10, AFA's performance is not affected by the presence of noisy clients, when comparing to the performance obtained for the clean scenario."
    },
    {
        "id": "816",
        "table": {
            "header": [
                "[BOLD] \u00a0FMNIST",
                "[BOLD] 10 Clients  [BOLD] Clean",
                "[BOLD] 10 Clients  [BOLD] Byzantine",
                "[BOLD] 10 Clients  [BOLD] Flipping",
                "[BOLD] 10 Clients  [BOLD] Noisy",
                "[BOLD] 100 Clients  [BOLD] Clean",
                "[BOLD] 100 Clients  [BOLD] Byzantine",
                "[BOLD] 100 Clients  [BOLD] Flipping",
                "[BOLD] 100 Clients  [BOLD] Noisy"
            ],
            "rows": [
                [
                    "AFA",
                    "14.72\u00b11.89",
                    "14.11\u00b11.16",
                    "[BOLD] 15.45\u00b1 [BOLD] 1.88",
                    "15.27\u00b11.89",
                    "15.72\u00b12.11",
                    "16.08\u00b11.88",
                    "[BOLD] 15.99\u00b1 [BOLD] 2.00",
                    "17.48\u00b11.32"
                ],
                [
                    "FA",
                    "[BOLD] 13.60\u00b1 [BOLD] 1.62",
                    "89.27\u00b10.81",
                    "24.52\u00b111.23",
                    "15.55\u00b11.80",
                    "16.96\u00b11.50",
                    "89.55\u00b11.18",
                    "48.46\u00b121.44",
                    "18.95\u00b10.15"
                ],
                [
                    "MKRUM",
                    "14.23\u00b10.21",
                    "14.28\u00b10.27",
                    "34.79\u00b13.12",
                    "14.21\u00b10.24",
                    "15.05\u00b10.36",
                    "15.00\u00b10.20",
                    "[BOLD] 15.14\u00b1 [BOLD] 0.30",
                    "[BOLD] 15.25\u00b1 [BOLD] 0.52"
                ],
                [
                    "COMED",
                    "[BOLD] 12.68\u00b1 [BOLD] 0.22",
                    "[BOLD] 13.33\u00b1 [BOLD] 0.23",
                    "24.02\u00b10.76",
                    "14.43\u00b10.32",
                    "17.05\u00b11.37",
                    "15.95\u00b11.63",
                    "18.11\u00b10.35",
                    "18.38\u00b10.13"
                ]
            ],
            "title": "Table 1: Averaged test error over 10 random training data splits for AFA, FA, MKRUM and COMED in different scenarios: clean datasets for all clients, Byzantine adversaries, label flipping attacks, and noisy clients. For each scenario we considered two settings with 10 and 100 clients with 3 and 30 bad/malicious clients respectively. Best results with statistical significance at the 5% level, according to a Wilcoxon Rank-Sum test, are highlighted in bold."
        },
        "insight": "Thus, for each experiment in Table 1 we highlight those results where one or two algorithms perform significantly better than the others (according to the outcome of the statistical test). From the results we can observe that AFA is robust in all scenarios for the 4 datasets, i.e. the test error of the algorithm is not significantly degraded compared to the performance achieved in the clean scenarios. As expected, FA\u2019s performance is significantly affected under thepresence of bad clients, especially in byzantine and flipping scenarios. It is also interesting to observe that both MKRUM and COMED are very sensitive to label flipping attacks, especially in MNIST, FMNIST and CIFAR-10 for the case of 10 clients. For example, in MNIST, MKRUM and COMED test errors increases up to 78.98% and 11.55% respectively in the flipping attack scenario with 10 clients, whereas AFA\u2019s error is just 2.96%, similar to its performance in the clean scenario. MKRUM also offers a worse performance in the clean scenario compared to the other algorithms, which is especially noticeable in the case of CIFAR-10, where the error rate is 53.02%, i.e. a 20% more than FA\u2019s error. This is not the case for COMED, whose performance is similar toAFA and FA when no bad clients are present, except for FM-NIST, where COMED exhibits a better performance. How- ever, COMED is not consistently robust to all the scenariosconsidered in the experiment."
    },
    {
        "id": "817",
        "table": {
            "header": [
                "Method",
                "[BOLD] CIFAR100 IS",
                "[BOLD] CIFAR100 FID"
            ],
            "rows": [
                [
                    "Real data",
                    "14.79 (\u00b1 .18)",
                    "[EMPTY]"
                ],
                [
                    "Baseline",
                    "10.88 (\u00b1 .19)",
                    "[BOLD] 11.53"
                ],
                [
                    "MHingeGAN",
                    "[BOLD] 14.36 (\u00b1 .17)",
                    "17.3"
                ],
                [
                    "MHingeGAN CSC",
                    "8.79 (\u00b1 .08)",
                    "21.03"
                ],
                [
                    "MHingeGAN FM",
                    "7.86 (\u00b1 .03)",
                    "31.55"
                ],
                [
                    "SNGAN ",
                    "9.30 (\u00b1 .08)",
                    "15.6"
                ],
                [
                    "MSGAN\u2020 ",
                    "-",
                    "19.74"
                ]
            ],
            "title": "Table 1: Inception Scores and FIDs for supervised image generation. Each row corresponds to a single trained model. The models we trained were chosen by minimizing the IS within 100,000 iterations. Methods that are not class conditional are indicated by the \u2020. The best number per column is bold faced."
        },
        "insight": "Compared to other methods, our MHingeGAN yields a higher IS and a lower FID. [CONTINUE] Tables 1 and 2 show state of the art performance of MHingeGAN on the CIFAR10 dataset. The class specific loss on the generator is shown to be essential for superior performance, and stores more conditional information in the discriminator/classifier and the generator. MHingeGAN is able to perform well in both fully supervised and semi-supervised settings, and learns an accurate classifier concurrently with a high quality generator."
    },
    {
        "id": "818",
        "table": {
            "header": [
                "Model",
                "Generator Arch.",
                "[ITALIC] Gb Score",
                "Best  [ITALIC] Ei Score",
                "\u223cParams"
            ],
            "rows": [
                [
                    "1",
                    "Small DCGAN ( [ITALIC] d=256)",
                    "5.02\u00b10.05",
                    "[EMPTY]",
                    "700,000"
                ],
                [
                    "2",
                    "[BOLD] Tiny DCGAN ( [ITALIC] d=128) + Editors",
                    "[BOLD] 5.00\u00b10.09",
                    "[BOLD] 5.67\u00b10.07 (Edit 3)",
                    "[BOLD] 866,000"
                ],
                [
                    "3",
                    "[BOLD] Small DCGAN + Editors (one Critic)",
                    "[BOLD] 5.24\u00b10.04",
                    "[BOLD] 6.05\u00b10.08 (Edit 2)",
                    "[BOLD] 1,250,000"
                ],
                [
                    "4",
                    "[BOLD] Small DCGAN + Editors (multi Critic)",
                    "[BOLD] 5.63\u00b10.06",
                    "[BOLD] 5.86\u00b10.05 (Edit 2)",
                    "[BOLD] 1,250,000"
                ],
                [
                    "5",
                    "Small DCGAN + Editors (end-to-end)",
                    "[EMPTY]",
                    "2.56\u00b10.02",
                    "1,250,000"
                ],
                [
                    "6",
                    "DCGAN - WGAN+GP ( [ITALIC] d=512)",
                    "5.18\u00b10.05",
                    "[EMPTY]",
                    "1,730,000"
                ],
                [
                    "7",
                    "Small ResNet",
                    "5.75\u00b10.05",
                    "[EMPTY]",
                    "720,000"
                ],
                [
                    "8",
                    "[BOLD] Small ResNet + Editors",
                    "[BOLD] 6.35\u00b10.09",
                    "[BOLD] 6.70\u00b10.03 (Edit 3)",
                    "[BOLD] 1,000,000"
                ],
                [
                    "9",
                    "ResNet - WGAN+GP",
                    "6.86\u00b10.04",
                    "[EMPTY]",
                    "1,220,000"
                ]
            ],
            "title": "Table 1: Inception scores on unlabelled CIFAR with and without the chain generator approach trained with WGAN+GP. d refers to the depth of the first conv layer in DCGAN architectures. See section 7.4 for details about architecture."
        },
        "insight": "All models compared are trained to the same number of epochs using the WGAN formulation with a gradient penalty term (WGAN+GP). [CONTINUE] Based on inception scores, our DCGAN variant of the chain generator using multiple critics (model 2 and 4) was able to outperform the base DCGAN model (model 6) while using fewer parameters. [CONTINUE] To ensure that our generator architecture and training scheme are responsible for the performance, we run experiments with a single critic and see that results do not degrade (see model 3 in Table 1); but with end-to-end backpropagation through the chain generator results degraded significantly (see model 5 in Table 1)."
    },
    {
        "id": "819",
        "table": {
            "header": [
                "Method",
                "PSNR (mean \u00b1 std)"
            ],
            "rows": [
                [
                    "Semantic Inpainting",
                    "33.85 \u00b1 4.67"
                ],
                [
                    "Context Encoders",
                    "26.31 \u00b1 4.48"
                ],
                [
                    "Contextual Attention",
                    "31.80 \u00b1 5.19"
                ]
            ],
            "title": "Table 1: Comparison of PSNR mean and standard deviation on the 880 healthy inpainted patches."
        },
        "insight": "Table 1 provides the means and standard deviations of the peak signal-to-noise ratio (PSNR) over all 880 healthy images in the test set. The PSNR value is computed with respect to the center 64 \u00d7 64 missing patch rather than the entire patch. We note the semantic inpainting model achieves the highest average PSNR in this case; however, this quantitative assessment doesn't fully correlate with the visual results."
    },
    {
        "id": "820",
        "table": {
            "header": [
                "[BOLD] Algorithms",
                "[BOLD] False Alarm Rate Abnormal Videos"
            ],
            "rows": [
                [
                    "Proposed Method(3D ResNet+constr.+loss)",
                    "0.72"
                ],
                [
                    "Proposed Method(3D ResNet+constr.+new rank.loss)",
                    "[BOLD] 0.67"
                ]
            ],
            "title": "TABLE III: Comparison of false alarm rate on abnormal videos of UCF-Crime testing dataset."
        },
        "insight": "AUC in all, therefore in Table III, we have just shown the comparison of false alarm rate at the threshold of 50% on the basis of variation of our proposed method. Abnormal activities happen rarely in the real-world scenarios, but still detecting the abnormal activity accurately is an important task. Therefore the detector should give less false alarm rate for abnormal videos. As shown in Table III, our proposed method along with the new ranking loss function gives the lowest false alarm rate on abnormal videos. The table shows the effectiveness of using the new loss function."
    },
    {
        "id": "821",
        "table": {
            "header": [
                "Method",
                "error_rate"
            ],
            "rows": [
                [
                    "Genetic DCNN ",
                    "5.4"
                ],
                [
                    "CNN ",
                    "7.46"
                ],
                [
                    "Soft",
                    "6.30"
                ],
                [
                    "[BOLD] Arc",
                    "[BOLD] 6.04"
                ],
                [
                    "ArcFace",
                    "6.27"
                ],
                [
                    "Center",
                    "[BOLD] 6.04"
                ]
            ],
            "title": "Table 5: The error rate (%) on Fashion-MNIST dataset."
        },
        "insight": "The experiment on Fashion-MNIST, shown in Table 5, reveals our method and center loss are best."
    },
    {
        "id": "822",
        "table": {
            "header": [
                "Methods",
                "Observer accuracy in 2AFC test"
            ],
            "rows": [
                [
                    "Semantic Inpainting",
                    "66.66%"
                ],
                [
                    "Context Encoders",
                    "59.45%"
                ],
                [
                    "Contextual Attention",
                    "37.03 %"
                ]
            ],
            "title": "Table 3: Human observer performance for the task of choosing the real x-ray. An accuracy of 50% corresponds to chance behavior; an accuracy of 100% indicates the observer can perfectly identify the unaltered chest x-ray in a pair."
        },
        "insight": "Table 3 presents the results of this study. The contextual attention model is the most successful one to fool the expert, because more than half of the time the expert would select the image with an inpainted region as the real chest x-ray. On the other hand, the semantic inpainting model has a more difficult time to fool the expert, where only one third of the time its results are considered 'real'. [CONTINUE] context encoder's results are usually also not detectable with observer accuracy close to chance levels. These findings are consistent with the results of the previous sections."
    },
    {
        "id": "823",
        "table": {
            "header": [
                "[BOLD] Approach",
                "#  [BOLD] of encrypted model copies",
                "[BOLD] Ciphertext size (dimension)"
            ],
            "rows": [
                [
                    "Threshold HE\u00a0[asharov2012multiparty]",
                    "[BOLD] P",
                    "1\u00d72"
                ],
                [
                    "Multi-Key HE\u00a0[chen2017batched]",
                    "1",
                    "( [BOLD] N+1)\u00d72"
                ],
                [
                    "This work",
                    "1",
                    "[BOLD] 2\u00d72"
                ]
            ],
            "title": "TABLE I: A comparison of different approaches."
        },
        "insight": "Suppose we have [CONTINUE] model owners and P clients, as illustrated in Fig. 1, for every client we need to generate a joint key for this client and the group of [CONTINUE] model owners; that is, we will need to produce and maintain P joint keys. This also means that each model owner has to provide P copies of the encrypted model to the cloud. [CONTINUE] For our collaborative machine learning scenario, an MKHE solution will require each ciphertext to be extended to [CONTINUE] + 1 different keys (i.e., the model owners keys plus the key of the requesting client) before any computation. The efficiency of the system is affected especially if the number of model owners is large because it proportionally increases the ciphertext size. [CONTINUE] Also, we reduce the number of encrypted models from P to 1 and the dimension of the extended ciphertexts from (N + 1) \u00d7 2 to 2 \u00d7 2. [CONTINUE] Table [CONTINUE] provides a summary of our comparison. [CONTINUE] This approach also means that each model owner must prepare P encrypted copies of their models to the cloud, one for each distinct joint key. [CONTINUE] As an alternative, participants can encrypt their inputs under their individual keys, but extend them to additional keys using the MKHE scheme at evaluation. This way, the model owners delegate one encryption copy of their models to the cloud. When a client Ci requests an evaluation, each SWHE ciphertext c = (c0, c1) is extended to one in the form \u00afc = {cM1| . . . |cMN |cCi } \u2208 R2(N +1)q under the set of [CONTINUE] + 1 keys {pkM1 , . . . , pkMN , pkCi}, resulting in linear expansion of the ciphertext size. Our proposed approach leverages both the threshold and multi-key HE techniques. There is no need to set up a joint key for each client with the model owners, and ciphertexts are extended only under two different keys instead of [CONTINUE] + 1 keys resulting in a reduction in the ciphertext size. [CONTINUE] reduces the number of encryption of the same model, and (iii) reduces the ciphertext size with dimension reduction from (N + 1) \u00d7 2 to 2 \u00d7 2. We presented the detail design of this approach and analyzed the complexity and security."
    },
    {
        "id": "824",
        "table": {
            "header": [
                "Mechanism",
                "[ITALIC] \u03bb",
                "[ITALIC] \u03b7",
                "[ITALIC] \u03b5pos [m]",
                "[ITALIC] \u03b5ori [rad]"
            ],
            "rows": [
                [
                    "3-DoF",
                    "0",
                    "12.81%",
                    "8.76e-3\u00b18.25e-3",
                    "1.20e-2\u00b11.41e-2"
                ],
                [
                    "[EMPTY]",
                    "1",
                    "1.27%",
                    "1.38e-2\u00b12.67e-2",
                    "1.61e-2\u00b13.09e-2"
                ],
                [
                    "[EMPTY]",
                    "2",
                    "0.97%",
                    "1.42e-2\u00b12.60e-2",
                    "1.57e-2\u00b12.91e-2"
                ],
                [
                    "6-DoF",
                    "0",
                    "71.22%",
                    "8.39e-3\u00b16.80e-3",
                    "3.15e-2\u00b15.03e-2"
                ],
                [
                    "[EMPTY]",
                    "1",
                    "3.30%",
                    "3.50e-2\u00b14.57e-2",
                    "1.27e-1\u00b11.53e-1"
                ],
                [
                    "[EMPTY]",
                    "2",
                    "1.89%",
                    "3.72e-2\u00b15.28e-2",
                    "1.19e-1\u00b11.55e-1"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "1.16%",
                    "3.95e-2\u00b15.47e-2",
                    "1.45e-1\u00b12.22e-1"
                ],
                [
                    "15-DoF",
                    "0",
                    "100%",
                    "6.93e-3\u00b19.67e-3",
                    "1.80e-2\u00b13.46e-2"
                ],
                [
                    "[EMPTY]",
                    "1",
                    "6.12%",
                    "1.82e-2\u00b13.80e-2",
                    "5.56e-2\u00b11.47e-1"
                ],
                [
                    "[EMPTY]",
                    "2",
                    "3.02%",
                    "1.65e-2\u00b13.01e-2",
                    "4.95e-2\u00b11.20e-1"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "1.59%",
                    "1.31e-2\u00b12.67e-2",
                    "4.13e-2\u00b11.13e-1"
                ]
            ],
            "title": "TABLE II: Percentage of configurations with at least one infeasible joint angle \u03b7 and the mean and standard deviations of the position error \u03b5pos and orientation error \u03b5ori for different penalty factors \u03bb in DT. The number of training samples is 800 (3-DoF) or 32000 (6-DoF and 15-DoF)."
        },
        "insight": "Results are listed in Table II. We can see that DT yields lots of infeasible predictions if we do not take joint limits into account (\u03bb = 0). The more joints are used, [CONTINUE] the higher the chance to violate at least one joint limit. If the penalty factor \u03bb is increased, the percentage of infeasible predictions \u03b7 decreases to about 1% and for higher values of \u03bb potentially even further. The downside of this approach is a significant increase of the position and orientation errors. The highest increase is noticed for the 6-DoF mechanism where the errors increase approximately by a factor of four. This means that the sample efficiency of DT is worse if we consider joint limits as we need a much larger number of training samples"
    },
    {
        "id": "825",
        "table": {
            "header": [
                "Mecha- nism",
                "Test Set",
                "Analy- tical",
                "Numerical TRAC-IK",
                "DT (800)",
                "DT (6400)"
            ],
            "rows": [
                [
                    "3-DoF",
                    "Uniform",
                    "100%",
                    "100%",
                    "62.88%",
                    "98.08%"
                ],
                [
                    "[EMPTY]",
                    "Singular",
                    "100%",
                    "100%",
                    "61.81%",
                    "99.08%"
                ],
                [
                    "[EMPTY]",
                    "Nonsingular",
                    "100%",
                    "100%",
                    "59.17%",
                    "97.25%"
                ],
                [
                    "Mecha-",
                    "Test Set",
                    "Analy-",
                    "Numerical",
                    "DT",
                    "DT"
                ],
                [
                    "nism",
                    "[EMPTY]",
                    "tical",
                    "TRAC-IK",
                    "(32000)",
                    "(256000)"
                ],
                [
                    "6-DoF",
                    "Uniform",
                    "100%",
                    "98.44%",
                    "53.58%",
                    "94.53%"
                ],
                [
                    "[EMPTY]",
                    "Singular",
                    "100%",
                    "86.95%",
                    "52.27%",
                    "95.26%"
                ],
                [
                    "[EMPTY]",
                    "Nonsingular",
                    "100%",
                    "99.08%",
                    "51.58%",
                    "95.98%"
                ],
                [
                    "15-DoF",
                    "Uniform",
                    "-",
                    "93.23%",
                    "82.96%",
                    "97.73%"
                ],
                [
                    "[EMPTY]",
                    "Near-sing.",
                    "-",
                    "89.96%",
                    "78.21%",
                    "96.60%"
                ],
                [
                    "[EMPTY]",
                    "Nonsingular",
                    "-",
                    "95.34%",
                    "82.96%",
                    "98.12%"
                ]
            ],
            "title": "TABLE III: Solve rates for error thresholds \u03b5pos<0.01m and \u03b5ori<0.03rad and different test sets. Numbers in brackets below DT denote the number of training samples."
        },
        "insight": "Results are shown in Table III. The analytical solution is exact. [CONTINUE] While there occur no failures for the simple planar 3-DoF mechanism, solve rates are clearly lower for singular configurations with the 6- and 15-DoF manipulators. On the dataset from which near-singular configurations were removed we also see an increase of the overall solve rate. DT seems to be more robust against singularities. While there are slight deviations in the solve rate, no clear trend can be seen so that we assume that these differences are not caused by singularities."
    },
    {
        "id": "826",
        "table": {
            "header": [
                "Method",
                "LEN-CHANGE NLL",
                "LEN-CHANGE MSE",
                "VAR-CHANGE NLL",
                "VAR-CHANGE MSE",
                "Gazebo:Env1 NLL",
                "Gazebo:Env1 MSE",
                "Gazebo:Env2 NLL",
                "Gazebo:Env2 MSE",
                "Gazebo:Env3 NLL",
                "Gazebo:Env3 MSE"
            ],
            "rows": [
                [
                    "BOCPD",
                    "0.99\u00b10.38",
                    "0.47\u00b10.32",
                    "1.13\u00b10.61",
                    "0.55\u00b10.58",
                    "2.07\u00b10.51",
                    "0.14\u00b10.05",
                    "2.24\u00b10.48",
                    "0.57\u00b10.26",
                    "0.28\u00b10.12",
                    "0.11\u00b10.03"
                ],
                [
                    "CS-BOCPD",
                    "0.95\u00b10.32",
                    "0.43\u00b10.30",
                    "1.05\u00b10.42",
                    "0.48\u00b10.40",
                    "-0.11\u00b10.33",
                    "0.12\u00b10.04",
                    "1.82\u00b11.51",
                    "0.55\u00b10.27",
                    "0.25\u00b10.16",
                    "0.10\u00b10.02"
                ],
                [
                    "CBOCPD",
                    "[BOLD] 0.79\u00b10.33",
                    "[BOLD] 0.34\u00b10.24",
                    "[BOLD] 0.89\u00b10.36",
                    "[BOLD] 0.41\u00b10.34",
                    "[BOLD] -0.31\u00b10.34",
                    "[BOLD] 0.11\u00b10.04",
                    "[BOLD] 0.69\u00b10.36",
                    "[BOLD] 0.45\u00b10.19",
                    "[BOLD] -0.99\u00b10.47",
                    "0.10\u00b10.04"
                ]
            ],
            "title": "Table 1: Comparison of BOCPD, CS-BOCPD, and CBOCPD with NLL and MSE on synthetic and Gazebo robot simulation datasets."
        },
        "insight": "We compare the proposed CBOCPD with BOCPD and CS (CUSUM)-BOCPD. [CONTINUE] Table 1: Comparison of BOCPD, CS-BOCPD, and CBOCPD with NLL and MSE on synthetic and Gazebo robot simulation datasets. [CONTINUE] The last three columns in Table 1 show that CBOCPD outperforms other methods in all environments. Interestingly, the results show that CUSUM test does not help BOCPD much when the variance decreases as in Env3 but proposed likelihood ratio test improves BOCPD in all the cases. [CONTINUE] The first two columns in Table 1 present the quantitative results from 100 runs of each algorithm after training on the first 100 points and testing on the remaining 300 points. CBOCPD outperforms BOCPD in terms of both negative log-likelihood (NLL) and mean squared error (MSE)."
    },
    {
        "id": "827",
        "table": {
            "header": [
                "Method Nile Data (200 training points, 463 test points)",
                "NLL Nile Data (200 training points, 463 test points)",
                "p-value Nile Data (200 training points, 463 test points)",
                "MSE Nile Data (200 training points, 463 test points)",
                "p-value Nile Data (200 training points, 463 test points)"
            ],
            "rows": [
                [
                    "ARGP",
                    "1.07\u00b10.64",
                    "<0.0001",
                    "5.06\u00b10.86",
                    "0.0005"
                ],
                [
                    "ARGP-BOCPD",
                    "0.78\u00b10.72",
                    "<0.0001",
                    "4.94\u00b10.87",
                    "0.0017"
                ],
                [
                    "GPTS",
                    "0.86\u00b10.64",
                    "<0.0001",
                    "4.78\u00b10.81",
                    "0.0100"
                ],
                [
                    "BOCPD",
                    "0.57\u00b10.77",
                    "0.0014",
                    "4.73\u00b10.82",
                    "0.0115"
                ],
                [
                    "CBOCPD",
                    "[BOLD] 0.00\u00b10.80",
                    "[EMPTY]",
                    "[BOLD] 4.32\u00b10.74",
                    "[EMPTY]"
                ],
                [
                    "Well Log Data (500 training points, 3050 test points)",
                    "Well Log Data (500 training points, 3050 test points)",
                    "Well Log Data (500 training points, 3050 test points)",
                    "Well Log Data (500 training points, 3050 test points)",
                    "Well Log Data (500 training points, 3050 test points)"
                ],
                [
                    "ARGP",
                    "7.20\u00b10.60",
                    "<0.0001",
                    "17.3\u00b12.6",
                    "<0.0001"
                ],
                [
                    "ARGP-BOCPD",
                    "[BOLD] 0.00\u00b10.30",
                    "[EMPTY]",
                    "[BOLD] 4.68\u00b10.46",
                    "[EMPTY]"
                ],
                [
                    "GPTS",
                    "3.73\u00b10.42",
                    "<0.0001",
                    "8.27\u00b10.61",
                    "<0.0001"
                ],
                [
                    "BOCPD",
                    "4.35\u00b10.31",
                    "<0.0001",
                    "19.2\u00b11.3",
                    "<0.0001"
                ],
                [
                    "CBOCPD",
                    "0.30\u00b10.27",
                    "0.0010",
                    "4.92\u00b10.44",
                    "0.2124"
                ],
                [
                    "Snow Data (500 training points, 13380 test points)",
                    "Snow Data (500 training points, 13380 test points)",
                    "Snow Data (500 training points, 13380 test points)",
                    "Snow Data (500 training points, 13380 test points)",
                    "Snow Data (500 training points, 13380 test points)"
                ],
                [
                    "ARGP",
                    "17.48\u00b10.82",
                    "<0.0001",
                    "14.82\u00b10.57",
                    "<0.0001"
                ],
                [
                    "ARGP-BOCPD",
                    "0.06\u00b10.39",
                    "<0.0001",
                    "9.65\u00b10.39",
                    "<0.0001"
                ],
                [
                    "GPTS",
                    "16.60\u00b10.22",
                    "<0.0001",
                    "8.76\u00b10.36",
                    "<0.0001"
                ],
                [
                    "BOCPD",
                    "[BOLD] 0.00\u00b10.39",
                    "[EMPTY]",
                    "9.43\u00b10.38",
                    "[EMPTY]"
                ],
                [
                    "CBOCPD",
                    "1.92\u00b10.37",
                    "<0.0001",
                    "[BOLD] 6.34\u00b10.27",
                    "<0.0001"
                ]
            ],
            "title": "Table 2: Results of predictive performance on Nile data, Well Log Data and Snow Data. The results are provided with 95% of confidence interval and the p-value of the null hypothesis that a method is equivalent to the best performing method according to NLL, using a one sided t-test."
        },
        "insight": "Table 2: Results of predictive performance on Nile data, Well Log Data and Snow Data. The results are provided with 95% of confidence interval and the p-value of the null hypothesis that a method is equivalent to the best performing method according to NLL, using a one sided t-test. [CONTINUE] Further, we compare our proposed algorithm with autoregressive GP (ARGP), autoregressive GP with BOCPD (ARGPBOCPD), GP time series with BOCPD (BOCPD) and GP time series model (GPTS). [CONTINUE] Table 2 shows the predictive performance of our proposed algorithm compared to other GP based BOCPD methods. In Nile data, we see that combining BOCPD method with GP improves the performance. CBOCPD further improves the performance by leveraging the proposed statistical hypothesis tests and outperforms all other algorithms. In Well Log data, the nonlinear temporal correlations within each regime give a slight advantage to underlying predictive model of ARGP. However, CBOCPD still shows the competitive result. In Snowfall data, CBOCPD well detects the difference in noise levels and achieves the best performance in MSE."
    },
    {
        "id": "828",
        "table": {
            "header": [
                "model",
                "RMS",
                "average",
                "std. dev.",
                "max"
            ],
            "rows": [
                [
                    "MLP",
                    "0.76",
                    "-0.29",
                    "0.70",
                    "-4.94"
                ],
                [
                    "CNN",
                    "0.60",
                    "-0.39",
                    "0.46",
                    "-2.33"
                ]
            ],
            "title": "Table 2: Comparison of the longitudinal performances of the MLP and CNN controllers (in m/s)."
        },
        "insight": "The control signals of the CNN model remains always smooth and within a reasonable range of values. Secondly, both the longitudinal errors are smaller for the CNN than the MLP as shown in Table 2."
    },
    {
        "id": "829",
        "table": {
            "header": [
                "Method",
                "Params(M)",
                "CIFAR10",
                "CIFAR10+"
            ],
            "rows": [
                [
                    "EM-Softmax ",
                    "15.2",
                    "-",
                    "6.69"
                ],
                [
                    "Maxout ",
                    "-",
                    "9.38",
                    "-"
                ],
                [
                    "All-CNN ",
                    "1.3",
                    "9.08",
                    "7.25"
                ],
                [
                    "Softmax",
                    "11.22",
                    "13.16\u00b10.31",
                    "5.73\u00b10.22"
                ],
                [
                    "Center",
                    "11.22",
                    "12.22\u00b10.45",
                    "5.40\u00b10.21"
                ],
                [
                    "[BOLD] Arc",
                    "11.22",
                    "[BOLD] 11.77\u00b10.31",
                    "[BOLD] 5.23\u00b10.19"
                ],
                [
                    "ArcFace",
                    "11.22",
                    "12.11\u00b10.45",
                    "5.34\u00b10.12"
                ]
            ],
            "title": "Table 6: Recognition error rate (mean\u00b1std%) on CIFAR10 without data augmentation and CIFAR10+ with data augmentation. Every result is evaluated five times."
        },
        "insight": "Table 6 and 7 reveal Arc is the best and ArcFace is better."
    },
    {
        "id": "830",
        "table": {
            "header": [
                "[BOLD] City",
                "[BOLD] Venues",
                "[BOLD] Unique Movements"
            ],
            "rows": [
                [
                    "Chicago",
                    "13,904",
                    "5,396,723"
                ],
                [
                    "New York",
                    "32,971",
                    "5,296,809"
                ]
            ],
            "title": "TABLE I: Number of Venues and Venue Movements for each city"
        },
        "insight": "The check-in information describes an aggregated count of all movements from one venue to another, separated by month and five time intervals: Morning, Midday, Afternoon, Night and Overnight. In the collected data, we focus on the two cities mentioned above for the year of 2018. The aggregated number of venues and number of total venue movements are summarised in Table I."
    },
    {
        "id": "831",
        "table": {
            "header": [
                "Feature Name",
                "Chicago Morning",
                "Chicago Midday",
                "Chicago A'noon",
                "Chicago Night",
                "Chicago O\u2019Night",
                "New York Morning",
                "New York Midday",
                "New York A'noon",
                "New York Night",
                "New York O\u2019Night"
            ],
            "rows": [
                [
                    "Check-in Entropy",
                    "[BOLD] 0.365",
                    "[BOLD] 0.277",
                    "[BOLD] 0.273",
                    "[BOLD] 0.360",
                    "0.326",
                    "[BOLD] 0.387",
                    "[BOLD] 0.264",
                    "[BOLD] 0.304",
                    "[BOLD]  0.300",
                    "0.250"
                ],
                [
                    "In Check-in Density",
                    "0.025",
                    "0.043",
                    "0.031",
                    "0.006",
                    "-0.011",
                    "0.109",
                    "0.059",
                    "0.035",
                    "0.178",
                    "0.021"
                ],
                [
                    "Out Check-in Density",
                    "0.051",
                    "0.006",
                    "0.027",
                    "0.048",
                    "-0.022",
                    "0.088",
                    "-0.025",
                    "0.075",
                    "0.055",
                    "0.017"
                ],
                [
                    "Stationary Density",
                    "-0.064",
                    "-0.047",
                    "-0.056",
                    "-0.047",
                    "0.025",
                    "-0.158",
                    "-0.025",
                    "-0.089",
                    "-0.157",
                    "-0.027"
                ],
                [
                    "Mean",
                    "0.133",
                    "0.025",
                    "0.021",
                    "0.088",
                    "0.362",
                    "0.058",
                    "0.022",
                    "0.053",
                    "0.158",
                    "0.352"
                ],
                [
                    "Median",
                    "0.124",
                    "0.015",
                    "0.015",
                    "0.079",
                    "0.348",
                    "-0.007",
                    "-0.026",
                    "0.017",
                    "0.112",
                    "0.336"
                ],
                [
                    "Risk Count",
                    "[BOLD] 0.210",
                    "0.088",
                    "0.077",
                    "0.095",
                    "[BOLD] 0.611",
                    "[BOLD] 0.402",
                    "[BOLD] 0.424",
                    "[BOLD] 0.512",
                    "0.282",
                    "[BOLD] 0.421"
                ],
                [
                    "Risk Ratio",
                    "0.113",
                    "-0.010",
                    "0.002",
                    "0.042",
                    "0.326",
                    "-0.018",
                    "-0.036",
                    "0.000",
                    "0.037",
                    "0.249"
                ],
                [
                    "Self Risk",
                    "0.154",
                    "0.095",
                    "0.070",
                    "0.116",
                    "[BOLD] 0.422",
                    "0.113",
                    "0.050",
                    "0.076",
                    "0.225",
                    "[BOLD] 0.355"
                ],
                [
                    "Neighbourhood Crimes",
                    "0.001",
                    "0.039",
                    "-0.026",
                    "0.035",
                    "0.322",
                    "0.357",
                    "0.074",
                    "0.053",
                    "[BOLD] 0.309",
                    "0.040"
                ],
                [
                    "Historical Density",
                    "[BOLD] 0.637",
                    "[BOLD] 0.867",
                    "[BOLD] 0.872",
                    "[BOLD] 0.671",
                    "[BOLD] 0.794",
                    "[BOLD] 0.936",
                    "[BOLD] 0.983",
                    "[BOLD] 0.723",
                    "[BOLD] 0.961",
                    "[BOLD] 0.921"
                ],
                [
                    "Arts & Entertainment",
                    "-0.018",
                    "-0.030",
                    "-0.021",
                    "0.005",
                    "0.084",
                    "0.161",
                    "0.137",
                    "0.174",
                    "0.149",
                    "0.055"
                ],
                [
                    "College & University",
                    "-0.072",
                    "-0.063",
                    "-0.054",
                    "-0.042",
                    "0.041",
                    "0.054",
                    "0.058",
                    "0.066",
                    "0.049",
                    "0.255"
                ],
                [
                    "Event",
                    "-0.059",
                    "-0.023",
                    "-0.012",
                    "-0.009",
                    "0.031",
                    "-0.02",
                    "-0.028",
                    "-0.024",
                    "-0.035",
                    "-0.067"
                ],
                [
                    "Food",
                    "0.175",
                    "[BOLD] 0.116",
                    "[BOLD] 0.119",
                    "[BOLD] 0.228",
                    "0.262",
                    "0.034",
                    "0.016",
                    "0.018",
                    "0.031",
                    "0.233"
                ],
                [
                    "Nightlife Spot",
                    "0.047",
                    "0.007",
                    "-0.009",
                    "0.120",
                    "0.351",
                    "0.086",
                    "0.149",
                    "0.146",
                    "0.153",
                    "0.221"
                ],
                [
                    "Outdoors & Rec.",
                    "-0.072",
                    "-0.087",
                    "-0.077",
                    "-0.117",
                    "-0.095",
                    "-0.128",
                    "-0.128",
                    "-0.131",
                    "-0.128",
                    "-0.127"
                ],
                [
                    "Professional & Other",
                    "-0.038",
                    "0.017",
                    "0.007",
                    "-0.061",
                    "-0.123",
                    "0.107",
                    "0.077",
                    "0.148",
                    "0.105",
                    "-0.052"
                ],
                [
                    "Residence",
                    "0.068",
                    "0.016",
                    "-0.010",
                    "-0.021",
                    "0.020",
                    "0.180",
                    "0.183",
                    "0.154",
                    "0.185",
                    "0.158"
                ],
                [
                    "Shops & Service",
                    "0.050",
                    "0.031",
                    "0.047",
                    "0.077",
                    "0.162",
                    "-0.045",
                    "0.022",
                    "0.019",
                    "-0.031",
                    "0.104"
                ],
                [
                    "Travel & Transport",
                    "-0.086",
                    "-0.056",
                    "-0.059",
                    "-0.165",
                    "-0.269",
                    "-0.06",
                    "-0.103",
                    "-0.122",
                    "-0.087",
                    "-0.276"
                ],
                [
                    "Total POI",
                    "[BOLD] 0.517",
                    "[BOLD] 0.651",
                    "[BOLD] 0.631",
                    "[BOLD] 0.638",
                    "[BOLD] 0.451",
                    "[BOLD] 0.509",
                    "[BOLD] 0.655",
                    "[BOLD] 0.673",
                    "[BOLD] 0.586",
                    "[BOLD] 0.524"
                ],
                [
                    "Venue Diversity",
                    "[BOLD] 0.318",
                    "[BOLD] 0.287",
                    "[BOLD] 0.255",
                    "[BOLD] 0.366",
                    "[BOLD] 0.428",
                    "[BOLD] 0.410",
                    "[BOLD] 0.367",
                    "[BOLD] 0.356",
                    "[BOLD] 0.375",
                    "[BOLD] 0.361"
                ]
            ],
            "title": "TABLE II: Feature Correlation Analysis for the New York and Chicago"
        },
        "insight": "The correlation values between the features and crime count are illustrated in Table II for Chicago and New York City across different periods of a day. We observe that crime count is highly correlated with Crime Event History in both cities over many time intervals. According to the Near Repeat theory , crime tends to happen in the vicinity of past crime events. The features derived from region risk analysis also have good correlation with crime count for both cities, especially overnight. Such positive correlation highlights the importance of such features in crime count. Surprisingly, many POI densities are negatively correlated with crime count. However, venue diversity has good correlation with crime which verifies that mixed land use has good impact on crime."
    },
    {
        "id": "832",
        "table": {
            "header": [
                "[BOLD] Time of Day",
                "[BOLD] Error",
                "[BOLD] Chicago 1",
                "[BOLD] Chicago 2",
                "[BOLD] New York 1",
                "[BOLD] New York 2"
            ],
            "rows": [
                [
                    "Morning",
                    "MAE",
                    "1.068",
                    "1.082",
                    "7.755",
                    "7.649"
                ],
                [
                    "Morning",
                    "RMSE",
                    "1.557",
                    "1.490",
                    "11.109",
                    "10.736"
                ],
                [
                    "Midday",
                    "MAE",
                    "1.764",
                    "1.817",
                    "12.376",
                    "12.399"
                ],
                [
                    "Midday",
                    "RMSE",
                    "2.791",
                    "2.792",
                    "16.362",
                    "15.764"
                ],
                [
                    "Afternoon",
                    "MAE",
                    "1.846",
                    "1.850",
                    "21.287",
                    "22.021"
                ],
                [
                    "Afternoon",
                    "RMSE",
                    "3.074",
                    "3.046",
                    "42.552",
                    "44.461"
                ],
                [
                    "Night",
                    "MAE",
                    "1.961",
                    "1.997",
                    "12.628",
                    "13.986"
                ],
                [
                    "Night",
                    "RMSE",
                    "2.946",
                    "2.966",
                    "16.558",
                    "17.515"
                ],
                [
                    "Overnight",
                    "MAE",
                    "1.821",
                    "2.029",
                    "13.390",
                    "18.104"
                ],
                [
                    "Overnight",
                    "RMSE",
                    "2.772",
                    "3.061",
                    "17.509",
                    "21.933"
                ],
                [
                    "1. All Features Present",
                    "1. All Features Present",
                    "1. All Features Present",
                    "1. All Features Present",
                    "1. All Features Present",
                    "1. All Features Present"
                ],
                [
                    "2.  [ITALIC] Region Risk Features Omitted",
                    "2.  [ITALIC] Region Risk Features Omitted",
                    "2.  [ITALIC] Region Risk Features Omitted",
                    "2.  [ITALIC] Region Risk Features Omitted",
                    "2.  [ITALIC] Region Risk Features Omitted",
                    "2.  [ITALIC] Region Risk Features Omitted"
                ]
            ],
            "title": "TABLE III: MAE and RMSE for different times of the day for Chicago and New York after applying Linear Regression"
        },
        "insight": "The experimental results using different regression algorithms including LR, NB and RF are shown in tables III to V respectively. The experimental results show that the model with Region Risk based features has less error compare to the model without Region Risk based features in most of the time intervals. Among three different regression algorithm, RF shows the consistent and better performance result."
    },
    {
        "id": "833",
        "table": {
            "header": [
                "[BOLD] Dataset %",
                "[BOLD] Office Products & Movies\u00a0and\u00a0TV 80",
                "[BOLD] Office Products & Movies\u00a0and\u00a0TV 90",
                "[BOLD] Sports\u00a0and\u00a0Outdoors & CDs\u00a0and\u00a0Vinyl 80",
                "[BOLD] Sports\u00a0and\u00a0Outdoors & CDs\u00a0and\u00a0Vinyl 90",
                "[BOLD] Android\u00a0Apps & Video\u00a0Games 80",
                "[BOLD] Android\u00a0Apps & Video\u00a0Games 90",
                "[BOLD] Toys\u00a0and\u00a0Games & Automotive 80",
                "[BOLD] Toys\u00a0and\u00a0Games & Automotive 90"
            ],
            "rows": [
                [
                    "CMF",
                    "1.0825",
                    "1.0583",
                    "2.1043",
                    "2.0578",
                    "2.0784",
                    "2.1295",
                    "2.2636",
                    "2.1855"
                ],
                [
                    "CDTF",
                    "1.0577",
                    "1.0389",
                    "2.0702",
                    "2.0126",
                    "2.0461",
                    "1.9965",
                    "2.2234",
                    "2.1680"
                ],
                [
                    "FM-CDCF",
                    "1.0272",
                    "0.9925",
                    "2.0580",
                    "1.9942",
                    "1.9963",
                    "1.9722",
                    "2.1908",
                    "2.1534"
                ],
                [
                    "CoNet",
                    "0.9782",
                    "0.9689",
                    "1.9835",
                    "1.9587",
                    "1.9744",
                    "1.9562",
                    "2.1507",
                    "2.1356"
                ],
                [
                    "U-DARec",
                    "0.9985",
                    "0.9821",
                    "2.0123",
                    "1.9808",
                    "1.9876",
                    "1.9550",
                    "2.1734",
                    "2.1405"
                ],
                [
                    "[BOLD] I-DARec",
                    "[BOLD] 0.9665",
                    "[BOLD] 0.9582",
                    "[BOLD] 1.9673",
                    "[BOLD] 1.9408",
                    "[BOLD] 1.9546",
                    "[BOLD] 1.9318",
                    "[BOLD] 2.1152",
                    "[BOLD] 2.0723"
                ]
            ],
            "title": "Table 2: Comparison with Baselines (%: percentage of training data)"
        },
        "insight": "Table 2 reports the comparison results among the selected baselines for CDR. Note that in each domain pair, the chosen categories are as irrelevant as possible, so that we can better examine the performance of our model on learning and transfering knowledge from the rating matrix only. It is obvious that DL-based approaches (i.e., CoNet and DARec) are superior to non-deep models. This can be explained by the [CONTINUE] ability of nonlinearity modelling from the deep neural networks. Also, MF-based mehtods perform better when the source domain dataset is relatively dense, but the datasets we consider in this work are of high sparsity (over 99.8%). For example, in the Office Products and Movies & TV dataset with 90% training data, CoNet gains 2.38% RMSE improvement over FM-CDCF and I-DARec achieves 3.45% over FMCDCF. This result shows that DL-based methods are more suitable for extreme sparse datasets, which is in consistency with the findings in [Hu, 2018]. Furthermore, we notice that CoNet performs better than U-DARec as U-DARec adopts adversarial training to balance the extraction of shared user rating patterns and the pursuit of high rating prediction accuracy. This weakness is overcome by I-DARec which minimizes the losses of domain classifier and rating predictor at the same time. Thus, as we can see, I-DARec achieves the best performance among the baselines."
    },
    {
        "id": "834",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Loudness ( [ITALIC] L1)",
                "[BOLD] F0 ( [ITALIC] L1)",
                "[BOLD] F0 Outliers"
            ],
            "rows": [
                [
                    "[BOLD] Supervised",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "WaveRNN\u00a0(Hantrakul et\u00a0al.,  2019 )",
                    "0.10",
                    "1.00",
                    "0.07"
                ],
                [
                    "DDSP Autoencoder",
                    "[BOLD] 0.07",
                    "[BOLD] 0.02",
                    "[BOLD] 0.003"
                ],
                [
                    "[BOLD] Unsupervised",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "DDSP Autoencoder",
                    "0.09",
                    "0.80",
                    "0.04"
                ]
            ],
            "title": "Table 1: Resynthesis accuracies. Comparison of DDSP models to SOTA WaveRNN model provided the same conditioning information. The supervised DDSP Autoencoder and WaveRNN models use the fundamental frequency from a pretrained CREPE model, while the unsupervised DDSP autoencoder learns to infer the frequency from the audio during training."
        },
        "insight": "Table 1: Resynthesis accuracies. Comparison of DDSP models to SOTA WaveRNN model provided the same conditioning information. The supervised DDSP Autoencoder and WaveRNN models use the fundamental frequency from a pretrained CREPE model, while the unsupervised DDSP autoencoder learns to infer the frequency from the audio during training. [CONTINUE] For the NSynth dataset, we quantitatively compare the quality of DDSP resynthesis with that of a state-of-the-art baseline using WaveRNN (Hantrakul et al., 2019). The models are comparable as they are trained on the same data, provided the same conditioning, and both targeted towards realtime synthesis applications. In Table 1, we compute loudness and fundamental frequency (F0) L1 metrics described in Section [CONTINUE] of the appendix. Despite the strong performance of the baseline, the supervised DDSP autoencoder still outperforms it, especially in F0 L1. This is not unexpected, as the additive synthesizer directly uses the conditioning frequency to synthesize audio. The unsupervised DDSP autoencoder must learn to infer its own F0 conditioning signal directly from the audio. As described in Section B.4, we improve optimization by also adding a perceptual loss in the form of a pretrained CREPE network (Kim et al., 2018). While not as accurate as the supervised DDSP version, the model does a fair job at learning to generate sounds with the correct frequencies without supervision, outperforming the supervised WaveRNN model."
    },
    {
        "id": "835",
        "table": {
            "header": [
                "[EMPTY]",
                "NMF",
                "RPCA",
                "KAM",
                "Proposed DAP"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "(spiertz2009source__)",
                    "(huang2012singing)",
                    "yela2018does",
                    "[EMPTY]"
                ],
                [
                    "SDR",
                    "-6.369",
                    "-5.395",
                    "-3.375",
                    "[BOLD] -1.581"
                ],
                [
                    "SIR",
                    "-1.934",
                    "-0.227",
                    "-0.753",
                    "[BOLD] 3.859"
                ],
                [
                    "LSD",
                    "2.301",
                    "2.011",
                    "2.321",
                    "[BOLD] 1.959"
                ]
            ],
            "title": "Table 1: Comparison between DAP/NMF/RPCA/KAM on numerical metrics: SDR/SIR/LSD. For SDR/SIR, higher is better and for LSD, smaller is better. The best results on the three metrics are from our method, DAP."
        },
        "insight": "DAP outperforms these three BSS methods in all three numerical metrics, as shown in Table 1."
    },
    {
        "id": "836",
        "table": {
            "header": [
                "Methods",
                "Number of raw images for averaging 1",
                "Number of raw images for averaging 2",
                "Number of raw images for averaging 4",
                "Number of raw images for averaging 8",
                "Number of raw images for averaging 16",
                "Time"
            ],
            "rows": [
                [
                    "Raw",
                    "27.22\u00a0/\u00a00.5442",
                    "30.08\u00a0/\u00a00.6800",
                    "32.86\u00a0/\u00a00.7981",
                    "36.03\u00a0/\u00a00.8892",
                    "39.70\u00a0/\u00a00.9487",
                    "-"
                ],
                [
                    "VST+NLM ",
                    "31.25\u00a0/\u00a00.7503",
                    "32.85\u00a0/\u00a00.8116",
                    "34.92\u00a0/\u00a00.8763",
                    "37.09\u00a0/\u00a00.9208",
                    "40.04\u00a0/\u00a00.9540",
                    "137.10 s"
                ],
                [
                    "VST+BM3D ",
                    "32.71\u00a0/\u00a00.7922",
                    "34.09\u00a0/\u00a00.8430",
                    "36.05\u00a0/\u00a00.8970",
                    "38.01\u00a0/\u00a00.9336",
                    "40.61\u00a0/\u00a00.9598",
                    "5.67 s"
                ],
                [
                    "VST+KSVD ",
                    "32.02\u00a0/\u00a00.7746",
                    "33.69\u00a0/\u00a00.8327",
                    "35.84\u00a0/\u00a00.8933",
                    "37.79\u00a0/\u00a00.9314",
                    "40.36\u00a0/\u00a00.9585",
                    "341.21 s"
                ],
                [
                    "VST+KSVD(D) ",
                    "31.77\u00a0/\u00a00.7712",
                    "33.45\u00a0/\u00a00.8292",
                    "35.67\u00a0/\u00a00.8908",
                    "37.69\u00a0/\u00a00.9300",
                    "40.32\u00a0/\u00a00.9579",
                    "67.96 s"
                ],
                [
                    "VST+KSVD(G) ",
                    "31.98\u00a0/\u00a00.7752",
                    "33.64\u00a0/\u00a00.8327",
                    "35.83\u00a0/\u00a00.8930",
                    "37.82\u00a0/\u00a00.9312",
                    "40.44\u00a0/\u00a00.9584",
                    "58.82 s"
                ],
                [
                    "VST+EPLL ",
                    "32.61\u00a0/\u00a00.7876",
                    "34.07\u00a0/\u00a00.8414",
                    "36.08\u00a0/\u00a00.8970",
                    "38.12\u00a0/\u00a00.9349",
                    "40.83\u00a0/\u00a00.9618",
                    "288.63 s"
                ],
                [
                    "VST+WNNM ",
                    "32.52\u00a0/\u00a00.7880",
                    "34.04\u00a0/\u00a00.8419",
                    "36.04\u00a0/\u00a00.8973",
                    "37.95\u00a0/\u00a00.9334",
                    "40.45\u00a0/\u00a00.9587",
                    "451.89 s"
                ],
                [
                    "PURE-LET ",
                    "31.95\u00a0/\u00a00.7664",
                    "33.49\u00a0/\u00a00.8270",
                    "35.29\u00a0/\u00a00.8814",
                    "37.25\u00a0/\u00a00.9212",
                    "39.59\u00a0/\u00a00.9450",
                    "[BOLD] 2.61 s"
                ],
                [
                    "DnCNN ",
                    "34.88\u00a0/\u00a00.9063",
                    "36.02\u00a0/\u00a0 [BOLD] 0.9257",
                    "37.57\u00a0/\u00a00.9460",
                    "39.28\u00a0/\u00a00.9588",
                    "[BOLD] 41.57\u00a0/\u00a00.9721",
                    "3.07 s\u2020"
                ],
                [
                    "Noise2Noise ",
                    "[BOLD] 35.40\u00a0/\u00a0 [BOLD] 0.9187",
                    "[BOLD] 36.40\u00a0/\u00a00.9230",
                    "[BOLD] 37.59\u00a0/\u00a00.9481",
                    "[BOLD] 39.43\u00a0/\u00a00.9601",
                    "41.45\u00a0/\u00a0 [BOLD] 0.9724",
                    "2.94 s\u2020"
                ]
            ],
            "title": "Table 2: Denoising performance using the mixed test set, which includes confocal, two-photon, and wide-field microscopy images. PSNR (dB), SSIM, and denoising time (seconds) are obtained by averaging over 48 noise realizations in the mixed test set for each of 5 noise levels. Results of DnCNN and Noise2Noise are obtained by training on dataset with all noise levels. All 50 captures of each FOV (except the 19-th FOV which is reserved for test) are included in the training set, with 1 (DnCNN) or 2 (Noise2Noise) samples of which randomly selected from each FOV when forming mini-batches during training for 400 epochs. \u2020Note that test time for deep learning models on GPU is faster in orders of magnitude, i.e. 0.62 ms for DnCNN and 0.99 ms for Noise2Noise on single GPU in our experiment."
        },
        "insight": "The benchmark denoising results on the mixed test set is shown in Table 2, including PSNR, structural similarity index (SSIM)  and denoising time. From the table, BM3D (in combination with VST) is still the most versatile traditional denoising algorithm regarding its high PSNR and relatively fast denoising speed. PURE-LET, though its PSNR is not the highest, is the fastest denoising method among all the benchmarked algorithms thanks to its specific design for Poisson-Gaussian denoising. Finally, deep learning models outperform the other 8 methods by a significant margin in all noise levels, both in terms of PSNR and SSIM, even thought they are blind to noise levels. This is different from the observation made before in [1, 24], probably because the nature of Poisson dominated noise is different from Gaussian noise while most of the denoising methods are developed for Gaussian noise model. Even if we applied the VST before Gaussian denoising, the transformed noise may still be different from a pure Gaussian one. More importantly, here the models are re-trained with our FMD dataset instead of pre-trained on other datasets."
    },
    {
        "id": "837",
        "table": {
            "header": [
                "[EMPTY]",
                "DAP vs. NMF",
                "DAP vs. RPCA",
                "DAP vs. KAM"
            ],
            "rows": [
                [
                    "SDR",
                    "75.3",
                    "66.0",
                    "56.0"
                ],
                [
                    "SIR",
                    "76.7",
                    "62.7",
                    "66.7"
                ],
                [
                    "LSD",
                    "71.4",
                    "58.0",
                    "71.4"
                ]
            ],
            "title": "Table 2: Better sample ratio (%) comparing with NMF/RPCA/KAM in terms of numerical metrics: SDR/SIR/LSD on Universal-150. All results are larger than 50%, which shows that our DAP can achieve better results than the compared methods on a majority of testing samples."
        },
        "insight": "For example, compared to the NMF in terms of SDR, the ratio is 0.753, which shows that our method is better than the NMF on 75.3% testing samples. The comparison results are shown in Table 2. We can see that our DAP achieve better results on a majority of testing samples comparing to NMF/RPCA/KAM."
    },
    {
        "id": "838",
        "table": {
            "header": [
                "[EMPTY]",
                "NMF",
                "RPCA",
                "KAM",
                "Proposed DAP"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "(spiertz2009source__)",
                    "(huang2012singing)",
                    "yela2018does",
                    "[EMPTY]"
                ],
                [
                    "SDR",
                    "-3.36",
                    "-4.49",
                    "-2.16",
                    "[BOLD] -1.37"
                ],
                [
                    "SIR",
                    "-0.93",
                    "-1.00",
                    "-0.32",
                    "[BOLD] 0.82"
                ],
                [
                    "LSD",
                    "0.56",
                    "0.57",
                    "2.19",
                    "[BOLD] 0.48"
                ]
            ],
            "title": "Table 3: Comparison between DAP/NMF/RPCA/KAM in terms of numerical metrics: SDR/SIR/LSD on a standard source separation benchmark\u00a0(vincent2007oracle). For SDR/SIR, higher is better and for LSD, smaller is better. The best results on the three metrics are from our method, DAP."
        },
        "insight": "As shown in Table 3, We can see that our method outperforms the three compared methods in terms of SDR, SIR, and LSD."
    },
    {
        "id": "839",
        "table": {
            "header": [
                "[EMPTY]",
                "DAP vs. NMF",
                "DAP vs. RPCA",
                "DAP vs. KAM"
            ],
            "rows": [
                [
                    "SDR",
                    "60.0",
                    "70.0",
                    "50.0"
                ],
                [
                    "SIR",
                    "55.0",
                    "40.0",
                    "55.0"
                ],
                [
                    "LSD",
                    "70.0",
                    "65.0",
                    "95.0"
                ]
            ],
            "title": "Table 4: Better sample ratio (%) comparing with NMF/RPCA/KAM in terms of numerical metrics: SDR/SIR/LSD on the source separation benchmark\u00a0(vincent2007oracle). Most ratio results are larger than 50%, which shows that our DAP can achieve overall better results than the compared methods."
        },
        "insight": "The Table 4 illustrates better ratio results. We can also find that our DAP still achieves overall better results."
    },
    {
        "id": "840",
        "table": {
            "header": [
                "Model",
                "Network Prediction",
                "Evaluation Ground Truth",
                "[ITALIC] Fmean",
                "MR"
            ],
            "rows": [
                [
                    "CNN ",
                    "Single activity label",
                    "Last sample\u2019s label",
                    "0.890",
                    "87.4%"
                ],
                [
                    "CNN ",
                    "Single activity label",
                    "Actual labels",
                    "0.793",
                    "54.7%"
                ]
            ],
            "title": "TABLE I: Performance evaluation of the baseline CNN architecture [2] trained with multi-class formulated objective against both the approximated ground truth (equivalent to the last observed sample annotation) as well as the actual ground-truth for Opportunity dataset."
        },
        "insight": "In Table I, we report performance of the resulting HAR system by comparing the generated predictions against both the approximate ground truth (obtained from the last observed sample annotation) as well as the actual multi-label ground truth for Opportunity dataset. [CONTINUE] In Table I, the lower performance measures obtained from the evaluation against the actual ground truth labels as compared with the approximated ground truth suggest that there are sensory segments in the HAR dataset that convey measurements of multiple activities in the time span of the sliding window\u2014see the result for MR in Table I."
    },
    {
        "id": "841",
        "table": {
            "header": [
                "Dataset",
                "Model",
                "MR",
                "MR0",
                "MR1",
                "MR2",
                "MR3"
            ],
            "rows": [
                [
                    "Actitracker",
                    "(Baseline) Deep-BCE",
                    "90.1%",
                    "-",
                    "91.1%",
                    "60.2%",
                    "-"
                ],
                [
                    "Actitracker",
                    "(Ours) Auto-BCE",
                    "92.9%",
                    "-",
                    "93.9%",
                    "62.7%",
                    "-"
                ],
                [
                    "Actitracker",
                    "(Ours) Deep-Set",
                    "93.2%",
                    "-",
                    "93.9%",
                    "71.5%",
                    "-"
                ],
                [
                    "Actitracker",
                    "[BOLD] (Ours) Auto-Set",
                    "[BOLD] 94.9%",
                    "-",
                    "[BOLD] 95.5%",
                    "[BOLD] 75.1%",
                    "-"
                ],
                [
                    "Opportunity (locomotions)",
                    "(Baseline) Deep-BCE",
                    "82.0%",
                    "70.7%",
                    "85.0%",
                    "84.9%",
                    "68.3%"
                ],
                [
                    "Opportunity (locomotions)",
                    "(Ours) Auto-BCE",
                    "83.1%",
                    "73.7%",
                    "85.1%",
                    "85.3%",
                    "69.9%"
                ],
                [
                    "Opportunity (locomotions)",
                    "(Ours) Deep-Set",
                    "83.9%",
                    "78.2%",
                    "86.8%",
                    "84.9%",
                    "68.7%"
                ],
                [
                    "Opportunity (locomotions)",
                    "[BOLD] (Ours) Auto-Set",
                    "[BOLD] 84.9%",
                    "[BOLD] 80.2%",
                    "[BOLD] 87.1%",
                    "[BOLD] 85.6%",
                    "[BOLD] 75.6%"
                ]
            ],
            "title": "TABLE II: Comparison of our proposed Deep Auto-Set network against the baselines according to the obtained exact match ratio for each dataset. The best results are highlighted with boldface. Note that for the Actitracker dataset, sensor segments with cardinality of 0 (corresponding to Null segments) and 3 do not exist."
        },
        "insight": "Moreover, the match ratios in Table II suggest that Deep Auto-Set is a robust activity recognition system capable of: i) distinguishing different activity classes accurately (implied from MR0 and MR1 values); ii) identifying activity transition segments (implied from MR2 values); as well as iii) recognizing short duration human activities (implied from MR3 values). [CONTINUE] The performance results of our Deep Auto-Set network and the baseline models on the two HAR representative datasets are shown in Table II and Table III for different evaluation metrics. [CONTINUE] From the reported results, we can see that our novel Deep Auto-Set network consistently outperforms the baselines on Actitracker and Opportunity datasets in terms of both F1score and exact match ratio performance metrics."
    },
    {
        "id": "842",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Position  [BOLD] \u21132 [m]  [BOLD] @3s",
                "[BOLD] Position  [BOLD] \u21132 [m]  [BOLD] @6s",
                "[BOLD] Heading [deg]  [BOLD] @3s",
                "[BOLD] Heading [deg]  [BOLD] @6s",
                "[BOLD] Infeasible  [BOLD] [%]"
            ],
            "rows": [
                [
                    "UKF",
                    "3.99",
                    "10.58",
                    "7.50",
                    "18.88",
                    "[BOLD] 0.0"
                ],
                [
                    "S-LSTM\u00a0",
                    "4.72",
                    "8.20",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "poly-1",
                    "1.71",
                    "5.79",
                    "3.97",
                    "8.00",
                    "[BOLD] 0.0"
                ],
                [
                    "poly-2",
                    "1.37",
                    "4.39",
                    "10.19",
                    "31.44",
                    "20.8"
                ],
                [
                    "poly-3",
                    "1.45",
                    "4.66",
                    "10.16",
                    "16.09",
                    "15.5"
                ],
                [
                    "UM",
                    "[BOLD] 1.34",
                    "4.25",
                    "4.82",
                    "7.69",
                    "26.0"
                ],
                [
                    "UM-velo",
                    "1.37",
                    "4.28",
                    "4.76",
                    "7.55",
                    "27.3"
                ],
                [
                    "UM-LSTM",
                    "1.35",
                    "4.25",
                    "4.22",
                    "7.20",
                    "22.9"
                ],
                [
                    "UM-heading",
                    "1.37",
                    "4.32",
                    "4.65",
                    "7.09",
                    "20.7"
                ],
                [
                    "CTRA",
                    "1.56",
                    "4.61",
                    "3.60",
                    "8.68",
                    "[BOLD] 0.0"
                ],
                [
                    "DKM",
                    "[BOLD] 1.34",
                    "[BOLD] 4.21",
                    "[BOLD] 3.38",
                    "[BOLD] 4.92",
                    "[BOLD] 0.0"
                ]
            ],
            "title": "TABLE I: Comparison of prediction errors (lower is better)"
        },
        "insight": "The results are presented in Table I. [CONTINUE] the na\u00a8\u0131ve non-learned UKF baseline gave the largest errors, Expectedly, [CONTINUE] Furthermore, we found that poly-n methods of varying degrees yielded very different results. [CONTINUE] Never [CONTINUE] The second- and third-degree polynomials (constant-acceleration and constant-jerk models, respectively) further improved position errors, although with degraded heading and feasibility metrics. [CONTINUE] Moving forward, we see that the four unconstrained models, UM, UM-velo, UM-LSTM, and UM-heading gave strong results in terms of the center position prediction. [CONTINUE] the results are near-identical, and both have about 30% of their trajectories being kinematically infeasible, [CONTINUE] It is interesting to discuss UM-LSTM, which showed good performance in terms of position errors but poor in terms of heading and infeasibility metrics. [CONTINUE] The UM-heading model, which predicts both positions and headings, has better heading and infeasibility metrics com [CONTINUE] pared to UM. However, around 20% of the predicted are still kinematically infeasible [CONTINUE] However, when compared to unconstrained models it is clear that the method underperformed, although it is competitive in terms of kinematic feasibility. [CONTINUE] The DKM method gave the best performance, as it improves the heading errors by a large margin and its trajectories are guaranteed to be kinematically feasible."
    },
    {
        "id": "843",
        "table": {
            "header": [
                "[BOLD] Feature",
                "[BOLD] LSTM Autoencoder\u00a0",
                "[BOLD] Our Approach"
            ],
            "rows": [
                [
                    "Steer Angle",
                    "0.0005",
                    "0.0003"
                ],
                [
                    "Steer Speed",
                    "0.0004",
                    "0.0003"
                ],
                [
                    "Speed",
                    "0.0004",
                    "0.0003"
                ],
                [
                    "Yaw",
                    "0.0004",
                    "0.0003"
                ],
                [
                    "Pedal Angle",
                    "0.0012",
                    "0.0012"
                ],
                [
                    "Pedal Pressure",
                    "0.0012",
                    "0.0003"
                ],
                [
                    "Combined",
                    "0.3043",
                    "0.2082"
                ]
            ],
            "title": "TABLE II: Comparison of normalized reconstruction MSE losses."
        },
        "insight": "Table II [CONTINUE] shows the average normalized reconstruction loss on test data for different modalities between our approach (multitask learning) and LSTM autoencoder. We can notice that, our approach results in lower reconstruction loss with 33% lower error (0.2) compared to the standalone autoencoder (0.3) in the 'combined' category."
    },
    {
        "id": "844",
        "table": {
            "header": [
                "[BOLD] Category",
                "[BOLD] LSTM Autoencoder\u00a0",
                "[BOLD] Our Approach",
                "[BOLD] Our Approach (Scaled Scores)"
            ],
            "rows": [
                [
                    "Speed",
                    "21.7%",
                    "21.7%",
                    "30.4%"
                ],
                [
                    "K-turns",
                    "13.0%",
                    "8.8%",
                    "17.4%"
                ],
                [
                    "U-turns",
                    "4.4%",
                    "-",
                    "-"
                ],
                [
                    "Lane Change",
                    "34.8%",
                    "47.8%",
                    "39.1%"
                ],
                [
                    "Normal",
                    "26.1%",
                    "21.7%",
                    "13.1%"
                ],
                [
                    "Total",
                    "100% (23)",
                    "100% (23)",
                    "100% (23)"
                ]
            ],
            "title": "TABLE III: Comparison of qualitative results by analyzing top 0.01% scores."
        },
        "insight": "We have summarized our analysis results in Table III. We can notice that, while autoencoder classifies U-turns as anomalous, our approach (both scaled and unscaled) does not. We can also notice that our scaled approach classifies lesser 'Normal' and more 'Speed' anomalies. By comparing, the percentage of 'Normal' cases classified as anomalous, we can tell that [CONTINUE] scaled approach performs better than unscaled, which in turns performs better than standalone autoencoder approach."
    },
    {
        "id": "845",
        "table": {
            "header": [
                "[BOLD] Feature",
                "[BOLD] Multi-class LSTM Autoencoder",
                "[BOLD] Our Approach"
            ],
            "rows": [
                [
                    "Steer Angle",
                    "0.0007",
                    "0.0004"
                ],
                [
                    "Steer Speed",
                    "0.0005",
                    "0.0004"
                ],
                [
                    "Speed",
                    "0.0006",
                    "0.0004"
                ],
                [
                    "Yaw",
                    "0.0006",
                    "0.0003"
                ],
                [
                    "Pedal Angle",
                    "0.0014",
                    "0.0013"
                ],
                [
                    "Pedal Pressure",
                    "0.0012",
                    "0.0004"
                ],
                [
                    "Combined",
                    "0.4058",
                    "0.2456"
                ]
            ],
            "title": "TABLE IV: Comparison of normalized reconstruction MSE losses (without u-turn data)."
        },
        "insight": "The final (after 300 epochs of training) reconstruction loss on the eval/test data for each feature is summarized in Table IV. We can notice that our approach achieves lower reconstruction error for all features, compared to multi-class approach."
    },
    {
        "id": "846",
        "table": {
            "header": [
                "[BOLD] Percentile Top Scores",
                "[BOLD] Multi-class LSTM Autoencoder",
                "[BOLD] Our Approach",
                "[BOLD] Our Approach (Scaled Scores)"
            ],
            "rows": [
                [
                    "0.001",
                    "0.39% (3/765)",
                    "1.70% (13/765)",
                    "7.97% (61/765)"
                ],
                [
                    "0.01",
                    "1.96% (15/765)",
                    "7.97% (61/765)",
                    "29.02% (222/765)"
                ],
                [
                    "0.1",
                    "13.33% (102/765)",
                    "17.25% (132/765)",
                    "48.63% (372/765)"
                ],
                [
                    "0.5",
                    "73.46% (562/765)",
                    "52.68% (403/765)",
                    "84.44% (646/765)"
                ],
                [
                    "1",
                    "100.00% (765/765)",
                    "99.87% (764/765)",
                    "100.00% (765/765)"
                ]
            ],
            "title": "TABLE V: Comparison of percentage of u-turns detected (qualitative results) by analyzing top anomaly scores."
        },
        "insight": "The results are shown in Table V. For our multi-task approach, we have two scenarios\u2014actual reconstruction loss and scaled reconstruction loss. Considering the especially the top percentiles, we can notice that our approach with scaled scores performs better than our approach with normal scores which in turn performs better than multi-class approach. For example, considering the top 0.001 percentile anomaly scores for each approach\u2014our approach with scaled scores is able to detect 7.97% i.e., 61 of a total 765 u-turn windows in test data (consisting of 228802 windows), while this number is 1.7% for our approach with actual scores and only 0.39% for multi-class autoencoder approach."
    },
    {
        "id": "847",
        "table": {
            "header": [
                "Dataset",
                "SRC",
                "KSRC",
                "AE-SRC",
                "VGG19-SRC",
                "InceptionV3-SRC",
                "Resnet50-SRC",
                "Denesnet169-SRC",
                "DSRC (ours)"
            ],
            "rows": [
                [
                    "USPS",
                    "87.78",
                    "91.34",
                    "88.65",
                    "91.27",
                    "93.51",
                    "95.75",
                    "95.26",
                    "[BOLD] 96.25"
                ],
                [
                    "SVHN",
                    "15.71",
                    "27.42",
                    "18.69",
                    "52.86",
                    "41.14",
                    "47.88",
                    "37.65",
                    "[BOLD] 67.75"
                ],
                [
                    "UMDAA-01",
                    "79.00",
                    "81.37",
                    "86.70",
                    "82.68",
                    "86.15",
                    "91.84",
                    "86.35",
                    "[BOLD] 93.39"
                ]
            ],
            "title": "TABLE II: Sparse representation-based classification accuracy of different methods."
        },
        "insight": "\u00d7 We compare our method with the standard SRC method , Kernel SRC (KSRC) , SRC on features extracted from an autoencoder with similar architecture to our network (AESRC), and SRC on features extracted from the state-of-theart pre-trained networks. In our experiment with the pretrained networks, the networks are pre-trained on the Imagenet dataset . For this purpose, we use the following [CONTINUE] four popular network architectures: VGG-19 , InceptionV3 , Resnet-50  and Densenet-169 . [CONTINUE] The first row of Table II shows the performance of various SRC methods. As can be observed from this table, the proposed method performs significantly better than the other methods including the classical and deep learning-based methods. [CONTINUE] The second row in Table II compares the performance of different SRC methods. This table demonstrates the advantage of our method. While the classification task is much more challenging on SVHN than MNIST, the gap between the performance of our method and the second best performance is even more. The next best performing method is VGG19-SRC which performs 14.86% behind the accuracy of our method. [CONTINUE] The performance of various SRC methods on the UMDAA01 dataset are tabulated in the third row of Table II. As can be seen, our proposed DSRC method similar to the experiments with SVHN provides remarkable improvements as compared to the other SRC methods. This clearly shows that more [CONTINUE] challenging datasets are better represented by our method."
    },
    {
        "id": "848",
        "table": {
            "header": [
                "[EMPTY]",
                "DSRC",
                "DSC-SRC",
                "DSRC0.5",
                "DSRC1.5",
                "DSRC2"
            ],
            "rows": [
                [
                    "USPS",
                    "[BOLD] 96.25",
                    "78.25",
                    "N/C",
                    "95.75",
                    "[BOLD] 96.25"
                ]
            ],
            "title": "TABLE III: The classification accuracy corresponding to the ablation study. N/C refers to the cases where the learning process did not converge."
        },
        "insight": "Table III reveals that while the regularization norm on the coefficient matrix is selected between (cid:96)1 and (cid:96)2, it does not have much effect on the performance of the classification task. [CONTINUE] However, in our experiments, we observed that for norms smaller than 1, the problem is not stable and often does not converges. [CONTINUE] In addition, DSC-SRC cannot provide a [CONTINUE] desirable performance. [CONTINUE] it is possible that testing features shape an isolated group that does not have a strong connection to the training features. This makes it more difficult to estimate a label for the test samples."
    },
    {
        "id": "849",
        "table": {
            "header": [
                "Class",
                "Avg.  [ITALIC] K- [ITALIC] \u03b1",
                "Drop Antr 1",
                "Drop Antr 2",
                "Drop Antr 3",
                "Drop Antr 4"
            ],
            "rows": [
                [
                    "Stanford Drone",
                    "Stanford Drone",
                    "Stanford Drone",
                    "Stanford Drone",
                    "Stanford Drone",
                    "Stanford Drone"
                ],
                [
                    "Person",
                    "0.463",
                    "-0.124",
                    "0.029",
                    "-0.004",
                    "0.050"
                ],
                [
                    "Vehicle",
                    "0.675",
                    "0.010",
                    "0.048",
                    "-0.095",
                    "0.049"
                ],
                [
                    "Bicycle",
                    "0.408",
                    "-0.045",
                    "0.056",
                    "0.015",
                    "-0.023"
                ],
                [
                    "VIRAT",
                    "VIRAT",
                    "VIRAT",
                    "VIRAT",
                    "VIRAT",
                    "VIRAT"
                ],
                [
                    "Person",
                    "0.817",
                    "-0.010",
                    "0.007",
                    "0.003",
                    "0.009"
                ],
                [
                    "Vehicle",
                    "0.897",
                    "-0.004",
                    "-0.004",
                    "0.011",
                    "-0.006"
                ],
                [
                    "Bicycle",
                    "0.197",
                    "0.036",
                    "-0.051",
                    "0.012",
                    "0.145"
                ]
            ],
            "title": "Table 4: Average Vitality score of each annotator calculated on individual classes"
        },
        "insight": "The quality metrics calculated for each of the 4 annotators in the professional team can be seen in Table 3 and Table 4. [CONTINUE] Table 4 shows that Annotator 1 on the professional team, had the most disagreement from the rest of the team with an average Vitality Rating of \u22120.078. From table 4, we can see that the class that Annotator 1 had the most disagreement on was with the person class, with an average Vitality Score of \u22120.124 when comparing average overall K-\u03b1 for the person class to the average K-\u03b1 with Annotator 1 removed."
    },
    {
        "id": "850",
        "table": {
            "header": [
                "[ITALIC] K- [ITALIC] \u03b1",
                "Expert",
                "Experienced",
                "Novice"
            ],
            "rows": [
                [
                    "Average",
                    "0.310",
                    "0.371",
                    "0.086"
                ],
                [
                    "Median",
                    "0.269",
                    "0.392",
                    "0.059"
                ]
            ],
            "title": "Table 5: Average and median K-\u03b1 rating of MTurk workers for Stanford Drone images annotated by both the professional team and MTurk workers"
        },
        "insight": "annotate. Each image was annotated by 10 workers from each MTurk group (expert, experienced, novice). K-\u03b1 was calculated from all 10 workers in each experience group. The average and median K-\u03b1 values for each group are found in Table 5. [CONTINUE] Table 5 shows that despite the restrictions placed on MTurk workers who could participate on the annotation tasks, the K-\u03b1 among MTurk groups was much lower than that observed from the professional team."
    },
    {
        "id": "851",
        "table": {
            "header": [
                "Methods",
                "mAP",
                "Coverage"
            ],
            "rows": [
                [
                    "MV3D(BV+FV)",
                    "0.118",
                    "0.15"
                ],
                [
                    "Frustum-Pointnet-v1",
                    "0.407",
                    "0.45"
                ],
                [
                    "Frustum-Pointnet-v2",
                    "0.425",
                    "0.48"
                ],
                [
                    "3D-VGG+RoIpool8",
                    "0.360",
                    "0.32"
                ],
                [
                    "3D-VGG+RoIpool8(w/o maxpool)",
                    "0.423",
                    "0.41"
                ],
                [
                    "3D-ResNet+RoIpool8",
                    "0.416",
                    "0.44"
                ],
                [
                    "3D-ResNet+RoIpool8(Raw volume)",
                    "0.610",
                    "0.55"
                ],
                [
                    "[ITALIC] A2-Net (APRoI8)",
                    "0.711",
                    "0.67"
                ],
                [
                    "[ITALIC] A2-Net w/o Neighbor Loss",
                    "0.865",
                    "0.72"
                ],
                [
                    "[ITALIC] A2-Net",
                    "[BOLD] 0.891",
                    "[BOLD] 0.91"
                ]
            ],
            "title": "Table 1: The results of detection and threading comparing with other 3D object detection methods."
        },
        "insight": "The quantitative amino acid detection results are reported in Table. 1. [CONTINUE] Table. 1 shows that our method outperformed them by a large margin."
    },
    {
        "id": "852",
        "table": {
            "header": [
                "Training Dataset for RetinaNet",
                "Prec.",
                "Rec.",
                "F1",
                "mis-class."
            ],
            "rows": [
                [
                    "Drop Antr 1",
                    "0.65",
                    "0.56",
                    "0.60",
                    "380"
                ],
                [
                    "Drop Antr 2",
                    "0.60",
                    "0.67",
                    "0.63",
                    "781"
                ],
                [
                    "Drop Antr 3",
                    "0.55",
                    "0.64",
                    "0.59",
                    "838"
                ],
                [
                    "Drop Antr 4",
                    "0.58",
                    "0.66",
                    "0.62",
                    "738"
                ]
            ],
            "title": "Table 7: RetinaNet trained with 4 datasets (Drop Antr 1, Drop Antr 2, Drop Antr 3, and Drop Antr 4) performance measured against custom ground truth gt1 dataset. Precision, recall, F1-score and number of mis-classified predictions for RetinaNet trained on four different sets of data."
        },
        "insight": "Results from the same experiments but using ground truth sets gt1 and gt2 are shown in tables 7 and 8 respectively. [CONTINUE] the performance looks very different. All versions of RetinaNet showed an increased level of performance for the inter annotator curated datasets, gt1 and gt2, than for the original dataset gt3. [CONTINUE] Another observation from these experiments is that for models trained with data from annotators that disagree with the majority, the model produces more candidate predictions for the same algorithm configuration (compare tables 7 and 8)."
    },
    {
        "id": "853",
        "table": {
            "header": [
                "Training Dataset for RetinaNet",
                "Prec.",
                "Rec.",
                "F1",
                "mis-class."
            ],
            "rows": [
                [
                    "Drop Antr 1",
                    "0.70",
                    "0.57",
                    "0.63",
                    "375"
                ],
                [
                    "Drop Antr 2",
                    "0.60",
                    "0.66",
                    "0.63",
                    "847"
                ],
                [
                    "Drop Antr 3",
                    "0.59",
                    "0.65",
                    "0.62",
                    "814"
                ],
                [
                    "Drop Antr 4",
                    "0.63",
                    "0.67",
                    "0.65",
                    "711"
                ]
            ],
            "title": "Table 8: RetinaNet trained with 4 datasets (Drop Antr 1, Drop Antr 2, Drop Antr 3, and Drop Antr 4) performance measured against custom ground truth gt2 dataset. Precision, recall, F1-score and number of mis-classified predictions for RetinaNet trained on four different sets of data."
        },
        "insight": "Results from the same experiments but using ground truth sets gt1 and gt2 are shown in tables 7 and 8 respectively. [CONTINUE] the performance looks very different. All versions of RetinaNet showed an increased level of performance for the inter annotator curated datasets, gt1 and gt2, than for the original dataset gt3. [CONTINUE] Another observation from these experiments is that for models trained with data from annotators that disagree with the majority, the model produces more candidate predictions for the same algorithm configuration (compare tables 7 and 8)."
    },
    {
        "id": "854",
        "table": {
            "header": [
                "Training Dataset for RetinaNet",
                "Prec.",
                "Rec.",
                "F1",
                "mis-class."
            ],
            "rows": [
                [
                    "Drop Antr 1",
                    "0.66",
                    "0.56",
                    "0.61",
                    "380"
                ],
                [
                    "Drop Antr 2",
                    "0.62",
                    "0.56",
                    "0.59",
                    "456"
                ],
                [
                    "Drop Antr 3",
                    "0.58",
                    "0.52",
                    "0.55",
                    "474"
                ],
                [
                    "Drop Antr 4",
                    "0.61",
                    "0.54",
                    "0.57",
                    "422"
                ]
            ],
            "title": "Table 9: RetinaNet trained with 4 datasets (Drop Antr 1, Drop Antr 2, Drop Antr 3, and Drop Antr 4) performance measured against custom ground truth gt1 dataset. Precision, recall, F1-score and number of misclassified predictions."
        },
        "insight": "Tables 9 and 10 show performance results of RetinaNet using ground truth from the best annotators (i.e. gt1, and gt2 sets). [CONTINUE] through these experiments we have seen the impact in model performance of the inclusion or removal of labeled data from annotators with larger degree of disagreement (see tables 9 and 10),"
    },
    {
        "id": "855",
        "table": {
            "header": [
                "Approachs",
                "PSNR",
                "MS-SSIM",
                "BPP"
            ],
            "rows": [
                [
                    "BPG",
                    "31.47",
                    "0.94824",
                    "0.144"
                ],
                [
                    "BPG+Post",
                    "32.01",
                    "0.95712",
                    "0.148"
                ],
                [
                    "H.266",
                    "31.72",
                    "0.96097",
                    "0.149"
                ],
                [
                    "H.266 + Post",
                    "32.09",
                    "0.96104",
                    "0.147"
                ],
                [
                    "H.266 + Post + Rotation",
                    "32.10",
                    "0.96124",
                    "0.149"
                ]
            ],
            "title": "Table 1: Evaluation results on CLIC 2019 validation dataset"
        },
        "insight": "We choose BPG and H.266 as baselines. For BPG, we mixed quality parameters at 40 and 41; As for H.266, we mixed quality parameters at 35, 36 and 37 for different configure setting. As shown in Table 1, it can be seen the post-processing module improves the PSNR and MS-SSIM metrics for both algorithms. The EDSR's rotation trick elevates about 1 percent of PSNR and 0.2 percent of MS-SSIM."
    },
    {
        "id": "856",
        "table": {
            "header": [
                "[BOLD] Dataset  [BOLD] (m,n)",
                "[BOLD] Vote  [BOLD] (232 (124,108), 16)",
                "[BOLD] Vote  [BOLD] (232 (124,108), 16)",
                "[BOLD] SPECT  [BOLD] (267(212,55),22)",
                "[BOLD] SPECT  [BOLD] (267(212,55),22)",
                "[BOLD] KR-vs-KP  [BOLD] (3196(1569,1527),35)",
                "[BOLD] KR-vs-KP  [BOLD] (3196(1569,1527),35)"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[BOLD] Mean",
                    "[BOLD] SD",
                    "[BOLD] Mean",
                    "[BOLD] SD",
                    "[BOLD] Mean",
                    "[BOLD] SD"
                ],
                [
                    "[BOLD] p",
                    "0.969",
                    "0.0218",
                    "0.732",
                    "0.0589",
                    "0.940",
                    "0.0109"
                ],
                [
                    "[BOLD] 0",
                    "0.966",
                    "0.0203",
                    "0.693",
                    "0.0746",
                    "0.938",
                    "0.0096"
                ],
                [
                    "[BOLD] 0.1",
                    "0.955",
                    "0.0363",
                    "0.680",
                    "0.0681",
                    "0.924",
                    "0.0106"
                ],
                [
                    "[BOLD] 0.2",
                    "0.865",
                    "0.0839",
                    "0.638",
                    "0.0797",
                    "0.900",
                    "0.0150"
                ],
                [
                    "[BOLD] 0.3",
                    "0.821",
                    "0.0941",
                    "0.625",
                    "0.0664",
                    "0.866",
                    "0.0193"
                ],
                [
                    "[BOLD] 0.35",
                    "0.694",
                    "0.0919",
                    "0.579",
                    "0.0955",
                    "0.795",
                    "0.0263"
                ],
                [
                    "[BOLD] 0.4",
                    "0.573",
                    "0.1041",
                    "0.505",
                    "0.0793",
                    "0.706",
                    "0.0393"
                ]
            ],
            "title": "Table 2: Average (Mean) test accuracy along with standard deviation (SD) over 15 trials obtained by using squared loss based linear classifier learnt on Sy-De (noise rate p) attribute noise corrupted data. Even though squared loss is theoretically shown to be Sy-De noise robust, it doesn\u2019t show good performance at high noise rates. This could be because the result in Theorem 1 is in expectation. In particular, finite sample size starts showing its effect at high noise rates and the performance deteriorates."
        },
        "insight": "In Tables 2 and 3, we present the values which we used to generate the plots in Figure 1. The results are averaged over 15 trials. We observe that at high noise rates, the theoretically proven robustness of squared loss doesn't work because of the finite samples. [CONTINUE] Table 2: Average (Mean) test accuracy along with standard deviation (SD) over 15 trials obtained by using squared loss based linear classifier learnt on Sy-De (noise rate p) attribute noise corrupted data. Even though squared loss is theoretically shown to be Sy-De noise robust, it doesn't show good performance at high noise rates. This could be because the"
    },
    {
        "id": "857",
        "table": {
            "header": [
                "[BOLD] Dataset  [BOLD] (m,n)",
                "[BOLD] Vote  [BOLD] (232 (124,108), 16)",
                "[BOLD] Vote  [BOLD] (232 (124,108), 16)",
                "[BOLD] SPECT  [BOLD] (267(212,55),22)",
                "[BOLD] SPECT  [BOLD] (267(212,55),22)",
                "[BOLD] KR-vs-KP  [BOLD] (3196(1569,1527),35)",
                "[BOLD] KR-vs-KP  [BOLD] (3196(1569,1527),35)"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[BOLD] Mean",
                    "[BOLD] SD",
                    "[BOLD] Mean",
                    "[BOLD] SD",
                    "[BOLD] Mean",
                    "[BOLD] SD"
                ],
                [
                    "[BOLD] p",
                    "0.970",
                    "0.0207",
                    "0.665",
                    "0.0639",
                    "0.940",
                    "0.0111"
                ],
                [
                    "[BOLD] 0",
                    "0.967",
                    "0.0193",
                    "0.613",
                    "0.1214",
                    "0.937",
                    "0.0097"
                ],
                [
                    "[BOLD] 0.1",
                    "0.956",
                    "0.0373",
                    "0.605",
                    "0.1338",
                    "0.923",
                    "0.0109"
                ],
                [
                    "[BOLD] 0.2",
                    "0.868",
                    "0.0839",
                    "0.568",
                    "0.1353",
                    "0.899",
                    "0.0152"
                ],
                [
                    "[BOLD] 0.3",
                    "0.823",
                    "0.0950",
                    "0.586",
                    "0.1188",
                    "0.866",
                    "0.0186"
                ],
                [
                    "[BOLD] 0.35",
                    "0.698",
                    "0.0903",
                    "0.544",
                    "0.1542",
                    "0.794",
                    "0.0261"
                ],
                [
                    "[BOLD] 0.4",
                    "0.575",
                    "0.1045",
                    "0.540",
                    "0.1472",
                    "0.706",
                    "0.0399"
                ]
            ],
            "title": "Table 3: Average (Mean) test AM value along with standard deviation (SD) over 15 trials obtained by using squared loss based linear classifier learnt on Sy-De (noise rate p) attribute noise corrupted data. Due to the imbalanced nature of SPECT dataset, AM is a more suitable evaluation metric."
        },
        "insight": "In Tables 2 and 3, we present the values which we used to generate the plots in Figure 1. The results are averaged over 15 trials. We observe that at high noise rates, the theoretically proven robustness of squared loss doesn't work because of the finite samples. [CONTINUE] Table 3: Average (Mean) test AM value along with standard deviation (SD) over 15 trials obtained by using squared loss based linear classifier learnt on Sy-De (noise rate p) attribute noise corrupted data. Due to the imbalanced nature of SPECT dataset, AM is a more suitable evaluation metric."
    },
    {
        "id": "858",
        "table": {
            "header": [
                "[EMPTY]",
                "NODE",
                "ANODE"
            ],
            "rows": [
                [
                    "MNIST",
                    "96.4% \u00b1 0.5",
                    "[BOLD] 98.2% \u00b1  [BOLD] 0.1"
                ],
                [
                    "CIFAR10",
                    "53.7% \u00b1 0.2",
                    "[BOLD] 60.6% \u00b1  [BOLD] 0.4"
                ],
                [
                    "SVHN",
                    "81.0% \u00b1 0.6",
                    "[BOLD] 83.5% \u00b1  [BOLD] 0.5"
                ]
            ],
            "title": "Table 1: Test accuracies and their standard deviation over 5 runs on various image datasets."
        },
        "insight": "As can be seen in Fig. 12 and Table 1, for MNIST, CIFAR10, and SVHN, ANODEs achieve lower test losses and higher test accuracies than NODEs, suggesting that ANODEs also generalize better on image datasets."
    },
    {
        "id": "859",
        "table": {
            "header": [
                "[BOLD] Sound Class",
                "[BOLD] Avg. Normalized Correlation Value  [BOLD] Model 1",
                "[BOLD] Avg. Normalized Correlation Value  [BOLD] Model 2"
            ],
            "rows": [
                [
                    "Break",
                    "0.76",
                    "0.93"
                ],
                [
                    "Car",
                    "0.68",
                    "0.65"
                ],
                [
                    "Clock",
                    "0.92",
                    "0.73"
                ],
                [
                    "Cutting",
                    "0.58",
                    "0.89"
                ],
                [
                    "Fire",
                    "0.88",
                    "0.72"
                ],
                [
                    "Footstep",
                    "0.68",
                    "0.95"
                ],
                [
                    "Gunshot",
                    "0.65",
                    "0.82"
                ],
                [
                    "Horse",
                    "0.87",
                    "0.63"
                ],
                [
                    "Rain",
                    "0.90",
                    "0.81"
                ],
                [
                    "Thunder",
                    "0.71",
                    "0.58"
                ],
                [
                    "Typing",
                    "0.69",
                    "0.60"
                ],
                [
                    "Waterfall",
                    "0.86",
                    "0.69"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] 0.77",
                    "[BOLD] 0.75"
                ]
            ],
            "title": "Table I: Sound Quality Matrix Analysis Results: Average normalized cross-correlation value obtained from comparing the original and generated audio signals for model 1 and 2 in all sound classes"
        },
        "insight": "For each sound category, we calculate the average normalized cross correlation values between our generated and original audio signals to know how much similarity exists between them. We present the correlation values for models in Table I. We notice all positive cross-correlation values (above 0.5) representing an expected analogy between the ground truth and our obtained result. Apart from the four most temporal action sensitive classes (e.g. breaking, cutting, footsteps, gunshots), model 1 provides higher correlation values than method 2."
    },
    {
        "id": "860",
        "table": {
            "header": [
                "Methods",
                "Coverage",
                "RMSD"
            ],
            "rows": [
                [
                    "DFS [ITALIC] o",
                    "0.65",
                    "3.5"
                ],
                [
                    "DFS [ITALIC] d",
                    "0.68",
                    "3.1"
                ],
                [
                    "DFS [ITALIC] d+PBNet",
                    "0.89",
                    "2.6"
                ],
                [
                    "MCTS",
                    "0.72",
                    "2.9"
                ],
                [
                    "MCTS+PBNet",
                    "[BOLD] 0.91",
                    "[BOLD] 2.0"
                ]
            ],
            "title": "Table 2: The results of threading by DFS-based methods and the proposed MCTS+PBNet."
        },
        "insight": "It can be seen in Table. 2 shows that both threading algorithms are largely improved with PBNet. PBNet achieved 89.8% accuracy for peptide bond recognition."
    },
    {
        "id": "861",
        "table": {
            "header": [
                "[BOLD] Existing Model",
                "[BOLD] Avg. Acc."
            ],
            "rows": [
                [
                    "Owens et al.[owens2016visually] (ResNet + spectrogram + GHD dataset)",
                    "22.70%"
                ],
                [
                    "POCAN [zhangvisually] (ResNet + spectrogram + GHD dataset)",
                    "36.32%"
                ],
                [
                    "Zhuo [zhou2017visual] (Flow method + VEGAS dataset)",
                    "45.47%"
                ]
            ],
            "title": "Table II: Top 1 sound class prediction accuracy of existing models"
        },
        "insight": "The purpose of this qualitative task is to evaluate if semantic information of the sound class are present in our synthesized sound. [CONTINUE] To evaluate our classifier performance and for better comparison, we also calculate the average prediction accuracy of the classifier network as 78.32% by using the original spectrograms from our test dataset. Since prior works have not considered the movie sound effects domain, it is difficult to compare with their works directly. We present the prediction accuracy results of the most related sound generation models and our proposed models on the same retrieval task using classifier in Table II and III respectively. Both of our average prediction accuracies for this sound retrieval qualitative experiment show leading results above 63% with our classifier for the generated sounds from AutoFoley Models."
    },
    {
        "id": "862",
        "table": {
            "header": [
                "[BOLD] Proposed Model",
                "[BOLD] Avg. Accuracy"
            ],
            "rows": [
                [
                    "AutoFoley (Frame-Sequence Network)",
                    "[BOLD] 65.79%"
                ],
                [
                    "AutoFoley (Frame-Relation Network)",
                    "[BOLD] 63.40%"
                ],
                [
                    "AutoFoley (Real sound tracks)",
                    "78.32%"
                ]
            ],
            "title": "Table III: Top 1 sound class prediction accuracy of proposed models with AutoFoley Dataset"
        },
        "insight": "The purpose of this qualitative task is to evaluate if semantic information of the sound class are present in our synthesized sound. [CONTINUE] To evaluate our classifier performance and for better comparison, we also calculate the average prediction accuracy of the classifier network as 78.32% by using the original spectrograms from our test dataset. Since prior works have not considered the movie sound effects domain, it is difficult to compare with their works directly. We present the prediction accuracy results of the most related sound generation models and our proposed models on the same retrieval task using classifier in Table II and III respectively. Both of our average prediction accuracies for this sound retrieval qualitative experiment show leading results above 63% with our classifier for the generated sounds from AutoFoley Models."
    },
    {
        "id": "863",
        "table": {
            "header": [
                "[BOLD] Average",
                "[BOLD] Train  [BOLD] Model 1",
                "[BOLD] Train  [BOLD] Model 2",
                "[BOLD] Test  [BOLD] Model 1",
                "[BOLD] Test  [BOLD] Model 2"
            ],
            "rows": [
                [
                    "Log Loss",
                    "0.002",
                    "0.035",
                    "0.166",
                    "0.194"
                ],
                [
                    "Accuracy",
                    "0.989",
                    "0.962",
                    "0.834",
                    "0.806"
                ]
            ],
            "title": "Table IV: Loss and Accuracy Calculation Result"
        },
        "insight": "We calculate the average log loss and accuracy during training and testing of our models and display the results in Table IV. For both models, lower training losses are calculated as compared to the test case. Here, Model 1 gives a smaller log loss than Model 2 resulting in increased average accuracy throughout training and testing the network."
    },
    {
        "id": "864",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Description",
                "[BOLD] Average Accuracy (%)"
            ],
            "rows": [
                [
                    "Ablation Model 1",
                    "Direct raw image frame (no SP) +VGG19 + simple LSTM",
                    "43.18"
                ],
                [
                    "Ablation Model 2",
                    "Direct raw image (no SP) + ResNet-50 + simple LSTM",
                    "44.52"
                ],
                [
                    "Ablation Model 3",
                    "SP+ VGG19 + simple LSTM",
                    "56.04"
                ],
                [
                    "Ablation Model 4",
                    "SP + VGG19 + FS-LSTM",
                    "63.88"
                ],
                [
                    "Frame Sequence Network",
                    "SP + ResNet50 + FS-LSTM",
                    "65.79"
                ]
            ],
            "title": "Table V: Comparison of the average accuracy (%) among the Ablation Models and our proposed method 1 (Frame Sequence Network) on AutoFoley Dataset."
        },
        "insight": "The Table V shows the performance comparisons among these ablation models and our proposed Frame Sequence model. There is a noticeable accuracy degradation for the models that are using direct raw images as video frame input to the CNN. This is because they often miss useful color and motion features from consecutive video frames. The use of a simple LSTM design reduces the computation time, however, it provides higher class prediction errors. Though accuracy computed from the last ablation model (including SP, VGG19, FS-LSTM) is close to the proposed Frame-Sequence Network result, our model outperforms all other cascaded models. [CONTINUE] To understand the significance of each component of this prediction network, we build the following alternative cascaded ablation models, train them with the AutoFoley dataset, then compute the prediction accuracy in each case and compare with the obtained result from our proposed Model 1 (Frame-Sequence Network):"
    },
    {
        "id": "865",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Description",
                "[BOLD] Average Accuracy (%)"
            ],
            "rows": [
                [
                    "Ablation Model 1",
                    "TRN (for 4 frame relations) + VGG19 (base CNN)",
                    "41.64"
                ],
                [
                    "Ablation Model 2",
                    "TRN (for 4 frame relations) + ResNet-50 (base CNN)",
                    "42.01"
                ],
                [
                    "Ablation Model 3",
                    "TRN (for 8 frame relations) +VGG19 (base CNN)",
                    "60.98"
                ],
                [
                    "Ablation Model 4",
                    "TRN (for 16 frame relations) +ResNet-50 (base CNN)",
                    "64.27"
                ],
                [
                    "Frame Relation Network",
                    "TRN (for 8 frame relations) +ResNet-50 (base CNN)",
                    "63.40"
                ]
            ],
            "title": "Table VI: Comparison of the average accuracy (%) among the Ablation Models and our proposed method 2 (Frame Relation Network) on AutoFoley Dataset."
        },
        "insight": "To evaluate the effects of parameters and models used in our second proposed model (Frame-Relation Network), we train and test the following ablation models and compare their performance. [CONTINUE] The detailed comparative results are shown in Table VI. Here, we observe lower accuracies in cases of the 4 frame relation TRN. The accuracy improves with the increment of Q. However, it shows a noticeable computational time increment after setting Q value to 16 compared to 8, resulting a marginal accuracy improvement. Hence, in our proposed network, we do not further increase the Q value above 8. We also find that, [CONTINUE] ResNet-50 outperforms VGG19 model as the base CNN."
    },
    {
        "id": "866",
        "table": {
            "header": [
                "[BOLD] Class",
                "[BOLD] Query 1  [BOLD] Method 1",
                "[BOLD] Query 1  [BOLD] Method 2",
                "[BOLD] Query 2  [BOLD] Method 1",
                "[BOLD] Query 2  [BOLD] Method 2"
            ],
            "rows": [
                [
                    "Break",
                    "52.40%",
                    "56.10%",
                    "32.30%",
                    "67.70%"
                ],
                [
                    "Car",
                    "71.53%",
                    "65.40%",
                    "55.76%",
                    "44.24%"
                ],
                [
                    "Clock",
                    "90.91%",
                    "73.80%",
                    "70.90%",
                    "29.10%"
                ],
                [
                    "Cutting",
                    "50.17%",
                    "45.35%",
                    "62.89%",
                    "37.11%"
                ],
                [
                    "Fire",
                    "85.43%",
                    "75.40%",
                    "57.70%",
                    "42.30%"
                ],
                [
                    "Footstep",
                    "61.72%",
                    "50.57%",
                    "72.13%",
                    "27.87%"
                ],
                [
                    "Gunshot",
                    "68.33%",
                    "61.84%",
                    "64.60%",
                    "35.40%"
                ],
                [
                    "Horse",
                    "89.38%",
                    "78.20%",
                    "53.80%",
                    "46.20%"
                ],
                [
                    "Rain",
                    "88.62%",
                    "75.80%",
                    "50.00%",
                    "50.00%"
                ],
                [
                    "Thunder",
                    "76.25%",
                    "72.17%",
                    "59.35%",
                    "40.65%"
                ],
                [
                    "Typing",
                    "64.27%",
                    "62.39%",
                    "66.20%",
                    "33.80%"
                ],
                [
                    "Waterfall",
                    "85.55%",
                    "74.40%",
                    "66.78%",
                    "33.22%"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] 73.71%",
                    "[BOLD] 65.95%",
                    "[BOLD] 59.37%",
                    "[BOLD] 40.63%"
                ]
            ],
            "title": "Table VIII: Human Evaluation Results: Selection percentage of each sound category for the first and second human survey questions"
        },
        "insight": "To evaluate the quality of our synthesized sounds, we conduct a survey among local college students. In the survey, the students are presented a video with two audio samples, the original sound and the synthesised sound. We then ask students to select the option they prefer using four questions: 1) Select the original (more realistic) sample. 2) Select the most suitable sample (most reasonable audio initiated from the video clip). 3) Select the sample with minimum noise. 4) Select the most synchronized sample. We assess the performance of our produced sound tracks for each of the categories. [CONTINUE] The detailed human evaluation results (selection percentages of survey queries) for each individual class are presented in Tables VIII and IX for Models 1 and 2 separately."
    },
    {
        "id": "867",
        "table": {
            "header": [
                "[BOLD] Class",
                "[BOLD] Query 3  [BOLD] Method 1",
                "[BOLD] Query 3  [BOLD] Method 2",
                "[BOLD] Query 4  [BOLD] Method 1",
                "[BOLD] Query 4  [BOLD] Method 2"
            ],
            "rows": [
                [
                    "Break",
                    "29.17%",
                    "70.83%",
                    "29.00%",
                    "71.00%"
                ],
                [
                    "Car",
                    "36.02%",
                    "63.08%",
                    "57.32%",
                    "42.68%"
                ],
                [
                    "Clock",
                    "57.69%",
                    "42.30%",
                    "61.50%",
                    "38.50%"
                ],
                [
                    "Cutting",
                    "62.50%",
                    "37.50%",
                    "55.41%",
                    "44.59%"
                ],
                [
                    "Fire",
                    "34.61%",
                    "65.38%",
                    "53.80%",
                    "46.20%"
                ],
                [
                    "Footstep",
                    "65.37%",
                    "34.62%",
                    "68.55%",
                    "31.45%"
                ],
                [
                    "Gunshot",
                    "75.81%",
                    "24.19%",
                    "74.60%",
                    "25.40%"
                ],
                [
                    "Horse",
                    "36.18%",
                    "63.82%",
                    "53.80%",
                    "46.20%"
                ],
                [
                    "Rain",
                    "33.33%",
                    "66.67%",
                    "55.80%",
                    "44.20%"
                ],
                [
                    "Thunder",
                    "76.92%",
                    "23.08%",
                    "51.62%",
                    "48.38%"
                ],
                [
                    "Typing",
                    "67.24%",
                    "32.76%",
                    "68.50%",
                    "31.50%"
                ],
                [
                    "Waterfall",
                    "51.85%",
                    "48.15%",
                    "52.43%",
                    "47.57%"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] 52.23%",
                    "[BOLD] 47.70%",
                    "[BOLD] 52.86%",
                    "[BOLD] 43.14%"
                ]
            ],
            "title": "Table IX: Human Evaluation Results: Selection percentage of each sound category for the third and fourth human survey questions"
        },
        "insight": "To evaluate the quality of our synthesized sounds, we conduct a survey among local college students. In the survey, the students are presented a video with two audio samples, the original sound and the synthesised sound. We then ask students to select the option they prefer using four questions: 1) Select the original (more realistic) sample. 2) Select the most suitable sample (most reasonable audio initiated from the video clip). 3) Select the sample with minimum noise. 4) Select the most synchronized sample. We assess the performance of our produced sound tracks for each of the categories. [CONTINUE] The detailed human evaluation results (selection percentages of survey queries) for each individual class are presented in Tables VIII and IX for Models 1 and 2 separately."
    },
    {
        "id": "868",
        "table": {
            "header": [
                "Paper",
                "Yelp\u201915",
                "IMDB",
                "Amazon"
            ],
            "rows": [
                [
                    "Zhang et al., 2015",
                    "59.9%",
                    "-",
                    "55.3%"
                ],
                [
                    "Tang et al., 2015",
                    "67.6%",
                    "45.3%",
                    "-"
                ],
                [
                    "Yang et al., 2016",
                    "71.0%",
                    "49.4%",
                    "63.6%"
                ]
            ],
            "title": "Table 1: State of the art accuracies on multi-class classification, results taken from [12]."
        },
        "insight": "However, if you look at data from Table 1, which shows state of the art results on multi-class classification dataset, it becomes clear that this is not a solved problem. Accuracies on Amazon and Yelp reviews datasets, where you have to predict the ranking of a review on 1 to 5 scale, hover around 60% to 70%, with nobody beating the 50% threshold on IMDB dataset for movie genre prediction with 15 categories."
    },
    {
        "id": "869",
        "table": {
            "header": [
                "Architecture",
                "Top-1 Error",
                "# Param",
                "MACs"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "(M)",
                    "(G)"
                ],
                [
                    "ResNet-50",
                    "[BOLD] 24.0%",
                    "26",
                    "4.1"
                ],
                [
                    "MorphNet",
                    "24.8%",
                    "15.5",
                    "-"
                ],
                [
                    "MobileNetV1",
                    "29.4%",
                    "4.2",
                    "0.569"
                ],
                [
                    "MobileNetV2",
                    "25.3%",
                    "6.9",
                    "0.585"
                ],
                [
                    "MnasNet-A",
                    "24.4%",
                    "4.8",
                    "[BOLD] 0.340"
                ],
                [
                    "EfficientNet-B0",
                    "23.7%",
                    "5.3",
                    "0.391"
                ],
                [
                    "DARTS",
                    "26.7%",
                    "4.7",
                    "0.574"
                ],
                [
                    "RandWire-WS",
                    "25.3%",
                    "5.6",
                    "0.583"
                ],
                [
                    "ShrinkCNN-A",
                    "26.1%",
                    "[BOLD] 3.6",
                    "0.385"
                ],
                [
                    "ShrinkCNN-B",
                    "24.9%",
                    "[BOLD] 3.6",
                    "0.385"
                ]
            ],
            "title": "Table 2: Performance comparison of various CNN architectures on ImageNet-1K dataset. All the evaluations are based on 50,000 images of ImageNet-1K validation dataset. The input resolution is set to 224\u00d7224."
        },
        "insight": "For example, compared to the hand-crafted MobileNetV2 model with ReLU activation, ShrinkCNN-A reduces 48% model parameters and 34% MACs. Compared to MNasNet-A which is crafted by conducting resource-aware neural architecture search, ShrinkCNN-B can further cut off 25% parameters with negligible impact on the top-1 error rate. [CONTINUE] Our ShrinkCNN-B requires slightly more MACs mainly because MnasNet-A applies architecture motifs (e.g., squeeze-and-excitation layers) to further increase the efficiency of MACs. [CONTINUE] Compared to EfficientNet (Tan and Le 2019) obtained by scaling up MobileNetV2 blocks using automated search, ShrinkCNN-B can save 32% parameters with the similar top-1 error rate and MACs."
    },
    {
        "id": "870",
        "table": {
            "header": [
                "Topology",
                "Nodes",
                "Accuracy",
                "MAC"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "(%)",
                    "(M)"
                ],
                [
                    "Complete-DAG",
                    "8",
                    "91.56",
                    "105.61"
                ],
                [
                    "WS",
                    "15",
                    "90.5\u00b10.41",
                    "16.04\u00b12.71"
                ],
                [
                    "ER",
                    "15",
                    "92.1\u00b10.38",
                    "34.40\u00b116.43"
                ],
                [
                    "BA",
                    "15",
                    "90.0\u00b10.34",
                    "32.61\u00b15.96"
                ],
                [
                    "ShrinkCNN",
                    "8",
                    "93.22",
                    "38.20"
                ]
            ],
            "title": "Table 4: Comparison with ShrinkCNN topology with prior graph topology. We use N=8 nodes when crafting CNN architectures with complete-DAG-based topology, and N=15 nodes when crafting CNN architectures with random graph priors. Experiments on random graph priors are conducted 10 times to reduce randomness as much as possible."
        },
        "insight": "We craft CNN architectures with the above topology respectively, train them on the full CIFAR-10 dataset with the same hyperparameter setting, and compare their test results with ShrinkCNN in Table 4. Table 4 shows that ShrinkCNN is able to achieve up to 1.66% higher accuracy than the complete-DAG-based topology, and up to 3% higher than the topology from random graphs, thanks to the topological knowledge accumulation in the AutoShrink process."
    },
    {
        "id": "871",
        "table": {
            "header": [
                "Cell Topology",
                "Edges",
                "Perplexity",
                "# Param (M)"
            ],
            "rows": [
                [
                    "Complete-DAG",
                    "15",
                    "63.1",
                    "33"
                ],
                [
                    "ShrinkRNN",
                    "8",
                    "56.5",
                    "23"
                ]
            ],
            "title": "Table 5: Demonstration of efficient RNN cell structures found by AutoShrink algorithm on full Penn Treebank dataset. We used N=6 nodes when crafting RNN architectures with complete-DAG-based topology."
        },
        "insight": "Table 5 indicates that our representative RNN cell structure can achieve a significantly lower (6.6) perplexity with 30% fewer parameters."
    },
    {
        "id": "872",
        "table": {
            "header": [
                "split",
                "#samples",
                "STCNN-T",
                "STCNN-T+I",
                "LSTM"
            ],
            "rows": [
                [
                    "0",
                    "3184",
                    "25.7",
                    "24.4",
                    "25.8"
                ],
                [
                    "1",
                    "1637",
                    "24.1",
                    "21.3",
                    "24.0"
                ],
                [
                    "2",
                    "2509",
                    "24.6",
                    "21.7",
                    "25.7"
                ],
                [
                    "3",
                    "4532",
                    "23.5",
                    "20.7",
                    "25.9"
                ],
                [
                    "4",
                    "3835",
                    "20.8",
                    "19.3",
                    "23.4"
                ],
                [
                    "avg",
                    "15697",
                    "23.5 \u00b1 0.8",
                    "21.3 \u00b1 0.8",
                    "25.0 \u00b1 0.5"
                ]
            ],
            "title": "Table 1: Quantitative results. Left: negative cross-entropy (mean and standard error over five-fold cross-validation) of the proposed model on SDD with only trajectory information (STCNN-T) or with additional reference images (STCNN-T+I). LSTM is a baseline architecture, see the main text. Right: top 10%-oracle error at different time points of the forecast, see main text for details."
        },
        "insight": "Quantitative results for SDD are provided in Table 1 [CONTINUE] For SDD, Table 1 (right) shows the top-10% oracle error at different time points in the forecasted trajectory, as in [1, 5]. [CONTINUE] Table 1 (right) shows that this \"shot gun\" baseline achieves oracle scores at least as good as all other methods, despite not even making use of the reference image."
    },
    {
        "id": "873",
        "table": {
            "header": [
                "[EMPTY]",
                "region",
                "landscape",
                "dbl.-page",
                "no text",
                "leading"
            ],
            "rows": [
                [
                    "CVS",
                    "90.89",
                    "100",
                    "94.74",
                    "97.37",
                    "81.58"
                ],
                [
                    "Test",
                    "90.21",
                    "99.29",
                    "95.93",
                    "94.19",
                    "73.85"
                ]
            ],
            "title": "TABLE I: Evaluation of simple document analysis (% GT)."
        },
        "insight": "Table [CONTINUE] lists the results obtained on the CVS set, as well as for the test data of TRACK A, since relevant regions are provided for this track. In both cases roughly 90% of the releveant areas are correctly detected."
    },
    {
        "id": "874",
        "table": {
            "header": [
                "[BOLD] Robot Joint Error",
                "[BOLD] Robot Joint Error",
                "[BOLD] Robot Joint Error  [BOLD] Ball",
                "[BOLD] Robot Joint Error  [BOLD] Shoe, IMU",
                "[BOLD] Robot Joint Error  [BOLD] Shoe, IMU Head",
                "[BOLD] Robot Joint Error  [BOLD] Shoe, IMU Head, Ball",
                "[BOLD] Robot Joint Error  [BOLD] All"
            ],
            "rows": [
                [
                    "43%",
                    "BIP",
                    "-",
                    "3.91\u00d71015",
                    "1.48\u00d71014",
                    "9.56\u00d71014",
                    "3.97\u00d71016"
                ],
                [
                    "43%",
                    "PF",
                    "-",
                    "0.37",
                    "0.28",
                    "0.28",
                    "0.26"
                ],
                [
                    "43%",
                    "eBIP\u2212",
                    "-",
                    "5.62\u00d7106",
                    "9.98\u00d7106",
                    "1.08\u00d7107",
                    "6.00\u00d7107"
                ],
                [
                    "43%",
                    "eBIP",
                    "-",
                    "0.18",
                    "0.19",
                    "0.18",
                    "0.14"
                ],
                [
                    "82%",
                    "BIP",
                    "1.04\u00d71015",
                    "1.59\u00d71017",
                    "4.22\u00d71015",
                    "3.66\u00d71015",
                    "6.52\u00d71017"
                ],
                [
                    "82%",
                    "PF",
                    "0.11",
                    "0.37",
                    "0.28",
                    "0.28",
                    "0.26"
                ],
                [
                    "82%",
                    "eBIP\u2212",
                    "10.52",
                    "9.46\u00d7106",
                    "1.36\u00d7107",
                    "1.68\u00d7107",
                    "7.66\u00d7107"
                ],
                [
                    "82%",
                    "eBIP",
                    "0.05",
                    "0.20",
                    "0.19",
                    "0.11",
                    "0.09"
                ]
            ],
            "title": "TABLE I: The left table indicates the mean squared error values for the first three joints of the robot at the time the ball is caught while the right table is the mean absolute error for the inferred ball position. A green box represents the best method and a gray box represents methods which are not statistically worse than the best method (Mann-Whitney U, p<0.05). The values 43% and 82% indicate inference is performed after 43% of the interaction is observed (corresponding to before the ball is thrown) and 82% is observed (the ball is partway through its trajectory). The ball itself is not visible for the first 43% as this is when it is occluded by the participant\u2019s hand. The standard error for eBIP is less than \u00b10.01 in all cases for the joint MSE and \u00b10.015 for the ball MAE."
        },
        "insight": "both methods fared much better, however, eBIP significantly more so as it achieved the best result in every category (see green box). [CONTINUE] The inference errors for the robot joints are shown in Table I, along with the errors for the estimation of the location of the ball at the time of interception. [CONTINUE] Results in Table [CONTINUE] show that attempting to model the demonstrations with a parametric Gaussian model yields a poor approximation and leads to an incorrect estimate of the initial uncertainty. This is supported by the fact that both BIP (Gaussian prior) and eBIP\u2212 (mixture model prior) produce predictions that are many orders of magnitude from the true state. [CONTINUE] However, we see from Table [CONTINUE] that BIP yields a 1.04 \u00d7 1015 joint prediction MSE error when 82% of the ball trajectory is observed while eBIP\u2212 only yields a joint prediction MSE of 10.52. [CONTINUE] These results also show that both the PF and eBIP\u2212 produce worse inference results as the number of active modalities increases. For example, the PF predictions result in a joint MSE of 0.11 when only the trajectory of the ball was observed, but a MSE of 0.26 when all modalities were observed. [CONTINUE] Lastly, we observe that the introduction of additional modalities does not always yield an increase in inference accuracy, although it may provide other benefits depending on the modalities. This is evident when comparing the joint MSE prediction errors of eBIP for the {Shoe, IMU} data set and that of the {Shoe, IMU, Head} data set. The introduction of the head modality actually increases the inference error when 43% of the trajectory is observed, which is when the head modality is most relevant."
    },
    {
        "id": "875",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Method F#",
                "[BOLD] Method messytables",
                "[BOLD] Method readr",
                "[BOLD] Method TDDA",
                "[BOLD] Method Trifacta",
                "[BOLD] Method ptype-hc",
                "[BOLD] Method ptype"
            ],
            "rows": [
                [
                    "Overall",
                    "0.73",
                    "0.72",
                    "0.69",
                    "0.61",
                    "0.90",
                    "0.92",
                    "[BOLD] 0.93"
                ],
                [
                    "Accuracy",
                    "0.73",
                    "0.72",
                    "0.69",
                    "0.61",
                    "0.90",
                    "0.92",
                    "[BOLD] 0.93"
                ],
                [
                    "Boolean",
                    "0.55",
                    "0.56",
                    "0.00",
                    "0.00",
                    "0.49",
                    "0.75",
                    "[BOLD] 0.83"
                ],
                [
                    "Date",
                    "0.35",
                    "0.17",
                    "0.10",
                    "0.00",
                    "[BOLD] 0.68",
                    "0.67",
                    "0.67"
                ],
                [
                    "Float",
                    "0.60",
                    "0.57",
                    "0.59",
                    "0.42",
                    "0.87",
                    "[BOLD] 0.93",
                    "0.91"
                ],
                [
                    "Integer",
                    "0.55",
                    "0.55",
                    "0.57",
                    "0.46",
                    "[BOLD] 0.88",
                    "0.85",
                    "[BOLD] 0.88"
                ],
                [
                    "String",
                    "0.61",
                    "0.61",
                    "0.58",
                    "0.51",
                    "0.83",
                    "[BOLD] 0.90",
                    "0.89"
                ]
            ],
            "title": "Table 2: Performance of the methods using the Jaccard index and overall accuracy, for the types Boolean, Date, Float, Integer and String."
        },
        "insight": "We present the performance of the methods in Table 2, which indicates that our method performs better than the others for all types, except for the date type where it is slightly worse than Trifacta. In the table ptype denotes the discriminatively trained model, and ptype-hc the version with handcrafted parameters. [CONTINUE] Notice that the discriminative training improves the performance, specifically for Boolean and integer types. [CONTINUE] Note that the training has a slightly negative effect on float and string types, but it still performs better than the other methods. [CONTINUE] It is noticeable from Table 2 that dates are difficult to infer accurately. [CONTINUE] The large performance gap for Booleans suggests that our method handles confusions with integers and strings better. Analysis shows that such confusions occur respectively in the presence of {0, 1}, and {Yes, No}. We further note that F#, messytables, and readr perform similarly, especially on floats, integers, and strings; which is most likely explained by the fact that they employ similar heuristics."
    },
    {
        "id": "876",
        "table": {
            "header": [
                "Method",
                "FPs",
                "FNs",
                "TPs"
            ],
            "rows": [
                [
                    "Trifacta",
                    "0.67",
                    "3.96",
                    "1.57"
                ],
                [
                    "ptype",
                    "1.13",
                    "0.20",
                    "5.34"
                ]
            ],
            "title": "Table 3: The percentages of FPs, FNs, and TPs for Trifacta and ptype on type/non-type detection."
        },
        "insight": "Lastly, we compare Trifacta and ptype in terms of percentages of TPs, FPs, and FNs which are presented in Table 3, where the labels for ptype are generated by applying a threshold of 0.5 on the posterior distributions. Note that here we aggregate the predictions over the datasets. As per the table, ptype results in a higher number of FPs than Trifacta, but Trifacta produces a higher number of FNs and a lower number of TPs than ptype. Note that here we denote non-type and type entries as Positive and Negative respectively. This indicates that ptype is more likely to identify non-types than Trifacta, but it can also label type entries as non-types more often."
    },
    {
        "id": "877",
        "table": {
            "header": [
                "Experiment",
                "Min",
                "Max",
                "Avg",
                "S.D."
            ],
            "rows": [
                [
                    "Not Learnable",
                    "99.71",
                    "[BOLD] 99.79",
                    "0.997500",
                    "0.0002190"
                ],
                [
                    "Random Init.",
                    "99.72",
                    "[BOLD] 99.78",
                    "0.997512",
                    "0.0001499"
                ],
                [
                    "Ones Init.",
                    "99.70",
                    "99.77",
                    "0.997397",
                    "0.0001885"
                ]
            ],
            "title": "Table 2: Individual Models"
        },
        "insight": "For each of our three experiments, we ran 32 trials, each of which with weights randomly initialized prior to training and, due to the stochastic nature of the data augmentation, a different set of training images. As a result, training progressed to different points in the loss surface resulting in a range of values for the top accuracies that were achieved on the test set. See Table 2. The trial that achieved 99.79% test accuracy establishes a new state of the art for a single model where the previous state of the art was 99.77% . 4 additional trials achieved 99.78% test accuracy, also surpassing the previous state of the art."
    },
    {
        "id": "878",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] Mean RMSE",
                "[BOLD] Mean RMSE"
            ],
            "rows": [
                [
                    "[BOLD] Model Type",
                    "[BOLD] Lanes Known",
                    "[BOLD] Lanes Unknown"
                ],
                [
                    "GP  [ITALIC] \u03c4=8",
                    "13.928",
                    "23.676"
                ],
                [
                    "DNN  [ITALIC] h=2, [ITALIC] g=10",
                    "6.328",
                    "9.145"
                ],
                [
                    "LR",
                    "6.290",
                    "8.878"
                ]
            ],
            "title": "TABLE III: Best performing model of each type. Values closer to zero are better, best values are highlighted in bold."
        },
        "insight": "For each model type among GP, DNN, and LR, Table III summarizes the predictive quality of the best performing model as measured through mean RMSE (Eq. 11) over all [CONTINUE] experiments. [CONTINUE] As may be expected, all models perform better when blocked lanes are known than otherwise. [CONTINUE] LR is the best performing model type when lanes are either known or unknown. GP performs much worse than both LR and DNN, [CONTINUE] The poor fit of GP [CONTINUE] The predictive performance of DNN is a little worse than that of LR, [CONTINUE] and they demonstrate (Table III) that higher precision of incident location indeed yields better predictions across all model types."
    },
    {
        "id": "879",
        "table": {
            "header": [
                "[BOLD] Conditions",
                "[ITALIC] U [BOLD]  and  [ITALIC] D",
                "[BOLD] MSD",
                "[BOLD] MAE",
                "[BOLD] RMSE"
            ],
            "rows": [
                [
                    "Normal",
                    "with",
                    "\u22120.183",
                    "3.555",
                    "4.839"
                ],
                [
                    "Normal",
                    "w/o",
                    "\u22120.025",
                    "3.693",
                    "5.259"
                ],
                [
                    "Incident",
                    "with",
                    "3.040",
                    "6.862",
                    "9.513"
                ],
                [
                    "Incident",
                    "w/o",
                    "0.440",
                    "5.913",
                    "9.085"
                ]
            ],
            "title": "TABLE IV: Performance of Mordinary under normal vs. incident conditions"
        },
        "insight": "we measure how worse Mordinary performs on incident conditions, for which it was not trained. The results are summarized in Table IV, and we see that Mordinary deteriorates in comparison with its performance under normal conditions. The deterioration occurs whether or not Mordinary has access to information about uplink [CONTINUE] and downlink D."
    },
    {
        "id": "880",
        "table": {
            "header": [
                "Methods",
                "0.1%",
                "1%",
                "10%",
                "30%",
                "50%",
                "70%",
                "90%"
            ],
            "rows": [
                [
                    "Peephole ",
                    "0.4556",
                    "0.4769",
                    "0.4963",
                    "0.4977",
                    "0.4972",
                    "0.4975",
                    "0.4951"
                ],
                [
                    "E2EPP ",
                    "0.5038",
                    "0.6734",
                    "0.7009",
                    "0.6997",
                    "0.7011",
                    "0.6992",
                    "0.6997"
                ],
                [
                    "ReNAS-1 (type matrix + MSE)",
                    "0.3465",
                    "0.5911",
                    "0.7914",
                    "0.8229",
                    "0.8277",
                    "0.8344",
                    "0.8350"
                ],
                [
                    "ReNAS-2 (tensor + MSE)",
                    "0.4856",
                    "0.6090",
                    "0.8103",
                    "0.8430",
                    "0.8399",
                    "0.8504",
                    "0.8431"
                ],
                [
                    "ReNAS-3 (type matrix + L1)",
                    "0.6039",
                    "0.7943",
                    "0.8752",
                    "0.8894",
                    "0.8949",
                    "0.8976",
                    "0.8995"
                ],
                [
                    "ReNAS-4 (tensor + L1)",
                    "0.6335",
                    "0.8136",
                    "0.8762",
                    "[BOLD] 0.8900",
                    "[BOLD] 0.8957",
                    "[BOLD] 0.8979",
                    "[BOLD] 0.8997"
                ],
                [
                    "ReNAS-5 (type matrix + L)",
                    "0.6096",
                    "0.7949",
                    "0.8756",
                    "0.8854",
                    "0.8898",
                    "0.8911",
                    "0.8918"
                ],
                [
                    "ReNAS-6 (tensor + L)",
                    "[BOLD] 0.6574",
                    "[BOLD] 0.8161",
                    "[BOLD] 0.8763",
                    "0.8873",
                    "0.8910",
                    "0.8923",
                    "0.8954"
                ]
            ],
            "title": "Table 2: The Kendall\u2019s Tau (KTau) of Peephole, E2EPP and the proposed algorithms on the NASBench dataset with different proportions of training samples."
        },
        "insight": "The experimental results are shown in Tab. 2. [CONTINUE] we focus on the second column when using only 0.1% (424 models) of the NASBench dataset as training set. Different proportions are used in the experiment for integrity. The results show that the proposed encoding method can represent an architecture better than without using it, and the KTau indicator increases about 0.14 when using MSE loss and 0.05 when using pairwise loss. When using pairwise loss instead of element-wise MSE loss, the KTau indicator increases about 0.26 when using only the type matrix as feature, and about 0.17 when using the proposed feature tensor. It means that pairwise loss is better than MSE loss at ranking regardless of input feature. [CONTINUE] Peephole [CONTINUE] shows better result than proposed v1 method [CONTINUE] However, it performs worse than proposed v2 method [CONTINUE] E2EPP used random forest as predictor, which has advantages only when the training samples are extremely rare. When using limited training data, the proposed method with loss function L (Eq. 5) achieves the best KTau performance, while the proposed [CONTINUE] method with L1 loss (Eq. 3) is better when more training data is used. The results show that generating features with continuity has advantageous to model ranking when little training data is used, which is often the case in reality."
    },
    {
        "id": "881",
        "table": {
            "header": [
                "Method",
                "accuracy(%)",
                "ranking(%)"
            ],
            "rows": [
                [
                    "Peephole ",
                    "92.63 \u00b1 0.31",
                    "12.32"
                ],
                [
                    "E2EPP ",
                    "93.47 \u00b1 0.44",
                    "1.23"
                ],
                [
                    "Proposed v1",
                    "92.36 \u00b1 0.27",
                    "16.93"
                ],
                [
                    "Proposed v2",
                    "93.03 \u00b1 0.21",
                    "6.09"
                ],
                [
                    "Proposed v3",
                    "93.43 \u00b1 0.26",
                    "1.50"
                ],
                [
                    "Proposed v4",
                    "93.90 \u00b1 0.21",
                    "0.04"
                ],
                [
                    "Proposed v5",
                    "93.48 \u00b1 0.18",
                    "1.21"
                ],
                [
                    "Proposed v6",
                    "[BOLD] 93.95 \u00b1 0.11",
                    "[BOLD] 0.02"
                ]
            ],
            "title": "Table 3: The classification accuracy (%) on CIFAR-10 dataset and the ranking (%) among different architectures in NASBench dataset using EA algorithm with the proposed predictor and the peer competitors. Predictors are trained with 0.1% samples randomly selected from NASBench dataset."
        },
        "insight": "The second column represents the classification accuracies of the selected models on CIFAR-10 test set, and the third column represents the true ranking of the selected models among all the 423k different models in NASBench dataset. The proposed method outperforms other competitors, and finds an network architecture with top 0.02% performance among the search space using only 0.1% dataset."
    },
    {
        "id": "882",
        "table": {
            "header": [
                "Method",
                "Peephole ",
                "E2EPP ",
                "Proposed"
            ],
            "rows": [
                [
                    "top-1 acc (%)",
                    "73.58",
                    "75.49",
                    "[BOLD] 78.56"
                ],
                [
                    "top-5 acc (%)",
                    "91.97",
                    "92.77",
                    "[BOLD] 94.17"
                ]
            ],
            "title": "Table 4: The classification accuracy (%) on CIFAR-100 dataset among different architectures in NASBench using EA algorithm with the proposed predictor and the peer competitors. Predictors are trained with 424 samples randomly selected from NASBench."
        },
        "insight": "we randomly select 424 architectures and train them on CIFAR-100 dataset from scratch and get the ground-truth labels. [CONTINUE] The results in Tab. 4 show the priority of the proposed method."
    },
    {
        "id": "883",
        "table": {
            "header": [
                "Dataset",
                "Classes",
                "[ITALIC] n",
                "[ITALIC] ntest",
                "[ITALIC] d",
                "Base Error",
                "Input Constraints"
            ],
            "rows": [
                [
                    "MNIST-1-7",
                    "2",
                    "13007",
                    "2163",
                    "784",
                    "0.7%\u00a0( [ITALIC] \u03bb=0.01)",
                    "[0,1]"
                ],
                [
                    "Dogfish",
                    "2",
                    "1800",
                    "600",
                    "2048",
                    "1.3%\u00a0( [ITALIC] \u03bb=1.10)",
                    "[BOLD] R"
                ],
                [
                    "Enron",
                    "2",
                    "4137",
                    "1035",
                    "5116",
                    "2.9%\u00a0( [ITALIC] \u03bb=0.09)",
                    "[BOLD] Z\u22650"
                ],
                [
                    "IMDB",
                    "2",
                    "25000",
                    "25000",
                    "89527",
                    "11.9%\u00a0( [ITALIC] \u03bb=0.01)",
                    "[BOLD] Z\u22650"
                ],
                [
                    "MNIST",
                    "10",
                    "60000",
                    "10000",
                    "784",
                    "7.5% \\lx @ [ITALIC] notemarkfootnote",
                    "[0,1]"
                ]
            ],
            "title": "Table 1: Characteristics of the datasets we consider, together with the base test errors that an SVM achieves on them (with regularization parameters \u03bb selected by validation). The input covariates for Enron and IMDB must be non-negative integers."
        },
        "insight": "Summaries of each dataset are given in Table 1, including the number of training points n, the dimension of each point d, and the base accuracy of an SVM trained only on the clean data. Each dataset has its own characteristics and input constraints: [CONTINUE] The MNIST-1-7 image dataset (LeCun et al., 1998) requires each input feature to lie within the interval [0, 1] (representing normalized pixels). [CONTINUE] It is easily linearly separable: an SVM achieves 0.7% error on the clean data. [CONTINUE] The Dogfish image dataset (Koh and Liang, 2017) has no input constraints; [CONTINUE] paper we allow to take any value in Rd. Compared to the MNIST-1-7 dataset (where n (cid:29) d), the Dogfish dataset has n \u2248 d, allowing attackers to potentially exploit overfitting. The Dogfish dataset also has a low base error of 1.3%. [CONTINUE] The Enron spam classification text dataset (Metsis et al., 2006) requires input to be non-negative and integer valued, [CONTINUE] As with the Dogfish dataset, the Enron dataset has n \u2248 d and a relatively low base error of 3.0%. [CONTINUE] The IMDB sentiment classification text dataset (Maas et al., 2011) similarly requires input to be non-negative and integer valued. Compared to the other datasets, the IMDB dataset has significantly larger n and d, presenting computational challenges. It also has n (cid:28) d and is not as linearly separable as the other datasets, with a high base error of 11.9%."
    },
    {
        "id": "884",
        "table": {
            "header": [
                "[EMPTY]",
                "kNN",
                "CelebA 28.00",
                "LSUN 36.30"
            ],
            "rows": [
                [
                    "Accuracy",
                    "Eigenface\u00a0",
                    "53.28",
                    "-"
                ],
                [
                    "(%)",
                    "PRNU\u00a0",
                    "86.61",
                    "67.84"
                ],
                [
                    "[EMPTY]",
                    "Ours",
                    "[BOLD] 99.43",
                    "[BOLD] 98.58"
                ],
                [
                    "FD ratio",
                    "Inception\u00a0",
                    "2.36",
                    "5.27"
                ],
                [
                    "[EMPTY]",
                    "Our fingerprint",
                    "[BOLD] 454.76",
                    "[BOLD] 226.59"
                ]
            ],
            "title": "Table 1: Evaluation on {real, ProGAN, SNGAN, CramerGAN, MMDGAN}. The best performance is highlighted in bold."
        },
        "insight": "Table 1 shows that we can effectively differentiate GANgenerated images from real ones and attribute generated images to their sources, just using a regular CNN classifier."
    },
    {
        "id": "885",
        "table": {
            "header": [
                "[EMPTY]",
                "kNN",
                "CelebA 10.88",
                "LSUN 10.58"
            ],
            "rows": [
                [
                    "Accuracy",
                    "Eigenface\u00a0",
                    "23.12",
                    "-"
                ],
                [
                    "(%)",
                    "PRNU\u00a0",
                    "89.40",
                    "69.73"
                ],
                [
                    "[EMPTY]",
                    "Ours",
                    "[BOLD] 99.14",
                    "[BOLD] 97.04"
                ],
                [
                    "[EMPTY]",
                    "Our visNet",
                    "97.07",
                    "96.58"
                ],
                [
                    "FD ratio",
                    "Inception\u00a0",
                    "1.10",
                    "1.29"
                ],
                [
                    "[EMPTY]",
                    "Our fingerprint",
                    "[BOLD] 80.28",
                    "[BOLD] 36.48"
                ]
            ],
            "title": "Table 3: Evaluation on {real, ProGAN_seed_v#i}. The best performance is highlighted in bold. \u201cOur visNet\u201d row indicates our fingerprint visualization network described in Section\u00a03.3 and evaluated in Section\u00a04.5."
        },
        "insight": "In this setup of {real, ProGAN seed v#i} where i \u2208 {1, ..., 10}, we show the performance evaluation in Table 3. [CONTINUE] All the FD ratio measures consistently decreased compared to Table 3. [CONTINUE] We show the attribution performance in the \"Our visNet\" row in Table 3,"
    },
    {
        "id": "886",
        "table": {
            "header": [
                "[BOLD] Layer",
                "[BOLD] Shapes"
            ],
            "rows": [
                [
                    "conv2d_1",
                    "5\u00d75\u00d71\u00d732"
                ],
                [
                    "conv2d_1",
                    "32"
                ],
                [
                    "conv2d_2",
                    "5\u00d75\u00d732\u00d764"
                ],
                [
                    "conv2d_2",
                    "64"
                ],
                [
                    "dense_1",
                    "1024\u00d7512"
                ],
                [
                    "dense_1",
                    "512"
                ],
                [
                    "dense_2",
                    "512\u00d710"
                ],
                [
                    "dense_2",
                    "10"
                ]
            ],
            "title": "TABLE II: Parameters settings for the CNN."
        },
        "insight": "The architecture of the CNN used for the MNIST task has two 5 \u00d7 5 convolution layers (the first one has 32 channels and the other has 64) and 2 \u00d7 2 max pooling layers. Then a fully connected layer with 512 units and ReLu activation is followed. The output layer is a softmax unit. The parameter settings for the CNN are listed in Table II."
    },
    {
        "id": "887",
        "table": {
            "header": [
                "[BOLD] Layer",
                "[BOLD] Shapes"
            ],
            "rows": [
                [
                    "lstm_1",
                    "9\u00d7100"
                ],
                [
                    "lstm_1",
                    "25\u00d7100"
                ],
                [
                    "lstm_1",
                    "100"
                ],
                [
                    "lstm_2",
                    "25\u00d7100"
                ],
                [
                    "lstm_2",
                    "25\u00d7100"
                ],
                [
                    "lstm_2",
                    "100"
                ],
                [
                    "dense_1",
                    "25\u00d7256"
                ],
                [
                    "dense_1",
                    "256"
                ],
                [
                    "dense_2",
                    "256\u00d76"
                ],
                [
                    "dense_2",
                    "6"
                ]
            ],
            "title": "TABLE III: Parameter settings for the LSTM."
        },
        "insight": "The architecture of the LSTM used in this study has two 5 \u00d7 5 LSTM layers (the first one with cell size = 20 and time steps = 128 and the other with cell size = 10 and the same time steps), a fully connected layer with 128 units and the ReLu activation, and a softmax output layer. The corresponding parameters of the LSTM is given in Table III."
    },
    {
        "id": "888",
        "table": {
            "header": [
                "[ITALIC]  [BOLD] freq",
                "[BOLD] Accuracy*",
                "[BOLD] Round**"
            ],
            "rows": [
                [
                    "3/15",
                    "96.72% (0.0037)",
                    "115.74 (32.19)"
                ],
                [
                    "5/15",
                    "97.08% (0.0037)",
                    "87.82 (20.03)"
                ],
                [
                    "7/15",
                    "97.12% (0.0046)",
                    "95.43 (14.64)"
                ]
            ],
            "title": "TABLE IV: Experimental results on freq."
        },
        "insight": "a = e/2, K = 20, and m = 2. Two metrics are adopted in this work for measuring the performance of the compared algorithms. One is the best accuracy of the central model within 200 rounds and the other is to the required rounds before the central model's accuracy reaches 95.0%. Note that the same computing rounds means the same communication cost. All experiments are independently run for ten times, and their average (AVG) and standard deviation (STDEV) values are presented in Tables IV, V, and VI. In the tables, the average value is listed before the standard deviation in parenthesis. Based on the results in Tables IV, V, and VI, the following observations can be made. [CONTINUE] \u2022 Analysis on freq: From the results presented in Table the lower the exchange IV, we can conclude that frequency is, the less communication costs will be required, which is very much expected. However, a too low f req will deteriorate the accuracy of the central model."
    },
    {
        "id": "889",
        "table": {
            "header": [
                "[ITALIC]  [BOLD] a",
                "[BOLD] Accuracy*",
                "[BOLD] Round**"
            ],
            "rows": [
                [
                    "e***",
                    "96.92% (0.0041)",
                    "75.26 (2.09)"
                ],
                [
                    "e/2",
                    "97.08% (0.0037)",
                    "87.82 (20.03)"
                ]
            ],
            "title": "TABLE V: Experimental results on a."
        },
        "insight": "a = e/2, K = 20, and m = 2. Two metrics are adopted in this work for measuring the performance of the compared algorithms. One is the best accuracy of the central model within 200 rounds and the other is to the required rounds before the central model's accuracy reaches 95.0%. Note that the same computing rounds means the same communication cost. All experiments are independently run for ten times, and their average (AVG) and standard deviation (STDEV) values are presented in Tables IV, V, and VI. In the tables, the average value is listed before the standard deviation in parenthesis. Based on the results in Tables IV, V, and VI, the following observations can be made. [CONTINUE] The results in Table V indicate that e/2 is a better option for the CNN on the 1@MNIST dataset. When a = 1, the algorithm is reduced to AFL, meaning that the parameters uploaded in different rounds will be of equal importance in the aggregation."
    },
    {
        "id": "890",
        "table": {
            "header": [
                "[BOLD] Scalability  [ITALIC]  [BOLD] K  [BOLD] m",
                "[BOLD] FedAVG",
                "[BOLD] TWAFL"
            ],
            "rows": [
                [
                    "20 2",
                    "96.83% (0.0097)",
                    "97.16% (0.0096)"
                ],
                [
                    "10 1",
                    "94.26% (0.0083)",
                    "94.76% (0.0102)"
                ],
                [
                    "10 2",
                    "96.16% (0.0167)",
                    "96.11% (0.0909)"
                ]
            ],
            "title": "TABLE VI: Experimental results on the scalability of the algorithm."
        },
        "insight": "a = e/2, K = 20, and m = 2. Two metrics are adopted in this work for measuring the performance of the compared algorithms. One is the best accuracy of the central model within 200 rounds and the other is to the required rounds before the central model's accuracy reaches 95.0%. Note that the same computing rounds means the same communication cost. All experiments are independently run for ten times, and their average (AVG) and standard deviation (STDEV) values are presented in Tables IV, V, and VI. In the tables, the average value is listed before the standard deviation in parenthesis. Based on the results in Tables IV, V, and VI, the following observations can be made. [CONTINUE] separately assessed. Based on the results presented in Table VI, the following three conclusions can be made. First, a larger number of involved clients (a larger m) leads to a higher recognition accuracy. Second, TWAFL outperforms FedAVG when the active client fraction is [CONTINUE] = 0.1, as indicated in the results in rows one and two in the table. Third, FedAVG is slightly better when the total number of clients is smaller and [CONTINUE] is higher, as shown by the results listed in the last row of the table. This implies that the advantage of the proposed algorithm over the traditional federated learning will become more apparent as the number of clients increases. This is encouraging since in most realworld problems, the number of clients is typically very large."
    },
    {
        "id": "891",
        "table": {
            "header": [
                "[BOLD] Dataset_ID@Task",
                "[BOLD] FedAVG  [ITALIC]  [BOLD] Round (Accuracy)\u22c6  [BOLD] C. Cost\u22c6\u22c6",
                "[BOLD] TEFL  [ITALIC]  [BOLD] Round (Accuracy)\u22c6  [BOLD] C. Cost\u22c6\u22c6",
                "[BOLD] TWAFL  [ITALIC]  [BOLD] Round (Accuracy)\u22c6  [BOLD] C. Cost\u22c6\u22c6",
                "[BOLD] AFL  [ITALIC]  [BOLD] Round (Accuracy)\u22c6  [BOLD] C. Cost\u22c6\u22c6"
            ],
            "rows": [
                [
                    "1@MNIST*",
                    "75 (97.2%) 6.16",
                    "[BOLD] 31 (97.9%) 0.74",
                    "106 (97.7%) 1",
                    "175 (95.4%) 2.46"
                ],
                [
                    "2@MNIST*",
                    "85 (97.2%) 6.76",
                    "[BOLD] 32 (98.5%) 1.33",
                    "61 (98.1%)  [BOLD] 1",
                    "\u2014(94.8%)\u2020 \u2014\u2020"
                ],
                [
                    "3@MNIST*",
                    "73 (97.7%) 5.99",
                    "[BOLD] 31 (98.7%) 1.13",
                    "70 (97.9%)  [BOLD] 1",
                    "\u2014(94.9%)\u2020 \u2014\u2020"
                ],
                [
                    "4@MNIST*",
                    "196 (95.2%) 8.18",
                    "[BOLD] 61 (98.0%) 1.14",
                    "136 (96.1%)  [BOLD] 1",
                    "\u2014(93.1%)\u2020 \u2014\u2020"
                ],
                [
                    "5@MNIST*",
                    "98 (97.1%) 3.28",
                    "[BOLD] 76 (97.8%) 3.17",
                    "61 (96.1%)  [BOLD] 1",
                    "109 (96.7%) 2.66"
                ],
                [
                    "1@HAR**",
                    "526 (92.3%) 4.72",
                    "[BOLD] 166 (94.0%)  [BOLD] 0.69",
                    "358  [BOLD] (94.6%) 1",
                    "\u2014(89.2%) \u2021 \u2014\u2021"
                ],
                [
                    "2@HAR**",
                    "451 (94.7%) 5.65",
                    "[BOLD] 119 (95.4%)  [BOLD] 0.57",
                    "313  [BOLD] (95.9%) 1",
                    "\u2014(82.9%) \u2021 \u2014\u2021"
                ],
                [
                    "3@HAR**",
                    "\u2014(87.3%) \u2021 \u2014\u2021",
                    "[BOLD] 174 (94.4%) 0.69",
                    "376 (93.2%) 1",
                    "\u2014(88.5%) \u2021 \u2014\u2021"
                ],
                [
                    "4@HAR**",
                    "856 (90.2%) 7.05",
                    "[BOLD] 181 (95.1%) 1.28",
                    "211 (94.1%)  [BOLD] 1",
                    "751 (91.2%) 5.30"
                ],
                [
                    "5@HAR**",
                    "571 (92.6%) 5.49",
                    "[BOLD] 155 (93.6%) 0.57",
                    "404 (93.1%) 1",
                    "646 (92.5%) 2.38"
                ]
            ],
            "title": "TABLE VII: Experiments on Performance ."
        },
        "insight": "Finally, the comparative results of the four algorithms on ten test cases generated from MNIST and HAR tasks are given in Table VII. The listed metrics include the number of rounds needed, the classification accuracy (listed in parenthesis), and total communication cost (C. Cost for short). From these results, the following observations can be made: \u2022 Both TWAFL and TEFL outperform FedAVG on most cases in terms of the total number of rounds, the best accuracy, and the total communication cost. \u2022 TEFL achieves the best performance on most tasks in terms of the total number of rounds and the best accuracy. The temporally weighted aggregation strategy accelerates the convergence of the learning and improved the learning performance. \u2022 TWAFL performs slightly better than TEFL on MNIST in terms of the total communication cost, while TEFL works better than TWAFL on the HAR datasets. The asynchronous model update strategy significantly contributes to reducing the communication cost per round. \u2022 AFL performs the worst among the four compared algorithms. When comparing the performance of the AFL only adopting the asynchronous strategy and the TWAFL using both of them, the asynchronous one always needs the help of the other strategy."
    },
    {
        "id": "892",
        "table": {
            "header": [
                "[EMPTY]",
                "LSUN  [ITALIC] Noise",
                "LSUN  [ITALIC] Noise",
                "LSUN  [ITALIC] Blur",
                "LSUN  [ITALIC] Blur",
                "LSUN  [ITALIC] Cropping",
                "LSUN  [ITALIC] Cropping",
                "LSUN  [ITALIC] Compression",
                "LSUN  [ITALIC] Compression",
                "LSUN  [ITALIC] Relighting",
                "LSUN  [ITALIC] Relighting",
                "LSUN  [ITALIC] Combination",
                "LSUN  [ITALIC] Combination"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs"
                ],
                [
                    "PRNU\u00a0",
                    "[BOLD] 39.59",
                    "40.97",
                    "26.92",
                    "30.79",
                    "9.30",
                    "9.42",
                    "18.27",
                    "23.66",
                    "60.86",
                    "63.31",
                    "16.54",
                    "16.89"
                ],
                [
                    "Ours",
                    "11.80",
                    "[BOLD] 95.30",
                    "[BOLD] 74.48",
                    "[BOLD] 96.68",
                    "[BOLD] 86.20",
                    "[BOLD] 97.30",
                    "[BOLD] 24.73",
                    "[BOLD] 92.40",
                    "[BOLD] 62.21",
                    "[BOLD] 97.36",
                    "[BOLD] 24.44",
                    "[BOLD] 83.42"
                ]
            ],
            "title": "Table 8: Classification accuracy (%) of our network w.r.t. different perturbation attacks before or after immunization on LSUN bedroom {real, ProGAN_seed_v#i}. The best performance is highlighted in bold."
        },
        "insight": "Given perturbed images and the setup of {real, ProGAN seed v#i}, we show the pre-trained classifier performances in the \"Akt\" columns in Table 7 and Table 8. [CONTINUE] Given perturbed images and the setup of {real, ProGAN seed v#i}, we show the finetuned classifier performance in the \"Dfs\" columns in Table 7 and Table 8."
    },
    {
        "id": "893",
        "table": {
            "header": [
                "BSD-68 Methods",
                "BSD-68 BM3D",
                "BSD-68 DnCNN-SURE",
                "BSD-68 DnCNN-SURE*",
                "BSD-68 DnCNN-N2N",
                "BSD-68 DnCNN-eSURE",
                "BSD-68 DnCNN-MSE"
            ],
            "rows": [
                [
                    "[ITALIC] \u03c3=25",
                    "28.56",
                    "28.92",
                    "29.00",
                    "[BOLD] 29.08",
                    "[BOLD] 29.08",
                    "29.20"
                ],
                [
                    "[ITALIC] \u03c3=50",
                    "25.62",
                    "26.00",
                    "26.07",
                    "26.13",
                    "[BOLD] 26.15",
                    "26.22"
                ],
                [
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12"
                ],
                [
                    "[ITALIC] \u03c3=25",
                    "29.97",
                    "30.04",
                    "30.13",
                    "30.30",
                    "[BOLD] 30.31",
                    "30.42"
                ],
                [
                    "[ITALIC] \u03c3=50",
                    "26.67",
                    "26.87",
                    "26.97",
                    "[BOLD] 27.07",
                    "[BOLD] 27.07",
                    "27.16"
                ],
                [
                    "Noisy dataset",
                    "-",
                    "1",
                    "2",
                    "2",
                    "2",
                    "\u221e"
                ]
            ],
            "title": "Table 1: PSNR results of blind denoisers on BSD68 and Set12 datasets."
        },
        "insight": "The performance of our approach along with the state-of- the-art methods was tabulated in Table 1. Our network training approach demonstrates almost identical quantitative results with Noise2Noise trained DnCNN (DnCNN-N2N) in both test sets. [CONTINUE] Moreover, quantitative analysis on BSD68 test set reveals that our eSURE is consistently better than conventional BM3D for about 0.5dB and outperforms DnCNN-SURE for about 0.15 dB in lower and higher noise cases. The performance gap between the proposed method, BM3D and DnCNN-SURE are still similar for Set12 test set. In addition, we can observe that minimizing MC-SURE with twice more dataset (DnCNN-SURE*) provides a little improvement, but not enough to reach DnCNN-eSURE and DnCNN-N2N."
    },
    {
        "id": "894",
        "table": {
            "header": [
                "BSD-68  [ITALIC] \u03c3noisy",
                "BSD-68 25",
                "BSD-68 25",
                "BSD-68 25",
                "BSD-68 50",
                "BSD-68 50",
                "BSD-68 50",
                "BSD-68 50"
            ],
            "rows": [
                [
                    "[ITALIC] \u03c3gt",
                    "1",
                    "5",
                    "10",
                    "1",
                    "5",
                    "10",
                    "20"
                ],
                [
                    "BM3D",
                    "28.56",
                    "28.56",
                    "28.56",
                    "25.62",
                    "25.62",
                    "25.62",
                    "25.62"
                ],
                [
                    "DnCNN-SURE",
                    "29.05",
                    "29.01",
                    "29.02",
                    "25.95",
                    "25.97",
                    "25.90",
                    "25.92"
                ],
                [
                    "DnCNN-N2N",
                    "29.23",
                    "29.15",
                    "28.37",
                    "[BOLD] 26.28",
                    "26.24",
                    "25.91",
                    "24.69"
                ],
                [
                    "DnCNN-eSURE",
                    "[BOLD] 29.23",
                    "[BOLD] 29.23",
                    "[BOLD] 29.21",
                    "26.27",
                    "[BOLD] 26.24",
                    "[BOLD] 26.27",
                    "[BOLD] 26.25"
                ],
                [
                    "DnCNN-MSE",
                    "29.23",
                    "29.23",
                    "29.23",
                    "26.28",
                    "26.28",
                    "26.28",
                    "26.28"
                ],
                [
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12",
                    "Set 12"
                ],
                [
                    "BM3D",
                    "29.97",
                    "29.97",
                    "29.97",
                    "26.67",
                    "26.67",
                    "26.67",
                    "26.67"
                ],
                [
                    "DnCNN-SURE",
                    "30.23",
                    "30.19",
                    "30.19",
                    "26.77",
                    "26.85",
                    "26.73",
                    "26.74"
                ],
                [
                    "DnCNN-N2N",
                    "30.41",
                    "30.39",
                    "29.46",
                    "[BOLD] 27.28",
                    "27.20",
                    "27.08",
                    "25.39"
                ],
                [
                    "DnCNN-eSURE",
                    "[BOLD] 30.47",
                    "[BOLD] 30.48",
                    "[BOLD] 30.44",
                    "27.27",
                    "[BOLD] 27.27",
                    "[BOLD] 27.25",
                    "[BOLD] 27.23"
                ],
                [
                    "DnCNN-MSE",
                    "30.47",
                    "30.47",
                    "30.47",
                    "27.28",
                    "27.28",
                    "27.28",
                    "27.28"
                ]
            ],
            "title": "Table 2: Results of denoising methods on BSD68 and Set 12 datasets (Performance in dB)."
        },
        "insight": "We simulated the case with imperfect ground truth images (or slightly noisy ground truth), by adding synthetic Gaussian noise with \u03c3gt to the noiseless clean oracle images. Consequently, noisy training images were generated by adding i.i.d. Gaussian noise on the top of the imperfect ground truth dataset. [CONTINUE] Table 2 shows the performance of denoising methods trained for a fixed noise given a noisy groundtruth images with \u03c3gt = {1, 5, 10, 20}. The higher the \u03c3gt is, the more correlated noise is in a training set. We notice that at low level of ground-truth noise (\u03c3gt = 1), both Noise2Noise (DnCNN-N2N) and eSURE (DnCNN-eSURE) yield the best PSNR results and even comparable to the DnCNNMSE. However, as noise correlation gets severe (\u03c3gt = 5, 10), Noise2Noise fails to achieve high performance that is consistent with our theoretical derivation. In contrast, the proposed DnCNN [CONTINUE] eSURE produces the best quantitative results in a stable manner. Although, DnCNN-SURE is not susceptible to noise correlation, it still yielded worse performance than DnCNN-eSURE."
    },
    {
        "id": "895",
        "table": {
            "header": [
                "RGB BSD-68  [ITALIC] \u03c3noisy",
                "RGB BSD-68 25",
                "RGB BSD-68 25",
                "RGB BSD-68 25",
                "RGB BSD-68 50",
                "RGB BSD-68 50",
                "RGB BSD-68 50",
                "RGB BSD-68 50"
            ],
            "rows": [
                [
                    "[ITALIC] \u03c3gt",
                    "1",
                    "5",
                    "10",
                    "1",
                    "5",
                    "10",
                    "20"
                ],
                [
                    "CBM3D",
                    "30.70",
                    "30.70",
                    "30.70",
                    "27.38",
                    "27.38",
                    "27.38",
                    "27.38"
                ],
                [
                    "CDnCNN-SURE",
                    "30.97",
                    "30.98",
                    "30.99",
                    "27.63",
                    "27.68",
                    "27.64",
                    "27.63"
                ],
                [
                    "CDnCNN-N2N",
                    "31.18",
                    "31.08",
                    "29.83",
                    "27.89",
                    "27.87",
                    "27.61",
                    "25.62"
                ],
                [
                    "CDnCNN-eSURE",
                    "[BOLD] 31.20",
                    "[BOLD] 31.18",
                    "[BOLD] 31.19",
                    "[BOLD] 27.94",
                    "[BOLD] 27.91",
                    "[BOLD] 27.90",
                    "[BOLD] 27.78"
                ],
                [
                    "DnCNN-MSE",
                    "31.20",
                    "31.20",
                    "31.20",
                    "27.93",
                    "27.93",
                    "27.93",
                    "27.93"
                ]
            ],
            "title": "Table 3: Results of denoising methods on RGB color BSD68 dataset(Performance in dB)."
        },
        "insight": "The experimental results for RGB color image denoising case are tabulated in Table 3. In this case, we observe the same performance degradation pattern of DnCNN-N2N as the noise in ground truth image increases (more results in the supplementary material)."
    },
    {
        "id": "896",
        "table": {
            "header": [
                "Method",
                "ML-100k",
                "ML-1M"
            ],
            "rows": [
                [
                    "GCMC (original)",
                    "0.910",
                    "0.832"
                ],
                [
                    "GCMC (new split)",
                    "1.2259\u00b10.0001",
                    "1.1414\u00b10.0002"
                ],
                [
                    "GCMC-LSTM\u2020",
                    "1.2556\u00b10.0509",
                    "1.1857\u00b10.0193"
                ],
                [
                    "GCMC-GRU\u2020",
                    "1.2446\u00b10.0404",
                    "1.2056\u00b10.0171"
                ],
                [
                    "GCMC-LSTM\u2021",
                    "1.0725\u00b10.0217",
                    "[BOLD] 1.0394\u00b10.0230"
                ],
                [
                    "GCMC-GRU\u2021",
                    "[BOLD] 1.0630\u00b10.0141",
                    "1.0407\u00b10.0336"
                ]
            ],
            "title": "Table 1: RMSE values for all models on the ML-100k and ML-1M datasets using the new training and test sets, averaged over 5 different runs, along with standard deviations. GCMC (original) results are the same as those reported in previous work\u00a0[1]. The \u2020 symbol denotes experiments using disjoint edge sets in every M(i); \u2021 denotes the representation where every step contains all edges from previous steps."
        },
        "insight": "At first glance, the most striking result is the difference in performance GCMC displays when the training and test sets were changed to the new ones. As the original training and test sets are closer to a uniform sample taken from the whole dataset, their distributions are similar. Once we view this from a temporal perspective, that is, unseen data is comprised of future ratings, it is evident that such a change in the distribution severely affects GCMC. This highlights the importance of taking into account the evolution of user-item relationships happening over time. [CONTINUE] With this in mind, we see that the recurrent models attain better results than the baseline only in one of the representations. While both LSTM and GRU variants displayed very similar performance, the disjoint edge representation performs worse than its alternative. [CONTINUE] As the recurrent models have the exact same configuration in both scenarios, the representation is clearly the main factor influencing the performance of the model. When using all edges from previous steps at every step, the performance gap is evident."
    },
    {
        "id": "897",
        "table": {
            "header": [
                "[EMPTY]",
                "CelebA  [ITALIC] Noise",
                "CelebA  [ITALIC] Noise",
                "CelebA  [ITALIC] Blur",
                "CelebA  [ITALIC] Blur",
                "CelebA  [ITALIC] Cropping",
                "CelebA  [ITALIC] Cropping",
                "CelebA  [ITALIC] Compression",
                "CelebA  [ITALIC] Compression",
                "CelebA  [ITALIC] Relighting",
                "CelebA  [ITALIC] Relighting",
                "CelebA  [ITALIC] Combination",
                "CelebA  [ITALIC] Combination"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs",
                    "Atk",
                    "Dfs"
                ],
                [
                    "PRNU\u00a0",
                    "[BOLD] 57.88",
                    "63.82",
                    "27.37",
                    "42.43",
                    "9.84",
                    "10.68",
                    "[BOLD] 26.15",
                    "44.55",
                    "86.59",
                    "87.02",
                    "[BOLD] 19.93",
                    "21.77"
                ],
                [
                    "Ours",
                    "9.14",
                    "[BOLD] 93.02",
                    "[BOLD] 49.64",
                    "[BOLD] 97.20",
                    "[BOLD] 46.80",
                    "[BOLD] 98.28",
                    "8.77",
                    "[BOLD] 88.02",
                    "[BOLD] 94.02",
                    "[BOLD] 98.66",
                    "19.31",
                    "[BOLD] 72.64"
                ]
            ],
            "title": "Table 7: Classification accuracy (%) of our network w.r.t. different perturbation attacks before or after immunization on CelebA {real, ProGAN_seed_v#i}. The best performance is highlighted in bold."
        },
        "insight": "Given perturbed images and the setup of {real, ProGAN seed v#i}, we show the pre-trained classifier performances in the \"Akt\" columns in Table 7 and Table 8. [CONTINUE] Given perturbed images and the setup of {real, ProGAN seed v#i}, we show the finetuned classifier performance in the \"Dfs\" columns in Table 7 and Table 8."
    },
    {
        "id": "898",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Uncalibrated Freq.",
                "Uncalibrated MC Dropout",
                "Uncalibrated MC Dropout",
                "TS Calibrated Freq.",
                "TS Calibrated MC Dropout",
                "TS Calibrated MC Dropout"
            ],
            "rows": [
                [
                    "Data Set",
                    "Model",
                    "ECE",
                    "ECE",
                    "UCE",
                    "ECE",
                    "ECE",
                    "UCE"
                ],
                [
                    "CIFAR-10",
                    "ResNet-18",
                    "8.95",
                    "8.41",
                    "7.60",
                    "1.40",
                    "[BOLD] 0.47",
                    "[BOLD] 5.27"
                ],
                [
                    "CIFAR-100",
                    "ResNet-101",
                    "29.63",
                    "24.62",
                    "30.33",
                    "3.50",
                    "[BOLD] 1.92",
                    "[BOLD] 2.41"
                ],
                [
                    "CIFAR-100",
                    "DenseNet-169",
                    "30.62",
                    "23.98",
                    "29.62",
                    "6.10",
                    "[BOLD] 2.89",
                    "[BOLD] 2.69"
                ]
            ],
            "title": "Table 1: ECE and UCE test set results in % (M=15 bins). 0\u2009% means perfect calibration. In TS calibration with MC dropout the same value of T was used to report both ECE and UCE."
        },
        "insight": "Tab. 1 reports test set results for different networks [17, 18] and data sets used to evaluate the performance of temperature scaling for dropout variational inference. The proposed UCE metric is used to quantify calibration of uncertainty. [CONTINUE] Uncalibrated ECE shows, that MC dropout already reduces miscalibration of model likelihood by up to 6.6 percentage points. With TS calibration, MC dropout reduces ECE by 45\u201366 % and UCE drops drastically (especially for larger networks)."
    },
    {
        "id": "899",
        "table": {
            "header": [
                "[BOLD] Attempt",
                "[BOLD] Response Time",
                "[BOLD] Error Rate",
                "[BOLD] Score"
            ],
            "rows": [
                [
                    "[BOLD] First",
                    "0.802(0.113)",
                    "0.069(0.054)",
                    "0.395(0.373)"
                ],
                [
                    "[BOLD] Second",
                    "0.844(0.168)",
                    "0.096(0.066)",
                    "0.010(0.500)"
                ],
                [
                    "[BOLD] p-value",
                    "0.0075",
                    "0.0006",
                    "<0.0001"
                ]
            ],
            "title": "TABLE III: Mean value and standard deviation for the critical blocks of first and second attempts; p-value indicating the significance of the difference between the first and second attempts"
        },
        "insight": "Table III shows the mean and standard deviation of the response times, error rates, and IAT scores for both the first and second IAT attempts [CONTINUE] The last row shows the p-value of a two-tailed dependent t-test for paired samples. These values indicate a strong statistical difference between the first and second attempts. [CONTINUE] the average IAT score was 0.39 [CONTINUE] The mean score decreased to 0.01"
    },
    {
        "id": "900",
        "table": {
            "header": [
                "[BOLD] ML Method",
                "[BOLD] Unpruned ( [ITALIC] n=154)",
                "[BOLD] Pruned ( [ITALIC] n=102)"
            ],
            "rows": [
                [
                    "Naive Bayes",
                    "0.721",
                    "0.735"
                ],
                [
                    "SVM",
                    "0.728",
                    "0.779"
                ],
                [
                    "Logistic",
                    "0.747",
                    "0.716"
                ],
                [
                    "Multilayer Perceptron",
                    "0.719",
                    "0.812"
                ],
                [
                    "Simple Logistic",
                    "0.700",
                    "0.764"
                ],
                [
                    "JRip",
                    "0.675",
                    "0.712"
                ],
                [
                    "Random Forest",
                    "0.678",
                    "0.745"
                ]
            ],
            "title": "TABLE IV: Results of various machine learning methods. Pruning the data entailed of removing deception attempts that resulted in a <1 SD score change."
        },
        "insight": "For each subset, we used 7 distinct machine learning algorithms provided by Weka (using the default parameters): naive Bayes, support vector machines, multinomial logistic regression, multilayer perceptron, simple logistic regression, propositional rule learner (JRip), and random forest. [CONTINUE] Table IV shows the weighted F1 scores across each subset and with each ML method. [CONTINUE] On the unpruned data, multinomial logistic performed the best achieving an F1 score of 0.75 followed closely by naive bayes, SVM, and multilayer perceptron. The multilayer perceptron performed the best on the pruned data achieving an F1 score of 0.81. All models, with the exception of multinomial logistic, performed better on the pruned data. Overall, the more complex models (i.e., SVM and multilayer perceptron) performed the best across the pruned and unpruned data."
    },
    {
        "id": "901",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] F1 Unpruned",
                "[BOLD] F1 Pruned"
            ],
            "rows": [
                [
                    "MLP",
                    "0.78",
                    "0.81"
                ],
                [
                    "Ratio",
                    "0.64",
                    "0.58"
                ]
            ],
            "title": "TABLE V: F1 scores for TensorFlow MLP and ratio-based method from [1]"
        },
        "insight": "Table V shows our top performing model (the TensorFlow MLP) compared against the ratio-based model both on the unpruned and pruned data. [CONTINUE] The ratio-based model showed accuracies of 70% or lower as opposed to 80%"
    },
    {
        "id": "902",
        "table": {
            "header": [
                "Dataset Models",
                "Cora GCN",
                "Cora SGC",
                "Cora DeepWalk",
                "Cora LINE",
                "Citeseer GCN",
                "Citeseer SGC",
                "Citeseer DeepWalk",
                "Citeseer LINE",
                "Pubmed GCN",
                "Pubmed SGC",
                "Pubmed DeepWalk",
                "Pubmed LINE"
            ],
            "rows": [
                [
                    "(unattacked)",
                    "80.20",
                    "78.82",
                    "77.23",
                    "76.75",
                    "72.50",
                    "69.68",
                    "69.68",
                    "65.15",
                    "80.40",
                    "80.21",
                    "78.69",
                    "72.12"
                ],
                [
                    "[ITALIC] Random",
                    "-1.90",
                    "-1.22",
                    "-1.76",
                    "-1.84",
                    "-2.86",
                    "-1.47",
                    "-6.62",
                    "-1.78",
                    "-1.75",
                    "-1.77",
                    "-1.25",
                    "-1.01"
                ],
                [
                    "[ITALIC] Degree",
                    "-2.21",
                    "-4.42",
                    "-3.08",
                    "-12.40",
                    "-4.68",
                    "-5.21",
                    "-9.67",
                    "-12.55",
                    "-3.86",
                    "-4.44",
                    "-2.43",
                    "-13.05"
                ],
                [
                    "[ITALIC] RL-S2V",
                    "-5.20",
                    "-5.62",
                    "-5.24",
                    "-10.38",
                    "-6.50",
                    "-4.08",
                    "-12.13",
                    "-20.10",
                    "-6.40",
                    "-6.11",
                    "-6.10",
                    "-13.21"
                ],
                [
                    "A [ITALIC] class",
                    "-3.62",
                    "-2.96",
                    "[BOLD] -6.29",
                    "-7.55",
                    "-3.48",
                    "-2.83",
                    "[BOLD] -12.56",
                    "-10.28",
                    "-4.21",
                    "-2.25",
                    "-3.05",
                    "-6.75"
                ],
                [
                    "[ITALIC] GF-Attack",
                    "[BOLD] -7.60",
                    "[BOLD] -9.73",
                    "-5.31",
                    "[BOLD] -13.27",
                    "[BOLD] -7.78",
                    "[BOLD] -6.19",
                    "-12.50",
                    "[BOLD] -22.11",
                    "[BOLD] -7.96",
                    "[BOLD] -7.20",
                    "[BOLD] -7.43",
                    "[BOLD] -14.16"
                ]
            ],
            "title": "Table 1: Summary of the change in classification accuracy (in percent) compared to the clean/original graph. Single edge perturbation under RBA setting. Lower is better."
        },
        "insight": "Table 1 summaries the attack results of different attackers on Graph Convolutional Networks. Our GF-Attack attacker outperforms other attackers on all datasets and all models. Moreover, GFAttack performs quite well on 2 layers GCN with nonlinear activation. This implies the generalization ability of our attacker on Graph Convolutional Networks. [CONTINUE] Table 1 also summaries the attack results of different attackers on sampling-based graph embedding models. As expected, our attacker achieves the best performance nearly on all target [CONTINUE] models. It validates the effectiveness of our method on attacking sampling-based models. [CONTINUE] Another interesting observation is that the attack performance on LINE is much better than that on DeepWalk. This result may due to the deterministic structure of LINE, while the random sampling procedure in DeepWalk may help raise the resistance to adversarial attack. Moreover, GF-Attack on all graph filters successfully drop the classification accuracy on both Graph Convolutional Networks and sampling-based models, which again indicates the transferability of our general model in practice."
    },
    {
        "id": "903",
        "table": {
            "header": [
                "[EMPTY]",
                "BPE\u2013BPE",
                "BPE\u2013char",
                "char\u2013char"
            ],
            "rows": [
                [
                    "source vocab",
                    "83,227",
                    "24,440",
                    "304"
                ],
                [
                    "target vocab",
                    "91,000",
                    "302",
                    "302"
                ],
                [
                    "source emb.",
                    "512",
                    "512",
                    "128"
                ],
                [
                    "source conv.",
                    "-",
                    "-",
                    ""
                ],
                [
                    "target emb.",
                    "512",
                    "512",
                    "512"
                ],
                [
                    "encoder",
                    "gru",
                    "gru",
                    "gru"
                ],
                [
                    "encoder size",
                    "1024",
                    "512",
                    "512"
                ],
                [
                    "decoder",
                    "gru_cond",
                    "two_layer_gru_decoder",
                    "two_layer_gru_decoder"
                ],
                [
                    "decoder size",
                    "1024",
                    "1024",
                    "1024"
                ],
                [
                    "minibatch size",
                    "128",
                    "128",
                    "64"
                ],
                [
                    "optmizer",
                    "adam",
                    "adam",
                    "adam"
                ],
                [
                    "learning rate",
                    "0.0001",
                    "0.0001",
                    "0.0001"
                ],
                [
                    "beam size",
                    "12",
                    "20",
                    "20"
                ],
                [
                    "training time",
                    "\u2248 1 week",
                    "\u2248 2 weeks",
                    "\u2248 2 weeks"
                ],
                [
                    "(minibatches)",
                    "240,000",
                    "510,000",
                    "540,000"
                ]
            ],
            "title": "Table 2: NMT hyperparameters. \u2018decoder\u2019 refers to function implemented in Nematus (for BPE-to-BPE) and dl4mt-c2c (for *-to-char)."
        },
        "insight": "We report hyperparameters in Table 2. They correspond to those used by Lee et al. (2016) for BPE-to-char and char-tochar; for BPE-to-BPE, we also adopt some hyperparameters from Sennrich et al. (2016b), most importantly, we extract a joint BPE vocabulary of size 89 500 from the parallel corpus. We trained the BPE-to-BPE system for one week, following Sennrich et al. (2016a), and the *-to-char systems for two weeks, following Lee et al. (2016), on a single Titan X GPU."
    },
    {
        "id": "904",
        "table": {
            "header": [
                "system",
                "2014",
                "2015",
                "2016"
            ],
            "rows": [
                [
                    "(test set and size\u2192)",
                    "3003",
                    "2169",
                    "2999"
                ],
                [
                    "BPE-to-BPE",
                    "20.1 (21.0)",
                    "23.2 (23.0)",
                    "26.7 (26.5)"
                ],
                [
                    "BPE-to-char",
                    "19.4 (20.5)",
                    "22.7 (22.6)",
                    "26.0 (25.9)"
                ],
                [
                    "char-to-char",
                    "19.7 (20.7)",
                    "22.9 (22.7)",
                    "26.2 (26.1)"
                ],
                [
                    "",
                    "25.4 (26.5)",
                    "28.1 (28.3)",
                    "34.2 (34.2)"
                ]
            ],
            "title": "Table 3: Case-sensitive Bleu scores (EN-DE) on WMT newstest. We report scores with detokenized NIST Bleu (mteval-v13a.pl), and in brackets, tokenized Bleu with multi-bleu.perl."
        },
        "insight": "Firstly, we report case-sensitive BLEU scores for all systems we trained for comparison to previous work.10 Results are shown in Table 3. The results confirm that our systems are comparable to previously reported results (Sennrich et al., 2016a; Chung et al., 2016), and that performance of the three systems is relatively close in terms of BLEU."
    },
    {
        "id": "905",
        "table": {
            "header": [
                "system",
                "agreement noun phrase",
                "agreement subject-verb",
                "verb particle",
                "polarity (negation) insertion",
                "polarity (negation) deletion",
                "transliteration"
            ],
            "rows": [
                [
                    "(category and size\u2192)",
                    "21813",
                    "35105",
                    "2450",
                    "22760",
                    "4043",
                    "3490"
                ],
                [
                    "BPE-to-BPE",
                    "[BOLD] 95.6",
                    "[BOLD] 93.4",
                    "[BOLD] 91.1",
                    "97.9",
                    "[BOLD] 91.5",
                    "96.1"
                ],
                [
                    "BPE-to-char",
                    "93.9",
                    "91.2",
                    "88.0",
                    "[BOLD] 98.5",
                    "88.4",
                    "[BOLD] 98.6"
                ],
                [
                    "char-to-char",
                    "93.9",
                    "91.5",
                    "86.7",
                    "[BOLD] 98.5",
                    "89.3",
                    "[BOLD] 98.3"
                ],
                [
                    "",
                    "98.7",
                    "96.6",
                    "96.1",
                    "98.7",
                    "92.7",
                    "96.4"
                ],
                [
                    "human",
                    "99.4",
                    "99.8",
                    "99.8",
                    "99.9",
                    "98.5",
                    "99.0"
                ]
            ],
            "title": "Table 4: Accuracy (in percent) of models on different categories of contrastive errors. Best single model result in bold (multiple bold results indicate that difference to best system is not statistically significant)."
        },
        "insight": "Our main result is the assessment via contrastive translation pairs, shown in Table 4. We find that despite obtaining similar BLEU scores, the models have learned different structures to a different degree. The models with character decoder make fewer transliteration errors than the BPE-to-BPE model. However,"
    },
    {
        "id": "906",
        "table": {
            "header": [
                "system",
                "negation insertion  [ITALIC] nicht",
                "negation insertion  [ITALIC] kein",
                "negation insertion  [ITALIC] un-",
                "negation deletion  [ITALIC] nicht",
                "negation deletion  [ITALIC] kein",
                "negation deletion  [ITALIC] un-"
            ],
            "rows": [
                [
                    "(category and size \u2192)",
                    "1297",
                    "10219",
                    "11244",
                    "2919",
                    "538",
                    "586"
                ],
                [
                    "BPE-to-BPE",
                    "[BOLD] 94.8",
                    "[BOLD] 99.1",
                    "97.1",
                    "[BOLD] 93.0",
                    "[BOLD] 88.7",
                    "[BOLD] 86.5"
                ],
                [
                    "BPE-to-char",
                    "92.7",
                    "[BOLD] 98.9",
                    "[BOLD] 98.7",
                    "91.0",
                    "85.1",
                    "78.8"
                ],
                [
                    "char-to-char",
                    "92.1",
                    "[BOLD] 98.9",
                    "[BOLD] 98.8",
                    "91.5",
                    "86.4",
                    "80.5"
                ],
                [
                    "",
                    "97.1",
                    "99.7",
                    "98.0",
                    "93.6",
                    "92.0",
                    "88.4"
                ]
            ],
            "title": "Table 5: Accuracy (in percent) of models on different categories of contrastive errors related to polarity. Best single model result in bold."
        },
        "insight": "Polarity shifts between the source and target text are a well-known translation problem, and our analysis shows that the main type of error is the deletion of negation markers, in line with with findings of previous studies (Fancellu and Webber, 2015). We consider the relatively high number of errors related to polarity an important problem in machine translation, and hope that future work will try to improve upon our results, shown in more detail in Table 5."
    },
    {
        "id": "907",
        "table": {
            "header": [
                "system",
                "sentence",
                "cost"
            ],
            "rows": [
                [
                    "source",
                    "Since then we have only played in the Swedish  [BOLD] league which is not the same level.",
                    "[EMPTY]"
                ],
                [
                    "reference",
                    "Seitdem haben wir nur in der Schwedischen  [BOLD] Liga gespielt,  [BOLD] die nicht das gleiche Niveau  [BOLD] hat.",
                    "0.149"
                ],
                [
                    "contrastive",
                    "Seitdem haben wir nur in der Schwedischen  [BOLD] Liga gespielt,  [BOLD] die nicht das gleiche Niveau  [BOLD] haben.",
                    "0.137"
                ],
                [
                    "1-best",
                    "Seitdem haben wir nur in der schwedischen  [BOLD] Liga gespielt,  [BOLD] die nicht die gleiche Stufe  [BOLD] sind.",
                    "0.090"
                ],
                [
                    "source",
                    "FriendsFest: the  [BOLD] comedy show that taught us serious lessons about male friendship.",
                    "[EMPTY]"
                ],
                [
                    "reference",
                    "FriendsFest: die  [BOLD] Comedy-Show, die uns ernsthafte Lektionen \u00fcber M\u00e4nnerfreundschaften  [BOLD] erteilt",
                    "0.276"
                ],
                [
                    "contrastive",
                    "FriendsFest: die  [BOLD] Comedy-Show, die uns ernsthafte Lektionen \u00fcber M\u00e4nnerfreundschaften  [BOLD] erteilen",
                    "0.262"
                ],
                [
                    "1-best",
                    "FriendsFest: die  [BOLD] Kom\u00f6die zeigt,  [BOLD] dass uns ernsthafte Lehren aus m\u00e4nnlichen Freundschaften",
                    "0.129"
                ],
                [
                    "source",
                    "Robert Lewandowski  [BOLD] had the best opportunities in the first half.",
                    "[EMPTY]"
                ],
                [
                    "reference",
                    "Die besten Gelegenheiten in H\u00e4lfte eins  [BOLD] hatte Robert Lewandowski.",
                    "0.551"
                ],
                [
                    "contrastive",
                    "Die besten Gelegenheiten in H\u00e4lfte eins  [BOLD] hatten Robert Lewandowski.",
                    "0.507"
                ],
                [
                    "1-best",
                    "Robert Lewandowski  [BOLD] hatte in der ersten H\u00e4lfte die besten M\u00f6glichkeiten.",
                    "0.046"
                ]
            ],
            "title": "Table 6: Examples where char-to-char model prefers contrastive translation (subject-verb agreement errors). 1-best translation can make error of same type (example 1), different type (translation of taught is missing in example 2), or no error (example 3)."
        },
        "insight": "Table 6 shows different examples of the where the contrastive translation is scored higher than the reference by the char-to-char model, and the corresponding 1-best translation. In the first one, our method automatically recognizes an error that also appears in the 1-best translation. In the second example, the 1-best translation is missing the verb."
    },
    {
        "id": "908",
        "table": {
            "header": [
                "Approach",
                "People",
                "Clothing",
                "Body",
                "Animals",
                "Vehicles",
                "Instruments",
                "Scene",
                "Other",
                "All"
            ],
            "rows": [
                [
                    "[BOLD] Non-scalable methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "GroundeR\u00a0Rohrbach et\u00a0al. ( 2016 )",
                    "61.00",
                    "38.12",
                    "10.33",
                    "62.55",
                    "68.75",
                    "36.42",
                    "58.18",
                    "29.08",
                    "47.81"
                ],
                [
                    "Multimodal compact bilinear\u00a0Fukui et\u00a0al. ( 2016 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "48.69"
                ],
                [
                    "PGN+QRN\u00a0Chen et\u00a0al. ( 2017 )",
                    "75.08",
                    "55.90",
                    "20.27",
                    "73.36",
                    "68.95",
                    "45.68",
                    "65.27",
                    "38.80",
                    "60.21"
                ],
                [
                    "[BOLD] Non-scalable and joint localization methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Structured matching\u00a0Wang et\u00a0al. ( 2016b )",
                    "57.89",
                    "34.61",
                    "15.87",
                    "55.98",
                    "52.25",
                    "23.46",
                    "34.22",
                    "26.23",
                    "42.08"
                ],
                [
                    "SPC+PPC\u00a0Plummer et\u00a0al. ( 2017a )",
                    "71.69",
                    "50.95",
                    "25.24",
                    "76.25",
                    "66.50",
                    "35.80",
                    "51.51",
                    "35.98",
                    "55.85"
                ],
                [
                    "QRC net\u00a0Chen et\u00a0al. ( 2017 )",
                    "76.32",
                    "59.58",
                    "25.24",
                    "[BOLD] 80.50",
                    "[BOLD] 78.25",
                    "50.62",
                    "67.12",
                    "43.60",
                    "65.14"
                ],
                [
                    "[BOLD] Scalable methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Structure-preserving embedding\u00a0Wang et\u00a0al. ( 2016a )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "43.89"
                ],
                [
                    "CCA+Detector+Size+Color\u00a0Plummer et\u00a0al. ( 2017b )",
                    "64.73",
                    "46.88",
                    "17.21",
                    "65.83",
                    "68.75",
                    "37.65",
                    "51.39",
                    "31.77",
                    "50.89"
                ],
                [
                    "[BOLD] Query-Adaptive R-CNN (proposed)",
                    "[BOLD] 78.17",
                    "[BOLD] 61.99",
                    "[BOLD] 35.25",
                    "74.41",
                    "76.16",
                    "[BOLD] 56.69",
                    "[BOLD] 68.07",
                    "[BOLD] 47.42",
                    "[BOLD] 65.21"
                ]
            ],
            "title": "Table 1: Phrase localization accuracy on Flickr30k Entities dataset."
        },
        "insight": "non-scalable or joint localization methods. Moreover, it significantly outperformed the scalable methods, which suggests the approach of predicting the classifier is better than learning a common subspace for the open-vocabulary detection problem."
    },
    {
        "id": "909",
        "table": {
            "header": [
                "Architecture",
                "Params",
                "IoU 0.5",
                "IoU 0.6",
                "IoU 0.7",
                "IoU 0.8",
                "IoU 0.9"
            ],
            "rows": [
                [
                    "w/o regression",
                    "-",
                    "[BOLD] 65.21",
                    "53.19",
                    "35.70",
                    "14.32",
                    "1.88"
                ],
                [
                    "300\u201316(\u20134096)",
                    "[BOLD] 0.3M",
                    "64.14",
                    "57.66",
                    "48.22",
                    "33.04",
                    "9.29"
                ],
                [
                    "300\u201364(\u20134096)",
                    "1.1M",
                    "63.87",
                    "57.43",
                    "[BOLD] 49.05",
                    "33.84",
                    "[BOLD] 10.55"
                ],
                [
                    "300\u2013256(\u20134096)",
                    "4.3M",
                    "63.84",
                    "57.70",
                    "48.71",
                    "33.87",
                    "10.05"
                ],
                [
                    "300\u20131024(\u20134096)",
                    "17M",
                    "64.29",
                    "[BOLD] 58.05",
                    "48.49",
                    "[BOLD] 33.94",
                    "10.09"
                ],
                [
                    "300(\u2013256\u20134096)",
                    "4.5M",
                    "62.82",
                    "56.28",
                    "48.02",
                    "32.71",
                    "9.89"
                ],
                [
                    "300\u20134096",
                    "1.2M",
                    "63.23",
                    "56.92",
                    "48.17",
                    "32.66",
                    "9.20"
                ]
            ],
            "title": "Table 2: Comparison of various bounding box regressors on Flickr30k Entities for different IoU thresholds. The number of parameters in Gr is also shown."
        },
        "insight": "Table 2 shows the results with and without regressor. The regressor significantly improved the accuracy with high IoU thresholds, which demonstrates that the regressor improved the localization accuracy. In addition, the accuracy did not decrease as a result of sharing the hidden layer or reducing the number of units in the hidden layer."
    },
    {
        "id": "910",
        "table": {
            "header": [
                "[EMPTY]",
                "NPA",
                "WN",
                "VG",
                "[BOLD] Visual Genome mAP",
                "[BOLD] Visual Genome PR@10",
                "[BOLD] Visual Genome PR@100",
                "[BOLD] VOC mAP"
            ],
            "rows": [
                [
                    "CCA",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "3.18",
                    "20.40",
                    "15.64",
                    "28.23"
                ],
                [
                    "[BOLD] Query- Adaptive R-CNN",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "9.15",
                    "52.60",
                    "36.85",
                    "29.14"
                ],
                [
                    "[BOLD] Query- Adaptive R-CNN",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "10.90",
                    "60.10",
                    "43.21",
                    "36.74"
                ],
                [
                    "[BOLD] Query- Adaptive R-CNN",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "11.53",
                    "61.80",
                    "45.91",
                    "37.07"
                ],
                [
                    "[BOLD] Query- Adaptive R-CNN",
                    "\u2713",
                    "[EMPTY]",
                    "\u2713",
                    "11.65",
                    "65.40",
                    "46.85",
                    "41.32"
                ],
                [
                    "[BOLD] Query- Adaptive R-CNN",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[BOLD] 12.19",
                    "[BOLD] 65.70",
                    "[BOLD] 48.45",
                    "[BOLD] 42.81"
                ]
            ],
            "title": "Table 3: Open-vocabulary object detection performance on Visual Genome and PASCAL VOC 2007 datasets. WN and VG are the strategies to remove mutually non-exclusive phrases."
        },
        "insight": "Table 3 compares different training strategies. NPA significantly improved the performance: more than 25% relative improvement for all metrics. Removing mutually non-exclusive words also contributed the performance: WN and VG both improved performance (5.8% and 6.9% relative AP gain, respectively). Performance improved even further by combining them (11.8% relative AP gain), which shows they are complementary. AP was much improved by NPA for the PASCAL dataset as well (47% relative gain)."
    },
    {
        "id": "911",
        "table": {
            "header": [
                "Query girl",
                "Most confusing class man",
                "Most confusing class 19",
                "Most confusing class \u2192",
                "Most confusing class 3",
                "2nd most confusing class boy",
                "2nd most confusing class 4",
                "2nd most confusing class \u2192",
                "2nd most confusing class 2"
            ],
            "rows": [
                [
                    "skateboard",
                    "surfboard",
                    "12",
                    "\u2192",
                    "0",
                    "snowboard",
                    "11",
                    "\u2192",
                    "0"
                ],
                [
                    "train",
                    "bus",
                    "17",
                    "\u2192",
                    "1",
                    "oven",
                    "3",
                    "\u2192",
                    "0"
                ],
                [
                    "helmet",
                    "hat",
                    "18",
                    "\u2192",
                    "1",
                    "cap",
                    "6",
                    "\u2192",
                    "4"
                ],
                [
                    "elephant",
                    "bear",
                    "14",
                    "\u2192",
                    "0",
                    "horse",
                    "6",
                    "\u2192",
                    "0"
                ]
            ],
            "title": "Table 4: Number of false alarms in top 100 results for five queries (w/o NPA \u2192 w/ NPA). The top 2 confusing categories are shown for each query."
        },
        "insight": "Table 4 shows the most confused category and its total count in the top 100 search results for each query, which shows what concept is confusing for each query and how much the confusion is reduced by NPA.9 This shows that visually similar categories resulted in false positive without NPA, while their number was suppressed by training with NPA."
    },
    {
        "id": "912",
        "table": {
            "header": [
                "Database size",
                "10K",
                "50K",
                "100K",
                "500K",
                "1M"
            ],
            "rows": [
                [
                    "Time (ms)",
                    "183\u00b116",
                    "196\u00b121",
                    "242\u00b128",
                    "314\u00b190",
                    "484\u00b1165"
                ],
                [
                    "Memory (GB)",
                    "0.46",
                    "1.23",
                    "2.19",
                    "9.87",
                    "19.47"
                ]
            ],
            "title": "Table 5: Speed/memory in large-scale experiments."
        },
        "insight": "and standard deviation of speed are computed over 20 queries in PASCAL VOC dataset. Our system could retrieve objects from one million images in around 0.5 seconds."
    },
    {
        "id": "913",
        "table": {
            "header": [
                "# of Objs.",
                "Ep.1 9",
                "Ep.1 9",
                "Ep.2 15",
                "Ep.2 15",
                "Ep.3 20",
                "Ep.3 20",
                "Ep.4 25",
                "Ep.4 25"
            ],
            "rows": [
                [
                    "duration",
                    "277(s)",
                    "277(s)",
                    "328(s)",
                    "328(s)",
                    "510(s)",
                    "510(s)",
                    "520(s)",
                    "520(s)"
                ],
                [
                    "# of  [ITALIC] Hyp.",
                    "130",
                    "130",
                    "353",
                    "353",
                    "694",
                    "694",
                    "759",
                    "759"
                ],
                [
                    "# pnp tasks",
                    "3",
                    "3",
                    "4",
                    "4",
                    "6",
                    "6",
                    "10",
                    "10"
                ],
                [
                    "filters",
                    "Off",
                    "On",
                    "Off",
                    "On",
                    "Off",
                    "On",
                    "Off",
                    "On"
                ],
                [
                    "# objs. in bs.",
                    "15",
                    "9",
                    "26",
                    "20",
                    "49",
                    "28",
                    "54",
                    "31"
                ]
            ],
            "title": "Table II: Summary of episodes of the conducted experiments"
        },
        "insight": "Table III reports the performance measures of one-shot perception for each three symbols (shape, color, class-label) separately. [CONTINUE] The object class results represent an interesting scenario. Even though the average accuracy and precision of the classification is high, only 82.2 % of hypotheses generated in the four episodes are annotated. [CONTINUE] Table III presents the performance measures on the four episodes using these parameters. [CONTINUE] Overall, in the case of shape and color we can notice a general increase in all metrics, while in the case of classification there is a slight decrease in accuracy and precision but with a 10 % increase in coverage."
    },
    {
        "id": "914",
        "table": {
            "header": [
                "[BOLD] Algorithm",
                "| [ITALIC] A| [BOLD]  = 25  [BOLD] P = 0.1",
                "| [ITALIC] A| [BOLD]  = 25  [BOLD] P = 0.6",
                "| [ITALIC] A| [BOLD]  = 50  [BOLD] P = 0.1",
                "| [ITALIC] A| [BOLD]  = 50  [BOLD] P = 0.6",
                "| [ITALIC] A| [BOLD]  = 75  [BOLD] P = 0.1",
                "| [ITALIC] A| [BOLD]  = 75  [BOLD] P = 0.6"
            ],
            "rows": [
                [
                    "DSA-C",
                    "432",
                    "5725",
                    "2605",
                    "27163",
                    "7089",
                    "65519"
                ],
                [
                    "DSA-SDP",
                    "325",
                    "5635",
                    "2365",
                    "27210",
                    "6701",
                    "65600"
                ],
                [
                    "GDBA",
                    "386",
                    "5465",
                    "2475",
                    "26950",
                    "6867",
                    "65156"
                ],
                [
                    "MGM-2",
                    "352",
                    "5756",
                    "2481",
                    "27421",
                    "6962",
                    "65988"
                ],
                [
                    "PD-Gibbs",
                    "398",
                    "5875",
                    "2610",
                    "27350",
                    "7178",
                    "65650"
                ],
                [
                    "MS_ADVP",
                    "400",
                    "5805",
                    "2550",
                    "27400",
                    "7058",
                    "66008"
                ],
                [
                    "DSAN",
                    "408",
                    "5802",
                    "2639",
                    "27413",
                    "7224",
                    "66085"
                ],
                [
                    "[BOLD] DPSA",
                    "[BOLD] 268",
                    "[BOLD] 5358",
                    "[BOLD] 2136",
                    "[BOLD] 26240",
                    "[BOLD] 6276",
                    "[BOLD] 63998"
                ]
            ],
            "title": "Table 1: Comparison of DPSA and the benchmarking algorithms on difference configuration of random DCOPs."
        },
        "insight": "From the results in Table 1, it can be observed that DPSA produces solutions that are 21% 6.7% better than DSASDP depending on the number of agents. However, when the density is high (p = 0.6), GDBA is the closest competitor to DPSA. In dense settings, DPSA outperforms GDBA by 1.8% 1.9%. Other competing algorithms perform equal or worse than GDBA and DSA-SDP and produce even bigger performance difference with DPSA (up to 15% - 61% in sparse settings and 9.6% - 3.2% in dense settings). Also note that the optimal cost for ( = 25, p = 0.1), which we | generate using the well-known DPOP (Petcu and Faltings 2005) algorithm, is 253, while DPSA produces 268 in the same setting. \u2212 \u2212 A |"
    },
    {
        "id": "915",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Direct.",
                "[BOLD] Discuss",
                "[BOLD] Eating",
                "[BOLD] Greet",
                "[BOLD] Phone",
                "[BOLD] Photo",
                "[BOLD] Pose",
                "[BOLD] Purch."
            ],
            "rows": [
                [
                    "Zhou et al. ",
                    "54.8",
                    "60.7",
                    "58.2",
                    "71.4",
                    "62.0",
                    "53.8",
                    "55.6",
                    "75.2"
                ],
                [
                    "Dabral et al. ",
                    "44.8",
                    "50.4",
                    "44.7",
                    "49.0",
                    "52.9",
                    "61.4",
                    "43.5",
                    "45.5"
                ],
                [
                    "Yang et al. ",
                    "51.5",
                    "58.9",
                    "50.4",
                    "57.0",
                    "62.1",
                    "65.4",
                    "49.8",
                    "52.7"
                ],
                [
                    "Luo et al. ",
                    "49.2",
                    "57.5",
                    "53.9",
                    "55.4",
                    "62.2",
                    "73.9",
                    "52.1",
                    "60.9"
                ],
                [
                    "Sun et al. ",
                    "42.1",
                    "44.3",
                    "45.0",
                    "45.4",
                    "51.5",
                    "[BOLD] 43.2",
                    "[BOLD] 41.3",
                    "59.3"
                ],
                [
                    "Martinez et al. (GT) ",
                    "37.7",
                    "44.4",
                    "40.3",
                    "42.1",
                    "48.2",
                    "54.9",
                    "44.4",
                    "42.1"
                ],
                [
                    "Ours (GT)",
                    "[BOLD] 35.74",
                    "[BOLD] 42.39",
                    "[BOLD] 39.06",
                    "[BOLD] 40.55",
                    "[BOLD] 44.37",
                    "52.54",
                    "42.86",
                    "[BOLD] 38.83"
                ],
                [
                    "[BOLD] Method",
                    "[BOLD] Sitting",
                    "[BOLD] SittingD",
                    "[BOLD] Smoke",
                    "[BOLD] Wait",
                    "[BOLD] WalkD",
                    "[BOLD] Walk",
                    "[BOLD] WalkT",
                    "[BOLD] Avg."
                ],
                [
                    "Zhou et al. ",
                    "111.6",
                    "64.1",
                    "65.5",
                    "66.0",
                    "51.4",
                    "63.2",
                    "55.3",
                    "64.9"
                ],
                [
                    "Dabral et al. ",
                    "63.1",
                    "87.3",
                    "51.7",
                    "48.5",
                    "52.2",
                    "37.6",
                    "41.9",
                    "52.1"
                ],
                [
                    "Yang et al. ",
                    "69.2",
                    "85.2",
                    "57.4",
                    "58.4",
                    "43.6",
                    "60.1",
                    "47.7",
                    "58.6"
                ],
                [
                    "Luo et al. ",
                    "73.8",
                    "96.5",
                    "60.4",
                    "55.6",
                    "69.5",
                    "46.6",
                    "52.4",
                    "61.3"
                ],
                [
                    "Sun et al. ",
                    "73.3",
                    "[BOLD] 51.0",
                    "53.0",
                    "44.0",
                    "[BOLD] 38.3",
                    "48.0",
                    "44.8",
                    "48.3"
                ],
                [
                    "Martinez et al. (GT)",
                    "54.6",
                    "58.0",
                    "45.1",
                    "46.4",
                    "47.6",
                    "36.4",
                    "40.4",
                    "45.5"
                ],
                [
                    "Ours (GT)",
                    "[BOLD] 53.08",
                    "53.90",
                    "[BOLD] 42.10",
                    "[BOLD] 43.36",
                    "43.92",
                    "[BOLD] 33.31",
                    "[BOLD] 36.54",
                    "[BOLD] 42.84"
                ]
            ],
            "title": "TABLE I: MPJE (Mean Per Joint Error, mm) metric on Human3.6m dataset under defined protocol i.e. no rigid alignment of predicted 3d pose with ground truth 3d pose. GT denotes training on ground-truth 2d pose labels. Except Martinez et al. all other state-of-the-art methods use images for training instead of 2d pose labels. Our model achieves least MPJE for majority of the actions."
        },
        "insight": "Evaluation on test datasets is done using standard 3d pose estimation metrics, MPJE (Mean Per Joint Error in mm) for Human3.6m dataset, along with PCK (Percent of Correct Keypoints) and AUC (Area Under the Curve) , , , ,  for MPI-INF-3DHP, which are more robust and stronger metrics in identifying the incorrect joint predictions. [CONTINUE] projection loss). Our model captures all t Quantitative results are given in Table I. [CONTINUE] Human3.6m: Table [CONTINUE] shows results on Human3.6m under defined protocol in  using Model I, which is trained on Human3.6m dataset under full supervision. As shown in the table, we achieve greater accuracy than the state-of-the-art methods on most of the actions including difficult actions such as Sitting, Greeting, etc in terms of MPJE (Mean Per [CONTINUE] Joint Error, mm). On average we have an overall improvement of 6% over our baseline method , which is also trained on 2d pose ground-truth. This improvement in accuracy can be attributed to the 3d-to-2d re-projection loss minimization and geometric constraints. We also outperform state-of-theart method  which was trained on the input images from both Human3.6m and MPII, using our Model [CONTINUE] trained on Human3.6m alone."
    },
    {
        "id": "916",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Training Data",
                "[BOLD] PCK GS",
                "[BOLD] PCK NoGS",
                "[BOLD] PCK Outdoor",
                "[BOLD] PCK ALL",
                "[BOLD] AUC ALL"
            ],
            "rows": [
                [
                    "Zhou et al. ",
                    "H36m",
                    "45.6",
                    "45.1",
                    "14.4",
                    "37.7",
                    "20.9"
                ],
                [
                    "Martinez et al.",
                    "H36m",
                    "62.8*",
                    "58.5*",
                    "62.2*",
                    "62.2*",
                    "27.7*"
                ],
                [
                    "Mehta et al.",
                    "H36m",
                    "70.8",
                    "62.3",
                    "58.5",
                    "64.7",
                    "31.7"
                ],
                [
                    "Luo et al. ",
                    "H36m",
                    "71.3*",
                    "59.4*",
                    "65.7*",
                    "65.6*",
                    "33.2*"
                ],
                [
                    "Yang et al.",
                    "H36M+MPII",
                    "-",
                    "-",
                    "-",
                    "69.0",
                    "32.0"
                ],
                [
                    "Zhou et al.",
                    "H36m+MPII",
                    "71.1",
                    "64.7",
                    "[BOLD] 72.7",
                    "69.2",
                    "32.5"
                ],
                [
                    "Ours (Model I)",
                    "H36m",
                    "66.9*",
                    "63.0*",
                    "67.4*",
                    "65.8*",
                    "31.2*"
                ],
                [
                    "Ours (Model II)",
                    "H36m+MPII",
                    "[BOLD] 74.2*",
                    "[BOLD] 66.9*",
                    "71.4*",
                    "[BOLD] 70.8*",
                    "[BOLD] 34.5*"
                ]
            ],
            "title": "TABLE II: Results on MPI-INF-3DHP test-set by scene. Higher PCK(%) and AUC indicates better performance. \u2212 means values are not given in original paper. \u2217 denotes re-targeting of predicted 3d pose using ground truth limb length. Our model shows best performance among state-of-the-art methods while fine tuned on MPII dataset."
        },
        "insight": "Evaluation on test datasets is done using standard 3d pose estimation metrics, MPJE (Mean Per Joint Error in mm) for Human3.6m dataset, along with PCK (Percent of Correct Keypoints) and AUC (Area Under the Curve) , , , ,  for MPI-INF-3DHP, which are more robust and stronger metrics in identifying the incorrect joint predictions. [CONTINUE] MPI-INF-3DHP: For MPI-INF-3DHP dataset, quantitative aluation has been done using standard metrics PCK, AUC d MPJE as used in state-of-the-art methods , , , 0], . [CONTINUE] (a) Cross Dataset Evaluation: Table II shows evaluation sults on MPI-INF-3DHP with our Model [CONTINUE] (trained on uman 3.6M) and Model II (trained on Human 3.6M + PII) in terms of PCK and AUC for all three different ttings (GS, NoGS and Outdoor) for the 2935 testing images. n an average we see an improvement of 2.3% on PCK ith a threshold of 150 mm) and 6.2% on AUC over the st performing state-of-the-art method. This establishes the [CONTINUE] Quantitative results are given in Table II and Table III."
    },
    {
        "id": "917",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Training Data",
                "[BOLD] Walk PCK",
                "[BOLD] Exer. PCK",
                "[BOLD] Sit PCK",
                "[BOLD] Reach PCK",
                "[BOLD] Floor PCK",
                "[BOLD] Sport PCK",
                "[BOLD] Misc. PCK",
                "[BOLD] Total PCK",
                "[BOLD] Total AUC",
                "[BOLD] Total MPJE"
            ],
            "rows": [
                [
                    "Mehta et al. ",
                    "(MPII+LSP)H3.6M+3DHP [ITALIC] a",
                    "86.6",
                    "75.3",
                    "74.8",
                    "73.7",
                    "52.2",
                    "82.1",
                    "77.5",
                    "75.7",
                    "39.3",
                    "117.6"
                ],
                [
                    "Mehta et al.",
                    "(MPII+LSP)H3.6M+3DHP [ITALIC] a",
                    "87.7",
                    "77.4",
                    "74.7",
                    "72.9",
                    "51.3",
                    "83.3",
                    "80.1",
                    "76.6",
                    "40.4",
                    "124.7"
                ],
                [
                    "Dabral et al.",
                    "H3.6M+3DHP",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "76.7",
                    "39.1",
                    "103.8"
                ],
                [
                    "Luo et al.",
                    "(MPII)H3.6M+3DHP",
                    "90.5*",
                    "80.9*",
                    "90.0*",
                    "85.6*",
                    "70.2*",
                    "93.0*",
                    "92.9*",
                    "83.8*",
                    "47.7*",
                    "85.0*"
                ],
                [
                    "Ours",
                    "H3.6M+3DHP",
                    "[BOLD] 97.3*",
                    "[BOLD] 93.0*",
                    "[BOLD] 92.3*",
                    "[BOLD] 95.3*",
                    "[BOLD] 86.4*",
                    "[BOLD] 94.6*",
                    "[BOLD] 94.3*",
                    "[BOLD] 85.4*",
                    "[BOLD] 55.8*",
                    "[BOLD] 71.40*"
                ]
            ],
            "title": "TABLE III: Activity-wise performance on MPI-INF-3DHP test-set using standard metrics PCK (%), AUC and MPJE (mm). (MPII) means pretrained on MPII dataset. a denotes background augmentation in training data. \u2212 means values are not given in original paper. \u2217 denotes the re-targeting of predicted 3d pose using ground truth limb length. Higher PCK, AUC and lower MPJE indicates better performance. We have achieved significantly better performance than the state-of-the-art methods on all the actions in terms of all the metrics."
        },
        "insight": "Evaluation on test datasets is done using standard 3d pose estimation metrics, MPJE (Mean Per Joint Error in mm) for Human3.6m dataset, along with PCK (Percent of Correct Keypoints) and AUC (Area Under the Curve) , , , ,  for MPI-INF-3DHP, which are more robust and stronger metrics in identifying the incorrect joint predictions. [CONTINUE] MPI-INF-3DHP: For MPI-INF-3DHP dataset, quantitative aluation has been done using standard metrics PCK, AUC d MPJE as used in state-of-the-art methods , , , 0], . [CONTINUE] (b) Results after Fine-tuning: We also present a performance analysis of our Model III (Model [CONTINUE] fine-tuned on MPI-INF-3DHP dataset) in Table III. It shows a comparative analysis of the activity-wise performance of Model III with all recent state-of-the-art methods. We have achieved significant improvement over the state-of-the-art on all the actions in terms of all the metrics. On an average we exceed the best accuracy achieved by methods fully supervised on MPI-INF3DHP by 2% on PCK, 17% on AUC and 16% on MPJE. [CONTINUE] Quantitative results are given in Table II and Table III."
    },
    {
        "id": "918",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] PCK",
                "[BOLD] AUC"
            ],
            "rows": [
                [
                    "2d-to-3d (supervised loss) ",
                    "62.2",
                    "27.7"
                ],
                [
                    "Ours 2d-to-3d+3d-to-2d (re-projection loss)",
                    "64.2",
                    "29.7"
                ],
                [
                    "Ours 2d-to-3d+3d-to-2d (re-projection loss+ bone symmetry loss)",
                    "65.8",
                    "31.2"
                ]
            ],
            "title": "TABLE IV: Ablation Study for different losses on MPI-INF-3DHP dataset."
        },
        "insight": "Table IV and Table V shows ablative analysis of different network design parameters and losses used during training. [CONTINUE] Table IV shows, the addition of 2d re-projection loss with the supervised 3d loss in baseline network, increases PCK by 3.2% and AUC by 7.2% on MPI-INF-3DHP dataset, during crossdataset validation. Using bone length symmetry loss with reprojection and supervised loss advances network performance further with 6% and 13% of improvement in PCK and AUC respectively for similar test-setup."
    },
    {
        "id": "919",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Re-projection error",
                "\u0394"
            ],
            "rows": [
                [
                    "w/o batch normalization",
                    "36.2",
                    "30.7"
                ],
                [
                    "w/o dropout",
                    "6.49",
                    "0.99"
                ],
                [
                    "w/o dropout + w/o batch normalization",
                    "34.79",
                    "29.29"
                ]
            ],
            "title": "TABLE V: Ablation study on different network parameters for our (3d-to-2d re-projection module) in terms of re-projection error on Human3.6m. \u0394 defines re-projection error differences between current training setup (as mentioned in section IV-D) and the above setups."
        },
        "insight": "Table IV and Table V shows ablative analysis of different network design parameters and losses used during training. [CONTINUE] architecture as the baseline network. To understand the optimality of performance of our 3d-to-2d module we have performed an ablation study on different choice of design parameters. Table V, represents error between input ground truth 2d and re-projected 2d from the 3d-to-2d module for various design choices of the network. This error is measured in terms of Euclidean distance between joints in 2d co-ordinate space. The re-projection error is quite high when the network is trained without dropout or batch normalization between [CONTINUE] module. \u2206 defines the re-projection error differences between current training setup and different choices of training setups as mentioned in Table V."
    },
    {
        "id": "920",
        "table": {
            "header": [
                "Dataset",
                "Length (mins)",
                "Missing Obs.",
                "Accuracy"
            ],
            "rows": [
                [
                    "1",
                    "2.0",
                    "11%",
                    "83%"
                ],
                [
                    "2",
                    "3.7",
                    "18%",
                    "74%"
                ],
                [
                    "3",
                    "4.8",
                    "6%",
                    "83%"
                ],
                [
                    "4",
                    "4.4",
                    "9%",
                    "95%"
                ],
                [
                    "5",
                    "3.5",
                    "30%",
                    "81%"
                ],
                [
                    "6",
                    "5.7",
                    "20%",
                    "73%"
                ]
            ],
            "title": "TABLE I: Location estimation results"
        },
        "insight": "Localization accuracy was tested on 6 hand-annotated datasets gathered by an AV on public, multi-lane roads near Nissan Research Center in Silicon Valley. Each dataset was recorded over about a mile of stop-and-go traffic and ranged in time from 2 to 6 minutes. All road segments had between 3 and 6 lanes, corresponding to between 5 and 11 states. In these experiments no metric location information, such as GPS, was used, and topological ground truth was provided. Because of the intermittent nature of the lane line and vehicle detections, not all timesteps possess enough observations to disambiguate lane-states. Thus, in the results presented in Table [CONTINUE] we do not consider instances in which either no observations were recorded or the observations voted for at least half of all states, such as seeing only a single lane line immediately to the left of the vehicle. These instances are labeled \"Missing Observations\". Given sequences of timesteps with little or no observations, it is possible to have multiple states tie for the same belief. Localization is considered correct if the true state is among those with maximum belief, and incorrect otherwise. Length and observation quality are shown so as to give an idea about the difficulty of the dataset. Predictions are made at 100Hz."
    },
    {
        "id": "921",
        "table": {
            "header": [
                "[ITALIC] PE  [ITALIC] K\u03c3",
                "[ITALIC] PE  [ITALIC] K\u03c3",
                "0.9 1",
                "0.9 2",
                "0.9 3",
                "0.7 1",
                "0.7 2",
                "0.7 3",
                "0.5 1",
                "0.5 2",
                "0.5 3"
            ],
            "rows": [
                [
                    "[ITALIC] PM",
                    "0.9",
                    "96",
                    "95",
                    "92",
                    "94",
                    "88",
                    "83",
                    "83",
                    "78",
                    "72"
                ],
                [
                    "[ITALIC] PM",
                    "0.8",
                    "87",
                    "85",
                    "80",
                    "84",
                    "79",
                    "72",
                    "63",
                    "60",
                    "54"
                ],
                [
                    "[ITALIC] PM",
                    "0.7",
                    "75",
                    "74",
                    "72",
                    "66",
                    "65",
                    "61",
                    "55",
                    "50",
                    "50"
                ],
                [
                    "[ITALIC] PM",
                    "0.6",
                    "66",
                    "63",
                    "63",
                    "62",
                    "63",
                    "59",
                    "51",
                    "48",
                    "49"
                ]
            ],
            "title": "TABLE II: Local topological structure estimation accuracy. Results are reported as the percent of timesteps during which the correct topological structure was estimated with highest probability (lowest entropy). PM is probability of sampling from the correct topology. PE is the probability of emitting observations. K\u03c3 is the amount by which the variance is scaled. Each entry in the table was computed from performance over 1000 timesteps."
        },
        "insight": "Testing topological structure estimation was done using simulated data, since there were too few cases in the real world data to draw concrete conclusions. To simulate false positive data, lane line and vehicle detections are generated according to the topology given by the map with probability PM , and according to some other, randomly selected topology with probability (1 \u2212 PM ). Further, to simulate the intermittent nature of real-world data, with probability (1 \u2212 PE) no observations are emitted. To see how our approach handles increased sensor noise, we tested different levels of variance. Given a variance \u03c3 based on real-world data, simulated observations are generated with variance K\u03c3\u03c3, where K\u03c3 is an experimental parameter. Locations of lane lines and vehicle detections are sampled from multivariate normal distributions with means as a function of the given topology, and variances K\u03c3\u03c3. The observation generation process runs independently for each lane line and vehicle detection. Table II displays the results of local topological structure estimation."
    },
    {
        "id": "922",
        "table": {
            "header": [
                "[BOLD] Algorithm",
                "[BOLD] Metric",
                "[ITALIC]  [BOLD] DS1",
                "[ITALIC]  [BOLD] DS2",
                "[ITALIC]  [BOLD] DS3"
            ],
            "rows": [
                [
                    "[ITALIC] Random Forest",
                    "Acc.",
                    "1.0",
                    "0.99970",
                    "0.99997"
                ],
                [
                    "[ITALIC] Random Forest",
                    "F1",
                    "1.0",
                    "0.99985",
                    "0.99999"
                ],
                [
                    "[ITALIC] SVM",
                    "Acc.",
                    "1.0",
                    "1.0",
                    "0.99994"
                ],
                [
                    "[ITALIC] SVM",
                    "F1",
                    "1.0",
                    "1.0",
                    "0.99997"
                ],
                [
                    "[ITALIC] k-nearest Neighbour",
                    "Acc.",
                    "0.99710",
                    "0.99912",
                    "0.99941"
                ],
                [
                    "[ITALIC] k-nearest Neighbour",
                    "F1",
                    "0.99853",
                    "0.99956",
                    "0.99971"
                ],
                [
                    "[ITALIC] k Means Clustering",
                    "Acc.",
                    "0.98102",
                    "0.55624",
                    "0.63362"
                ],
                [
                    "[ITALIC] k Means Clustering",
                    "F1",
                    "0.99038",
                    "0.71485",
                    "0.77573"
                ]
            ],
            "title": "TABLE III: Results of Packet-based Anomaly Detection"
        },
        "insight": "The results are shown in Table III. [CONTINUE] It can be seen that SVM and Random Forest perform very well with near perfect scores, while k-nearest Neighbour performs well in certain areas and k Means Clustering does not perform satisfactorily at all, allowing too many false positives."
    },
    {
        "id": "923",
        "table": {
            "header": [
                "Model",
                "Removed Pixels",
                "ADE20k mIoU",
                "ADE20k Acc"
            ],
            "rows": [
                [
                    "Upernet",
                    "-",
                    "0.377",
                    "78.31"
                ],
                [
                    "DA (random)",
                    "Ignore",
                    "0.320",
                    "75.2"
                ],
                [
                    "DA (sizebased)",
                    "Ignore",
                    "0.379",
                    "78.31"
                ],
                [
                    "DA (hard negative)",
                    "Ignore",
                    "0.375",
                    "77.8"
                ],
                [
                    "DA (sizebased)",
                    "Negative",
                    "0.377",
                    "78.25"
                ],
                [
                    "DA (hard negative)",
                    "Negative",
                    "[BOLD] 0.385",
                    "[BOLD] 78.47"
                ]
            ],
            "title": "Table 3: Data augmentation results on ADE20k dataset"
        },
        "insight": "We can see that random sampling strategy, which worked well in image classification, fails here leading to drop in performance. This is because, many object categories in ADE20k dataset are large and difficult to remove like bed, sofa and mountain and random strategy suffers by picking these. Instead when we switch to size-based and hard-negative based sampling, we see that the performance improves and the the size-based sampling model achieves the best mIoU of the three models (0.379). Applying negative likelihood loss on the removed object class gets further improvement when combined with hard negative sampling. This model also improves upon the Upernet base- line (achieving 0.385 IoU vs 0.377 by Upernet),"
    },
    {
        "id": "924",
        "table": {
            "header": [
                "Model",
                "all (407 images) Road",
                "all (407 images) Sidewalk",
                "with car (258) Road",
                "with car (258) Sidewalk",
                "without car (149) Road",
                "without car (149) Sidewalk"
            ],
            "rows": [
                [
                    "Upernet",
                    "0.81",
                    "0.59",
                    "[BOLD] 0.86",
                    "[BOLD] 0.67",
                    "0.68",
                    "0.40"
                ],
                [
                    "DataAug",
                    "[BOLD] 0.82",
                    "[BOLD] 0.60",
                    "[BOLD] 0.86",
                    "0.65",
                    "[BOLD] 0.72",
                    "[BOLD] 0.46"
                ]
            ],
            "title": "Table 2: Comparing the performance of road and sidewalk segmentation on natural images with and without cars."
        },
        "insight": "On the full set and on the split with cars, we see that the performance of the baseline Upernet and our augmented model (DA hard negative with negative loss) is equivalent. However, when we look at only images without car, the Upernet model performs significantly worse in both road (0.68 vs 0.72 for ours) and sidewalk (0.40 vs 0.46 for ours) segmentation. This quantitatively shows that the baseline model struggles to distinguish between road and sidewalk without car in the image, whereas our data augmentation is more robust and performs well even without context (car)."
    },
    {
        "id": "925",
        "table": {
            "header": [
                "Model",
                "Training Data",
                "Full",
                "Only Cooccur",
                "Only Single"
            ],
            "rows": [
                [
                    "Upernet",
                    "Full\u00a0(5k)",
                    "0.774",
                    "0.797",
                    "0.670"
                ],
                [
                    "Data Aug",
                    "Full\u00a0(5k)",
                    "0.742",
                    "0.754",
                    "[BOLD] 0.675"
                ],
                [
                    "Upernet",
                    "Co-occur\u00a0(3.3k)",
                    "0.680",
                    "0.713",
                    "0.520"
                ],
                [
                    "Data Aug",
                    "Co-occur\u00a0(3.3k)",
                    "[BOLD] 0.82",
                    "[BOLD] 0.86",
                    "0.646"
                ]
            ],
            "title": "Table 4: Experiments in three class setting on ADE20k"
        },
        "insight": "First we can see that when we switch from training on Full training data to Co-occur split (containing only images with atleast two objects), the performance of the Upernet greatly drops on the Single test split (from 0.67 to 0.52). This is indicates that the model overfits to the context it sees, and is not able to segment objects when it seeing them out of context. However, with data-augmentation we generate images of objects without context, and can recover most of this performance loss (0.646). Surprisingly, data-augmented model trained on smaller co-occur data also outperforms the baseline trained with Full data when tested on the co-occur split."
    },
    {
        "id": "926",
        "table": {
            "header": [
                "dataset",
                "| [ITALIC] X|",
                "| [ITALIC] S|",
                "| [ITALIC] P|",
                "#leaves",
                "#edges",
                "#layers",
                "setup (s) 32b",
                "setup (s) 64b",
                "setup (GB) 32b",
                "setup (GB) 64b",
                "online (s) 32b",
                "online (s) 64b",
                "online (MB) 32b",
                "online (MB) 64b"
            ],
            "rows": [
                [
                    "accidents",
                    "111",
                    "22",
                    "4420",
                    "11100",
                    "27161",
                    "7",
                    "364.790",
                    "824.759",
                    "4.344846884",
                    "9.827901661",
                    "22.483",
                    "55.546",
                    "15.476771",
                    "30.949928"
                ],
                [
                    "baudio",
                    "100",
                    "22",
                    "4420",
                    "10000",
                    "26061",
                    "7",
                    "359.144",
                    "811.645",
                    "4.278353718",
                    "9.672070918",
                    "22.116",
                    "53.575",
                    "14.350017",
                    "28.696772"
                ],
                [
                    "bbc",
                    "1058",
                    "2",
                    "880",
                    "42320",
                    "44721",
                    "5",
                    "247.946",
                    "577.277",
                    "2.945730031",
                    "6.868429682",
                    "18.644",
                    "41.921",
                    "43.779831",
                    "87.525628"
                ],
                [
                    "bnetflix",
                    "100",
                    "2",
                    "4400",
                    "20000",
                    "32001",
                    "5",
                    "264.369",
                    "604.219",
                    "3.145345198",
                    "7.196644400",
                    "17.079",
                    "40.841",
                    "22.531776",
                    "45.060293"
                ],
                [
                    "book",
                    "500",
                    "2",
                    "880",
                    "20000",
                    "22401",
                    "5",
                    "134.476",
                    "311.635",
                    "1.596517534",
                    "3.706472180",
                    "9.806",
                    "22.328",
                    "20.906229",
                    "41.796343"
                ],
                [
                    "c20ng",
                    "910",
                    "2",
                    "880",
                    "36400",
                    "38801",
                    "5",
                    "217.939",
                    "523.526",
                    "2.587875051",
                    "6.029774956",
                    "16.381",
                    "37.619",
                    "37.712999",
                    "75.396714"
                ],
                [
                    "cr52",
                    "889",
                    "10",
                    "1768",
                    "35560",
                    "41985",
                    "7",
                    "304.390",
                    "700.521",
                    "3.619301009",
                    "8.340147404",
                    "21.114",
                    "49.053",
                    "38.085061",
                    "76.141513"
                ],
                [
                    "cwebkb",
                    "839",
                    "10",
                    "1768",
                    "33560",
                    "39985",
                    "7",
                    "294.29858",
                    "676.65497",
                    "3.4984",
                    "8.0568",
                    "20.6358",
                    "46.8293",
                    "36.035452",
                    "72.043907"
                ],
                [
                    "dna",
                    "180",
                    "22",
                    "4420",
                    "18000",
                    "34061",
                    "7",
                    "400.175",
                    "906.7995",
                    "4.76194",
                    "10.80538",
                    "24.4088",
                    "59.44160",
                    "22.5445",
                    "45.083"
                ],
                [
                    "jester",
                    "100",
                    "2",
                    "4400",
                    "20000",
                    "32001",
                    "5",
                    "264.355",
                    "604.262",
                    "3.14534",
                    "7.1966",
                    "17.10386",
                    "40.8777",
                    "22.5317",
                    "45.0602"
                ],
                [
                    "kdd",
                    "64",
                    "10",
                    "1768",
                    "2560",
                    "8985",
                    "7",
                    "136.70284",
                    "307.8906",
                    "1.6245",
                    "3.6652",
                    "8.5308",
                    "20.8365",
                    "4.266557",
                    "8.531005"
                ],
                [
                    "kosarek",
                    "190",
                    "2",
                    "2200",
                    "19000",
                    "25001",
                    "5",
                    "178.182",
                    "409.5368",
                    "2.11687",
                    "4.87362",
                    "12.3329875",
                    "28.7319",
                    "20.4866",
                    "40.967"
                ],
                [
                    "msnbc",
                    "17",
                    "10",
                    "1768",
                    "680",
                    "7105",
                    "7",
                    "126.9435",
                    "285.3938",
                    "1.5108",
                    "3.3989",
                    "7.96834",
                    "19.07186",
                    "2.339926",
                    "4.679258"
                ],
                [
                    "msweb",
                    "294",
                    "22",
                    "4420",
                    "29400",
                    "45461",
                    "7",
                    "457.6946",
                    "1042.26683",
                    "5.451",
                    "12.420",
                    "29.0565",
                    "68.9942",
                    "34.221849",
                    "68.434206"
                ],
                [
                    "plants",
                    "69",
                    "2",
                    "4400",
                    "13800",
                    "25801",
                    "5",
                    "233.0806",
                    "530.5189",
                    "2.770567",
                    "6.318326",
                    "14.73527",
                    "35.330077",
                    "16.181983",
                    "32.361699"
                ],
                [
                    "pumsb_star",
                    "163",
                    "2",
                    "4400",
                    "32600",
                    "44601",
                    "5",
                    "328.2498",
                    "754.047",
                    "3.9069",
                    "8.9816",
                    "22.075988",
                    "52.048611",
                    "35.436199",
                    "70.867117"
                ],
                [
                    "tmovie",
                    "500",
                    "10",
                    "1768",
                    "20000",
                    "26425",
                    "7",
                    "225.16933",
                    "515.2254",
                    "2.6787",
                    "6.1358",
                    "14.937111",
                    "35.160811",
                    "22.139125",
                    "44.262135"
                ],
                [
                    "tretail",
                    "135",
                    "2",
                    "4400",
                    "27000",
                    "39001",
                    "5",
                    "299.88872",
                    "687.527",
                    "3.56848",
                    "8.1882965",
                    "19.84694",
                    "47.23953",
                    "29.7009",
                    "59.3974"
                ],
                [
                    "nltcs",
                    "16",
                    "2",
                    "880",
                    "640",
                    "3041",
                    "5",
                    "36.0104",
                    "81.1858",
                    "0.4262",
                    "0.9636",
                    "2.6949",
                    "5.8308",
                    "1.066",
                    "2.1315"
                ],
                [
                    "nltcs",
                    "16",
                    "2",
                    "2200",
                    "1600",
                    "7601",
                    "5",
                    "89.586",
                    "202.298",
                    "1.065",
                    "2.4087",
                    "5.857",
                    "13.845",
                    "2.664",
                    "5.426"
                ],
                [
                    "nltcs",
                    "16",
                    "2",
                    "4400",
                    "3200",
                    "15201",
                    "5",
                    "178.932",
                    "404.3435",
                    "2.1298",
                    "4.8166",
                    "11.312",
                    "27.351",
                    "5.3258",
                    "10.651193"
                ],
                [
                    "nltcs",
                    "16",
                    "10",
                    "1768",
                    "640",
                    "7065",
                    "7",
                    "126.802",
                    "284.8314",
                    "1.50844",
                    "3.39321",
                    "7.9706",
                    "19.55636",
                    "2.298934",
                    "4.597"
                ],
                [
                    "nltcs",
                    "16",
                    "22",
                    "4420",
                    "1600",
                    "17661",
                    "7",
                    "316.42786",
                    "711.7047",
                    "3.7706",
                    "8.4821",
                    "19.258",
                    "47.6545",
                    "5.746",
                    "11.491"
                ],
                [
                    "nips",
                    "100",
                    "7",
                    "17",
                    "1061",
                    "1084",
                    "11",
                    "26.04271",
                    "71.28372",
                    "0.2977530",
                    "0.83463",
                    "2.06278",
                    "5.0131688",
                    "1.300823",
                    "2.601594"
                ],
                [
                    "nips",
                    "100",
                    "7",
                    "17",
                    "1061",
                    "1084",
                    "11",
                    "28.54619\u2020",
                    "76.21134\u2020",
                    "0.327307\u2020",
                    "0.89374\u2020",
                    "2.16724\u2020",
                    "5.252681\u2020",
                    "1.762611\u2020",
                    "3.063386\u2020"
                ],
                [
                    "nips",
                    "100",
                    "15",
                    "43",
                    "2750",
                    "2807",
                    "15",
                    "65.803",
                    "182.21",
                    "0.77022",
                    "2.160",
                    "4.55309",
                    "12.382",
                    "3.043669",
                    "6.087289"
                ],
                [
                    "nips",
                    "100",
                    "15",
                    "43",
                    "2750",
                    "2807",
                    "15",
                    "72.702458\u2020",
                    "196.289\u2020",
                    "0.854092\u2020",
                    "2.32806\u2020",
                    "4.822130\u2020",
                    "12.872\u2020",
                    "4.354196\u2020",
                    "7.397817\u2020"
                ]
            ],
            "title": "Table 1: Benchmarks of private SPN inference with CryptoSPN in a\u00a0WAN network. The SPN has\u00a0|X|\u00a0RVs,\u00a0|S| sum nodes, and\u00a0|P| product nodes. Setup and online runtime as well as communication are measured for both\u00a032- and 64-bit precision. All SPNs are\u00a0RAT-SPNs with\u00a0Bernoulli leaves except the ones for nips, which are regular\u00a0SPNs with\u00a0Poisson leaves. \u2020 indicates usage of a selection network for hiding RV assignments."
        },
        "insight": "Our benchmarks are given in Table 1. [CONTINUE] Generally, our results shown in Table 1 demonstrate that we achieve tractable setup and highly efficient online performance for medium-sized SPNs. Specifically, the setup phase requires costs in the order of minutes and gigabytes, while the online phase takes only a few seconds and megabytes. [CONTINUE] While no single parameter appears to be decisive for the runtimes, we observe that some parameters are much more significant: 1. The number of sums has a significantly larger effect than products or leaves, which is expected given the log2 and exp2 operations. But, since the absolute amount of sums is still relatively small, the additional input weights do not affect online communication. 2. Though differences in the number of RVs, product nodes, leaves, and edges do influence the runtimes, deviations have to be very large to take an effect. For instance, when examining the SPNs [CONTINUE] for accidents, baudio, and msweb, it takes roughly twice the amount of RVs and edges (the SPN for msweb) compared to the others to reach a significant runtime deviation. 3. When looking at the SPNs for nltcs, the first three SPNs have roughly the same density and the runtime seems to scale according to their size. The last two SPNs, however, have a noticeably higher density but comparable size and result in much higher runtimes. Thus, density (especially the amount of edges) is a much more significant parameter than plain network size. Yet, depending on the SPN, the costs of other, less important parameters can outweigh the costs of individual parameters. This is in line with our theoretical analysis in Section 3.2: the circuit's size depends on the number of children (with different costs for sums and products) as well as the number of RVs and leaves. The amount of layers has no direct effect because the round complexity of Yao's GC protocol is independent of the depth. As for the regular SPNs for nips, one can observe that the effects of hiding RV assignments are insignificant compared to the overall performance. Using 64-bit precision roughly doubles the costs of 32-bit precision, which is expected as the sub-circuits are about twice the size ."
    },
    {
        "id": "927",
        "table": {
            "header": [
                "Alg.",
                "Acc.",
                "Prec.",
                "Rec.",
                "F-score",
                "AUC"
            ],
            "rows": [
                [
                    "RF-A",
                    "[BOLD] 0.9987",
                    "[BOLD] 0.9965",
                    "[BOLD] 0.9971",
                    "[BOLD] 0.9968",
                    "[BOLD] 0.9997"
                ],
                [
                    "RF-S",
                    "0.9986",
                    "0.9962",
                    "0.9966",
                    "0.9964",
                    "[BOLD] 0.9997"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0002",
                    "0.0003",
                    "0.0006",
                    "0.0005",
                    "0.0000"
                ],
                [
                    "ET-A",
                    "0.9981",
                    "0.9951",
                    "0.9951",
                    "0.9951",
                    "0.9994"
                ],
                [
                    "ET-S",
                    "0.9980",
                    "0.9950",
                    "0.9950",
                    "0.9950",
                    "0.9994"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0001",
                    "0.0002",
                    "0.0001",
                    "0.0001",
                    "0.0000"
                ],
                [
                    "ANN-A",
                    "0.9802",
                    "0.9155",
                    "0.9908",
                    "0.9516",
                    "0.9984"
                ],
                [
                    "ANN-S",
                    "0.9740",
                    "0.8929",
                    "0.9860",
                    "0.9372",
                    "0.9968"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0062",
                    "0.0226",
                    "0.0047",
                    "0.0145",
                    "0.0017"
                ],
                [
                    "SVM-A",
                    "0.9109",
                    "0.6996",
                    "0.9595",
                    "0.8092",
                    "0.9780"
                ],
                [
                    "SVM-S",
                    "0.8869",
                    "0.6433",
                    "0.9565",
                    "0.7692",
                    "0.9746"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0239",
                    "0.0563",
                    "0.0030",
                    "0.0400",
                    "0.0034"
                ],
                [
                    "GB-A",
                    "0.9960",
                    "0.9854",
                    "0.9944",
                    "0.9899",
                    "0.9995"
                ],
                [
                    "GB-S",
                    "0.9957",
                    "0.9840",
                    "0.9945",
                    "0.9892",
                    "0.9996"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0003",
                    "0.0014",
                    "(0.0001)",
                    "0.0007",
                    "(0.0001)"
                ],
                [
                    "NB-A",
                    "0.7753",
                    "0.4371",
                    "0.4888",
                    "0.4615",
                    "0.8601"
                ],
                [
                    "NB-S",
                    "0.7621",
                    "0.4144",
                    "0.5019",
                    "0.4539",
                    "0.8508"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0132",
                    "0.0228",
                    "(0.0131)",
                    "0.0076",
                    "0.0093"
                ]
            ],
            "title": "Table 3: Performance using all features vs selected features"
        },
        "insight": "Comparing the performance using all features vs selected features, Table 3 shows that models using all features (denoted with an appended -A, for instance RF-A) tend to show better results in terms of all performance metrics. However,"
    },
    {
        "id": "928",
        "table": {
            "header": [
                "Alg.",
                "Acc.",
                "Prec.",
                "Rec.",
                "F-score",
                "AUC"
            ],
            "rows": [
                [
                    "RF-D1",
                    "[BOLD] 0.9973",
                    "[BOLD] 0.9920",
                    "[BOLD] 0.9945",
                    "[BOLD] 0.9932",
                    "[BOLD] 0.9993"
                ],
                [
                    "RF-D2",
                    "0.9511",
                    "0.9446",
                    "0.7985",
                    "0.8654",
                    "0.9572"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0463",
                    "0.0475",
                    "0.1960",
                    "0.1278",
                    "0.0421"
                ],
                [
                    "ET-D1",
                    "0.9969",
                    "0.9913",
                    "0.9932",
                    "0.9923",
                    "0.9989"
                ],
                [
                    "ET-D2",
                    "0.9756",
                    "0.9321",
                    "0.9448",
                    "0.9384",
                    "0.9954"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0214",
                    "0.0592",
                    "0.0483",
                    "0.0538",
                    "0.0036"
                ],
                [
                    "ANN-D1",
                    "0.9497",
                    "0.8300",
                    "0.9362",
                    "0.8799",
                    "0.9865"
                ],
                [
                    "ANN-D2",
                    "0.5952",
                    "0.3241",
                    "0.9721",
                    "0.4862",
                    "0.7921"
                ],
                [
                    "[ITALIC] Difference",
                    "0.3544",
                    "0.5059",
                    "(0.0359)",
                    "0.3937",
                    "0.1945"
                ],
                [
                    "SVM-D1",
                    "0.8489",
                    "0.5747",
                    "0.8968",
                    "0.7005",
                    "0.9252"
                ],
                [
                    "SVM-D2",
                    "0.7195",
                    "0.3739",
                    "0.6281",
                    "0.4687",
                    "0.7886"
                ],
                [
                    "[ITALIC] Difference",
                    "0.1294",
                    "0.2008",
                    "0.2687",
                    "0.2318",
                    "0.1366"
                ],
                [
                    "GB-D1",
                    "0.9881",
                    "0.9513",
                    "0.9904",
                    "0.9705",
                    "0.9986"
                ],
                [
                    "GB-D2",
                    "0.9230",
                    "0.7692",
                    "0.8701",
                    "0.8165",
                    "0.9789"
                ],
                [
                    "[ITALIC] Difference",
                    "0.0652",
                    "0.1821",
                    "0.1204",
                    "0.1539",
                    "0.0198"
                ],
                [
                    "NB-D1",
                    "0.7982",
                    "0.4881",
                    "0.5028",
                    "0.4953",
                    "0.8553"
                ],
                [
                    "NB-D2",
                    "0.5591",
                    "0.2687",
                    "0.7195",
                    "0.3913",
                    "0.6591"
                ],
                [
                    "[ITALIC] Difference",
                    "0.2391",
                    "0.2194",
                    "(0.2167)",
                    "0.1040",
                    "0.1962"
                ]
            ],
            "title": "Table 4: Performance using domain features vs constructed features"
        },
        "insight": "So we further compare the performance of models using the two best settings all features (i.e., baseline) vs domain features. We find that, among all models, RF using all features (denoted with an appended -A, for instance RF-A) performs better than all other algorithms (see Table 5)."
    },
    {
        "id": "929",
        "table": {
            "header": [
                "Attack",
                "Count",
                "All(%)",
                "Sel.(%)",
                "Dom.(%)",
                "Cons.(%)"
            ],
            "rows": [
                [
                    "Ddos",
                    "4184",
                    "99.90",
                    "99.90",
                    "99.90",
                    "62.86"
                ],
                [
                    "PortScan",
                    "4973",
                    "99.90",
                    "99.94",
                    "99.94",
                    "66.28"
                ],
                [
                    "Bot",
                    "54",
                    "77.78",
                    "77.78",
                    "75.93",
                    "22.22"
                ],
                [
                    "Infiltration",
                    "1",
                    "100",
                    "100",
                    "100",
                    "0.00"
                ],
                [
                    "Web Attack-BF",
                    "49",
                    "95.92",
                    "95.92",
                    "91.84",
                    "75.51"
                ],
                [
                    "Web Attack-XSS",
                    "23",
                    "95.65",
                    "95.65",
                    "91.30",
                    "65.22"
                ],
                [
                    "Web Attack-Sql",
                    "1",
                    "[BOLD] 0.00",
                    "[BOLD] 0.00",
                    "[BOLD] 100",
                    "[BOLD] 0.00"
                ],
                [
                    "FTP-Patator",
                    "251",
                    "99.20",
                    "100",
                    "99.20",
                    "81.67"
                ],
                [
                    "SSH-Patator",
                    "198",
                    "98.99",
                    "99.49",
                    "96.97",
                    "75.76"
                ],
                [
                    "DoS slowloris",
                    "188",
                    "99.47",
                    "99.47",
                    "98.94",
                    "61.70"
                ],
                [
                    "DoS Slowloris",
                    "174",
                    "99.43",
                    "99.43",
                    "96.55",
                    "31.61"
                ],
                [
                    "Dos Hulk",
                    "7319",
                    "99.71",
                    "99.73",
                    "99.34",
                    "96.19"
                ],
                [
                    "DoS GoldenEye",
                    "314",
                    "99.36",
                    "99.68",
                    "98.41",
                    "85.03"
                ],
                [
                    "Heartbleed",
                    "1",
                    "100",
                    "100",
                    "100",
                    "100"
                ]
            ],
            "title": "Table 6: Performance of unseen attack detection using RF"
        },
        "insight": "As shown in Table 6, we see that except for the constructed feature settings (denoted by Cons.), the performances of other feature settings (all, selected, and domain) are similar."
    },
    {
        "id": "930",
        "table": {
            "header": [
                "[BOLD] Metric",
                "[BOLD] Direct",
                "[BOLD] Hierarchical"
            ],
            "rows": [
                [
                    "Average Total time (in seconds)",
                    "51.908",
                    "10.695"
                ],
                [
                    "Average Final Error",
                    "[\u22120.168,0.172] [ITALIC] T",
                    "[0.086,0.198] [ITALIC] T"
                ],
                [
                    "Average Final Maximum Belief Uncertainty",
                    "0.696",
                    "0.625"
                ]
            ],
            "title": "Table 1: Comparison of direct and hierarchical planning. Values are averaged over 5 runs. Planning horizon: 20 steps. Belief start: [5,5]T. actual start: [3.5,2.0]T. Termination condition: Maximum likelihood estimate of belief converged within a ball of 0.2\u00a0unit radius around the goal ([0,0]T) with max covariance of 1\u00a0unit."
        },
        "insight": "Additional statistical analysis to compare the two approaches in terms of total planning time, final error and final belief uncertainty are presented in Table 1. It can be seen from Table 1 that, for [CONTINUE] comparable final error and final belief uncertainty, the hierarchical planner is able to find a solution approximately 5 times faster than the direct planning approach."
    },
    {
        "id": "931",
        "table": {
            "header": [
                "Embedding",
                "Distance \u21131",
                "Distance \u21132",
                "Distance Cosine"
            ],
            "rows": [
                [
                    "FACSNet-CL-F",
                    "47.1",
                    "47.1",
                    "40.7"
                ],
                [
                    "FACSNet-CL-P",
                    "45.3",
                    "44.2",
                    "48.3"
                ],
                [
                    "AFFNet-CL-F",
                    "49.0",
                    "47.7",
                    "49.0"
                ],
                [
                    "AFFNet-CL-P",
                    "52.4",
                    "51.6",
                    "53.3"
                ],
                [
                    "AFFNet-TL",
                    "-",
                    "49.6",
                    "-"
                ],
                [
                    "FECNet-16d",
                    "-",
                    "81.8",
                    "-"
                ]
            ],
            "title": "Table 2: Triplet prediction accuracy on the FEC test set."
        },
        "insight": "Table 2 shows the triplet prediction accuracy of various embeddings on the FEC test set using different distance functions. Among all the AFFNet and FACSNet embeddings, the combination of AFFNet-CL-P and cosine distance gives the best accuracy, [CONTINUE] It is worth noting that the proposed FECNet16d (81.8%) performs significantly better than the best competing approach (AFFNet-CL-P + Cosine distance; 53.3%)."
    },
    {
        "id": "932",
        "table": {
            "header": [
                "Dataset",
                "Male",
                "Female",
                "Organization",
                "Total"
            ],
            "rows": [
                [
                    "ILLAE",
                    "353",
                    "451",
                    "630",
                    "[BOLD] 1,434"
                ],
                [
                    "ILLAE",
                    "[ITALIC] 24.62%",
                    "[ITALIC] 31.45%",
                    "[ITALIC] 43.93%",
                    "[ITALIC] 100%"
                ],
                [
                    "CrowdFlower",
                    "3,698",
                    "4,024",
                    "2,464",
                    "[BOLD] 10,186"
                ],
                [
                    "CrowdFlower",
                    "[ITALIC] 36.30%",
                    "[ITALIC] 39.51%",
                    "[ITALIC] 24.19%",
                    "[ITALIC] 100%"
                ]
            ],
            "title": "Table 1: Datasets with user distributions"
        },
        "insight": "We collected the dataset from Twitter using streaming API and based on three hashtags #ILookLikeAnEngineer, #LookLikeAnEngineer and #LookLikeEngineer as we had found instances of all of them being used in conjunction. The time frame for the data ranges from August 3rd, 2015, the day the hashtag was first used, until October 15th, 2015, which is about 2 months after the first initial surge of the campaign. The dataset consists of 19,492 original tweets from 13,270 unique users (individuals and organizations). Three reviewers from our research team individually annotated 1,434 user profiles as female, male or organization. The annotated user profiles were reviewed by the team and in case of a disagreement, the user type was decided by mutual discussion. The distribution of each user type for ILLAE is shown in Table 1."
    },
    {
        "id": "933",
        "table": {
            "header": [
                "[BOLD] Methods  [BOLD] Baselines",
                "[BOLD] Methods  [BOLD] Majority (female)",
                "[BOLD] Methods  [BOLD] Majority (female)",
                "[BOLD] Accuracy(%) 39.51",
                "[ITALIC] F1 [BOLD] -Org(%) 0",
                "[ITALIC] F1 [BOLD] -Female(%) 56.63",
                "[ITALIC] F1 [BOLD] -Male(%) 0"
            ],
            "rows": [
                [
                    "[BOLD] Baselines",
                    "[BOLD] Organization/Individual",
                    "[BOLD] Organization/Individual",
                    "14.50",
                    "68.74",
                    "0",
                    "0"
                ],
                [
                    "[BOLD] Baselines",
                    "[BOLD] Name database",
                    "[BOLD] Name database",
                    "46.67",
                    "0",
                    "68.66",
                    "70.11"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Text",
                    "SVM",
                    "62.02",
                    "70.90",
                    "65.02",
                    "52.10"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Text",
                    "RF",
                    "61.43",
                    "70.53",
                    "64.45",
                    "51.66"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Image",
                    "SVM",
                    "63.42",
                    "72.94",
                    "62.78",
                    "57.30"
                ],
                [
                    "\u2260\u2260\u2260\u2260\u2260\u2260\u2260\u2260\u2260\u00b1\u00b1\u00b1\u00b1",
                    "[BOLD] Image",
                    "RF",
                    "64.55",
                    "75.71",
                    "64.83",
                    "56.63"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Metadata",
                    "SVM",
                    "48.70",
                    "52.07",
                    "56.47",
                    "34.36"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Metadata",
                    "RF",
                    "47.17",
                    "51.95",
                    "50.68",
                    "39.85"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Proposed",
                    "[BOLD] SVM",
                    "[BOLD] 78.61",
                    "[BOLD] 81.27",
                    "[BOLD] 79.83",
                    "[BOLD] 75.40"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Proposed",
                    "RF",
                    "76.56",
                    "78.86",
                    "78.72",
                    "72.64"
                ]
            ],
            "title": "Table 3: Classifiers results in different feature sets for CrowdFlower dataset"
        },
        "insight": "In Table 3, you will see the result of CrowdFlower dataset. But in this dataset, organization/individual classifier with 68.74% F1 score for organization class is outperformed by two classifiers beside the combined one: text (70.90% and 70.53% F1 score-org) and image (72.94% and 75.71% F1 score-org). Image and text features alone are enough to classify organizations and there is no need to gather extra information about the historical tweets or behavioral features in the user profile. Again, the name database has higher F1 score-female (70.11%) than the other classifiers except the combined one. Proposed framework with SVM classifier outperforms the rest of classifiers with the average overall accuracy of 78.61%."
    },
    {
        "id": "934",
        "table": {
            "header": [
                "[BOLD] Methods  [BOLD] Baselines",
                "[BOLD] Methods  [BOLD] Majority (org)",
                "[BOLD] Methods  [BOLD] Majority (org)",
                "[BOLD] Accuracy(%) 43.93",
                "[ITALIC] F1 [BOLD] -Org(%) 61.01",
                "[ITALIC] F1 [BOLD] -Female(%) 0",
                "[ITALIC] F1 [BOLD] -Male(%) 0"
            ],
            "rows": [
                [
                    "[BOLD] Baselines",
                    "[BOLD] Organization/Individual",
                    "[BOLD] Organization/Individual",
                    "37.31",
                    "84.85",
                    "0",
                    "0"
                ],
                [
                    "[BOLD] Baselines",
                    "[BOLD] Name database",
                    "[BOLD] Name database",
                    "45.08",
                    "0",
                    "71.41",
                    "73.20"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Text",
                    "SVM",
                    "67.50",
                    "82.41",
                    "63.37",
                    "37.14"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Text",
                    "RF",
                    "66.39",
                    "81.26",
                    "61.78",
                    "34.11"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Image",
                    "SVM",
                    "70.00",
                    "87.77",
                    "57.90",
                    "49.02"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Image",
                    "RF",
                    "72.24",
                    "91.65",
                    "63.53",
                    "44.54"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Metadata",
                    "SVM",
                    "57.67",
                    "70.02",
                    "50.62",
                    "38.85"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Metadata",
                    "RF",
                    "52.65",
                    "67.24",
                    "39.89",
                    "39.87"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Proposed",
                    "SVM",
                    "83.34",
                    "89.55",
                    "79.00",
                    "75.32"
                ],
                [
                    "[BOLD] Framework",
                    "[BOLD] Proposed",
                    "[BOLD] RF",
                    "[BOLD] 85.99",
                    "[BOLD] 92.21",
                    "[BOLD] 83.13",
                    "[BOLD] 77.25"
                ]
            ],
            "title": "Table 2: Classifiers results in different feature sets for ILLAE dataset"
        },
        "insight": "In Table 2, the performance of each baseline is calculated based on overall accuracy and F1 score for each class. As you can see, organization/individual classifier does the best job among baselines with 84.85% F1 score for organization class as it serves its main purpose. But the classifier with image feature set outperforms that classifier with 87.77% and 91.65% F1 score in both classifiers. It means image is a more discriminant feature than historical tweets. On the other hand, name database is effective in gender classification of individuals and it outperforms all of the other classifiers except the proposed framework. At the end, the proposed classifier with RF outperforms all of the other classifiers with the average overall accuracy of 85.99%."
    },
    {
        "id": "935",
        "table": {
            "header": [
                "[EMPTY]",
                "Male",
                "Female",
                "Organization",
                "Total"
            ],
            "rows": [
                [
                    "User",
                    "2,222",
                    "6,362",
                    "4,686",
                    "[BOLD] 13,270"
                ],
                [
                    "User",
                    "[ITALIC] 16.74%",
                    "[ITALIC] 47.95%",
                    "[ITALIC] 35.31%",
                    "[ITALIC] 100%"
                ]
            ],
            "title": "Table 4: User type distribution in entire ILLAE dataset"
        },
        "insight": "The user distribution of the ILLAE dataset, Table 4, shows that female users are the most dominant user types in this online social campaign with 47.95% which is followed by organizations with 35.31%. Organiza"
    },
    {
        "id": "936",
        "table": {
            "header": [
                "[EMPTY]",
                "Male",
                "Female",
                "Organization",
                "Total"
            ],
            "rows": [
                [
                    "Tweet",
                    "2,995",
                    "8,993",
                    "7,504",
                    "[BOLD] 19,492"
                ],
                [
                    "Tweet",
                    "[ITALIC] 15.36%",
                    "[ITALIC] 46.14%",
                    "[ITALIC] 38.50%",
                    "[ITALIC] 100%"
                ],
                [
                    "Retweet",
                    "8,464",
                    "47,764",
                    "33,422",
                    "[BOLD] 89,650"
                ],
                [
                    "Retweet",
                    "[ITALIC] 9.44%",
                    "[ITALIC] 53.28%",
                    "[ITALIC] 37.28%",
                    "[ITALIC] 100%"
                ],
                [
                    "Favorite",
                    "15,830",
                    "82,009",
                    "45,019",
                    "[BOLD] 142,858"
                ],
                [
                    "Favorite",
                    "[ITALIC] 11.08%",
                    "[ITALIC] 57.41%",
                    "[ITALIC] 31.51%",
                    "[ITALIC] 100%"
                ]
            ],
            "title": "Table 5: Tweet, retweet and favorite distribution in entire ILLAE dataset"
        },
        "insight": "On the same note, we would like to see the tweet, retweet and favorite distribution for each user types in the entire ILLAE dataset. In Table 5, female is the most dominant user type in all of the categories as it shows the importance of this campaign among women. Organizations' impact on this campaign is noteworthy in a sense that one of the main factors of sustainability in an online social campaign is the high level of organization engagement to involve more people in the cause. This is also confirmed by the social network analysis in the previous section that in the top 25 users with highest in-degree, 76% of them are of organization type."
    },
    {
        "id": "937",
        "table": {
            "header": [
                "Triplet type",
                "AFFNet-CL-P",
                "FECNet-16d",
                "Median rater"
            ],
            "rows": [
                [
                    "One-class",
                    "49.2",
                    "77.1",
                    "85.3"
                ],
                [
                    "Two-class",
                    "59.8",
                    "85.1",
                    "89.3"
                ],
                [
                    "Three-class",
                    "50.4",
                    "82.6",
                    "87.2"
                ],
                [
                    "All triplets",
                    "53.3",
                    "81.8",
                    "87.5"
                ]
            ],
            "title": "Table 3: Triplet prediction accuracy for different types of triplets in the FEC test set."
        },
        "insight": "Table 3 shows the triplet prediction accuracy of median rater, FECNet-16d and AFFNet-CL-P for each triplet type in the FEC test set. [CONTINUE] the performance is best (85.1%) for two-class triplets, and is lowest (77.1%) for one-class triplets,"
    },
    {
        "id": "938",
        "table": {
            "header": [
                "[EMPTY]",
                "KL-D",
                "CC"
            ],
            "rows": [
                [
                    "Predicted attention weights",
                    "0.49",
                    "0.74"
                ],
                [
                    "Distance related weights",
                    "0.99",
                    "0.61"
                ],
                [
                    "Self-attention weights in ",
                    "0.92",
                    "0.63"
                ]
            ],
            "title": "TABLE I: Similarity of attention estimates. KL-D denotes Kullback-Leibler divergence and CC denotes Correlation Coefficient."
        },
        "insight": "As shown in Table. IVA, the KL divergence for the predicted attention weights is 50.5% smaller than for the distance related weights and 46.7% smaller than for the self-attention weights. Similarly, the CC for the predicted attention weights is 21.3% larger than for the distance related weights and 17.5 % larger than for the self-attention weights."
    },
    {
        "id": "939",
        "table": {
            "header": [
                "[EMPTY]",
                "Success Rate Student003",
                "Success Rate NYC-GC",
                "Success Rate Zara2",
                "Success Rate Hotel",
                "Success Rate ETH",
                "Success Rate  [BOLD] AVG",
                "Navigation Time Student003",
                "Navigation Time NYC-GC",
                "Navigation Time Zara2",
                "Navigation Time Hotel",
                "Navigation Time ETH",
                "Navigation Time  [BOLD] AVG"
            ],
            "rows": [
                [
                    "SARL",
                    "0.692",
                    "0.358",
                    "0.815",
                    "0.581",
                    "0.657",
                    "0.621",
                    "13.1",
                    "13.8",
                    "12.9",
                    "13.4",
                    "13.9",
                    "13.4"
                ],
                [
                    "SA-GCNRL",
                    "0.616",
                    "0.431",
                    "0.838",
                    "0.683",
                    "0.782",
                    "0.670",
                    "12.0",
                    "12.0",
                    "13.2",
                    "12.8",
                    "13.0",
                    "12.6"
                ],
                [
                    "G-GCNRL",
                    "[BOLD] 0.753",
                    "[BOLD] 0.453",
                    "[BOLD] 0.936",
                    "[BOLD] 0.703",
                    "[BOLD] 0.831",
                    "[BOLD] 0.735",
                    "[BOLD] 11.2",
                    "[BOLD] 11.8",
                    "[BOLD] 10.9",
                    "[BOLD] 11.1",
                    "[BOLD] 11.1",
                    "[BOLD] 11.2"
                ]
            ],
            "title": "TABLE II: Comparison to the state-of-the-art, SARL."
        },
        "insight": "As shown in Table II, SA-GCNRL achieves on average a 7.9% higher success rate and a 6.0% shorter navigation time than SARL. [CONTINUE] As shown in Table II, G-GCNRL achieves a 9.7% higher success rate and an 11.1% shorter navigation time than SA-GCNRL."
    },
    {
        "id": "940",
        "table": {
            "header": [
                "[EMPTY]",
                "Success Rate Student003",
                "Success Rate NYC-GC",
                "Success Rate Zara2",
                "Success Rate Hotel",
                "Success Rate ETH",
                "Success Rate  [BOLD] AVG",
                "Navigation Time Student003",
                "Navigation Time NYC-GC",
                "Navigation Time Zara2",
                "Navigation Time Hotel",
                "Navigation Time ETH",
                "Navigation Time  [BOLD] AVG"
            ],
            "rows": [
                [
                    "G-GCNRL",
                    "[BOLD] 0.753",
                    "[BOLD] 0.453",
                    "[BOLD] 0.936",
                    "[BOLD] 0.703",
                    "[BOLD] 0.831",
                    "[BOLD] 0.735",
                    "[BOLD] 11.2",
                    "[BOLD] 11.8",
                    "10.9",
                    "11.1",
                    "11.1",
                    "11.2"
                ],
                [
                    "SA-GCNRL",
                    "0.616",
                    "0.431",
                    "0.838",
                    "0.683",
                    "0.782",
                    "0.670",
                    "12.0",
                    "12.0",
                    "13.2",
                    "12.8",
                    "13.0",
                    "12.6"
                ],
                [
                    "D-GCNRL",
                    "0.556",
                    "0.405",
                    "0.876",
                    "0.699",
                    "0.790",
                    "0.665",
                    "12.7",
                    "13.8",
                    "11.5",
                    "9.3",
                    "10.3",
                    "11.5"
                ],
                [
                    "U-GCNRL",
                    "0.671",
                    "0.387",
                    "0.928",
                    "0.687",
                    "0.827",
                    "0.700",
                    "11.2",
                    "12.0",
                    "[BOLD] 10.1",
                    "[BOLD] 9.3",
                    "[BOLD] 10.2",
                    "[BOLD] 10.6"
                ]
            ],
            "title": "TABLE III: Additional ablation study to show the advantage of the attention weights trained based on human gaze data."
        },
        "insight": "As shown in Table III, the success rate of G-GCNRL was 10.5% higher than that of D-GCNRL and 5.0% higher than that of U-GCNRL."
    },
    {
        "id": "941",
        "table": {
            "header": [
                "[EMPTY]",
                "Success Rate Student003",
                "Success Rate NYC-GC",
                "Success Rate Zara2",
                "Success Rate Hotel",
                "Success Rate ETH",
                "Success Rate AVG",
                "Navigation Time Student003",
                "Navigation Time NYC-GC",
                "Navigation Time Zara2",
                "Navigation Time Hotel",
                "Navigation Time ETH",
                "Navigation Time AVG"
            ],
            "rows": [
                [
                    "SA-GCNRL",
                    "0.616",
                    "[BOLD] 0.431",
                    "[BOLD] 0.838",
                    "[BOLD] 0.683",
                    "[BOLD] 0.782",
                    "[BOLD] 0.670",
                    "[BOLD] 12.0",
                    "[BOLD] 12.0",
                    "13.2",
                    "[BOLD] 12.8",
                    "[BOLD] 13.0",
                    "[BOLD] 12.6"
                ],
                [
                    "SARL",
                    "[BOLD] 0.692",
                    "0.358",
                    "0.815",
                    "0.581",
                    "0.657",
                    "0.621",
                    "13.1",
                    "13.8",
                    "[BOLD] 12.9",
                    "13.4",
                    "13.9",
                    "13.4"
                ],
                [
                    "U-GCNRL",
                    "[BOLD] 0.671",
                    "[BOLD] 0.387",
                    "[BOLD] 0.928",
                    "[BOLD] 0.687",
                    "[BOLD] 0.827",
                    "[BOLD] 0.700",
                    "[BOLD] 11.2",
                    "[BOLD] 12.0",
                    "[BOLD] 10.1",
                    "[BOLD] 9.3",
                    "[BOLD] 10.2",
                    "[BOLD] 10.6"
                ],
                [
                    "UARL",
                    "0.591",
                    "0.310",
                    "0.816",
                    "0.477",
                    "0.636",
                    "0.566",
                    "12.7",
                    "13.1",
                    "14.0",
                    "14.0",
                    "15.0",
                    "13.8"
                ]
            ],
            "title": "TABLE IV: Additional ablation study to show the advantage of the graph structure."
        },
        "insight": "As shown in Table IV, U-GCNRL achieves 23.7% higher success rate and 23.2% shorter navigation time. [CONTINUE] As shown in Table IV, SA-GCNRL achieves 7.9% higher success rate and 6.0% shorter navigation time."
    },
    {
        "id": "942",
        "table": {
            "header": [
                "Album",
                "BO",
                "CB",
                "DT",
                "GB",
                "HC",
                "JL",
                "JC",
                "KM",
                "LJ",
                "LS"
            ],
            "rows": [
                [
                    "FECNet-16d vs AFFNet-CL-P",
                    "5-2",
                    "9-1",
                    "5-1",
                    "9-0",
                    "10-0",
                    "9-0",
                    "7-1",
                    "10-0",
                    "1-4",
                    "1-6"
                ]
            ],
            "title": "Table 4: Number of votes received by the summaries generated by the FECNet-16d and AFFNet-CL-P embeddings."
        },
        "insight": "Table 4 shows the number of votes received by both the embeddings for all the albums. Humans prefer the summaries generated by the proposed FECNet-16d embedding for eight out of ten albums."
    },
    {
        "id": "943",
        "table": {
            "header": [
                "dataset",
                "train #imgs",
                "train #sents",
                "validation #imgs",
                "validation #sents",
                "test #imgs",
                "test #sents",
                "#objs per sent",
                "#rels per sent",
                "#attrs per obj",
                "#words per sent"
            ],
            "rows": [
                [
                    "VisualGenome",
                    "96,738",
                    "3,397,459",
                    "4,925",
                    "172,290",
                    "4,941",
                    "171,759",
                    "2.09",
                    "0.95",
                    "0.47",
                    "5.30"
                ],
                [
                    "MSCOCO",
                    "112,742",
                    "475,117",
                    "4,970",
                    "20,851",
                    "4,979",
                    "20,825",
                    "2.93",
                    "1.56",
                    "0.51",
                    "10.28"
                ]
            ],
            "title": "Table 1: Statistics of VisualGenome and MSCOCO datasets for controllable image captioning with ASGs."
        },
        "insight": "As shown in Table 1, the ASGs in MSCOCO are more complex than those in VisualGenome dataset since they contain more relationships and the captions are longer."
    },
    {
        "id": "944",
        "table": {
            "header": [
                "Method",
                "VisualGenome B4",
                "VisualGenome M",
                "VisualGenome R",
                "VisualGenome C",
                "VisualGenome S",
                "VisualGenome G",
                "VisualGenome G [ITALIC] o",
                "VisualGenome G [ITALIC] a",
                "VisualGenome G [ITALIC] r",
                "MSCOCO B4",
                "MSCOCO M",
                "MSCOCO R",
                "MSCOCO C",
                "MSCOCO S",
                "MSCOCO G",
                "MSCOCO G [ITALIC] o",
                "MSCOCO G [ITALIC] a",
                "MSCOCO G [ITALIC] r"
            ],
            "rows": [
                [
                    "ST ",
                    "11.1",
                    "17.0",
                    "34.5",
                    "139.9",
                    "31.1",
                    "1.2",
                    "0.5",
                    "0.7",
                    "0.5",
                    "10.5",
                    "16.8",
                    "36.2",
                    "100.6",
                    "24.1",
                    "1.8",
                    "0.8",
                    "1.1",
                    "1.0"
                ],
                [
                    "BUTD ",
                    "10.9",
                    "16.9",
                    "34.5",
                    "139.4",
                    "31.4",
                    "1.2",
                    "0.5",
                    "0.7",
                    "0.5",
                    "11.5",
                    "17.9",
                    "37.9",
                    "111.2",
                    "26.4",
                    "1.8",
                    "0.8",
                    "1.1",
                    "1.0"
                ],
                [
                    "C-ST",
                    "12.8",
                    "19.0",
                    "37.6",
                    "157.6",
                    "36.6",
                    "1.1",
                    "0.4",
                    "0.7",
                    "0.4",
                    "14.4",
                    "20.1",
                    "41.4",
                    "135.6",
                    "32.9",
                    "1.6",
                    "0.6",
                    "1.0",
                    "0.8"
                ],
                [
                    "C-BUTD",
                    "12.7",
                    "19.0",
                    "37.9",
                    "159.5",
                    "36.8",
                    "1.1",
                    "0.4",
                    "0.7",
                    "0.4",
                    "15.5",
                    "20.9",
                    "42.6",
                    "143.8",
                    "34.9",
                    "1.5",
                    "0.6",
                    "1.0",
                    "0.8"
                ],
                [
                    "Ours",
                    "[BOLD] 17.6",
                    "[BOLD] 22.1",
                    "[BOLD] 44.7",
                    "[BOLD] 202.4",
                    "[BOLD] 40.6",
                    "[BOLD] 0.7",
                    "[BOLD] 0.3",
                    "[BOLD] 0.3",
                    "[BOLD] 0.3",
                    "[BOLD] 23.0",
                    "[BOLD] 24.5",
                    "[BOLD] 50.1",
                    "[BOLD] 204.2",
                    "[BOLD] 42.1",
                    "[BOLD] 0.7",
                    "[BOLD] 0.4",
                    "[BOLD] 0.3",
                    "[BOLD] 0.3"
                ]
            ],
            "title": "Table 2: Comparison with carefully designed baselines for controllable image caption generation conditioning on ASGs."
        },
        "insight": "Table 2 presents the comparison result. It is worth noting that controllable baselines outperform non-controllable baselines due to the awareness of control signal ASG. We can also see that baseline models are struggling to generate designated attributes compared to objects and relationships according to detailed graph structure metrics."
    },
    {
        "id": "945",
        "table": {
            "header": [
                "#",
                "Enc role",
                "Enc rgcn",
                "Dec ctn",
                "Dec flow",
                "Dec gupdt",
                "Dec bs",
                "VisualGenome B4",
                "VisualGenome M",
                "VisualGenome R",
                "VisualGenome C",
                "VisualGenome S",
                "MSCOCO B4",
                "MSCOCO M",
                "MSCOCO R",
                "MSCOCO C",
                "MSCOCO S"
            ],
            "rows": [
                [
                    "1",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "11.2",
                    "18.3",
                    "36.7",
                    "146.9",
                    "35.6",
                    "13.6",
                    "19.7",
                    "41.3",
                    "130.2",
                    "32.6"
                ],
                [
                    "2",
                    "[EMPTY]",
                    "[EMPTY]",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "10.7",
                    "18.2",
                    "36.9",
                    "146.3",
                    "35.5",
                    "14.5",
                    "20.4",
                    "42.2",
                    "135.7",
                    "34.6"
                ],
                [
                    "3",
                    "\u2713",
                    "[EMPTY]",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "14.2",
                    "20.5",
                    "40.9",
                    "176.9",
                    "38.1",
                    "18.2",
                    "22.5",
                    "44.9",
                    "166.9",
                    "37.8"
                ],
                [
                    "4",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "15.7",
                    "21.4",
                    "43.6",
                    "191.7",
                    "40.0",
                    "21.6",
                    "23.7",
                    "48.6",
                    "190.5",
                    "40.9"
                ],
                [
                    "5",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "[EMPTY]",
                    "15.9",
                    "21.5",
                    "44.0",
                    "193.1",
                    "40.1",
                    "22.3",
                    "24.0",
                    "49.4",
                    "196.2",
                    "41.5"
                ],
                [
                    "6",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "\u2713",
                    "[EMPTY]",
                    "15.8",
                    "21.4",
                    "43.5",
                    "191.6",
                    "39.9",
                    "21.8",
                    "24.1",
                    "49.1",
                    "194.2",
                    "41.4"
                ],
                [
                    "7",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[EMPTY]",
                    "16.1",
                    "21.6",
                    "44.1",
                    "194.4",
                    "40.1",
                    "22.6",
                    "24.4",
                    "50.0",
                    "199.8",
                    "41.8"
                ],
                [
                    "8",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "\u2713",
                    "[BOLD] 17.6",
                    "[BOLD] 22.1",
                    "[BOLD] 44.7",
                    "[BOLD] 202.4",
                    "[BOLD] 40.6",
                    "[BOLD] 23.0",
                    "[BOLD] 24.5",
                    "[BOLD] 50.1",
                    "[BOLD] 204.2",
                    "[BOLD] 42.1"
                ]
            ],
            "title": "Table 3: Ablation study to demonstrate contributions from different proposed components. (role: role-aware node embedding; rgcn: MR-GCN; ctn: graph content attention; flow: graph flow attention; gupdt: graph updating; bs: beam search)"
        },
        "insight": "In order to demonstrate contributions from different components in our model, we provide an extensive ablation study in Table 3. [CONTINUE] in Row 3, we add the role-aware node embedding in the encoder and the performance is largely improved, which indicates that it is important to distinguish different intention roles in the graph. Comparing Row 4 against Row 3 where the MR-GCN is employed for contextual graph encoding, we see that graph context is beneficial for the graph node encoding. Row 5 and 6 enhance the decoder with graph flow attention and graph updating respectively. The graph flow attention shows complementarity with the graph content attention via capturing the structure information in the graph, and outperforms Row 4 on two datasets. However, the graph updating mechanism is more effective on MSCOCO dataset where the number of graph nodes are larger than on VisualGenome dataset. Since the graph updating module explicitly records the status of graph nodes, the effectiveness might be more apparent when generating longer sentences for larger graphs. In Row 7, we incorporate all the proposed components which obtains further gains. Finally, we apply beam search on the proposed model and achieves the best performance."
    },
    {
        "id": "946",
        "table": {
            "header": [
                "[EMPTY]",
                "Method",
                "Div-1",
                "Div-2",
                "SelfCIDEr"
            ],
            "rows": [
                [
                    "Visual Genome",
                    "Region",
                    "0.41",
                    "0.43",
                    "0.47"
                ],
                [
                    "Visual Genome",
                    "Ours",
                    "[BOLD] 0.54",
                    "[BOLD] 0.63",
                    "[BOLD] 0.75"
                ],
                [
                    "MS COCO",
                    "BS ",
                    "0.21",
                    "0.29",
                    "-"
                ],
                [
                    "MS COCO",
                    "POS ",
                    "0.24",
                    "0.35",
                    "-"
                ],
                [
                    "MS COCO",
                    "SeqCVAE ",
                    "0.25",
                    "0.54",
                    "-"
                ],
                [
                    "MS COCO",
                    "BUTD-BS",
                    "0.29",
                    "0.39",
                    "0.58"
                ],
                [
                    "MS COCO",
                    "Ours",
                    "[BOLD] 0.43",
                    "[BOLD] 0.56",
                    "[BOLD] 0.76"
                ]
            ],
            "title": "Table 4: Comparison with state-of-the-art approaches for diverse image caption generation."
        },
        "insight": "As shown in Table 4, the generated captions of our approach are more diverse than compared methods especially on the SelfCider score  which focuses on semantic similarity."
    },
    {
        "id": "947",
        "table": {
            "header": [
                "[BOLD] System",
                "[BOLD] Input type",
                "[BOLD] Offline",
                "[BOLD] Online",
                "[BOLD] Hybrid"
            ],
            "rows": [
                [
                    "ESN-Oja rule",
                    "Feature",
                    "59.77%",
                    "49.22%",
                    "60.01%"
                ],
                [
                    "ESN-Oja rule",
                    "Signal",
                    "54.30%",
                    "58.98%",
                    "[BOLD] 60.29%"
                ],
                [
                    "ESN-BCM rule",
                    "Feature",
                    "61.72%",
                    "54.14%",
                    "[BOLD] 62.17%"
                ],
                [
                    "ESN-BCM rule",
                    "Signal",
                    "56.25%",
                    "50.00%",
                    "59.34%"
                ],
                [
                    "ESN-IP rule",
                    "Feature",
                    "61.21%",
                    "59.11%",
                    "62.39%"
                ],
                [
                    "ESN-IP rule",
                    "Signal",
                    "68.28% [fourati2017optimized]",
                    "62.98%",
                    "[BOLD] 69.23%"
                ],
                [
                    "SVM with PSD features [wichakam2014evaluation]",
                    "Feature",
                    "63.40%",
                    "63.40%",
                    "63.40%"
                ],
                [
                    "HMM [torres2014comparative]",
                    "Signal",
                    "55.00\u00b14.5%",
                    "55.00\u00b14.5%",
                    "55.00\u00b14.5%"
                ]
            ],
            "title": "TABLE IV: Arousal Discrimination Results"
        },
        "insight": "problems using DEAP dataset. For arousal classification, using ESN with offline training which is the linear regression yields better results than the online training using the delta rule. An exception is made for the case of the reservoir pretrained with the Oja's rule using the online training of output weights as depicted in Table 4. The comparison is handled with the work in  since it belongs to the same context as the current work. [CONTINUE] We highlight that using feature as input achieves higher the best accuracy results while using BCM rule. But, [CONTINUE] achieved is 69.23% with ESN pretrained with IP using the hybrid mode inputted directly with EEG channel signal. [CONTINUE] trained them first with linear regression. As a result, the performance increased up to 14% over the state of the art method . [CONTINUE] But, for the other problems the offline outperforms the online mode. [CONTINUE] Pretraining the reservoir with Synaptic plasticity rules achieves higher results using the feature vector as rules achieves higher results input than the raw EEG signal. [CONTINUE] In all cases, pretraining ESN using intrinsic plasticity rule has achieved the best results either with using feature as input or raw EEG signal. As a consequence, we recommend the use of IP rule and the hybrid mode for classification of EEG signals."
    },
    {
        "id": "948",
        "table": {
            "header": [
                "[BOLD] System",
                "[BOLD] Input type",
                "[BOLD] Offline",
                "[BOLD] Online",
                "[BOLD] Hybrid"
            ],
            "rows": [
                [
                    "ESN-Oja rule",
                    "Feature",
                    "59.77%",
                    "52.73%",
                    "60.81%"
                ],
                [
                    "ESN-Oja rule",
                    "Signal",
                    "61.26%",
                    "54.92%",
                    "[BOLD] 62.13%"
                ],
                [
                    "ESN-BCM rule",
                    "Feature",
                    "57.42%",
                    "46.88%",
                    "58.26%"
                ],
                [
                    "ESN-BCM rule",
                    "Signal",
                    "56.25%",
                    "41.67%",
                    "[BOLD] 59.31%"
                ],
                [
                    "ESN-IP rule",
                    "Feature",
                    "53.52%",
                    "55.86%",
                    "57.94%"
                ],
                [
                    "ESN-IP rule",
                    "Signal",
                    "71.03% [fourati2017optimized]",
                    "66.23%",
                    "[BOLD] 71.25%"
                ],
                [
                    "SVM with bandpower features [wichakam2014evaluation]",
                    "Feature",
                    "62.30%",
                    "62.30%",
                    "62.30%"
                ],
                [
                    "HMM [torres2014comparative]",
                    "Signal",
                    "58.75\u00b13.8%",
                    "58.75\u00b13.8%",
                    "58.75\u00b13.8%"
                ]
            ],
            "title": "TABLE V: Valence Discrimination Results"
        },
        "insight": "For valence classification, the reservoir pretrained with IP using the hybrid training reaches the highest result up to 71.25%. Our system, again, outperforms the existing work using HMM with signal as input  up to 13% as shown in Table 5. Furthermore, Oja's rule achieves higher result with signal as input 62.13% than the feature 60.81%. While, the BCM rule achieves higher performance 59.31% with signal than the feature input 58.26%. [CONTINUE] other problems the offline outperforms the online mode. [CONTINUE] In all cases, pretraining ESN using intrinsic plasticity rule has achieved the best results either with using feature as input or raw EEG signal. As a consequence, we recommend the use of IP rule and the hybrid mode for classification of EEG signals."
    },
    {
        "id": "949",
        "table": {
            "header": [
                "[BOLD] System",
                "[BOLD] Input type",
                "[BOLD] Offline",
                "[BOLD] Online",
                "[BOLD] Hybrid"
            ],
            "rows": [
                [
                    "ESN-Oja rule",
                    "Feature",
                    "35.49%",
                    "42.23%",
                    "48.29%"
                ],
                [
                    "ESN-Oja rule",
                    "Signal",
                    "54.29%",
                    "58.12",
                    "[BOLD] 59.29%"
                ],
                [
                    "ESN-BCM rule",
                    "Feature",
                    "32.42%",
                    "33.98%",
                    "41.58%"
                ],
                [
                    "ESN-BCM rule",
                    "Signal",
                    "56.69%",
                    "59.81%",
                    "[BOLD] 60.23%"
                ],
                [
                    "ESN-IP rule",
                    "Feature",
                    "38.22%",
                    "44.65%",
                    "49.58%"
                ],
                [
                    "ESN-IP rule",
                    "Signal",
                    "68.79% [fourati2017optimized]",
                    "69.25%",
                    "[BOLD] 69.95%"
                ],
                [
                    "SVM with FD features[liu2013eeg]",
                    "Feature",
                    "69.53%",
                    "69.53%",
                    "69.53%"
                ]
            ],
            "title": "TABLE VI: Emotional states Discrimination Results"
        },
        "insight": "discrimination of 8 emotional states [CONTINUE] ur work since the comThe online mode is more [CONTINUE] efficient here than the offline mode for both signal and feature input. The hybridization allows us to achieve the best result which is 69.95%. In the literature, most of the existing works classify arousal and valence levels. There is only one work which classifies 8 emotional states . Our ESN trained with linear regression followed with the delta rule outperforms the SVM classifier with FD features. [CONTINUE] According to Table 6, the ESN is more robust when using [CONTINUE] According to Table 6, t signal instead of feature. [CONTINUE] Overall, the online mode is better than the offline mode for the classification of the emotional states. [CONTINUE] We can conclude that the delta rule is more efficient when the complexity of the architecture is high than the linear regression."
    },
    {
        "id": "950",
        "table": {
            "header": [
                "[BOLD] System",
                "[BOLD] Input type",
                "[BOLD] Offline",
                "[BOLD] Online",
                "[BOLD] Hybrid"
            ],
            "rows": [
                [
                    "ESN-Oja rule",
                    "Feature",
                    "65.45%",
                    "47.27%",
                    "60.36%"
                ],
                [
                    "ESN-Oja rule",
                    "Signal",
                    "61.82%",
                    "65.45%",
                    "[BOLD] 67.27%"
                ],
                [
                    "ESN-BCM rule",
                    "Feature",
                    "[BOLD] 65.45%",
                    "50.91%",
                    "64.63%"
                ],
                [
                    "ESN-BCM rule",
                    "Signal",
                    "49.09%",
                    "54.55%",
                    "58.29%"
                ],
                [
                    "ESN-IP rule",
                    "Feature",
                    "69.06%",
                    "65.45%",
                    "[BOLD] 76.15%"
                ],
                [
                    "ESN-IP rule",
                    "Signal",
                    "41.82%",
                    "49.09%",
                    "61.27%"
                ],
                [
                    "SVM with entropy features[garcia2017symbolic]",
                    "Feature",
                    "81.31%",
                    "81.31%",
                    "81.31%"
                ]
            ],
            "title": "TABLE VII: Stress/Calm Discrimination Results"
        },
        "insight": "When discriminating stress and [CONTINUE] calm, we remark that ESN with synaptic plasticity achieves the same accuracy with signal as well feature input which is 65.45%. Combining linear regression and delta rule training enhanced the performance from 61.82% to 67.27%. While using ESN with IP, we can rise the accuracy to 76.15% with bandpower features. When inputting signal to ESN with IP, we obtained an accuracy of 41.82% and 49.09% with the offline mode and online mode, respectively. The hybrid mode shows its effectiveness in this case to reach 61.27% as recognition rate. We believe that existing work combining SVM classifier with entropy features achieves the best result up to 81.31%, but we highlight that we proved that feeding ESN with EEG signal directly can achieve better result than bandpower features which is the aim of our work as depicted in Table 7. [CONTINUE] sing the feature vector as The only case the synaptic plasticity reaches good accuracy rate with signal is the classification of stress / calm states using Oja's rule."
    },
    {
        "id": "951",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "[BOLD] Ep.1  [BOLD] A",
                "[BOLD] Ep.1  [BOLD] P",
                "[BOLD] Ep.1  [BOLD] R",
                "[BOLD] Ep.2  [BOLD] A",
                "[BOLD] Ep.2  [BOLD] P",
                "[BOLD] Ep.2  [BOLD] R",
                "[BOLD] Ep.3  [BOLD] A",
                "[BOLD] Ep.3  [BOLD] P",
                "[BOLD] Ep.3  [BOLD] R",
                "[BOLD] Ep.4  [BOLD] A",
                "[BOLD] Ep.4  [BOLD] P",
                "[BOLD] Ep.4  [BOLD] R",
                "[BOLD] Average  [BOLD] A",
                "[BOLD] Average  [BOLD] P",
                "[BOLD] Average  [BOLD] R"
            ],
            "rows": [
                [
                    "Shape",
                    "One-shot",
                    "0.68",
                    "0.82",
                    "0.69",
                    "0.65",
                    "0.67",
                    "0.65",
                    "0.66",
                    "0.67",
                    "0.66",
                    "0.63",
                    "0.64",
                    "0.64",
                    "0.65",
                    "0.7",
                    "0.66"
                ],
                [
                    "Shape",
                    "Amortized",
                    "0.76",
                    "0.83",
                    "0.76",
                    "0.68",
                    "0.71",
                    "0.69",
                    "0.73",
                    "0.75",
                    "0.73",
                    "0.68",
                    "0.68",
                    "0.68",
                    "0.71",
                    "0.74",
                    "0.71"
                ],
                [
                    "Color",
                    "One-shot",
                    "0.87",
                    "1.0",
                    "0.87",
                    "0.91",
                    "0.95",
                    "0.92",
                    "0.82",
                    "0.9",
                    "0.83",
                    "0.84",
                    "0.92",
                    "0.84",
                    "0.86",
                    "0.94",
                    "0.86"
                ],
                [
                    "Color",
                    "Amortized",
                    "0.89",
                    "1.0",
                    "0.89",
                    "0.98",
                    "0.99",
                    "0.99",
                    "0.86",
                    "0.93",
                    "0.87",
                    "0.85",
                    "0.91",
                    "0.85",
                    "0.89",
                    "0.95",
                    "0.9"
                ],
                [
                    "Class",
                    "One-shot",
                    "0.93",
                    "0.95",
                    "0.93",
                    "0.98",
                    "0.98",
                    "0.98",
                    "0.95",
                    "0.96",
                    "0.95",
                    "0.96",
                    "0.97",
                    "0.96",
                    "0.96",
                    "0.96",
                    "0.96"
                ],
                [
                    "Class",
                    "Amortized",
                    "0.99",
                    "0.99",
                    "0.99",
                    "0.96",
                    "0.96",
                    "0.96",
                    "0.92",
                    "0.94",
                    "0.92",
                    "0.89",
                    "0.90",
                    "0.90",
                    "0.92",
                    "0.92",
                    "0.93"
                ],
                [
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "Coverage one-shot",
                    "82.2 %",
                    "82.2 %",
                    "82.2 %"
                ],
                [
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "Coverage amortized",
                    "94.3 %",
                    "94.3 %",
                    "94.3 %"
                ]
            ],
            "title": "Table III: Accuracy (A), Precision(P) and Recall(R) for shape, color and class annotations of object hypotheses with and without the amortization effects."
        },
        "insight": "Table III reports the performance measures of one-shot perception for each three symbols (shape, color, class-label) separately. The object class results represent an interesting scenario. Even though the average accuracy and precision of the classification is high, only 82.2 % of hypotheses generated in the four episodes are annotated. Table III presents the performance measures on the four episodes using these parameters. Overall, in the case of shape and color we can notice a general increase in all metrics, while in the case of classification there is a slight decrease in accuracy and precision but with a 10 % increase in coverage."
    },
    {
        "id": "952",
        "table": {
            "header": [
                "Parameterization",
                "Surface",
                "BFF\u00a0",
                "PGCP",
                "PGCP with an additional M\u00f6bius transformation"
            ],
            "rows": [
                [
                    "Free-boundary",
                    "Sophie",
                    "0.18",
                    "0.21",
                    "0.18"
                ],
                [
                    "Free-boundary",
                    "Niccol\u00f2 da Uzzano",
                    "0.60",
                    "0.84",
                    "0.57"
                ],
                [
                    "Free-boundary",
                    "Mask",
                    "0.24",
                    "0.58",
                    "0.32"
                ],
                [
                    "Free-boundary",
                    "Max Planck",
                    "2.49",
                    "2.62",
                    "2.50"
                ],
                [
                    "Free-boundary",
                    "Bunny",
                    "2.68",
                    "3.32",
                    "2.95"
                ],
                [
                    "Free-boundary",
                    "Julius",
                    "0.28",
                    "1.04",
                    "0.52"
                ],
                [
                    "Free-boundary",
                    "Buddha",
                    "0.78",
                    "1.20",
                    "1.16"
                ],
                [
                    "Disk-boundary",
                    "Ogre",
                    "1.21",
                    "1.21",
                    "1.21"
                ],
                [
                    "Disk-boundary",
                    "Niccol\u00f2 da Uzzano",
                    "0.76",
                    "0.86",
                    "0.57"
                ],
                [
                    "Disk-boundary",
                    "Brain",
                    "2.17",
                    "2.13",
                    "2.13"
                ],
                [
                    "Disk-boundary",
                    "Gargoyle",
                    "3.90",
                    "3.90",
                    "3.87"
                ],
                [
                    "Disk-boundary",
                    "Hand",
                    "5.29",
                    "5.25",
                    "5.25"
                ],
                [
                    "Disk-boundary",
                    "Octopus",
                    "6.79",
                    "8.13",
                    "8.13"
                ],
                [
                    "Disk-boundary",
                    "Buddha",
                    "0.78",
                    "0.79",
                    "0.77"
                ],
                [
                    "Spherical",
                    "Horse",
                    "27.03",
                    "8.90",
                    "6.54"
                ],
                [
                    "Spherical",
                    "Bulldog",
                    "6.74",
                    "1.09",
                    "1.08"
                ],
                [
                    "Spherical",
                    "Chinese Lion",
                    "4.46",
                    "1.93",
                    "1.74"
                ],
                [
                    "Spherical",
                    "Duck",
                    "7.92",
                    "1.00",
                    "0.84"
                ],
                [
                    "Spherical",
                    "David",
                    "0.85",
                    "0.85",
                    "0.36"
                ],
                [
                    "Spherical",
                    "Octopus",
                    "26.95",
                    "26.44",
                    "26.19"
                ],
                [
                    "Spherical",
                    "Lion Vase",
                    "7.13",
                    "0.92",
                    "0.84"
                ]
            ],
            "title": "Table 7: The area distortion mean(|darea|) of the global conformal parameterizations produced by the boundary first flattening (BFF) method\u00a0[56], the proposed PGCP method, and the proposed PGCP method with an additional step of composing with a M\u00f6bius transformation ((41) for free-boundary conformal parameterization, (42) for disk-boundary conformal parameterization, and (41) together with the stereographic projection for spherical conformal parameterization)."
        },
        "insight": "Table 7 shows the area distortion of the BFF method  and the proposed PGCP method for various types of global conformal parameterizations. For spherical conformal parameterization, our method achieves a lower area distortion. For disk conformal parameterization, the two methods achieve similar area distortions. For free-boundary conformal parameterization, the BFF method possesses a lower area distortion. [CONTINUE] The final results are recorded in the rightmost column of Table 7. It can be observed that the area distortion is effectively reduced with the aid of M\u00a8obius transformations in many cases, especially for free-boundary conformal parameterizations and spherical conformal parameterizations. The improvement for disk conformal parameterizations is relatively less significant, possibly due to the smaller number of free parameters. To achieve an even lower area distortion may require the composition with some other conformal transformations, which we plan to explore in the future."
    },
    {
        "id": "953",
        "table": {
            "header": [
                "Surface",
                "# vertices",
                "CEM\u00a0 Time (s)",
                "CEM\u00a0 mean(| [ITALIC] d|)",
                "CEM\u00a0 with CMG Time (s)",
                "CEM\u00a0 with CMG mean(| [ITALIC] d|)",
                "PGCP Time (s)",
                "PGCP mean(| [ITALIC] d|)"
            ],
            "rows": [
                [
                    "Ogre",
                    "20K",
                    "0.3",
                    "2.6",
                    "0.3",
                    "2.6",
                    "0.5",
                    "1.5"
                ],
                [
                    "Niccol\u00f2 da Uzzano",
                    "25K",
                    "1.4",
                    "1.3",
                    "1.5",
                    "1.4",
                    "0.8",
                    "0.8"
                ],
                [
                    "Brain",
                    "48K",
                    "2.9",
                    "1.5",
                    "2.8",
                    "1.5",
                    "1.3",
                    "1.5"
                ],
                [
                    "Gargoyle",
                    "50K",
                    "2.8",
                    "2.1",
                    "3.2",
                    "2.1",
                    "1.4",
                    "1.9"
                ],
                [
                    "Hand",
                    "53K",
                    "3.4",
                    "1.2",
                    "3.2",
                    "1.4",
                    "1.4",
                    "1.2"
                ],
                [
                    "Octopus",
                    "150K",
                    "10.4",
                    "24.0",
                    "9.3",
                    "26.6",
                    "8.9",
                    "5.6"
                ],
                [
                    "Buddha",
                    "240K",
                    "25.1",
                    "0.7",
                    "18.5",
                    "0.9",
                    "11.4",
                    "0.7"
                ],
                [
                    "Nefertiti",
                    "1M",
                    "83.2",
                    "4.2",
                    "74.4",
                    "4.2",
                    "52.7",
                    "2.9"
                ]
            ],
            "title": "Table 8: The performance of the conformal energy minimization (CEM)\u00a0[26] method, CEM combined with the Combinatorial Multigrid (CMG)\u00a0[58], and our proposed PGCP method for disk conformal parameterization of simply-connected open surfaces."
        },
        "insight": "Table 8 shows the performance of CEM, CEM combined with CMG, and our proposed method. It can be observed that while CEM combined with CMG demonstrates an improvement in efficiency for large problems when compared to the original CEM, our proposed method is still more advantageous in terms of both the efficiency and conformality."
    },
    {
        "id": "954",
        "table": {
            "header": [
                "Bin",
                "Data size",
                "% of Jobs FB",
                "% of Jobs CMU",
                "% of Resources FB",
                "% of Resources CMU",
                "% of I/O FB",
                "% of I/O CMU",
                "Task Time (mins) FB",
                "Task Time (mins) CMU"
            ],
            "rows": [
                [
                    "A",
                    "0-128MB",
                    "74.4%",
                    "63.4%",
                    "25.0%",
                    "32.3%",
                    "3.2%",
                    "10.9%",
                    "76.7",
                    "119.5"
                ],
                [
                    "B",
                    "128-512MB",
                    "16.2%",
                    "29.1%",
                    "12.2%",
                    "27.9%",
                    "16.1%",
                    "30.5%",
                    "37.6",
                    "103.2"
                ],
                [
                    "C",
                    "0.5-1GB",
                    "4.0%",
                    "0.9%",
                    "7.3%",
                    "1.3%",
                    "12.0%",
                    "2.4%",
                    "22.3",
                    "5.0"
                ],
                [
                    "D",
                    "1-2GB",
                    "3.0%",
                    "4.9%",
                    "13.4%",
                    "21.0%",
                    "19.3%",
                    "23.3%",
                    "41.0",
                    "77.6"
                ],
                [
                    "E",
                    "2-5GB",
                    "1.6%",
                    "1.5%",
                    "20.8%",
                    "15.1%",
                    "21.9%",
                    "27.8%",
                    "63.9",
                    "55.7"
                ],
                [
                    "F",
                    "5-10GB",
                    "0.8%",
                    "0.3%",
                    "21.4%",
                    "2.5%",
                    "27.5%",
                    "5.2%",
                    "65.6",
                    "9.2"
                ]
            ],
            "title": "Table 3: Job size distributions. The jobs are binned by their data sizes in our FB and CMU workloads"
        },
        "insight": "Table 3 shows the distribution of jobs by count, cluster resources they consume, amount of I/O they generate, and aggregate task execution time. The jobs in both workloads exhibit a heavy-tailed distribution of input sizes,"
    },
    {
        "id": "955",
        "table": {
            "header": [
                "Code version",
                "Time (total) [s]",
                "Speedup (total)",
                "Time (fraction of total) (subfind_density) [s]",
                "Speedup (subfind_density)"
            ],
            "rows": [
                [
                    "[ITALIC] original",
                    "167.4",
                    "[EMPTY]",
                    "22.6 (13.5%)",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] tuned",
                    "142.1",
                    "1.2\u00d7",
                    "17.1 (12.1%)",
                    "1.3\u00d7"
                ],
                [
                    "[ITALIC] optimized",
                    "137.1",
                    "1.2\u00d7",
                    "12.7 (9.3%)",
                    "1.8\u00d7"
                ]
            ],
            "title": "TABLE III: Performance results of the P-Gadget3 tests in three different code versions, indicated in the first column. The table reports the time to solution and speedup with respect to the original version for the whole application (second and third column, respectively) and for the subfind_density function (fourth and fifth column."
        },
        "insight": "The performance results of our tests on these three code versions for P-GADGET3 are summarized in Table III. In the tuned version the time to solution is improved by a factor of 1.2 \u2212 1.3. The effect of the optimization (third line in Table III) adds no further speedup to the total timing of the code,"
    },
    {
        "id": "956",
        "table": {
            "header": [
                "Label",
                "Location",
                "vCPU cores",
                "GHz",
                "RAM",
                "Provider"
            ],
            "rows": [
                [
                    "okeanos",
                    "Greece",
                    "4",
                    "2.1",
                    "4 GB",
                    "Okeanos Global"
                ],
                [
                    "linode-SG",
                    "Singapore",
                    "1",
                    "2.8",
                    "2 GB",
                    "Linode, LLC"
                ],
                [
                    "linode-US",
                    "California (US)",
                    "1",
                    "2.0",
                    "1 GB",
                    "Linode, LLC"
                ]
            ],
            "title": "TABLE I: Execution testbed. All nodes equipped with Ubuntu Linux 18.04.2 LTS."
        },
        "insight": "As shown in Table 1 the PEU are running on virtual machines executed in different parts of the globe. [CONTINUE] Tables 1\u20133 describe the simulation setup and the related network parameters. [CONTINUE] In particular, as shown in Table 1, we used three hosts widely distributed around the globe, i.e. Greece, Singapore, and the US. The hosts have a different number of available vCPUs and RAM, as reported in the table. [CONTINUE] the specifications of the virtual machines that have been used (Table 1)."
    },
    {
        "id": "957",
        "table": {
            "header": [
                "Group",
                "Test User",
                "Zurich Hauptbahnhof",
                "ETH Zurich Hauptgeba\u00fcde"
            ],
            "rows": [
                [
                    "1",
                    "1",
                    "5. Walking",
                    "3. Tram"
                ],
                [
                    "1",
                    "2",
                    "3. Tram",
                    "5. Walking"
                ],
                [
                    "1",
                    "3",
                    "5. Walking",
                    "5. Walking"
                ],
                [
                    "2",
                    "1",
                    "3. Tram",
                    "4. Bike"
                ],
                [
                    "2",
                    "2",
                    "3. Tram",
                    "5. Walking"
                ],
                [
                    "2",
                    "3",
                    "4. Bike",
                    "3. Tram"
                ],
                [
                    "[EMPTY]",
                    "Mean:",
                    "3.8",
                    "4.17"
                ]
            ],
            "title": "Table 2: Transport sustainability responses for the two points of interest."
        },
        "insight": "Table II shows the choices of transport means made by each test user at each point of interest. Overall, none of the more unsustainable transport means, i.e. car, bus and train, are chosen by test users to visit the points of interest. Walking and tram are the most popular means given that ETH Zurich and the main train station are very well connected with tram and are in close proximity. The mean sustainability of 4.17 for ETH Zurich Hauptgeba\u00a8ude is slightly higher than the one of 3.8 at Zurich Hauptbahnhof."
    },
    {
        "id": "958",
        "table": {
            "header": [
                "Locations",
                "Test users:",
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "Mean",
                "Median",
                "Actual cycling risk\u00a0"
            ],
            "rows": [
                [
                    "Spot A",
                    "[EMPTY]",
                    "2",
                    "2",
                    "2",
                    "1",
                    "1",
                    "1",
                    "1",
                    "2",
                    "2",
                    "1",
                    "2",
                    "1.55",
                    "2",
                    "1.36"
                ],
                [
                    "Spot B",
                    "[EMPTY]",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "2",
                    "1",
                    "1",
                    "1",
                    "1.09",
                    "1",
                    "0.42"
                ],
                [
                    "Spot C",
                    "[EMPTY]",
                    "2",
                    "1",
                    "1",
                    "1",
                    "2",
                    "3",
                    "1",
                    "3",
                    "4",
                    "2",
                    "2",
                    "2.0",
                    "2",
                    "6.21"
                ],
                [
                    "Spot D",
                    "[EMPTY]",
                    "3",
                    "3",
                    "3",
                    "2",
                    "4",
                    "4",
                    "2",
                    "2",
                    "3",
                    "4",
                    "4",
                    "3.09",
                    "3",
                    "8.31"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Pearson correlation:",
                    "0.94",
                    "0.85",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Spearman correlation:",
                    "1.0",
                    "1.0",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 3: Perceived cycling risk acquired via the Smart Agora app vs. the actual cycling risk calculated via an empirical model of real-world data\u00a0[48] in the four urban spots of Figure\u00a09. Users\u2019 responses are in the range [1,5] with 1 for very safe and 5 for very dangerous."
        },
        "insight": "Table III compares the perceived cycling risk values from 11 test users to the actual baseline cycling risk values. All test users cycled over the route on 12.12.2018 around 15:00 with the same provided bike to minimize biases originated from weather, light condition and the condition of different bikes. Correlation values are calculated using the mean and median value of the perceived cycling risk for each urban spot across all users. The Pearson correlation is 0.94 and 0.85 for the mean and median respectively, while the Spearman correlation is 1.0 for both mean and median."
    },
    {
        "id": "959",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "[BOLD] cLTL+  [BOLD] (regular)",
                "[BOLD] cLTL+  [BOLD] (grid)",
                "[BOLD] cLTL+  [BOLD] (continuous)"
            ],
            "rows": [
                [
                    "[BOLD] N",
                    "4",
                    "10.54",
                    "9.74",
                    "72.04"
                ],
                [
                    "[BOLD] N",
                    "6",
                    "33.86",
                    "17.87",
                    "86.47"
                ],
                [
                    "[BOLD] N",
                    "8",
                    "1974.7",
                    "155.36",
                    "2286"
                ],
                [
                    "[BOLD] N",
                    "10",
                    "992.02",
                    "265.38",
                    "132.18"
                ],
                [
                    "[BOLD] h",
                    "30",
                    "389.97",
                    "853.75",
                    "108.87"
                ],
                [
                    "[BOLD] h",
                    "35",
                    "992.02",
                    "265.38",
                    "132.18"
                ],
                [
                    "[BOLD] h",
                    "40",
                    "2788.1",
                    "556.51",
                    "181.94"
                ],
                [
                    "[BOLD] h",
                    "45",
                    "1842.2",
                    "201.45",
                    "274.3"
                ],
                [
                    "[BOLD] h",
                    "50",
                    "2505",
                    "238.86",
                    "257.77"
                ],
                [
                    "[BOLD] h",
                    "55",
                    "2436.3",
                    "334.97",
                    "364.48"
                ],
                [
                    "[BOLD] h",
                    "60",
                    "3828.1",
                    "266.19",
                    "440.68"
                ],
                [
                    "[ITALIC] \u03c4",
                    "0",
                    "992.02",
                    "265.38",
                    "132.18"
                ],
                [
                    "[ITALIC] \u03c4",
                    "1",
                    "9367.1",
                    "507.96",
                    "829.41"
                ],
                [
                    "[ITALIC] \u03c4",
                    "2",
                    "34222.1",
                    "323.74",
                    "18234"
                ]
            ],
            "title": "TABLE I: Numerical results"
        },
        "insight": "We report results for three different implementations in Table I. [CONTINUE] The first implementation uses the encodings proposed in this paper. The second implementation is a special encoding that can only be used for 4-connected grid environments. That is, robots move in a two dimensional gridded environment only horizontally or vertically. In this implementation, the number of Boolean variables needed to denote the state of the robot on a x \u00d7 y gridded environment is x + y as opposed to xy for a general implementation. A smaller number of decision variables decreases the solution times significantly. We also implement the continuous-state extension proposed in Section VI-A. [CONTINUE] As can be seen in Table I, solution times can be reduced significantly if the encodings that are most appropriate for the problem at hand are used."
    },
    {
        "id": "960",
        "table": {
            "header": [
                "[BOLD] N",
                "10",
                "[BOLD] cLTL 10.86",
                "[BOLD] cLTL+ 2.64"
            ],
            "rows": [
                [
                    "[BOLD] N",
                    "20",
                    "10.12",
                    "5.87"
                ],
                [
                    "[BOLD] N",
                    "50",
                    "8.99",
                    "56.13"
                ],
                [
                    "[BOLD] N",
                    "500",
                    "12.72",
                    "[ITALIC] TO"
                ],
                [
                    "[BOLD] h",
                    "20",
                    "10.86",
                    "2.64"
                ],
                [
                    "[BOLD] h",
                    "40",
                    "26.84",
                    "5.32"
                ],
                [
                    "[BOLD] h",
                    "60",
                    "60.93",
                    "7.87"
                ]
            ],
            "title": "TABLE II: Numerical results"
        },
        "insight": "Additionally, we examine the solution times for different encodings when specifications are given in cLTL and the [CONTINUE] robots have identical dynamics. [CONTINUE] The results in Table II are obtained by varying either the number of robots [CONTINUE] = 10 or the time horizon h = 20 while keeping all the other parameters intact. Solution times in the first and second column are obtained by alternative cLTL encodings proposed in Section IV-F and regular cLTL+ encodings, respectively. Regular cLTL+ encodings could not find solutions for [CONTINUE] = 500 within the timeout threshold of 60 minutes. On the other hand, cLTL encodings scale much better with the number of robots and easily handle hundreds of robots in a matter of seconds. In fact, solution times are almost unaffected by the number of robots."
    },
    {
        "id": "961",
        "table": {
            "header": [
                "[EMPTY]",
                "Robust CBF",
                "Non-Robust CBF"
            ],
            "rows": [
                [
                    "Avg. WCT (ms)",
                    "4.473",
                    "4.048"
                ],
                [
                    "Var. of WCTs (ms2)",
                    "5.793",
                    "3.435"
                ],
                [
                    "Avg. Freq. (Hz)",
                    "222",
                    "247"
                ],
                [
                    "Time Violated (s)",
                    "0",
                    "138"
                ]
            ],
            "title": "Table II: Comparison of the Wall-Clock Times (WCTs) for solving the Quadratic Programs with and without the robust CBF formulation. The last entry is the duration during which the constraint was violated for each experiment."
        },
        "insight": "Focusing on the run time, solving the robust CBF QP averaged a wall-clock time of 4.5 ms, translating to a frequency of approximately 220 Hz, resulting in only a 25 Hz decrease compared to solving the non-robust CBF QP. The reason the frequency decreases slightly is that the only computation added is a min operation over p values, which is linear with respect to p (O(p)), where p is the number of points forming the convex hull of the disturbance. The comparison of the wall-clock times of both experiments is shown in Table II. [CONTINUE] The robust CBF formulation (left) encounters 0 collisions whereas the non-robust CBF formulation (right) periodically encounters collisions as shown in Table II."
    },
    {
        "id": "962",
        "table": {
            "header": [
                "Finch",
                "Island A",
                "Island B",
                "Island C",
                "Island D",
                "Island E",
                "Island F",
                "Island G",
                "Island H",
                "Island I",
                "Island J",
                "Island K",
                "Island L",
                "Island M",
                "Island N",
                "Island O",
                "Island P",
                "Island Q"
            ],
            "rows": [
                [
                    "1",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1"
                ],
                [
                    "2",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "3",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "4",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "0",
                    "0",
                    "1",
                    "0",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "1",
                    "1",
                    "1"
                ],
                [
                    "5",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "6",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "1",
                    "0",
                    "1",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "7",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "0",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "8",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "1",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "9",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "0",
                    "0",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "10",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0"
                ],
                [
                    "11",
                    "0",
                    "0",
                    "1",
                    "1",
                    "1",
                    "0",
                    "1",
                    "1",
                    "0",
                    "1",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "12",
                    "0",
                    "0",
                    "1",
                    "1",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "13",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1"
                ]
            ],
            "title": "Table 1: Occurrence Matrix occurrence matrix of the finches on the Galapagos islands."
        },
        "insight": "much attention in numerical ecology. In ecological studies, the binary matrix is called occurrence matrix. Rows usually corresponds to species, the columns, to locations. For example, the binary matrix shown on Table 1 is known as \"Darwin's Finch\" dataset, which comes from Darwin's studies of the finches on the Galapagos islands (an archipelago in the East Pacific). The matrix represents the presence/absence of 13 species of finches in 17 islands. A \"1\" or \"0\" in entry (i, j) indicates the presence or absence of species i at island j. It is clear from Table 1 that some pairs of species tend to occur together (for example, species 9 and 10) while some other pairs tend to be disjoint. Therefore, it is of our interest to investigate whether the cooperation/competition influences the distribution of species on islands, or the patterns found are just by chance. [CONTINUE] before each successful swap using 'Darwin's Finch' dataset. Assuming the randomly chosen row is mostly filled, such as row 1 in Table 1, the two random chosen entries in this row would most likely be [1, 1] but the row of a 'checkerboard unit' has to be either [1, 0] or [0, 1]. Therefore swapping rarely happens when the chosen row/column is mostly filled (or equivalently, mostly unfilled)."
    },
    {
        "id": "963",
        "table": {
            "header": [
                "Method",
                "Filled portion",
                "Number of swaps",
                "Time per swap (/s)"
            ],
            "rows": [
                [
                    "Rectangle Loop",
                    "1%",
                    "586",
                    "1.18\u00d710\u22125"
                ],
                [
                    "Swap",
                    "1%",
                    "8",
                    "3.67\u00d710\u22124"
                ],
                [
                    "Rectangle Loop",
                    "5%",
                    "977",
                    "5.30\u00d710\u22126"
                ],
                [
                    "Swap",
                    "5%",
                    "42",
                    "3.52\u00d710\u22125"
                ],
                [
                    "Rectangle Loop",
                    "10%",
                    "1838",
                    "3.23\u00d710\u22126"
                ],
                [
                    "Swap",
                    "10%",
                    "156",
                    "1.25\u00d710\u22125"
                ],
                [
                    "Rectangle Loop",
                    "20%",
                    "3271",
                    "2.64\u00d710\u22126"
                ],
                [
                    "Swap",
                    "20%",
                    "509",
                    "5.68\u00d710\u22126"
                ],
                [
                    "Rectangle Loop",
                    "30%",
                    "4222",
                    "2.10\u00d710\u22126"
                ],
                [
                    "Swap",
                    "30%",
                    "803",
                    "5.06\u00d710\u22126"
                ],
                [
                    "Rectangle Loop",
                    "40%",
                    "4794",
                    "1.27\u00d710\u22126"
                ],
                [
                    "Swap",
                    "40%",
                    "1160",
                    "4.98\u00d710\u22126"
                ],
                [
                    "Rectangle Loop",
                    "50%",
                    "5080",
                    "1.37\u00d710\u22126"
                ],
                [
                    "Swap",
                    "50%",
                    "1271",
                    "5.36\u00d710\u22126"
                ]
            ],
            "title": "Table 4: The comparison between swap algorithm and Rectangle Loop algorithm. Each algorithm is implemented 10000 iterations on 100\u00d7100 matrices with different filled portions. The third column records the number of successful swaps among the 10000 iterations, the last column records the average time per swap, respectively."
        },
        "insight": "When the filled portion p is small, the Rectangle Loop algorithm is extremely efficient, producing more than 73 times more swaps than the swap algorithm. For large p, the advantage of Rectangle Loop algorithm is reduced, but still very significant. For p = 0.5, the Rectangle Loop still produces 4 times more swaps than the swap algorithm. Noteworthy, the zeros and ones play the symmetric rule in a binary matrix, therefore it is not necessary to generate the random matrix for p > 0.5. We also record the time per swap for both algorithms, as shown in the last column of Table 4. It turns out that the Rectangle Loop algorithm still has a significant advantage than the swap algorithm after the running time issue is taken into account. For p = 0.01, the Rectangle Loop is about 31 times more efficient than the swap algorithm. Even for p = 0.5, Rectangle Loop algorithm is still about 4 times more efficient than the swap algorithm."
    },
    {
        "id": "964",
        "table": {
            "header": [
                "[ITALIC] \u03b4",
                "Hamming bound length of vector ( [ITALIC] m/log [ITALIC] n)",
                "Hamming bound exponent ( [ITALIC] \u03b3\u2032)",
                "GV bound length of vector ( [ITALIC] m/log [ITALIC] n)",
                "GV bound exponent ( [ITALIC] \u03b3\u2032)"
            ],
            "rows": [
                [
                    "0.01",
                    "1.0476",
                    "1.0742",
                    "1.0879",
                    "1.0770"
                ],
                [
                    "0.025",
                    "1.1074",
                    "1.1591",
                    "1.2029",
                    "1.1728"
                ],
                [
                    "0.05",
                    "1.2029",
                    "1.2844",
                    "1.4013",
                    "1.3313"
                ],
                [
                    "0.075",
                    "1.2999",
                    "1.4021",
                    "1.6242",
                    "1.5024"
                ],
                [
                    "0.1",
                    "1.4013",
                    "1.5171",
                    "1.8832",
                    "1.6949"
                ],
                [
                    "0.125",
                    "1.5090",
                    "1.6316",
                    "2.1909",
                    "1.9170"
                ],
                [
                    "0.133",
                    "1.5449",
                    "1.6684",
                    "2.3064",
                    "1.9989"
                ]
            ],
            "title": "Table 1: Running time of our algorithm when vector length m and relative distance \u03b4 meets the Hamming bound and GV bound"
        },
        "insight": "The running time of our algorithm depends on \u2014 in addition to the number of vectors n \u2014 both dimension m and \u03b4 := dmin/m. To illustrate its performance we choose two typical vector lengths m, namely those corresponding to the Hamming bound7 and the Gilbert-Varshamov (GV) bound8, and list the exponents \u03b3(cid:48) in the running time of the GV-code version of our algorithm as a function of dmin (in fact \u03b4) in Table 1. Here, we write the running of the algorithm as \u02dcO(n\u03b3(cid:48) ), where \u02dcO suppresses any polylogarithmic factor of n. One can see that our algorithm runs in subquadratic time when \u03b4 is small, or equivalently when the Hamming distance between the closest pair is small. For instance, when \u03b4 = 0.05, and the length m = 1.4013 log n, then the running time is O(n1.3313) if we use GV bound."
    },
    {
        "id": "965",
        "table": {
            "header": [
                "Data Source",
                "Frequency",
                "Percent"
            ],
            "rows": [
                [
                    "WOS (matched Unpaywall Only articles & reviews)",
                    "11,661,206",
                    "57.5%"
                ],
                [
                    "WOS (Only articles & reviews 2000-2017)",
                    "20,280,606",
                    "-"
                ],
                [
                    "Scopus (matched Unpaywall Only articles & reviews)",
                    "14,188,983",
                    "53.48%"
                ],
                [
                    "Scopus (Only articles & reviews 2000-2017)",
                    "26,532,295",
                    "-"
                ]
            ],
            "title": "Table 1: All publications from WOS (2000-2017) and Scopus (2000-2017) that have an equivalent record in Unpaywall database (joined by DOIs)"
        },
        "insight": "Table 1 shows the number of articles and review papers from WOS and Scopus with an equivalent record in Unpaywall database. It presents also the total number of articles and review papers in WOS/Scopus to provide a baseline for comparison. Unpaywall has higher than 50% coverage in both cases while coverage of WOS is slightly higher (can be due to different indexing philosophy or DOIs completeness). In the following tables (in Unpaywall results), publications are limited to only articles and review papers published in 2000-2017."
    },
    {
        "id": "966",
        "table": {
            "header": [
                "Number of licences per DOI",
                "Frequency of DOIs",
                "Percent"
            ],
            "rows": [
                [
                    "0",
                    "9,892,208",
                    "51.41"
                ],
                [
                    "1",
                    "8,520,158",
                    "44.28"
                ],
                [
                    "2",
                    "824,975",
                    "4.29"
                ],
                [
                    "3",
                    "5,770",
                    "0.03"
                ],
                [
                    "5",
                    "25",
                    "0.00"
                ],
                [
                    "6",
                    "7",
                    "0.00"
                ]
            ],
            "title": "Table 2: Number of licences per DOI found in Crossref"
        },
        "insight": "Table 2 presents a descriptive view on whether publications have licence information recorded in Crossref. It shows that about 50% of publications from WOS or Scopus with a matching DOI indexed in Crossref do not have a licence URL. Some of the publications had more than one licence information in Crossref (as an example, the number of DOIs that each have 6 licence records on Crossref are 7)."
    },
    {
        "id": "967",
        "table": {
            "header": [
                "Crossref OA Status",
                "Unpaywall OA Status",
                "Frequency",
                "Percent"
            ],
            "rows": [
                [
                    "Closed Access",
                    "Closed Access",
                    "4,452,185",
                    "38.18"
                ],
                [
                    "[EMPTY]",
                    "Closed Access",
                    "3,512,794",
                    "30.12"
                ],
                [
                    "[EMPTY]",
                    "Open Access",
                    "1,770,612",
                    "15.18"
                ],
                [
                    "Closed Access",
                    "Open Access",
                    "1,363,525",
                    "11.69"
                ],
                [
                    "Open Access",
                    "Open Access",
                    "435,516",
                    "3.73"
                ],
                [
                    "Open Access",
                    "Closed Access",
                    "126,354",
                    "1.08"
                ],
                [
                    "Closed Access",
                    "[EMPTY]",
                    "26",
                    "0.00"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "19",
                    "0.00"
                ]
            ],
            "title": "Table 3: OA status comparison between Unpaywall and Crossref in WOS publications"
        },
        "insight": "Tables 3 and 4 present the OA status comparison between Unpaywall and Crossref in WOS and Scopus publications, respectively. [CONTINUE] Overall contradictory cases amount to 27.95% in WOS and 27.57% in Scopus which might partly be explained by the wider scope of Unpaywall including also green OA publications that might not be identified via license information only. [CONTINUE] (22.98% in WOS and 22.91% in Scopus, these percentages are quite close to contradictions observed in the overall sample presented in Tables 3 and 4)."
    },
    {
        "id": "968",
        "table": {
            "header": [
                "PDF Manually accessible?",
                "Licence status",
                "Pub OA?",
                "Frequency",
                "Percent"
            ],
            "rows": [
                [
                    "PDF Accessible",
                    "Open Access",
                    "Unpaywall OA",
                    "104",
                    "46.85"
                ],
                [
                    "No Access to PDF",
                    "Closed Access",
                    "Unpaywall non-OA",
                    "44",
                    "19.82"
                ],
                [
                    "No Access to PDF",
                    "Open Access",
                    "Unpaywall non-OA",
                    "18",
                    "8.11"
                ],
                [
                    "No Access to PDF",
                    "Closed Access",
                    "Unpaywall OA",
                    "16",
                    "7.21"
                ],
                [
                    "PDF Accessible",
                    "Closed Access",
                    "Unpaywall OA",
                    "16",
                    "7.21"
                ],
                [
                    "PDF Accessible",
                    "Closed Access",
                    "Unpaywall non-OA",
                    "14",
                    "6.31"
                ],
                [
                    "No Access to PDF",
                    "Open Access",
                    "Unpaywall OA",
                    "5",
                    "2.25"
                ],
                [
                    "[EMPTY]",
                    "Closed Access",
                    "Unpaywall non-OA",
                    "1",
                    "0.45"
                ],
                [
                    "No Access to PDF",
                    "Closed Access",
                    "Missing on Unpaywall",
                    "1",
                    "0.45"
                ],
                [
                    "PDF Accessible",
                    "[EMPTY]",
                    "Unpaywall non-OA",
                    "1",
                    "0.45"
                ],
                [
                    "PDF Accessible",
                    "Open Access",
                    "Unpaywall non-OA",
                    "1",
                    "0.45"
                ],
                [
                    "PDF Accessible",
                    "[EMPTY]",
                    "Unpaywall OA",
                    "1",
                    "0.45"
                ]
            ],
            "title": "Table 5: Random sample OA status check on publications from WOS"
        },
        "insight": "Tables 5 and 6 present the result of our research assistant's manual check for accessibility to article's PDF file from publishers websites compared to the respecitve licence in Crossref and the OA status we manually assignded to those URLs in contrast to OA status from Unpaywall. It is interesting to see there are publications defined as Non-OA while their PDF is accessible from the publisher (14.42% in WOS and 14.98% in Scopus) or vice versa, OA publications (based on either Unpaywall, Crossref or both) that are not accessible online [CONTINUE] (17.57% in WOS and 16.74% in Scopus). Note also the contradictory cases between Crossref and Unpaywall, where metadata from one shows OA and the other Closed, which requires further probes"
    },
    {
        "id": "969",
        "table": {
            "header": [
                "Number of cells per node",
                "4096"
            ],
            "rows": [
                [
                    "Number of particles per cell",
                    "2048"
                ],
                [
                    "Compilation flags",
                    "-openmp, -mavx (Cluster),"
                ],
                [
                    "[EMPTY]",
                    "-xMIC-AVX512 (Booster)"
                ]
            ],
            "title": "TABLE II: xPic experiment setup in the Cluster-Booster architecture evaluation measurements."
        },
        "insight": "The experimental setup is summarized in table II. [CONTINUE] running only on the Cluster means executing the particle solver on one Cluster node first and, once finished, using the same node for the field solver."
    },
    {
        "id": "970",
        "table": {
            "header": [
                "#  [BOLD] buses",
                "[BOLD] Heuristic",
                "[BOLD] Avg Latency",
                "[BOLD] Conf. Int. (95%)",
                "[BOLD] Errors"
            ],
            "rows": [
                [
                    "60",
                    "Fixed Random",
                    "72.68 sec",
                    "[70.43, 74.94] sec",
                    "15.37%"
                ],
                [
                    "60",
                    "Dynamic Random",
                    "56.0 sec",
                    "[54.51, 57.5] sec",
                    "18.26%"
                ],
                [
                    "60",
                    "Adaptive RTT",
                    "22.99 sec",
                    "[22.69, 23.29] sec",
                    "0.81%"
                ],
                [
                    "120",
                    "Fixed Random",
                    "87.75 sec",
                    "[85.38, 90.12] sec",
                    "29.49%"
                ],
                [
                    "120",
                    "Dynamic Random",
                    "67.6 sec",
                    "[66.29, 68.9] sec",
                    "18.99%"
                ],
                [
                    "120",
                    "Adaptive RTT",
                    "27.35 sec",
                    "[27.11, 27.58] sec",
                    "1.1%"
                ],
                [
                    "240",
                    "Fixed Random",
                    "177.62 sec",
                    "[174.25, 181.0] sec",
                    "42.81%"
                ],
                [
                    "240",
                    "Dynamic Random",
                    "128.2 sec",
                    "[126.28, 130.12] sec",
                    "44.85%"
                ],
                [
                    "240",
                    "Adaptive RTT",
                    "73.26 sec",
                    "[72.68, 73.85] sec",
                    "7.55%"
                ]
            ],
            "title": "Table I: Results on IOTA, with 60, 120, 240 buses."
        },
        "insight": "The amount of errors is quite high, as well as the measured latencies. Thus, these tests seem to conclude that, at the time of writing, the IOTA DLT is not fully structured to support smart services for transportation systems. On the other hand, the good news is that if we carefully select the full node to issue a transaction, the performances definitely improve. In fact, our third scheme \"Adaptive RTT\" has a low amount of errors, on average around 0.8%. Measured latencies are lower than other approaches. Still, the average latency amounts to 23 seconds, which is far from a real-time update of the DLT. [CONTINUE] charts further confirm the better performance obtained by the \"Adaptive RTT\" scheme. These [CONTINUE] Results confirm that \"Adaptive RTT\" provides better results. Average latencies are definitely lower than other schemes. [CONTINUE] In all cases, average latencies increase with the number of buses. [CONTINUE] To better emphasize the outcomes, Table [CONTINUE] reports some summarized statistics (shown in the box plots) and the error rates. Actually, the main difference on the performance of the approaches is on the amount of errors. While the average error for \"Adaptive RTT\" is \u223c 1%, for the other two schemes we have errors well above 15%. These error rates are clearly unacceptable, meaning that these approaches are unusable."
    },
    {
        "id": "971",
        "table": {
            "header": [
                "#  [BOLD] buses",
                "[BOLD] Avg Latency",
                "[BOLD] Conf. Int. (95%)",
                "[BOLD] Errors"
            ],
            "rows": [
                [
                    "120",
                    "777.17 msec",
                    "[774.68, 779.65] msec",
                    "2.73%"
                ]
            ],
            "title": "Table II: Preliminary results on Radix."
        },
        "insight": "This gave us some preliminary results that we report in Table II. In the table, we show the average latency, confidence interval and error percentage to add transactions on this Radix alphanet. Results are averaged over an amount of 12 test repetitions. In this case, we obtained very low latencies (below 1 sec), with a non negligible (but low) error rate."
    },
    {
        "id": "972",
        "table": {
            "header": [
                "[EMPTY]",
                "Virginia (US)",
                "Japan",
                "India",
                "Australia"
            ],
            "rows": [
                [
                    "Virginia (US)",
                    "[BOLD] 198",
                    "238",
                    "306",
                    "303"
                ],
                [
                    "Japan",
                    "236",
                    "[BOLD] 167",
                    "239",
                    "246"
                ],
                [
                    "India",
                    "304",
                    "242",
                    "[BOLD] 229",
                    "305"
                ],
                [
                    "Australia",
                    "303",
                    "232",
                    "304",
                    "[BOLD] 229"
                ]
            ],
            "title": "TABLE I: Zyzzyva\u2019s\u00a0[14] latencies (in ms) in a geo-scale deployment with primary at different locations. Columns indicate the primary\u2019s location. Rows indicate average client-side latency for commands issued from that region. For example, the entry at the 4th row and the 3rd column shows the client-side latency for commands issued from India to the primary in Japan. Lowest latency per primary location is highlighted."
        },
        "insight": "To validate these hypotheses, we deployed Zyzzyva  in a 4-replica geo-scale setting with nodes located in the US (Virginia), Japan, India, and Australia, using Amazon's EC2 infrastructure . We deployed clients alongside each replica to inject requests, and measured the client-side latencies by changing the primary's location. Table [CONTINUE] shows the results. We observe that the lowest latencies are when the primary is colocated within the same region. [CONTINUE] Zyzzyva's  latencies (in ms) in a geo-scale deployment with primary at different locations. Columns indicate the primary's location. Rows indicate average client-side latency for commands issued from that region. For example, the entry at the 4th row and the 3rd column shows the clientside latency for commands issued from India to the primary in Japan. Lowest latency per primary location is highlighted."
    },
    {
        "id": "973",
        "table": {
            "header": [
                "[BOLD] Protocol",
                "[BOLD] PBFT",
                "[BOLD] Zyzzyva",
                "[BOLD] Aliph",
                "[BOLD] ezBFT"
            ],
            "rows": [
                [
                    "Resilience",
                    "[ITALIC] f< [ITALIC] n/3",
                    "[ITALIC] f< [ITALIC] n/3",
                    "[ITALIC] f< [ITALIC] n/3",
                    "[ITALIC] f< [ITALIC] n/3"
                ],
                [
                    "Best-case comm. steps",
                    "5",
                    "3",
                    "2",
                    "3"
                ],
                [
                    "Best-case comm. steps in absence of \u2026",
                    "Byz. Slow links",
                    "Byz. Slow links Contention",
                    "Byz. Slow links Contention",
                    "Byz. Slow links Contention"
                ],
                [
                    "Slow-path steps",
                    "-",
                    "2",
                    "n + 3",
                    "2"
                ],
                [
                    "Leader",
                    "Single",
                    "Single",
                    "Single",
                    "Leaderless"
                ]
            ],
            "title": "TABLE II: Comparison of existing BFT protocols and ezBFT."
        },
        "insight": "Zyzzyva ,  uses 3f + 1 nodes to solve consensus in three steps (including client communication), requiring a quorum of 3f +1 responses. The protocol tolerates f faults, so it takes an additional two steps when nodes are faulty. Zyzzyva uses the minimum number of nodes, communication steps, and one-to-one messages to achieve fast consensus. It is cheaper than the aforementioned protocols, but also more complex. Zyzzyva's performance boost is due to speculative execution, active participation of the clients in the agreement process, and tolerating temporary inconsistencies among replicas. EZBFT has the same node and quorum requirements as well as the number of communication steps as Zyzzyva. However, by minimizing the latency of the first communication step and alleviating the specialized role of the primary, EZBFT reduces the request processing latency. Aliph  builds a BFT protocol by composing three different sub-protocols, each handling a specific system factor such as contention, slow links, and byzantine faults. Under zero contention, the sub-protocol Quorum can deliver agreement in two steps with 3f + 1 nodes by allowing clients to send the requests directly to the nodes. However, as contention or link latency increases, or as faults occur, Aliph switches to the subprotocol Chain whose additional steps is equal to the number of nodes in the system, or to the sub-protocol Backup which takes at least three steps. [CONTINUE] EZBFT exploits the trade-off between the slow and fast path steps. EZBFT uses three steps compared to Aliph's two steps in the common case, and in return, provides slow path in only two extra communication steps unlike Aliph. [CONTINUE] Table II summarizes the comparison of existing work with EZBFT. Note that EZBFT and Zyzzyva have the same bestcase communication steps. However, for EZBFT, the latency for the first-step communication is minuscule (tending towards zero) when compared to Zyzzyva's first-step latency."
    },
    {
        "id": "974",
        "table": {
            "header": [
                "Addr.",
                "Exec. (s)",
                "Runt. (s)",
                "Transm. (s)",
                "Total (s)"
            ],
            "rows": [
                [
                    "Client",
                    "8.10 (0.21)",
                    "3.55 (0.11)",
                    "0.87 (-)",
                    "12.52 (0.32)"
                ],
                [
                    "AoT",
                    "9.94 (0.26)",
                    "3.77 (0.08)",
                    "20.44 (0.13)",
                    "34.15 (0.47)"
                ]
            ],
            "title": "TABLE I: Average runtimes of workflow parts in the ring scenario in client-only tests and using AoT addressing."
        },
        "insight": "Tables [CONTINUE] and II show the average time needed for the parts of a workflow (the numbers in brackets show the standard deviation) in seconds. Table II indicates that the overall workflow time highly depends on the worker assignment in [CONTINUE] As shown in Table I, the total execution time is about 12.52 seconds and is pretty stable with only about 300 ms deviation. Finally, the AoT mode needs about 34.15 seconds in total and is also stable with only about 400 ms deviation. Since in AoT mode a worker is always two hops away from the next hop, the transmission is even faster than using JiT mode with the best worker assignment."
    },
    {
        "id": "975",
        "table": {
            "header": [
                "Assign.",
                "Exec. (s)",
                "Runt. (s)",
                "Transm. (s)",
                "Total (s)"
            ],
            "rows": [
                [
                    "Recent",
                    "9.65 (0.26)",
                    "3.89 (0.09)",
                    "50.94 (10.20)",
                    "64.48 (10.50)"
                ],
                [
                    "Random",
                    "9.82 (0.16)",
                    "3.93 (0.09)",
                    "32.60 (4.27)",
                    "46.35 (4.25)"
                ],
                [
                    "Best",
                    "10.02 (0.28)",
                    "3.94 (0.08)",
                    "23.54 (9.63)",
                    "37.49 (9.99)"
                ],
                [
                    "Spread",
                    "9.95 (0.20)",
                    "3.95 (0.09)",
                    "24.05 (6.82)",
                    "37.94 (7.11)"
                ]
            ],
            "title": "TABLE II: Average runtimes of workflow parts in the ring scenario using JiT addressing and all four assignments."
        },
        "insight": "the JiT experiments. The recent worker assignment with an average of about 64.48 seconds requires the longest time, due to the long distance between the nodes, since their offers take longer to reach the client and thus arrive more recently. The standard deviation is also relatively high with more than 10 seconds, indicating long running tasks and differing results. The random worker assignment achieves better results with about 46.35 seconds on average and a deviation of 4.25 seconds, since closer workers are chosen. Always selecting the best available worker leads to significantly lower workflow times, requiring about 37.49 seconds, but with a standard deviation of 9.99 seconds. Finally, using the spread assignment algorithm, the workflow time does not significantly differ from the previous assignment algorithm, using about 37.94 seconds, but has a better standard deviation of 7.11 seconds. If all workers are equally capable, the workflow times using the best worker or the spread algorithm do not differ. This shows clearly that in terms of workflow time, the algorithm using the best workers and our spread approach outperform the other approaches. But since not all workers are equally capable in the different capability tests, tests using the best worker have a broader standard deviation, since the capable workers are further away in the topology. This means that always using the best worker is slightly faster than using the spread algorithm, but is more unpredictable in how long the execution of a workflow will take, since the very best workers will be worn up and worse workers have to be chosen consequently. Therefore, we propose our spread algorithm as the best available solution. [CONTINUE] Overall, as previously shown in Table II, the workflow times are nearly as good as always selecting the best worker."
    },
    {
        "id": "976",
        "table": {
            "header": [
                "Assign.",
                "Exec. (s)",
                "Runt. (s)",
                "Transm. (s)",
                "Total (s)"
            ],
            "rows": [
                [
                    "Recent",
                    "8.7 (0.64)",
                    "5.0 (1.89)",
                    "269.0 (336.37)",
                    "282.8 (338.91)"
                ],
                [
                    "Random",
                    "8.9 (1.02)",
                    "5.0 (1.84)",
                    "254.9 (300.75)",
                    "268.8 (303.61)"
                ],
                [
                    "Best",
                    "8.9 (0.62)",
                    "5.2 (1.80)",
                    "135.5 (191.26)",
                    "149.6 (193.68)"
                ],
                [
                    "Spread",
                    "8.9 (0.68)",
                    "5.1 (1.95)",
                    "234.2 (300.75)",
                    "248.1 (303.61)"
                ]
            ],
            "title": "TABLE III: Average runtimes of tasks in mobile JiT scenarios in seconds."
        },
        "insight": "using our spread algorithm gives better results than random assignment and using [CONTINUE] using a recent worker. Note that the transmission times (and thus also the total times) have a rather high standard deviation. This is due to the mobility of the nodes and potentially disappearing links between two nodes, resulting in re-transmissions. These increase the time, whereas many transmissions are successful within the first try, reducing the mean transmission time."
    },
    {
        "id": "977",
        "table": {
            "header": [
                "[BOLD] Nodes",
                "[BOLD] Avg. latency of sending message",
                "[BOLD] Avg. latency of getting message"
            ],
            "rows": [
                [
                    "[BOLD] chief",
                    "0.149 ( [ITALIC] \u03c3 = 0.031)",
                    "0.043 ( [ITALIC] \u03c3 = 0.039)"
                ],
                [
                    "[BOLD] worker 1",
                    "0.036 ( [ITALIC] \u03c3 = 0.020)",
                    "0.041 ( [ITALIC] \u03c3 = 0.028)"
                ],
                [
                    "[BOLD] worker 2",
                    "0.037 ( [ITALIC] \u03c3 = 0.029)",
                    "0.042 ( [ITALIC] \u03c3 = 0.025)"
                ],
                [
                    "[BOLD] worker 3",
                    "0.036 ( [ITALIC] \u03c3 = 0.033)",
                    "0.043 ( [ITALIC] \u03c3 = 0.021)"
                ],
                [
                    "[BOLD] worker 4",
                    "0.037 ( [ITALIC] \u03c3 = 0.026)",
                    "0.041 ( [ITALIC] \u03c3 = 0.022)"
                ]
            ],
            "title": "Table 2: Average latency of sending and getting a message from the BSMD"
        },
        "insight": "Table 2 shows the average latency of sending and receiving messages on BSMD for the dSA based estimation process. The latency of worker nodes remains under 0.05 secs, thus the response times are promising for implementing more complex distributed choice models over BSMD. In this particular case, the computational resources needed for running [CONTINUE] the model on worker nodes are minimal. [CONTINUE] The average latency of sending a massage from chief node is 0.149 secs, a bit higher than the workers. The increment is due to chief sending the information to the four workers in batches."
    },
    {
        "id": "978",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FNR",
                "[BOLD] Precision",
                "[BOLD] f1-score",
                "[BOLD] Accuracy",
                "[BOLD] FPR"
            ],
            "rows": [
                [
                    "SVM",
                    "04.81%",
                    "96.62%",
                    "95.90%",
                    "95.93%",
                    "3.33%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b11.15%)",
                    "(\u00b10.40%)",
                    "(\u00b10.80%)",
                    "(\u00b10.77%)",
                    "[EMPTY]"
                ],
                [
                    "XGBoost",
                    "02.97%",
                    "93.59%",
                    "95.28%",
                    "95.19%",
                    "6.64%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.20%)",
                    "(\u00b10.31%)",
                    "(\u00b10.23%)",
                    "(\u00b10.25%)",
                    "[EMPTY]"
                ],
                [
                    "DNN",
                    "13.50%",
                    "[BOLD] 98.77%",
                    "92.22%",
                    "92.70%",
                    "[BOLD] 1.00%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.64%)",
                    "(\u00b10.62%)",
                    "(\u00b10.46%)",
                    "(\u00b10.42%)",
                    "[EMPTY]"
                ],
                [
                    "RF",
                    "04.18%",
                    "92.52%",
                    "94.14%",
                    "94.03%",
                    "7.76%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.55%)",
                    "(\u00b10.98%)",
                    "(\u00b10.34%)",
                    "(\u00b10.38%)",
                    "[EMPTY]"
                ],
                [
                    "K-NN",
                    "02.80%",
                    "93.36%",
                    "95.25%",
                    "95.15%",
                    "6.91%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.48%)",
                    "(\u00b10.70%)",
                    "(\u00b10.55%)",
                    "(\u00b10.57%)",
                    "[EMPTY]"
                ],
                [
                    "ELM",
                    "03.00%",
                    "94.51%",
                    "95.76%",
                    "95.70%",
                    "5.60%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.30%)",
                    "(\u00b10.47%)",
                    "(\u00b10.31%)",
                    "(\u00b10.30%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics",
                    "[BOLD] 01.44%",
                    "96.45%",
                    "[BOLD] 97.36%",
                    "[BOLD] 97.33%",
                    "3.90%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.33%)",
                    "(\u00b10.45%)",
                    "(\u00b10.29%)",
                    "(\u00b10.30%)",
                    "[EMPTY]"
                ],
                [
                    "",
                    "06.37%",
                    "\u2212",
                    "\u2212",
                    "95.93%",
                    "3.96%"
                ],
                [
                    "",
                    "3.00%",
                    "95.00%",
                    "96.00%",
                    "\u2212",
                    "\u2212"
                ]
            ],
            "title": "TABLE II: The Mean and Std of Malytics and the baselines for Drebin Dataset."
        },
        "insight": "The performance of the models on DexShare is presented in table III. [CONTINUE] It is to be expected that all models perform weaker on the dataset compared to Drebin, since the detaset is more complicated to deal with. Malytics again outperforms all models on DexShare."
    },
    {
        "id": "979",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FNR",
                "[BOLD] Precision",
                "[BOLD] f1-score",
                "[BOLD] Accuracy",
                "[BOLD] FPR"
            ],
            "rows": [
                [
                    "SVM",
                    "08.00%",
                    "94.77%",
                    "93.34%",
                    "93.44%",
                    "05.07%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.48%)",
                    "(\u00b10.25%)",
                    "(\u00b10.18%)",
                    "(\u00b10.16%)",
                    "[EMPTY]"
                ],
                [
                    "XGBoost",
                    "10.12%",
                    "90.74%",
                    "90.30%",
                    "90.35%",
                    "09.17%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.55%)",
                    "(\u00b10.39%)",
                    "(\u00b10.36%)",
                    "(\u00b10.35%)",
                    "[EMPTY]"
                ],
                [
                    "DNN",
                    "24.40%",
                    "90.40%",
                    "82.23%",
                    "83.72%",
                    "08.13%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b13.53%)",
                    "(\u00b11.73%)",
                    "(\u00b11.45%)",
                    "(\u00b10.89%)",
                    "[EMPTY]"
                ],
                [
                    "RF",
                    "13.07%",
                    "92.73%",
                    "89.73%",
                    "90.05%",
                    "06.82%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.35%)",
                    "(\u00b10.32%)",
                    "(\u00b10.22%)",
                    "(\u00b10.21%)",
                    "[EMPTY]"
                ],
                [
                    "K-NN",
                    "7.45%",
                    "93.36%",
                    "91.38%",
                    "91.28%",
                    "10.00%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.48%)",
                    "(\u00b10.70%)",
                    "(\u00b10.55%)",
                    "(\u00b10.57%)",
                    "[EMPTY]"
                ],
                [
                    "ELM",
                    "16.50%",
                    "92.25%",
                    "87.66%",
                    "88.24%",
                    "07.00%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.29%)",
                    "(\u00b10.67%)",
                    "(\u00b10.30%)",
                    "(\u00b10.36%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics",
                    "[BOLD] 05.53%",
                    "[BOLD] 95.88%",
                    "[BOLD] 95.17%",
                    "[BOLD] 95.20%",
                    "[BOLD] 04.06%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.46%)",
                    "(\u00b10.40%)",
                    "(\u00b10.20%)",
                    "(\u00b10.20%)",
                    "[EMPTY]"
                ],
                [
                    "",
                    "11.60%",
                    "88.16%",
                    "\u2212",
                    "88.26%",
                    "\u2212"
                ],
                [
                    "[EMPTY]",
                    "(\u00b12.76%)",
                    "(\u00b11.8%)",
                    "\u2212",
                    "(\u00b11.73%)",
                    "[EMPTY]"
                ]
            ],
            "title": "TABLE III: The Mean and Std of Malytics and the baselines for DexShare Dataset."
        },
        "insight": "Table IV provides more inside into Malytics. [CONTINUE] Table IV shows the average FNR, precision, f1-score, accuracy and FPR of Malytics with our proposed feature representation. The main important index of the test is FNR as a measure that shows how well Malytics detected new families."
    },
    {
        "id": "980",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FNR",
                "[BOLD] Precision",
                "[BOLD] f1-score",
                "[BOLD] Accuracy",
                "[BOLD] FPR"
            ],
            "rows": [
                [
                    "Malytics (APK)",
                    "09.43%",
                    "91.76%",
                    "91.16%",
                    "91.22%",
                    "8.1%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.61%)",
                    "(\u00b10.73%)",
                    "(\u00b10.54%)",
                    "(\u00b10.48%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics (Dex, MBR=0.5)",
                    "05.33%",
                    "95.88%",
                    "95.17%",
                    "95.20%",
                    "4.1%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.46%)",
                    "(\u00b10.40%)",
                    "(\u00b10.20%)",
                    "(\u00b10.20%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics (Dex, MBR=0.2)",
                    "[BOLD] 05.27%",
                    "[BOLD] 98.45%",
                    "96.55%",
                    "98.65%",
                    "3.7%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.63%)",
                    "(\u00b10.69%)",
                    "(\u00b10.55%)",
                    "(\u00b10.21%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics (Zero-day)",
                    "[BOLD] 10.59%",
                    "[BOLD] 96.31%",
                    "[BOLD] 92.68%",
                    "92.99%",
                    "3.4%"
                ],
                [
                    "",
                    "12.00%",
                    "86.00%",
                    "87.00%",
                    "\u2212",
                    "\u2212"
                ]
            ],
            "title": "TABLE IV: The Mean and Std of Malytics for DexShare dataset on the APK, Dex. Also, the results when the dataset is imbalanced and for zero-day (novel families) detection."
        },
        "insight": "Table V presents the results of the experiment on both datasets. Malytics False positive improves slightly while hit-rate is very close to dense setting (see table II). The size of hashing provides richer hidden representation for DexShare samples. In addition to being more precise, Malytics has better hit-rate (see table III). The imbalanced setting shows Malytics performance for real word application."
    },
    {
        "id": "981",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] FNR",
                "[BOLD] Precision",
                "[BOLD] f1-score",
                "[BOLD] Accuracy",
                "[BOLD] FPR"
            ],
            "rows": [
                [
                    "Drebin",
                    "01.53%",
                    "96.68%",
                    "97.56%",
                    "97.54%",
                    "3.38%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.50%)",
                    "(\u00b10.45%)",
                    "(\u00b10.40%)",
                    "(\u00b10.30%)",
                    "[EMPTY]"
                ],
                [
                    "DexShare",
                    "04.72%",
                    "96.69%",
                    "95.96%",
                    "96.00%",
                    "3.30%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.50%)",
                    "(\u00b10.35%)",
                    "(\u00b10.38%)",
                    "(\u00b10.35%)",
                    "[EMPTY]"
                ],
                [
                    "DexShare (MBR=0.2)",
                    "[BOLD] 04.42%",
                    "98.91%",
                    "97.21%",
                    "98.90%",
                    "[BOLD] 2.60%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.27%)",
                    "(\u00b10.82%)",
                    "(\u00b10.53%)",
                    "(\u00b10.18%)",
                    "[EMPTY]"
                ]
            ],
            "title": "TABLE V: The Mean and Std of Malytics with sparse tf-simashing for both Drebin and DexShare Datasets."
        },
        "insight": "Table VI shows that Malytics and Wuechner outperform other models when it comes to distinguishing original Windows PE clean files from PE malware."
    },
    {
        "id": "982",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] FNR",
                "[BOLD] Precision",
                "[BOLD] f1-score",
                "[BOLD] Accuracy",
                "[BOLD] AUC",
                "[BOLD] FPR"
            ],
            "rows": [
                [
                    "SVM",
                    "1.30%",
                    "99.13%",
                    "98.91%",
                    "98.92%",
                    "98.92%",
                    "0.86%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.30%)",
                    "(\u00b10.25%)",
                    "(\u00b10.26%)",
                    "(\u00b10.26%)",
                    "(\u00b10.25%)",
                    "[EMPTY]"
                ],
                [
                    "XGBoost",
                    "1.30%",
                    "98.43%",
                    "98.56%",
                    "98.56%",
                    "98.57%",
                    "1.57%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.27%)",
                    "(\u00b10.26%)",
                    "(\u00b10.19%)",
                    "(\u00b10.19%)",
                    "(\u00b10.19%)",
                    "[EMPTY]"
                ],
                [
                    "DNN",
                    "2.51%",
                    "96.77%",
                    "97.11%",
                    "97.09%",
                    "97.09%",
                    "3.31%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.59%)",
                    "(\u00b12.04%)",
                    "(\u00b10.78%)",
                    "(\u00b10.83%)",
                    "(\u00b10.83%)",
                    "[EMPTY]"
                ],
                [
                    "RF",
                    "2.18%",
                    "98.44%",
                    "98.13%",
                    "98.14%",
                    "98.14%",
                    "1.55%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.35%)",
                    "(\u00b10.32%)",
                    "(\u00b10.22%)",
                    "(\u00b10.21%)",
                    "(\u00b10.25%)",
                    "[EMPTY]"
                ],
                [
                    "K-NN",
                    "1.56%",
                    "98.50%",
                    "98.47%",
                    "98.47%",
                    "98.47%",
                    "1.50%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.38%)",
                    "(\u00b10.04%)",
                    "(\u00b10.20%)",
                    "(\u00b10.20%)",
                    "(\u00b10.20%)",
                    "[EMPTY]"
                ],
                [
                    "ELM",
                    "1.00%",
                    "95.82%",
                    "97.38%",
                    "97.34%",
                    "97.79%",
                    "4.30%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.17%)",
                    "(\u00b10.35%)",
                    "(\u00b10.18%)",
                    "(\u00b10.19%)",
                    "(\u00b10.18%)",
                    "[EMPTY]"
                ],
                [
                    "Malytics",
                    "[BOLD] 0.55%",
                    "[BOLD] 99.20%",
                    "99.32%",
                    "[BOLD] 99.32%",
                    "[BOLD] 99.96%",
                    "[BOLD] 0.80%"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.23%)",
                    "(\u00b10.27%)",
                    "(\u00b10.12%)",
                    "(\u00b10.11%)",
                    "(\u00b10.19%)",
                    "[EMPTY]"
                ],
                [
                    "",
                    "1.00%",
                    "[BOLD] 99.20%",
                    "[BOLD] 99.70%",
                    "\u2212",
                    "99.30%",
                    "\u2212"
                ],
                [
                    "[EMPTY]",
                    "(\u00b10.00%)",
                    "(\u00b10.00%)",
                    "(\u00b10.5%)",
                    "\u2212",
                    "(\u00b10.1%)",
                    "[EMPTY]"
                ],
                [
                    "",
                    "0.80%",
                    "\u2212",
                    "99.10%",
                    "99.05%",
                    "[EMPTY]",
                    "1.10%"
                ]
            ],
            "title": "TABLE VI: The Mean and Std of Malytics and the baselines for WinPE and Mal2016 of the PEShare Dataset."
        },
        "insight": "Table VII shows that the proposed solution outperforms other models over all evaluation indices. [CONTINUE] An interesting result of trying to distinguish Mal2016 malware set from WinAppPE benign set is in the comparison of DNN with Raff . Raff  used deep CNN for detection. The results show deep learning models also can be competitive for malware application domain."
    },
    {
        "id": "983",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] s01",
                "[BOLD] s02",
                "[BOLD] s03",
                "[BOLD] s04",
                "[BOLD] s05",
                "[BOLD] s06",
                "[BOLD] s07",
                "[BOLD] s08",
                "[BOLD] s09",
                "[BOLD] s10",
                "[BOLD] Avg"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "[ITALIC] \u2019FPc\u2019",
                    "[ITALIC] \u2019FPc\u2019",
                    "[ITALIC] \u2019FPc\u2019",
                    "[ITALIC] \u2019Mc\u2019",
                    "[ITALIC] \u2019Mc\u2019",
                    "[ITALIC] \u2019Mc\u2019",
                    "[ITALIC] \u2019Mc\u2019",
                    "[ITALIC] \u2019FPc\u2019",
                    "[ITALIC] \u2019FPc\u2019",
                    "[ITALIC] \u2019Mc\u2019",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] Entropy",
                    "[BOLD] 0.83",
                    "0.74",
                    "[BOLD] 0.83",
                    "0.83",
                    "[BOLD] 0.73",
                    "[BOLD] 0.76",
                    "0.75",
                    "[BOLD] 0.85",
                    "[BOLD] 0.85",
                    "[BOLD] 0.82",
                    "[BOLD] 0.80 \u00b1 0.05"
                ],
                [
                    "[BOLD] PSD",
                    "0.74",
                    "[BOLD] 0.78",
                    "0.80",
                    "[BOLD] 0.85",
                    "0.60",
                    "0.69",
                    "[BOLD] 0.78",
                    "0.84",
                    "0.82",
                    "0.76",
                    "0.76 \u00b1 0.07"
                ]
            ],
            "title": "Table I: Single sample classification accuracy for each subject in classifying between IC and INC states. Comparison between the proposed entropy-based method and the PSD-based method taken from\u00a0[17]."
        },
        "insight": "Table [CONTINUE] shows the performance for each subject of the proposed entropy-based BCI in detecting periods of movement execution (IC) from periods of resting (INC). [CONTINUE] From the results, our system achieved comparable performance and better classification accuracy for 7 out of 10 subjects, with an average improve of 3.3% \u00b1 5.5%, even if not statistically significant (t-test, p>0.05). [CONTINUE] The group of subjects belonging to the 'FPc' configuration shows on average slightly better performances compared to subjects wearing the 'Mc' configuration, with classification accuracy of 82.0% \u00b1 4.6% and 77.0% \u00b1 4.4% respectively for the entropy-based BCI, and 79.0% \u00b1 3.8% and 73.0% \u00b1 9.5% respectively for the PSD-based BCI, but with no statistically significant difference (t-test, p>0.05). [CONTINUE] the entropy provides different additional information to motion detection respect to the spectrogram analysis, that could improve the identification of self-paced motion intention, as shown in Table I."
    },
    {
        "id": "984",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] config",
                "[BOLD] Single Sample Accuracy [%]",
                "[BOLD] Single Trial Accuracy [%]",
                "[BOLD] Single Trial Missed [%]",
                "[BOLD] Delay [sec]"
            ],
            "rows": [
                [
                    "s01",
                    "[ITALIC] \u2019FPc\u2019",
                    "0.78",
                    "0.90",
                    "0.10",
                    "1.5 \u00b1 0.7"
                ],
                [
                    "s02",
                    "[ITALIC] \u2019FPc\u2019",
                    "0.66",
                    "0.75",
                    "0.20",
                    "1.6 \u00b1 0.8"
                ],
                [
                    "s03",
                    "[ITALIC] \u2019FPc\u2019",
                    "0.70",
                    "0.75",
                    "0.10",
                    "1.5 \u00b1 1.2"
                ],
                [
                    "s04",
                    "[ITALIC] \u2019Mc\u2019",
                    "0.82",
                    "0.95",
                    "0.05",
                    "1.5 \u00b1 0.9"
                ],
                [
                    "s05",
                    "[ITALIC] \u2019Mc\u2019",
                    "0.75",
                    "0.70",
                    "0.25",
                    "1.9 \u00b1 0.4"
                ],
                [
                    "s06",
                    "[ITALIC] \u2019Mc\u2019",
                    "0.73",
                    "0.80",
                    "0.20",
                    "1.2 \u00b1 1.0"
                ],
                [
                    "s07",
                    "[ITALIC] \u2019Mc\u2019",
                    "0.78",
                    "0.85",
                    "0.10",
                    "1.5 \u00b1 0.5"
                ],
                [
                    "s08",
                    "[ITALIC] \u2019FPc\u2019",
                    "0.77",
                    "0.65",
                    "0.20",
                    "2.6 \u00b1 0.6"
                ],
                [
                    "s09",
                    "[ITALIC] \u2019FPc\u2019",
                    "0.71",
                    "0.65",
                    "0.15",
                    "1.8 \u00b1 0.3"
                ],
                [
                    "s10",
                    "[ITALIC] \u2019Mc\u2019",
                    "0.69",
                    "0.60",
                    "0.20",
                    "1.7 \u00b1 0.7"
                ],
                [
                    "Avg",
                    "[EMPTY]",
                    "0.74 \u00b1 0.05",
                    "0.76 \u00b1 0.11",
                    "0.15 \u00b1 0.06",
                    "1.7 \u00b1 0.7"
                ]
            ],
            "title": "Table II: Classification performance for each subject in classifying between INC and IC-cue states."
        },
        "insight": "the proposed entropy-based BCI has been trained to classify between INC state and IC-cue state. [CONTINUE] The system provides an overall classification accuracy of 74% \u00b1 5% with maximum performance of 82% for s04. The single trial classification performance are measured as the accuracy and the delay in predicting motion intention after the appearance of the ICcue. [CONTINUE] The proposed BCI is able to predict the intention of performing a motion with a single trial accuracy of 76% \u00b1 11% in 1.7 \u00b1 0.7 seconds on average [CONTINUE] On average, 15% \u00b1 6% of the trials are missed, while less than 9% are classified before the cue, showing the robustness of the system to missclassifications. [CONTINUE] Results in Table II shows acceptable classification accuracy of movement preparation respect to resting phases [CONTINUE] the proposed entropy-based BCI is able to predict motion intention also when no muscle activity is generated."
    },
    {
        "id": "985",
        "table": {
            "header": [
                "[BOLD] Env.#",
                "[BOLD] Navigation  [BOLD] distance (m)",
                "[BOLD] Navigation  [BOLD] time (s)",
                "[BOLD] Crash",
                "[BOLD] Total  [BOLD] reward"
            ],
            "rows": [
                [
                    "[BOLD] Env.1",
                    "60.00",
                    "57",
                    "N",
                    "12.15"
                ],
                [
                    "[BOLD] Env.1",
                    "60.00",
                    "57",
                    "N",
                    "11.89"
                ],
                [
                    "[BOLD] Env.1",
                    "60.00",
                    "57",
                    "N",
                    "10.94"
                ],
                [
                    "[BOLD] Env.1",
                    "60.00",
                    "58",
                    "N",
                    "12.66"
                ],
                [
                    "[BOLD] Env.1",
                    "60.00",
                    "57",
                    "N",
                    "11.51"
                ],
                [
                    "[BOLD] Env.2",
                    "60.00",
                    "58",
                    "N",
                    "11.93"
                ],
                [
                    "[BOLD] Env.2",
                    "60.00",
                    "57",
                    "N",
                    "12.49"
                ],
                [
                    "[BOLD] Env.2",
                    "60.00",
                    "58",
                    "N",
                    "12.43"
                ],
                [
                    "[BOLD] Env.2",
                    "60.00",
                    "57",
                    "N",
                    "12.28"
                ],
                [
                    "[BOLD] Env.2",
                    "60.00",
                    "58",
                    "N",
                    "11.83"
                ],
                [
                    "[BOLD] Env.3",
                    "60.00",
                    "58",
                    "N",
                    "10.35"
                ],
                [
                    "[BOLD] Env.3",
                    "60.00",
                    "58",
                    "N",
                    "11.93"
                ],
                [
                    "[BOLD] Env.3",
                    "60.00",
                    "57",
                    "N",
                    "12.16"
                ],
                [
                    "[BOLD] Env.3",
                    "60.00",
                    "58",
                    "N",
                    "11.96"
                ],
                [
                    "[BOLD] Env.3",
                    "60.00",
                    "58",
                    "N",
                    "12.14"
                ],
                [
                    "[BOLD] Env.4",
                    "60.00",
                    "79",
                    "N",
                    "3.82"
                ],
                [
                    "[BOLD] Env.4",
                    "60.00",
                    "71",
                    "N",
                    "4.44"
                ],
                [
                    "[BOLD] Env.4",
                    "48.85",
                    "120",
                    "N",
                    "3.61"
                ],
                [
                    "[BOLD] Env.4",
                    "29.96",
                    "42",
                    "Y",
                    "1.32"
                ],
                [
                    "[BOLD] Env.4",
                    "28.15",
                    "33",
                    "Y",
                    "1.25"
                ],
                [
                    "[BOLD] Env.5",
                    "60.00",
                    "78",
                    "N",
                    "3.41"
                ],
                [
                    "[BOLD] Env.5",
                    "60.00",
                    "75",
                    "N",
                    "3.58"
                ],
                [
                    "[BOLD] Env.5",
                    "60.00",
                    "71",
                    "N",
                    "3.53"
                ],
                [
                    "[BOLD] Env.5",
                    "21.35",
                    "28",
                    "Y",
                    "0.51"
                ],
                [
                    "[BOLD] Env.5",
                    "48.57",
                    "120",
                    "N",
                    "3.39"
                ],
                [
                    "[BOLD] Env.6",
                    "60.00",
                    "61",
                    "N",
                    "4.85"
                ],
                [
                    "[BOLD] Env.6",
                    "44.44",
                    "120",
                    "N",
                    "4.35"
                ],
                [
                    "[BOLD] Env.6",
                    "60.00",
                    "70",
                    "N",
                    "4.46"
                ],
                [
                    "[BOLD] Env.6",
                    "60.00",
                    "61",
                    "N",
                    "4.74"
                ],
                [
                    "[BOLD] Env.6",
                    "60.00",
                    "61",
                    "N",
                    "4.81"
                ],
                [
                    "[BOLD] Env.7",
                    "50.42",
                    "120",
                    "N",
                    "3.26"
                ],
                [
                    "[BOLD] Env.7",
                    "41.51",
                    "58",
                    "Y",
                    "0.53"
                ],
                [
                    "[BOLD] Env.7",
                    "15.90",
                    "77",
                    "Y",
                    "0.14"
                ],
                [
                    "[BOLD] Env.7",
                    "60.00",
                    "95",
                    "N",
                    "3.27"
                ],
                [
                    "[BOLD] Env.7",
                    "47.33",
                    "120",
                    "N",
                    "3.04"
                ],
                [
                    "[BOLD] Env.8",
                    "60.00",
                    "59",
                    "N",
                    "7.69"
                ],
                [
                    "[BOLD] Env.8",
                    "60.00",
                    "59",
                    "N",
                    "7.61"
                ],
                [
                    "[BOLD] Env.8",
                    "45.47",
                    "44",
                    "Y",
                    "5.82"
                ],
                [
                    "[BOLD] Env.8",
                    "45.39",
                    "45",
                    "Y",
                    "4.52"
                ],
                [
                    "[BOLD] Env.8",
                    "60.00",
                    "58",
                    "N",
                    "9.01"
                ],
                [
                    "[BOLD] Env.9",
                    "60.00",
                    "57",
                    "N",
                    "9.24"
                ],
                [
                    "[BOLD] Env.9",
                    "60.00",
                    "58",
                    "N",
                    "9.31"
                ],
                [
                    "[BOLD] Env.9",
                    "60.00",
                    "57",
                    "N",
                    "8.78"
                ],
                [
                    "[BOLD] Env.9",
                    "60.00",
                    "57",
                    "N",
                    "8.24"
                ],
                [
                    "[BOLD] Env.9",
                    "60.00",
                    "58",
                    "N",
                    "8.60"
                ],
                [
                    "[BOLD] Env.10",
                    "60.00",
                    "61",
                    "N",
                    "5.35"
                ],
                [
                    "[BOLD] Env.10",
                    "60.00",
                    "58",
                    "N",
                    "5.29"
                ],
                [
                    "[BOLD] Env.10",
                    "60.00",
                    "61",
                    "N",
                    "4.95"
                ],
                [
                    "[BOLD] Env.10",
                    "60.00",
                    "60",
                    "N",
                    "5.35"
                ],
                [
                    "[BOLD] Env.10",
                    "60.00",
                    "60",
                    "N",
                    "5.88"
                ]
            ],
            "title": "TABLE I: Evaluative test results in AirSim."
        },
        "insight": "Table [CONTINUE] states the values for reward, navigation distance, navigation time, and crash rate. [CONTINUE] In the first three environments, the agent is able to learn a policy which is close to the optimal. Successful navigation from the start in these relatively simple environments. The navigation time performance is also desirable as being close to 60s in 1m/s is observed for each trial to the goal setting. [CONTINUE] The crash rate is 0% in these environments, even for the narrow corridor in Env. 3. [CONTINUE] Still, the agent successfully reaches the goal position in 67% of the trials. In terms of safety, the agent completes 73% of the trials without a crash in these dense environments. [CONTINUE] On the other hand, there are some trials such as the third one in Env. 4 in which the agent cannot reach the goal position but does not take decisions which might yield a crash either. [CONTINUE] In the last three environments, the agent yields better performance as compared to the former group. It successfully reaches the goal position in 87% of the trials. The other 13% stands for two trials in Env. 8. The  agent yields fairly safe flights with collision-free flight percentage of 86% over the trials in AirSim. [CONTINUE] It reaches the goal point successfully during 88% of these safe flights in AirSim"
    },
    {
        "id": "986",
        "table": {
            "header": [
                "[BOLD] Env.#",
                "[BOLD] Navigation  [BOLD] distance (m)",
                "[BOLD] Navigation  [BOLD] time (s)",
                "[BOLD] Crash",
                "[BOLD] Total  [BOLD] reward"
            ],
            "rows": [
                [
                    "[BOLD] Env.1",
                    "3.50",
                    "21",
                    "N",
                    "0.44"
                ],
                [
                    "[BOLD] Env.1",
                    "3.50",
                    "18",
                    "N",
                    "0.46"
                ],
                [
                    "[BOLD] Env.1",
                    "3.50",
                    "21",
                    "N",
                    "0.44"
                ],
                [
                    "[BOLD] Env.1",
                    "3.50",
                    "19",
                    "N",
                    "0.43"
                ],
                [
                    "[BOLD] Env.1",
                    "3.50",
                    "22",
                    "N",
                    "0.43"
                ],
                [
                    "[BOLD] Env.2",
                    "0.65",
                    "30",
                    "N",
                    "0.34"
                ],
                [
                    "[BOLD] Env.2",
                    "1.03",
                    "15",
                    "Y",
                    "-0.70"
                ],
                [
                    "[BOLD] Env.2",
                    "1.55",
                    "30",
                    "N",
                    "0.32"
                ],
                [
                    "[BOLD] Env.2",
                    "1.38",
                    "30",
                    "N",
                    "0.34"
                ],
                [
                    "[BOLD] Env.2",
                    "0.80",
                    "30",
                    "N",
                    "0.37"
                ]
            ],
            "title": "TABLE II: Evaluative test results in real flights."
        },
        "insight": "Table II summarizes the real flight test results. [CONTINUE] In the first environment without obstacles, the agent's performance is desirable. There is no single crash and the agent reaches to the goal point with 100% success out of five trials as can be seen in Table II. The navigation time is relatively high as compared to the results in AirSim though. [CONTINUE] Still, the agent yields adequate end-to-end reasoning and completes the task with 100% accuracy in the first environment. [CONTINUE] It cannot reach the goal position out of five trials. The furthest navigation point on the initial rough path is recorded as 1.55m in the third trial as can be seen in Table II. On the other hand, the agent yields safe flights for 80% of the trials in this  environment. [CONTINUE] This number is 0% for real flights with relatively simpler environments. [CONTINUE] while this percentage is 56% for real flights."
    },
    {
        "id": "987",
        "table": {
            "header": [
                "[BOLD] GNG Parameters",
                "[BOLD] Device Mode",
                "[BOLD] Domain State"
            ],
            "rows": [
                [
                    "Max No. of Nodes",
                    "10000",
                    "20000"
                ],
                [
                    "Max. Edge Age",
                    "100",
                    "50"
                ],
                [
                    "Decay Rate: After Split",
                    "0.5",
                    "0.3"
                ],
                [
                    "Decay Rate: Error",
                    "0.995",
                    "0.9"
                ]
            ],
            "title": "TABLE I: Hyper-parameters for Growing Neural Gas Clustering."
        },
        "insight": "It is because of GNGs incremental nature that it is not necessary to decide on the number of nodes to use a priori, unlike the k-means clustering algorithm, where several trials may be required to determine an appropriate number of centres. GNG is, therefore, suitable for problems where we know nothing or little about the input distribution, or the cases in which deciding on network size and decaying parameters are very difficult or impossible."
    },
    {
        "id": "988",
        "table": {
            "header": [
                "[BOLD] Method",
                "[BOLD] Energy Saved (%)",
                "[BOLD] Considers User Behaviour"
            ],
            "rows": [
                [
                    "Autonomous Appliance Scheduling for Household Energy Management [12]",
                    "10.92",
                    "No"
                ],
                [
                    "Intelligent Household LED Lighting System Considering Energy Efficiency and User Satisfaction [13]",
                    "21.9",
                    "Yes"
                ],
                [
                    "Making Smart Home Smarter: Optimizing Energy Consumption with Human in the Loop",
                    "30",
                    "Yes"
                ]
            ],
            "title": "TABLE II: Comparison between different methods."
        },
        "insight": "In Table II, we show how the adaptive configuration of smart devices is quantitatively better than the other existing techniques. The mentioned methods are the closest to our problem statement, however, not exact. [CONTINUE] As explained in Section II, Byun et. al. do not perceive user behavior to the extent as our method does, while Adika et. al. do not take user preference into account at all , . Their result shows around 22% energy saving as opposed to a much better 30% energy saving achieved by our method. Hence, our work justifies which states are compromised to obtain energy efficiency, and performs better at this task while considering user device setting preferences against other methods."
    },
    {
        "id": "989",
        "table": {
            "header": [
                "Rendering",
                "Hospital (Generator)",
                "Hospital Game"
            ],
            "rows": [
                [
                    "SetPass Calls",
                    "106",
                    "136"
                ],
                [
                    "Draw Calls",
                    "252",
                    "298"
                ],
                [
                    "Total Batches",
                    "217",
                    "243"
                ],
                [
                    "Triangles",
                    "463.9K",
                    "504.9K"
                ],
                [
                    "Vertices",
                    "319.5K",
                    "361.1K"
                ]
            ],
            "title": "TABLE V: Rendering Results of the Hospital Game Generated by the Scenario-Based Video Game Generator vs. Hospital Game"
        },
        "insight": "The rendering profile used the SetPass Calls, Draw Calls, Total Batches, Triangles and Vertices as parameters. SetPass parameter is defined as \"the number of rendering passes\" , a Draw Call as a \"call to the graphics API to draw objects\"  and Batch as a \"package with data that will be sent to the GPU\"  on the Unity's Renderer Profiler page ."
    },
    {
        "id": "990",
        "table": {
            "header": [
                "[EMPTY]",
                "Energy 1 basic",
                "Energy 1 daily IMD",
                "Delay",
                "Prog-Mem footprint"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "session ( [ITALIC] \u03bcJ)",
                    "cycle\u2217\u00a0(J)",
                    "(ms)",
                    "(kB)"
                ],
                [
                    "Without security",
                    "16.61",
                    "16.60",
                    "2.17",
                    "\u223c1.00"
                ],
                [
                    "IMDfence (H/W)",
                    "108.31",
                    "17.69",
                    "15.73",
                    "6.86"
                ],
                [
                    "IMDfence (S/W)",
                    "213.50",
                    "19.89",
                    "57.22",
                    "7.86"
                ],
                [
                    "\u2217 Which includes a daily two-minute comm. session (see Section\u00a0 V-B )",
                    "\u2217 Which includes a daily two-minute comm. session (see Section\u00a0 V-B )",
                    "\u2217 Which includes a daily two-minute comm. session (see Section\u00a0 V-B )",
                    "\u2217 Which includes a daily two-minute comm. session (see Section\u00a0 V-B )",
                    "\u2217 Which includes a daily two-minute comm. session (see Section\u00a0 V-B )"
                ]
            ],
            "title": "TABLE IV: Summary of costs for running the IMDfence protocol on an IMD"
        },
        "insight": "Table IV summarizes the impact of IMDfence on an IMD in terms of energy, performance and program-memory footprint. For the hardware implementation of IMDfence it can be observed that, although the energy requirements increase by more than 6 times for a basic session, the total daily IMD consumption (that includes a two-minute communication session and electrical-stimulation costs) increases from 16.60 J to just 17.37 J, which amounts to a mere 4.64% increase, as previously shown in Fig. 12. The reason for this small increase is that the basic medical functionality, e.g., the continuous electrical stimulation of a pacemaker, dominates the security provisions since the reader accesses are far less frequent. In the case of software (AES-128) implementation of IMDfence, the total daily IMD consumption increases by 13.01% (as shown in Fig. 12). Moreover, there is a minimal increase in the computational delay and required program-memory size. In the context of current MCU technology, 6\u20137 kB of additional footprint is negligible. Hence, we conclude that there is no noticeable change in the IMD costs when IMDfence is employed."
    },
    {
        "id": "991",
        "table": {
            "header": [
                "[BOLD] Section",
                "[EMPTY]",
                "[BOLD] Description (NewsGuard points)",
                "[BOLD] Coloring"
            ],
            "rows": [
                [
                    "NewsGuard",
                    "1.",
                    "Does not repeatedly publish false content (22.0)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "2.",
                    "Gathers and presents information responsibly (18.0)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "3.",
                    "Regularly corrects or clarifies errors (12.5)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "4.",
                    "Handles the difference between news and opinion responsibly (12.5)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "5.",
                    "Avoids deceptive headlines (10.0)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "6.",
                    "Website discloses ownership and financing (7.5)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "7.",
                    "Clearly labels advertising (7.5)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "8.",
                    "Reveals who\u2019s in charge, including any possible conflicts of interest (5.0)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "9.",
                    "Provides information about content creators (5.0)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "10.",
                    "Aggregated score computed from 1-9",
                    "-"
                ],
                [
                    "[EMPTY]",
                    "11.",
                    "Column 10 thresholded at 60 points",
                    "[EMPTY]"
                ],
                [
                    "Pew Research Center",
                    "12.",
                    "Trust from consistently-liberals",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "13.",
                    "Trust from mostly-liberals",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "14.",
                    "Trust from mixed groups",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "15.",
                    "Trust from mostly-conservatives",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "16.",
                    "Trust from consistently-conservatives",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "17.",
                    "Aggregated trust from 12-16",
                    "[EMPTY]"
                ],
                [
                    "Wikipedia",
                    "18.",
                    "Existence of source on Wikipedia\u2019s list of fake news sources",
                    "[EMPTY]"
                ],
                [
                    "Open Sources",
                    "19.",
                    "Marked reliable",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "20.",
                    "Marked blog",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "21.",
                    "Marked clickbait",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "22.",
                    "Marked rumor",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "23.",
                    "Marked fake",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "24.",
                    "Marked unreliable",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "25.",
                    "Marked biased",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "26.",
                    "Marked conspiracy",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "27.",
                    "Marked hate speech",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "28.",
                    "Marked junk science",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "29.",
                    "Marked political",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "30.",
                    "Marked satire",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "31.",
                    "Marked state news",
                    "[EMPTY]"
                ],
                [
                    "Media Bias / Fact Check",
                    "32.",
                    "Factual reporting from 5 (good) down to 1 (bad)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "33.",
                    "Special label; conspiracy, pseudoscience or questionable source (purple), and satire (orange)",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "34.",
                    "Political leaning / bias from left to right.",
                    "[EMPTY]"
                ],
                [
                    "Allsides",
                    "35.",
                    "Political leaning / bias",
                    "[EMPTY]"
                ],
                [
                    "BuzzFeed",
                    "36.",
                    "Political leaning / bias, but only left and right",
                    "[EMPTY]"
                ],
                [
                    "PolitiFact",
                    "37.",
                    "Has brought story labelled as \u201dpants on Fire!\u201d",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "38.",
                    "Has brought story labelled as false",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "39.",
                    "Has brought story labelled as mostly false",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "40.",
                    "Has brought story labelled as half-true",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "41.",
                    "Has brought story labelled as mostly true",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "42.",
                    "Has brought story labelled as true",
                    "[EMPTY]"
                ],
                [
                    "Alexa Ranking",
                    "[EMPTY]",
                    "The Alexa ranking of the source.",
                    "Numerical"
                ],
                [
                    "# Articles",
                    "[EMPTY]",
                    "The number of articles collected from the source.",
                    "Numerical"
                ],
                [
                    "First Observed",
                    "[EMPTY]",
                    "The date of first articles collected from the source.",
                    "dd-mm-yyyy"
                ]
            ],
            "title": "Table 1: Details of the information for sources found in tables 2, 3 and 4. We generally use green-to-purple for good-to-poor reliability/credibility, with grey as inconclusive. For bias we use blue-to-red for left-to-right bias, with grey as unbiased. Orange is used for special cases. In NewsGuard data it represents missing information, in Open Sources it marks auxiliary labels and for Media Bias / Fact Check it marks satire."
        },
        "insight": "These assessment sites are: 1. NewsGuard 2. Pew Research Center 3. Wikipedia 4. OpenSources 5. Media Bias/Fact Check (MBFC) 6. AllSides 7. BuzzFeed News 8. Politifact [CONTINUE] Table 1 provide a detailed described of each assessment [CONTINUE] Their analysis produces 9 granular, binary labels for each site, with assigned point scores that sums to 100. Based on the sum of points the sites get an overall label for credibility - green for good score, red for bad score. Three additional overall labels exist for satire, user-produced content and sites with unfinished analysis. Table 1 describes the granular labels. [CONTINUE] conservatives. This analysis used 5 groups of people, ranging from liberals to conservatives, and each group provided a rating of how much they trust each source. The ratings are aggregated to show whether readers with different political leanings predominantly trust or distrust a specific source. We provide this trust label for each source and political leaning, as a label for congruency between bias a readership (rather than a factchecking label). [CONTINUE] We provide a fake-news tag for sources on the list. [CONTINUE] The site provides a list of sources with 1-3 tags per source (See Table 1). [CONTINUE] We were able to find a factual reporting label, which is derived from the previously mentioned scores. Many sources also had descriptive labels, some of which were related to reliability and some of which were related to bias. All these labels are described in Table 1. [CONTINUE] We include their bias label and feedback numbers (votes agreeing and votes disagreeing) for each source. The feedback number are not shown in the paper, but can be found in the dataset. [CONTINUE] They publish an associated dataset with news sources and their political leaning (left and right), which we include. [CONTINUE] We have counted the types of statements coming from each source, which could be used to indicate their truthfulness. [CONTINUE] We include the position of the sources in this rating in the dataset based on our access to Alexa on 13th of January 2019."
    },
    {
        "id": "992",
        "table": {
            "header": [
                "[EMPTY]",
                "Go",
                "Stop"
            ],
            "rows": [
                [
                    "Yield",
                    "18,10",
                    "15,4"
                ],
                [
                    "Walk",
                    "-500,-400",
                    "0,5"
                ],
                [
                    "Cycle",
                    "-600,-200",
                    "5,5"
                ]
            ],
            "title": "Table I: Normal form of the game in Section III with modified payoffs."
        },
        "insight": "For cyclists, the worst cases are when they cycle (\u2212600) or walk (\u2212500), respectively, and in both cases the AV drives; the values are lower than the previous ones because in this new setup the cyclist is also partially responsible for the accent. Both walk/stop and cycle/stop have been penalized with a fine of 15, which is assumed to be a certain punishment and it is common knowledge: then yield/stop (15) and yield/go (18) are the right solutions according to the new street regulations. For the AV, the worst outcomes are to hit the pedestrian (\u2212400) or the cyclist (\u2212200). If they both yield (4), they need to renegotiate the solution. Then walk/stop and cycle/stop (5) are the next worst outcomes because the AV had the right to pass. The best outcome is achieved when they play yield/go (10). [CONTINUE] The resulting game in normal form is represented by Table I. The unique NE of the game is yield/go since it is immediate to [CONTINUE] see that no player has anything to gain by changing only their own strategy, therefore in a fully autonomous environment the cyclists will yield and the AVs will go, thus validating the effectiveness of the changes."
    },
    {
        "id": "993",
        "table": {
            "header": [
                "[EMPTY]",
                "Canvas",
                "Canvas Font",
                "WebRTC",
                "Audio"
            ],
            "rows": [
                [
                    "(a)",
                    "8,503",
                    "1,387",
                    "1,313",
                    "534"
                ],
                [
                    "(b)",
                    "8,519",
                    "1,387",
                    "1,313",
                    "170"
                ],
                [
                    "Similarity",
                    "87.00%",
                    "100%",
                    "100%",
                    "31.34%"
                ]
            ],
            "title": "Table 3: Number of scripts found using heuristics from (a) Englehardt & Narayanan [12] and (b) Das et al. [30]"
        },
        "insight": "However, small implementation changes result in differences in the flagged scripts. We summarize this in Table 3 using the Jaccard similarity\u2014the size of the intersection of two sets relative to the size of the union of those sets."
    },
    {
        "id": "994",
        "table": {
            "header": [
                "[EMPTY]",
                "Score",
                "6,000",
                "10,000",
                "12,000",
                "14,000"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "n scripts over score",
                    "28,978",
                    "18,961",
                    "7,915",
                    "3,434"
                ],
                [
                    "[EMPTY]",
                    "% of dataset",
                    "7.1%",
                    "4.6%",
                    "1.9%",
                    "0.8%"
                ],
                [
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "n in heuristic list",
                    "5,962",
                    "5,875",
                    "5,722",
                    "1,476"
                ],
                [
                    "[EMPTY]",
                    "% of heuristic list",
                    "98.9%",
                    "97.5%",
                    "94.9%",
                    "24.5%"
                ],
                [
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "akam",
                    "108",
                    "108",
                    "108",
                    "0"
                ],
                [
                    "[EMPTY]",
                    "hs",
                    "1,857",
                    "1,857",
                    "0",
                    "0"
                ],
                [
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "charting",
                    "151",
                    "104",
                    "87",
                    "87"
                ],
                [
                    "[EMPTY]",
                    "modernizr",
                    "2,588",
                    "453",
                    "453",
                    "453"
                ],
                [
                    "[EMPTY]",
                    "sadbundle",
                    "335",
                    "281",
                    "258",
                    "258"
                ],
                [
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "17,977",
                    "10,283",
                    "[BOLD] 1,287",
                    "1,160"
                ]
            ],
            "title": "Table 4: Validation summary of scripts grouped at selected score thresholds. Uncharacterized scripts from the 12,000 score group were manually reviewed."
        },
        "insight": "The results summarized in Table 4. [CONTINUE] Of the 7,915 scripts given a score of 12,000 or higher by the model, 5,722 (94.9%) are fingerprinting as identified by the heuristic list. This increases to 5,962 (98.9%) at a score of 6,000."
    },
    {
        "id": "995",
        "table": {
            "header": [
                "[EMPTY]",
                "Score",
                "250",
                "350",
                "450"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "n scripts over score",
                    "28,927",
                    "7,305",
                    "4,491"
                ],
                [
                    "[EMPTY]",
                    "% of dataset",
                    "7.1%",
                    "1.8%",
                    "1.1%"
                ],
                [
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize",
                    "[BOLD] Pre-characterize"
                ],
                [
                    "[EMPTY]",
                    "n in heuristic list",
                    "5,929",
                    "5,723",
                    "4,255"
                ],
                [
                    "[EMPTY]",
                    "% of heuristic list",
                    "98.4%",
                    "94.9%",
                    "70.6%"
                ],
                [
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting",
                    "fingerprinting"
                ],
                [
                    "[EMPTY]",
                    "akam",
                    "108",
                    "108",
                    "108"
                ],
                [
                    "[EMPTY]",
                    "hs",
                    "1,857",
                    "0",
                    "0"
                ],
                [
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas",
                    "benign use of canvas"
                ],
                [
                    "[EMPTY]",
                    "charting",
                    "151",
                    "89",
                    "0"
                ],
                [
                    "[EMPTY]",
                    "modernizr",
                    "2,588",
                    "0",
                    "0"
                ],
                [
                    "[EMPTY]",
                    "sadbundle",
                    "335",
                    "270",
                    "0"
                ],
                [
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized",
                    "[BOLD] Remaining uncharacterized"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "17,959",
                    "1,115",
                    "128"
                ]
            ],
            "title": "Table 5: Validation Table - Summary of scripts flagged by keyword \u201cfingerprint\u201d grouped at selected score thresholds."
        },
        "insight": "Table 5 summarizes the performance as we did for Experiment 1. At the 350 score threshold we get very similar performance to that seen in Experiment 1 at a score of 12,000: we get essentially the same recall rate compared to the heuristic-based reference list and we picked up the same set of device-class fingerprinting scripts."
    },
    {
        "id": "996",
        "table": {
            "header": [
                "Experiment",
                "Precision",
                "Recall",
                "Accuracy",
                "F-Score"
            ],
            "rows": [
                [
                    "StreamSpot (baseline)",
                    "0.74",
                    "[EMPTY]",
                    "0.66",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] R=1",
                    "0.51",
                    "1.0",
                    "0.60",
                    "0.68"
                ],
                [
                    "[ITALIC] R=3",
                    "0.98",
                    "0.93",
                    "0.96",
                    "0.94"
                ]
            ],
            "title": "TABLE II: Comparison to StreamSpot on the StreamSpot dataset. We estimate StreamSpot\u2019s average accuracy and precision from the figure included in the paper\u00a0[manzoor2016fast], which does not report exact values. They did not report recall or F-score."
        },
        "insight": "We compare UNICORN to StreamSpot, using StreamSpot's own dataset. [CONTINUE] examine with different neighborhood sizes, R = 1 (equivalent to StreamSpot) and R = 3. [CONTINUE] UNICORN's ability to trivially consider larger neighborhoods (R = 3) produces significant precision/accuracy improvement."
    },
    {
        "id": "997",
        "table": {
            "header": [
                "Experiment",
                "# of Test Graphs",
                "# of FPs ( [ITALIC] R = 1)",
                "# of FPs ( [ITALIC] R = 3)"
            ],
            "rows": [
                [
                    "YouTube",
                    "25",
                    "14",
                    "0"
                ],
                [
                    "Gmail",
                    "25",
                    "19",
                    "0"
                ],
                [
                    "Download",
                    "25",
                    "25",
                    "2"
                ],
                [
                    "VGame",
                    "25",
                    "20",
                    "0"
                ],
                [
                    "CNN",
                    "25",
                    "18",
                    "0"
                ]
            ],
            "title": "TABLE III: Decomposition of Unicorn\u2019s false positive results of the StreamSpot dataset."
        },
        "insight": "Table III further show that analyzing larger neighborhoods greatly reduces the false positive rate. [CONTINUE] o UNICORN raises false positive alarms only on the Download dataset,"
    },
    {
        "id": "998",
        "table": {
            "header": [
                "Experiment",
                "Precision",
                "Recall",
                "Accuracy",
                "F-Score"
            ],
            "rows": [
                [
                    "SC-1",
                    "0.85",
                    "0.96",
                    "0.90",
                    "0.90"
                ],
                [
                    "SC-2",
                    "0.75",
                    "0.80",
                    "0.77",
                    "0.78"
                ]
            ],
            "title": "TABLE VIII: Experimental results of the supply-chain APT attack scenarios."
        },
        "insight": "Table VIII shows the experimental results. [CONTINUE] We see in Table VIII that UNICORN is able to detect attacks with high accuracy with only a small number of false alarms (as reflected in precision and recall)."
    },
    {
        "id": "999",
        "table": {
            "header": [
                "Two story building with nonlinear links Partitioned Domain Span: [0.10,1.0 [ITALIC] e04]-[1.0,5.0 [ITALIC] e04]",
                "Two story building with nonlinear links Partitioned Domain Span: [0.10,1.0 [ITALIC] e04]-[1.0,5.0 [ITALIC] e04]",
                "Two story building with nonlinear links Partitioned Domain Span: [0.10,1.0 [ITALIC] e04]-[1.0,5.0 [ITALIC] e04]",
                "Two story building with nonlinear links Partitioned Domain Span: [0.10,1.0 [ITALIC] e04]-[1.0,5.0 [ITALIC] e04]",
                "Two story building with nonlinear links Partitioned Domain Span: [0.10,1.0 [ITALIC] e04]-[1.0,5.0 [ITALIC] e04]"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Mean RE [BOLD] u",
                    "Max RE [BOLD] u",
                    "Mean RE [BOLD] rf",
                    "Max RE [BOLD] rf"
                ],
                [
                    "Partition A - Speed-up factor: 1.28",
                    "Partition A - Speed-up factor: 1.28",
                    "Partition A - Speed-up factor: 1.28",
                    "Partition A - Speed-up factor: 1.28",
                    "Partition A - Speed-up factor: 1.28"
                ],
                [
                    "Global Basis",
                    "3.18%",
                    "5.22%",
                    "4.84%",
                    "7.51%"
                ],
                [
                    "Local Basis",
                    "0.43%",
                    "0.68%",
                    "3.06%",
                    "3.90%"
                ],
                [
                    "Entries Interp.",
                    "0.40%",
                    "0.67%",
                    "3.25%",
                    "3.73%"
                ],
                [
                    "Coefficients Interp.",
                    "0.39%",
                    "0.67%",
                    "3.28%",
                    "3.69%"
                ],
                [
                    "Partition B - Speed-up factor: 1.31",
                    "Partition B - Speed-up factor: 1.31",
                    "Partition B - Speed-up factor: 1.31",
                    "Partition B - Speed-up factor: 1.31",
                    "Partition B - Speed-up factor: 1.31"
                ],
                [
                    "Global Basis",
                    "1.96%",
                    "5.91%",
                    "3.94%",
                    "8.28%"
                ],
                [
                    "Local Basis",
                    "0.15%",
                    "0.33%",
                    "1.83%",
                    "2.86%"
                ],
                [
                    "Entries Interp.",
                    "0.12%",
                    "0.30%",
                    "1.29%",
                    "2.86%"
                ],
                [
                    "Coefficients Interp.",
                    "0.12%",
                    "0.30%",
                    "1.30%",
                    "3.06%"
                ]
            ],
            "title": "Table 3: The performance of the pROM for the domain partition techniques presented in Figure 1(b). The RErf error of Equation (4.1) with respect to the restoring forces rf is evaluated. The average and max error across the domain is presented along with the speed up factor. The pROM variants compared are described in Table 2."
        },
        "insight": "The performance of the pROM variants presented in Table 2 is summarized in Table 3. The average and maximum error measure for validation samples spanning across the parametric domain are presented. The accuracy is evaluated with respect to the response time history and the time history of the nonlinear terms, namely the restoring forces. [CONTINUE] it seems that the use of a single projection basis on the Global Basis pROM of Table 3 is not capable of approximating the underlying phenomena accurately enough. [CONTINUE] addition, the proposed Coefficients Interpolation approach seems able to achieve a similar accuracy to the established element-wise Entries Interpolation and thus reproduce the high fidelity dynamic behavior and response. [CONTINUE] For this reason, the efficiency achieved is negligible due to the full evaluation of the nonlinear terms."
    },
    {
        "id": "1000",
        "table": {
            "header": [
                "[BOLD] Test-name",
                "[BOLD] P-value",
                "[BOLD] Result"
            ],
            "rows": [
                [
                    "Block Frequency (m = 100)",
                    "0.046169",
                    "Succeed"
                ],
                [
                    "Frequency",
                    "0.681211",
                    "Succeed"
                ],
                [
                    "Cusum (Forward)",
                    "0.878529",
                    "Succeed"
                ],
                [
                    "Cusum (Reverse)",
                    "0.674391",
                    "Succeed"
                ],
                [
                    "Long Runs of Ones",
                    "0.128851",
                    "Succeed"
                ],
                [
                    "Spectral DFT",
                    "0.149590",
                    "Succeed"
                ],
                [
                    "Rank",
                    "0.638151",
                    "Succeed"
                ],
                [
                    "Lempel Ziv Complexity",
                    "1.000000",
                    "Succeed"
                ],
                [
                    "Overlapping Templates (m = 9)",
                    "0.120402",
                    "Succeed"
                ],
                [
                    "NonOverlapping Templates (m = 9)",
                    "0.197506",
                    "Succeed"
                ],
                [
                    "Approximate Entropy (m = 10)",
                    "0.681211",
                    "Succeed"
                ],
                [
                    "Universal (L = 7, Q = 1280)",
                    "0.051599",
                    "Succeed"
                ],
                [
                    "Random Excursions (x = +1)",
                    "0.297235",
                    "Succeed"
                ],
                [
                    "Serial (m = 16)",
                    "0.343750",
                    "Succeed"
                ],
                [
                    "Random Excursions Variant (x = +1)",
                    "0.050388",
                    "Succeed"
                ],
                [
                    "Runs",
                    "0.499889",
                    "Succeed"
                ],
                [
                    "Linear Complexity (M = 500)",
                    "0.703017",
                    "Succeed"
                ]
            ],
            "title": "TABLE II: Test results for 1048576 bit strings"
        },
        "insight": "The NIST Randomness Tests The NIST  test suite is a statistical package that consisting of up to 15 tests. [CONTINUE] The 15 tests concentrate on a variety of different non-randomness types that could exist in a bit strings. The proposed mechanism produces a very random bit strings as reflected by the high p-values as shown in Table II. [CONTINUE] pretty good randomness properties and also high periodicity. [CONTINUE] the obtained bit strings have high periodicity and good randomness properties."
    },
    {
        "id": "1001",
        "table": {
            "header": [
                "[EMPTY]",
                "Perceived Fairness Model 1",
                "Perceived Fairness Model 2",
                "Perceived Fairness Model 3"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Coef.(S.E.)",
                    "Coef.(S.E.)",
                    "Coef.(S.E.)"
                ],
                [
                    "Unfavorable Outcome",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "vs. Favorable Outcome",
                    "\u22121.040\u2217\u2217\u2217 (0.112)",
                    "[EMPTY]",
                    "\u22121.034\u2217\u2217\u2217 (0.111)"
                ],
                [
                    "Biased Treatment",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "vs. Unbiased Treatment",
                    "[EMPTY]",
                    "\u22120.410\u2217\u2217\u2217 (0.119)",
                    "\u22120.396\u2217\u2217\u2217 (0.111)"
                ],
                [
                    "Self-expected Pass",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "vs. Self-expected Fail",
                    "0.682\u2217\u2217\u2217 (0.127)",
                    "0.637\u2217\u2217\u2217 (0.135)",
                    "0.655\u2217\u2217\u2217 (0.126)"
                ],
                [
                    "Constant",
                    "4.063\u2217\u2217\u2217 (0.122)",
                    "3.785\u2217\u2217\u2217 (0.133)",
                    "4.278\u2217\u2217\u2217 (0.135)"
                ],
                [
                    "R2",
                    "0.164",
                    "0.059",
                    "0.182"
                ],
                [
                    "Adjusted R2",
                    "0.161",
                    "0.055",
                    "0.177"
                ],
                [
                    "[ITALIC] Note:",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001"
                ]
            ],
            "title": "Table 3: Regression models predicting perceived fairness from (un)favorable outcome to an individual (Model\u00a01), (un)biased treatment across groups (Model\u00a02), and both factors together (Model\u00a03)."
        },
        "insight": "The models in Table 3 illustrate the impacts of (un)favorable outcome to individual and (un)biased treatment to group on the perception of fairness. Model 1 tests the effect of (un)favorable outcome to an individual, model 2 tests the effect of (un)biased treatment to a group, and model 3 includes both variables. All models include a control variable of self-expectation to control for the participants' own expectations of whether they would pass or fail. Model 1 predicts that participants who were told that they failed the Master qualification test will rate the fairness of the algorithm 1.040 (95% CI: [0.819, 1.260]) point lower on a [CONTINUE] Model 2 predicts that participants will rate the biased algorithm 0.410 (95% CI: [0.175, 0.644]) points lower on a 7-point scale (p<0.01), compared with the unbiased algorithm. Therefore, both H1a and H1b are supported. The effect of an unfavorable algorithm outcome is stronger than the effect of a biased algorithm. We compared the adjusted R2 for models 1 and 2, finding that algorithm outcome explains more variance of the perceived fairness than whether the algorithm is biased or not (model 1 adjusted R2=0.161, model 2 adjusted R2=0.055). Model 3 compares the effect more explicitly. The effect size of (un)favorable outcome on perceived fairness (Coef.=-1.034, p<0.001, 95% CI: [-1.253 -0.816]) is twice as much as the effect size of (un)biased algorithm (Coef.=-0.396, p<0.001, 95% CI: [-0.615 -0.177]), which supports H1c."
    },
    {
        "id": "1002",
        "table": {
            "header": [
                "[EMPTY]",
                "Perceived Fairness Model 1",
                "Perceived Fairness Model 2"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Coef. (S.E.)",
                    "Coef. (S.E.)"
                ],
                [
                    "Unfavorable Outcome vs. Favorable Outcome",
                    "\u22121.041\u2217\u2217\u2217 (0.113)",
                    "\u22120.887\u2217\u2217\u2217 (0.258)"
                ],
                [
                    "Biased Treatment vs. Unbiased Treatment",
                    "\u22120.395\u2217\u2217\u2217 (0.113)",
                    "\u22121.068\u2217\u2217\u2217 (0.257)"
                ],
                [
                    "CS Team vs. Outsourced",
                    "0.131 (0.138)",
                    "\u22120.614\u2217 (0.240)"
                ],
                [
                    "CS and HR vs. Outsourced",
                    "0.201 (0.137)",
                    "\u22120.352 (0.224)"
                ],
                [
                    "Machine Learning vs. Rules",
                    "0.138 (0.112)",
                    "0.086 (0.189)"
                ],
                [
                    "High Transparency vs. Low Transparency",
                    "\u22120.154 (0.114)",
                    "0.056 (0.191)"
                ],
                [
                    "Mixed Decision vs. Algorithm-only",
                    "0.090 (0.113)",
                    "0.220 (0.189)"
                ],
                [
                    "Unfavorable Outcome \u00d7 CS team",
                    "[EMPTY]",
                    "0.426 (0.272)"
                ],
                [
                    "Unfavorable Outcome \u00d7 CS and HR",
                    "[EMPTY]",
                    "0.015 (0.273)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Machine Learning",
                    "[EMPTY]",
                    "\u22120.238 (0.220)"
                ],
                [
                    "Unfavorable Outcome \u00d7 High Transparency",
                    "[EMPTY]",
                    "0.106 (0.224)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Mixed Decision",
                    "[EMPTY]",
                    "\u22120.420 (0.221)"
                ],
                [
                    "Biased Treatment \u00d7 CS team",
                    "[EMPTY]",
                    "0.971\u2217\u2217\u2217 (0.272)"
                ],
                [
                    "Biased Treatment \u00d7 CS and HR",
                    "[EMPTY]",
                    "1.070\u2217\u2217\u2217 (0.271)"
                ],
                [
                    "Biased Treatment \u00d7 Machine Learning",
                    "[EMPTY]",
                    "0.343 (0.220)"
                ],
                [
                    "Biased Treatment \u00d7 High Transparency",
                    "[EMPTY]",
                    "\u22120.486\u2217 (0.224)"
                ],
                [
                    "Biased Treatment \u00d7 Mixed Decision",
                    "[EMPTY]",
                    "0.174 (0.221)"
                ],
                [
                    "Self-expected Pass vs. Self-expected Fail",
                    "0.659\u2217\u2217\u2217 (0.127)",
                    "0.616\u2217\u2217\u2217 (0.125)"
                ],
                [
                    "Constant",
                    "4.131\u2217\u2217\u2217 (0.178)",
                    "4.438\u2217\u2217\u2217 (0.235)"
                ],
                [
                    "R2",
                    "0.190",
                    "0.235"
                ],
                [
                    "Adjusted R2",
                    "0.178",
                    "0.210"
                ],
                [
                    "[ITALIC] Note:",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001"
                ]
            ],
            "title": "Table 4: Regression models predicting perceived fairness from algorithm outcomes and development procedures. Model\u00a01 shows the main effects, and Model\u00a02 includes interaction terms."
        },
        "insight": "Hypothesis 2 relates to how different algorithm creation and deployment procedures affect perceived fairness. Model 1 in Table 4 examines the main effects of different algorithm development procedures on perceived fairness, which allows us to test hypothesis 2a and 2b. Model 2 in Table 4 examines the interaction effects between development procedures and algorithm outcomes. Model 1 in Table 4 shows that the algorithm development manipulations had no significant main effects on perceived fairness. Therefore, Hypothesis 2a and 2b are not supported. The five variables reflecting the algorithm's development procedures collectively explained less than 1% of the variance in perceived fairness. Although there is no significant main effect, Model 2 in Table 4 finds several interaction effects between development procedures and algorithm outcomes. The effects of algorithm bias are moderated by the procedural factors of design and transparency. The negative effect of a biased algorithm is strongest when the algorithm is built by an outsourced team, as compared with when it is built by computer science experts within the organization (Coef.=0.971, p<0.001, 95% CI: [0.437, 1.504]), or by a mix of computer science experts and other MTurk staff within the organization (Coef.=1.070, p<0.001, 95% CI: [0.537, 1.602])."
    },
    {
        "id": "1003",
        "table": {
            "header": [
                "[EMPTY]",
                "Perceived Fairness Model 1",
                "Perceived Fairness Model 2"
            ],
            "rows": [
                [
                    "[EMPTY]",
                    "Coef. (S.E.)",
                    "Coef. (S.E.)"
                ],
                [
                    "Unfavorable Outcome vs. Favorable Outcome",
                    "\u22121.057\u2217\u2217\u2217 (0.115)",
                    "\u22121.237 (0.651)"
                ],
                [
                    "Biased Treatment vs. Unbiased Treatment",
                    "\u22120.426\u2217\u2217\u2217 (0.114)",
                    "\u22121.070 (0.671)"
                ],
                [
                    "Low Literacy vs. High Literacy",
                    "\u22120.285\u2217 (0.117)",
                    "\u22120.384 (0.204)"
                ],
                [
                    "Above 45 vs. Between 25-45",
                    "\u22120.142 (0.150)",
                    "0.060 (0.271)"
                ],
                [
                    "Below 25 vs. Between 25-45",
                    "0.237 (0.208)",
                    "0.069 (0.347)"
                ],
                [
                    "Male vs. Female",
                    "0.209 (0.122)",
                    "\u22120.019 (0.203)"
                ],
                [
                    "Bachelor\u2019s Degree vs. Above Bachelor\u2019s Degree",
                    "\u22120.198 (0.188)",
                    "0.070 (0.376)"
                ],
                [
                    "Below Bachelor\u2019s Degree vs. Above Bachelor\u2019s Degree",
                    "\u22120.272 (0.189)",
                    "0.276 (0.371)"
                ],
                [
                    "Asian vs. Non-asian",
                    "0.106 (0.282)",
                    "0.031 (0.562)"
                ],
                [
                    "White vs. Non-white",
                    "\u22120.119 (0.220)",
                    "\u22120.534 (0.501)"
                ],
                [
                    "Black vs. Non-black",
                    "0.340 (0.244)",
                    "\u22120.103 (0.544)"
                ],
                [
                    "Native American vs. Non-native American",
                    "0.218 (0.390)",
                    "0.690 (0.926)"
                ],
                [
                    "Hispanic vs. Non-hispanic",
                    "\u22120.136 (0.240)",
                    "\u22120.770 (0.524)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Low Literacy",
                    "[EMPTY]",
                    "0.268 (0.240)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Above 45",
                    "[EMPTY]",
                    "\u22120.334 (0.307)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Below 25",
                    "[EMPTY]",
                    "0.215 (0.423)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Male",
                    "[EMPTY]",
                    "0.558\u2217 (0.242)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Bachelor\u2019s Degree",
                    "[EMPTY]",
                    "\u22120.723 (0.389)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Below Bachelor\u2019s Degree",
                    "[EMPTY]",
                    "\u22121.144\u2217\u2217 (0.393)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Asian",
                    "[EMPTY]",
                    "0.329 (0.624)"
                ],
                [
                    "Unfavorable Outcome \u00d7 White",
                    "[EMPTY]",
                    "0.601 (0.533)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Black",
                    "[EMPTY]",
                    "0.918 (0.584)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Native American",
                    "[EMPTY]",
                    "\u22120.600 (1.029)"
                ],
                [
                    "Unfavorable Outcome \u00d7 Hispanic",
                    "[EMPTY]",
                    "0.117 (0.568)"
                ],
                [
                    "Biased Treatment \u00d7 Low Literacy",
                    "[EMPTY]",
                    "\u22120.034 (0.239)"
                ],
                [
                    "Biased Treatment \u00d7 Above 45",
                    "[EMPTY]",
                    "\u22120.097 (0.306)"
                ],
                [
                    "Biased Treatment \u00d7 Below 25",
                    "[EMPTY]",
                    "0.135 (0.425)"
                ],
                [
                    "Biased Treatment \u00d7 Male",
                    "[EMPTY]",
                    "\u22120.097 (0.245)"
                ],
                [
                    "Biased Treatment \u00d7 Bachelor\u2019s Degree",
                    "[EMPTY]",
                    "0.394 (0.387)"
                ],
                [
                    "Biased Treatment \u00d7 Below Bachelor\u2019s Degree",
                    "[EMPTY]",
                    "0.181 (0.385)"
                ],
                [
                    "Biased Treatment \u00d7 Asian",
                    "[EMPTY]",
                    "0.011 (0.631)"
                ],
                [
                    "Biased Treatment \u00d7 White",
                    "[EMPTY]",
                    "0.452 (0.549)"
                ],
                [
                    "Biased Treatment \u00d7 Black",
                    "[EMPTY]",
                    "0.063 (0.583)"
                ],
                [
                    "Biased Treatment \u00d7 Native American",
                    "[EMPTY]",
                    "\u22120.241 (1.030)"
                ],
                [
                    "Biased Treatment \u00d7 Hispanic",
                    "[EMPTY]",
                    "1.234\u2217 (0.581)"
                ],
                [
                    "Self-expected Pass vs. Self-expected Fail",
                    "0.598\u2217\u2217\u2217 (0.133)",
                    "0.562\u2217\u2217\u2217 (0.137)"
                ],
                [
                    "Constant",
                    "4.649\u2217\u2217\u2217 (0.319)",
                    "4.877\u2217\u2217\u2217 (0.622)"
                ],
                [
                    "R2",
                    "0.220",
                    "0.262"
                ],
                [
                    "Adjusted R2",
                    "0.200",
                    "0.209"
                ],
                [
                    "[ITALIC] Note:",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001",
                    "\u2217p<0.05; \u2217\u2217p<0.01; \u2217\u2217\u2217p<0.001"
                ]
            ],
            "title": "Table 5: Regression models predicting perceived fairness from algorithm outcomes and individual differences. Model\u00a01 shows the main effects, and Model\u00a02 includes interaction terms."
        },
        "insight": "Table 5 describes a regression model predicting perceived fairness from individual differences (e.g., education level, computer literacy, and demographics). Model 1 describes main effects, while Model 2 explores the interaction effects between these variables and algorithm outcomes. Model 1 in Table 5 shows that computer literacy is positively correlated with perceived fairness. On average, participants with low computer literacy report significantly lower perceived fairness than participants with high computer literacy (Coef.=0.285, p<0.05, 95% CI: [-0.515, -0.055]), which supports hypothesis 3b.  [CONTINUE] Model 2 in Table 5 shows that certain groups react more strongly to an unfavorable outcome. For example, on average, female participants react more strongly to an unfavorable outcome than male participants (Coef.=0.558, p<0.05, 95% CI: [ 0.082, 1.034]) and participants with a lower education level react more strongly to an unfavorable outcome than participants with a higher education level (Coef.=-1.144, p<0.01, 95% CI: [-1.916, -0.372]). "
    },
    {
        "id": "1004",
        "table": {
            "header": [
                "Traj. type",
                "dCor",
                "RV",
                "GMCC"
            ],
            "rows": [
                [
                    "line 0.01",
                    "0.0006",
                    "0.0",
                    "0.001"
                ],
                [
                    "line 0.02",
                    "0.02",
                    "0.0",
                    "0.005"
                ],
                [
                    "line 0.04",
                    "0.01",
                    "0.0",
                    "0.02"
                ],
                [
                    "circle 0.01",
                    "0.0",
                    "0.0",
                    "0.001"
                ],
                [
                    "circle 0.02",
                    "0.0",
                    "0.0",
                    "0.002"
                ],
                [
                    "circle 0.04",
                    "0.0035",
                    "0.0",
                    "0.0039"
                ]
            ],
            "title": "TABLE I: Similarity distance measurements for line and circular trajectories with different noises for distance correlation(dCor), RV coefficient and GMCC"
        },
        "insight": "From Table I, it can be observed that in terms of noise the three similarity distances are pretty robust. GMCC is giving the worst results in comparison with dCor and RV, but from a general perspective, three of them remains robust in front of noisy signals as the obtained values are close to 0."
    },
    {
        "id": "1005",
        "table": {
            "header": [
                "Transf. type",
                "dCor",
                "RV",
                "GMCC"
            ],
            "rows": [
                [
                    "rotation",
                    "0.0",
                    "0.01",
                    "0.0"
                ],
                [
                    "scale",
                    "0.0",
                    "0.0",
                    "0.0"
                ],
                [
                    "reflection",
                    "0.0",
                    "0.009",
                    "0.0"
                ],
                [
                    "shear",
                    "0.12",
                    "0.18",
                    "0.0"
                ],
                [
                    "squeeze",
                    "0.11",
                    "0.14",
                    "0.0"
                ]
            ],
            "title": "TABLE II: Similarity distance measurements for circular trajectories with different transformations for distance correlation(dCor), RV coefficient and GMCC"
        },
        "insight": "These tables are representative of the real power of GMCC. While the RV coefficient is measuring big distances between the original line and the one transformed by shear mapping, rotation or squeezing, the GMCC remains robust in null distance. dCor's distance is also pretty close to 0, but in shear mapping and squeezing gives a little bit worse results. GMCC is the coefficient that works better for similarity invariant to linear transformations and will remain always in lowest distance for any linear transformations. GMCC is limited to linear transformations. GMCC distance starts increasing when no linear mapping can be set between the compared trajectories. Meanwhile, dCor is capable [CONTINUE] of finding also nonlinear mappings between trajectories. The distance measured by dCor will be smaller than the distance with GMCC for nonlinear correlations."
    },
    {
        "id": "1006",
        "table": {
            "header": [
                "Transf. type",
                "dCor",
                "RV",
                "GMCC"
            ],
            "rows": [
                [
                    "rotation",
                    "0.0",
                    "0.82",
                    "0.0"
                ],
                [
                    "scale",
                    "0.0",
                    "0.0",
                    "0.0"
                ],
                [
                    "reflection",
                    "0.0",
                    "0.819",
                    "0.0"
                ],
                [
                    "shear",
                    "0.007",
                    "0.88",
                    "0.0"
                ],
                [
                    "squeeze",
                    "0.0029",
                    "0.16",
                    "0.0"
                ]
            ],
            "title": "TABLE III: Similarity distance measurements for linear trajectories with different transformations for dCor, RV and GMCC"
        },
        "insight": "These tables are representative of the real power of GMCC. While the RV coefficient is measuring big distances between the original line and the one transformed by shear mapping, rotation or squeezing, the GMCC remains robust in null distance. dCor's distance is also pretty close to 0, but in shear mapping and squeezing gives a little bit worse results. GMCC is the coefficient that works better for similarity invariant to linear transformations and will remain always in lowest distance for any linear transformations. GMCC is limited to linear transformations. GMCC distance starts increasing when no linear mapping can be set between the compared trajectories. Meanwhile, dCor is capable [CONTINUE] of finding also nonlinear mappings between trajectories. The distance measured by dCor will be smaller than the distance with GMCC for nonlinear correlations."
    },
    {
        "id": "1007",
        "table": {
            "header": [
                "Transf. type",
                "dCor",
                "RV",
                "GMCC"
            ],
            "rows": [
                [
                    "line Vs circle",
                    "0.5258",
                    "0.27378",
                    "0.871"
                ]
            ],
            "title": "TABLE IV: Similarity distance measurements for line Vs circle"
        },
        "insight": "The distance measured by GMCC is 0.87 while the distance measured by dCor is 0.57. dCor is less sensible to nonlinear transformations than GMCC. our correlation coefficient is a worse tool for measuring nonlinear correlation but at the same time is a better metric for measuring only linear correlations."
    },
    {
        "id": "1008",
        "table": {
            "header": [
                "[ITALIC]  [BOLD] Action",
                "[ITALIC]  [BOLD] Usefulness  [BOLD] Me",
                "[ITALIC]  [BOLD] Usefulness  [BOLD] Other",
                "[ITALIC]  [BOLD] Task Progression",
                "[ITALIC]  [BOLD] Mutual Help  [BOLD] Me",
                "[ITALIC]  [BOLD] Mutual Help  [BOLD] Other",
                "[ITALIC]  [BOLD] Competition  [BOLD] Me",
                "[ITALIC]  [BOLD] Competition  [BOLD] Other"
            ],
            "rows": [
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] I am ahead",
                    "10",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] We are equal",
                    "10",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] I am ahead",
                    "10",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] We are equal",
                    "10",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] I am ahead",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] We are equal",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] I am ahead",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] We are equal",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] GIVE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] I am ahead",
                    "0",
                    "0",
                    "5",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] We are equal",
                    "10",
                    "0",
                    "5",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] YES",
                    "[ITALIC] I am behind",
                    "10",
                    "0",
                    "5",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] I am ahead",
                    "0",
                    "0",
                    "0",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] We are equal",
                    "0",
                    "0",
                    "0",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] YES",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "-5"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] I am ahead",
                    "10",
                    "0",
                    "5",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] We are equal",
                    "10",
                    "0",
                    "5",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] YES",
                    "[ITALIC] NO",
                    "[ITALIC] I am behind",
                    "10",
                    "0",
                    "5",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] I am ahead",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] We are equal",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] TAKE",
                    "[ITALIC] NO",
                    "[ITALIC] NO",
                    "[ITALIC] I am behind",
                    "0",
                    "0",
                    "0",
                    "0"
                ]
            ],
            "title": "TABLE I: Rewards attributed to each player based on the action a player performs, whether the letter is useful for them, how is their task progression compared to the other player, and the scoring system."
        },
        "insight": "The scoring systems are presented in Table I. [CONTINUE] when both players need the same letter, this version rewards players who take letters (their score increases by five points) and punishes the other players (the other players' score decreases by five points). [CONTINUE] they can progress on their task by taking letters that the other player does not want, where their score increases by five points. [CONTINUE] players can just punish their peers by taking a letter [CONTINUE] that the other needs, while they do not need it. This leads to a decrease in five points in the score of the other player."
    },
    {
        "id": "1009",
        "table": {
            "header": [
                "[BOLD] Plagiarized Code",
                "[BOLD] P1\u2019s Similarity Degree",
                "[BOLD] P2\u2019s Similarity Degree"
            ],
            "rows": [
                [
                    "A",
                    "70%",
                    "50%"
                ],
                [
                    "B",
                    "50%",
                    "40%"
                ],
                [
                    "C",
                    "60%",
                    "95%"
                ]
            ],
            "title": "TABLE I: Similarity degrees for illustrating how contradicting plagiarism pairs are selected"
        },
        "insight": "To provide more insight, suppose we have three plagiarized codes (A, B, and C); their similarity degree toward original code\u2014measured using two detection approaches: P1 and P2\u2014 can be seen on Table I. [CONTINUE] According to given example, P1's and P2's ranking order are {A, C, B} and {C, A, B} respectively. As a result, A and [CONTINUE] are considered as a contradicting pair; they satisfy (1) since P1(A) > P1(C) and P2(A) < P2(C). [CONTINUE] Two things should be considered while selecting contradicting pairs. First, plagiarized codes in each contradicting pair should be explicitly different to each other when perceived by human. Hence, it is preferable to select pairs that generate high delta similarity degree between evaluated approaches. Second, the difference between evaluated approaches should not be coincidental on selected contradicting pairs. Hence, it is important to assure that similarity degrees resulted from both approaches are significantly different to each other; wherein the significance can be measured using t-test."
    },
    {
        "id": "1010",
        "table": {
            "header": [
                "[BOLD]  Aspect",
                "[BOLD] Occurrences",
                "[BOLD] Relationship to Evaluated Approaches"
            ],
            "rows": [
                [
                    "Statement order",
                    "11",
                    "Related to SBA since order is a part of source code structure."
                ],
                [
                    "Semantic",
                    "5",
                    "Related to SBA since semantic is preserved based on source code structure."
                ],
                [
                    "Identifier name",
                    "3",
                    "Related to ABA since identifier name is a source code characteristic."
                ],
                [
                    "Structure",
                    "2",
                    "Obviously related to SBA."
                ],
                [
                    "Output",
                    "1",
                    "Related to SBA since output is defined based on source code structure."
                ],
                [
                    "Line of code",
                    "1",
                    "Related to ABA since line of code is a source code characteristic."
                ]
            ],
            "title": "TABLE II: Considered aspects while suspecting plagiarism cases"
        },
        "insight": "Table II displays those aspects, including their occurrences (calculated based on the number of respondents mentioning it) and their relationship to evaluated plagiarism detection approaches. [CONTINUE] Most of mentioned aspects are related to SBA. Hence, it can be stated that SBA is more effective than ABA from think-aloud evaluation perspective."
    },
    {
        "id": "1011",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Dimen-",
                "[BOLD] Hilbert",
                "[BOLD] Dimensions",
                "[BOLD] Reference",
                "[BOLD] RDB-tree"
            ],
            "rows": [
                [
                    "[BOLD] Dataset",
                    "[BOLD] sions,  [ITALIC] \u03bd",
                    "[BOLD] order,  [ITALIC] \u03c9",
                    "[BOLD] per curve,  [ITALIC] \u03b7",
                    "[BOLD] objects,  [ITALIC] m",
                    "[BOLD] leaf order, \u03a9"
                ],
                [
                    "SIFTn",
                    "128",
                    "8",
                    "16",
                    "10",
                    "63"
                ],
                [
                    "Yorck",
                    "128",
                    "32",
                    "16",
                    "10",
                    "36"
                ],
                [
                    "SUN",
                    "512",
                    "32",
                    "64",
                    "10",
                    "13"
                ],
                [
                    "Audio",
                    "192",
                    "32",
                    "24",
                    "10",
                    "28"
                ],
                [
                    "Enron",
                    "1369",
                    "16",
                    "37",
                    "10",
                    "18"
                ],
                [
                    "Glove",
                    "100",
                    "32",
                    "10",
                    "10",
                    "40"
                ]
            ],
            "title": "Table 3: RDB-tree leaf order (page size = 4 KB)."
        },
        "insight": "Table 3 lists the RDB-tree leaf orders for the datasets we use. [CONTINUE] Table 4 shows the details of the datasets including their domain sizes while Table 3 lists the Hilbert curve orders and other parameters. [CONTINUE] The Hilbert curve and RDB-tree orders are shown in Table 3."
    },
    {
        "id": "1012",
        "table": {
            "header": [
                "[BOLD] Dataset",
                "[BOLD] Query  [BOLD] Time (ms)",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] C2LSH",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] SRS",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] Multicurves",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] QALSH",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] OPQ",
                "[BOLD] Gain of HD-Index in Query Time over  [BOLD] HNSW",
                "[BOLD] MAP  [BOLD] @100",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] C2LSH",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] SRS",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] Multicurves",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] QALSH",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] OPQ",
                "[BOLD] Gain of HD-Index in MAP@100 over  [BOLD] HNSW"
            ],
            "rows": [
                [
                    "[BOLD] SIFT10K",
                    "19.46",
                    "0.06x",
                    "0.17x",
                    "[BOLD] 2.88x",
                    "0.20x",
                    "0.10x",
                    "0.004x",
                    "0.98",
                    "[BOLD] 2.44x",
                    "[BOLD] 4.45x",
                    "0.98x",
                    "[BOLD] 1.81x",
                    "0.99x",
                    "0.98x"
                ],
                [
                    "[BOLD] Audio",
                    "44.18",
                    "0.02x",
                    "0.88x",
                    "[BOLD] 2.44x",
                    "0.21x",
                    "0.02x",
                    "0.0007x",
                    "0.86",
                    "[BOLD] 14.33x",
                    "[BOLD] 6.61x",
                    "[BOLD] 3.05x",
                    "[BOLD] 1.28x",
                    "0.98x",
                    "0.99x"
                ],
                [
                    "[BOLD] SUN",
                    "105.78",
                    "0.12x",
                    "0.22x",
                    "[BOLD] NP",
                    "0.15x",
                    "0.02x",
                    "0.007x",
                    "0.69",
                    "[BOLD] 3.83x",
                    "[BOLD] 23.00x",
                    "[BOLD] NP",
                    "[BOLD] 1.72x",
                    "1.00x",
                    "0.88x"
                ],
                [
                    "[BOLD] SIFT1M",
                    "25.10",
                    "[BOLD] 5.30x",
                    "[BOLD] 1.56x",
                    "[BOLD] 22.98x",
                    "[BOLD] 11.27x",
                    "0.05x",
                    "0.002x",
                    "0.56",
                    "[BOLD] 2.80x",
                    "[BOLD] 28.00x",
                    "0.97x",
                    "[BOLD] 1.19x",
                    "1.00x",
                    "0.92x"
                ],
                [
                    "[BOLD] Yorck",
                    "262.29",
                    "0.27x",
                    "[BOLD] 27.56x",
                    "[BOLD] 2.21x",
                    "[BOLD] 33.54x",
                    "0.01x",
                    "0.002x",
                    "0.39",
                    "[BOLD] 1542.51x",
                    "[BOLD] 39.18x",
                    "[BOLD] 1.24x",
                    "[BOLD] 1.01x",
                    "1.00x",
                    "[BOLD] 1.02x"
                ],
                [
                    "[BOLD] SIFT100M",
                    "732.04",
                    "[BOLD] CR",
                    "[BOLD] 2.17x",
                    "[BOLD] 1.73x",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "0.40",
                    "[BOLD] CR",
                    "[BOLD] 75.72x",
                    "[BOLD] 1.13x",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR"
                ],
                [
                    "[BOLD] SIFT1B",
                    "4855.20",
                    "[BOLD] CR",
                    "[BOLD] DNF",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "0.25",
                    "[BOLD] CR",
                    "[BOLD] DNF",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR",
                    "[BOLD] CR"
                ],
                [
                    "[BOLD] Enron",
                    "242.69",
                    "0.02x",
                    "0.06x",
                    "[BOLD] NP",
                    "[BOLD] NP",
                    "0.06x",
                    "0.002x",
                    "0.92",
                    "[BOLD] 5.12x",
                    "[BOLD] 12.07x",
                    "[BOLD] NP",
                    "[BOLD] NP",
                    "0.99x",
                    "0.99x"
                ],
                [
                    "[BOLD] Glove",
                    "85.20",
                    "0.75x",
                    "0.06x",
                    "[BOLD] 2.6x",
                    "[BOLD] 2.28x",
                    "0.05x",
                    "0.0003x",
                    "0.24",
                    "[BOLD] 8.87x",
                    "[BOLD] 80.00x",
                    "[BOLD] 1.26x",
                    "[BOLD] 1.43x",
                    "0.99x",
                    "0.31x"
                ]
            ],
            "title": "Table 5: Comparison of HD-Index with other techniques. DNF indicates that index construction did not terminate even after running for 20 times the duration of the slowest among the other techniques. CR indicates that index construction crashed due to running out of resources. NP indicates that index construction is not at all possible due to an inherent limitation of the technique."
        },
        "insight": "Table 5 shows that HD-Index significantly outperforms all the other techniques except HNSW and OPQ in terms of quality."
    },
    {
        "id": "1013",
        "table": {
            "header": [
                "[BOLD] Data/Compression",
                "ZFP",
                "ISABELA",
                "SZ",
                "IDEALEM"
            ],
            "rows": [
                [
                    "A6BUS1C1MAG",
                    "9.99",
                    "5.57",
                    "16.36",
                    "242.3"
                ],
                [
                    "A6BUS1L1MAG",
                    "6.40",
                    "5.57",
                    "58.15",
                    "120.0"
                ],
                [
                    "BANK514C1MAG",
                    "7.50",
                    "5.56",
                    "14.38",
                    "248.4"
                ],
                [
                    "BANK514L1MAG",
                    "8.00",
                    "5.57",
                    "41.71",
                    "156.5"
                ],
                [
                    "A6BUS1C1ANG",
                    "8.76",
                    "5.36",
                    "67.25",
                    "86.89"
                ],
                [
                    "A6BUS1L1ANG",
                    "8.78",
                    "5.38",
                    "177.4",
                    "84.32"
                ],
                [
                    "BANK514C1ANG",
                    "8.76",
                    "5.36",
                    "59.13",
                    "96.39"
                ],
                [
                    "BANK514L1ANG",
                    "8.78",
                    "5.38",
                    "210.8",
                    "85.05"
                ]
            ],
            "title": "TABLE I: Compression Ratios"
        },
        "insight": "We present the compression ratio of IDEALEM with the ratios of other floating-point compres sion methods [CONTINUE] Table [CONTINUE] shows the compression ratios of four compression methods for \u00b5PMU data. [CONTINUE] In Table I, we can see that for many \u00b5PMU data, compression ratios reach close to the maximum achievable compression ratios: 256 for the standard mode and 99.56 for the residual mode. [CONTINUE] With the same parameter configurations used in Table I, Table II shows the reconstruction qualities of all compression methods along with original data in terms of these six measures. [CONTINUE] Considering its very high compression ratios, the results shown here confirms the effectiveness of IDEALEM as a new lossy compression method."
    },
    {
        "id": "1014",
        "table": {
            "header": [
                "[BOLD] Data/Compression A6BUS1C1MAG",
                "#1 20618980",
                "ZFP 1493207",
                "ISABELA 20678257",
                "SZ 18423314",
                "IDEALEM 41187683",
                "#2 6.04",
                "ZFP 83.34",
                "ISABELA 6.02",
                "SZ 6.75",
                "IDEALEM 3.02"
            ],
            "rows": [
                [
                    "A6BUS1L1MAG",
                    "20325437",
                    "1834458",
                    "20393068",
                    "7768430",
                    "41059933",
                    "6.12",
                    "67.84",
                    "6.10",
                    "16.02",
                    "3.03"
                ],
                [
                    "BANK514C1MAG",
                    "16198077",
                    "10951716",
                    "16829379",
                    "13386853",
                    "41154067",
                    "7.68",
                    "11.36",
                    "7.39",
                    "9.30",
                    "3.02"
                ],
                [
                    "BANK514L1MAG",
                    "17308827",
                    "251624",
                    "17410171",
                    "9991781",
                    "41108185",
                    "7.19",
                    "488.7",
                    "7.15",
                    "12.45",
                    "3.03"
                ],
                [
                    "A6BUS1C1ANG",
                    "21591680",
                    "857406",
                    "22104731",
                    "7493261",
                    "22281503",
                    "5.76",
                    "145.1",
                    "5.63",
                    "16.61",
                    "5.59"
                ],
                [
                    "A6BUS1L1ANG",
                    "622017",
                    "41255",
                    "1509371",
                    "34495",
                    "1112170",
                    "200.1",
                    "3016.3",
                    "82.44",
                    "3607.5",
                    "111.9"
                ],
                [
                    "BANK514C1ANG",
                    "16148705",
                    "1934105",
                    "16811692",
                    "7620889",
                    "17017975",
                    "7.71",
                    "64.33",
                    "7.40",
                    "16.33",
                    "7.31"
                ],
                [
                    "BANK514L1ANG",
                    "1561848",
                    "48359",
                    "2430822",
                    "89356",
                    "1456997",
                    "79.66",
                    "2572.9",
                    "51.19",
                    "1392.4",
                    "85.40"
                ],
                [
                    "[BOLD] Data/Compression",
                    "#3",
                    "ZFP",
                    "ISABELA",
                    "SZ",
                    "IDEALEM",
                    "#4",
                    "ZFP",
                    "ISABELA",
                    "SZ",
                    "IDEALEM"
                ],
                [
                    "A6BUS1C1MAG",
                    "0.27",
                    "0.23",
                    "0.27",
                    "0.28",
                    "0.23",
                    "0.35",
                    "2.14",
                    "0.36",
                    "0.40",
                    "0.48"
                ],
                [
                    "A6BUS1L1MAG",
                    "0.46",
                    "0.71",
                    "0.45",
                    "0.61",
                    "0.45",
                    "0.44",
                    "2.88",
                    "0.46",
                    "1.15",
                    "0.96"
                ],
                [
                    "BANK514C1MAG",
                    "5.64",
                    "7.79",
                    "5.54",
                    "6.26",
                    "5.85",
                    "9.86",
                    "14.98",
                    "9.66",
                    "11.93",
                    "11.54"
                ],
                [
                    "BANK514L1MAG",
                    "0.03",
                    "0.00",
                    "0.03",
                    "0.03",
                    "0.03",
                    "0.04",
                    "2.00",
                    "0.04",
                    "0.07",
                    "0.06"
                ],
                [
                    "A6BUS1C1ANG",
                    "0.91",
                    "19.41",
                    "1.02",
                    "1.80",
                    "0.91",
                    "1.25",
                    "26.55",
                    "1.51",
                    "3.60",
                    "0.87"
                ],
                [
                    "A6BUS1L1ANG",
                    "3.80",
                    "123.3",
                    "7.67",
                    "47.82",
                    "7.63",
                    "9.58",
                    "145.2",
                    "5.75",
                    "172.7",
                    "6.46"
                ],
                [
                    "BANK514C1ANG",
                    "1.53",
                    "10.81",
                    "1.65",
                    "2.17",
                    "1.23",
                    "1.68",
                    "12.31",
                    "1.99",
                    "3.55",
                    "0.97"
                ],
                [
                    "BANK514L1ANG",
                    "2.13",
                    "106.5",
                    "4.84",
                    "23.95",
                    "5.72",
                    "3.85",
                    "124.6",
                    "3.62",
                    "67.31",
                    "4.37"
                ],
                [
                    "[BOLD] Data/Compression",
                    "#5",
                    "ZFP",
                    "ISABELA",
                    "SZ",
                    "IDEALEM",
                    "#6",
                    "ZFP",
                    "ISABELA",
                    "SZ",
                    "IDEALEM"
                ],
                [
                    "A6BUS1C1MAG",
                    "67644",
                    "95868",
                    "60379",
                    "66764",
                    "833079",
                    "0.00",
                    "0.09",
                    "0.00",
                    "0.00",
                    "0.01"
                ],
                [
                    "A6BUS1L1MAG",
                    "32",
                    "37",
                    "28",
                    "33",
                    "686",
                    "0.32",
                    "0.21",
                    "0.32",
                    "0.33",
                    "0.32"
                ],
                [
                    "BANK514C1MAG",
                    "1062126",
                    "1070228",
                    "1079142",
                    "1058476",
                    "2415159",
                    "3.16",
                    "3.16",
                    "3.16",
                    "3.16",
                    "2.80"
                ],
                [
                    "BANK514L1MAG",
                    "39",
                    "251621",
                    "37",
                    "42",
                    "9824",
                    "0.11",
                    "0.00",
                    "0.11",
                    "0.12",
                    "0.12"
                ],
                [
                    "A6BUS1C1ANG",
                    "50755",
                    "67443",
                    "50755",
                    "50834",
                    "27152",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "A6BUS1L1ANG",
                    "19880",
                    "19271",
                    "20618",
                    "19447",
                    "25015",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "BANK514C1ANG",
                    "45882",
                    "57293",
                    "45882",
                    "45980",
                    "23277",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "BANK514L1ANG",
                    "19904",
                    "19561",
                    "20569",
                    "19752",
                    "22959",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ]
            ],
            "title": "TABLE II: Reconstruction Qualities with Six Measures"
        },
        "insight": "We present the reconstruction quality with the following measures: (#1) number of local maxima (peaks), (#2) mean distance between peaks, (#3) mean distance between the values of peaks, (#4) mean size of jumps, (#5) number of jumps higher than 10% of difference between the maximum and minimum values of series, and (#6) the percentage of points that lie outside the whiskers of the Tukey box plot (more than 1.5 interquartile ranges above the upper quartile or below the lower quartile). [CONTINUE] With the same parameter configurations used in Table I, Table II shows the reconstruction qualities of all compression methods along with original data in terms of these six measures. [CONTINUE] Note that in Table II, smaller difference between the first column of each measure and results of compression methods indicates better reconstruction quality. [CONTINUE] For (#1), ISABELA performs the best for the most part. [CONTINUE] For (#2), (#3), and (#4), IDEALEM is the second best compression method after ISABELA. [CONTINUE] For (#5), SZ shows overall the best performance. On the other hand, IDEALEM performs poorly in most cases. [CONTINUE] for (#6), IDEALEM is comparable with ISABELA and SZ. [CONTINUE] All in all, we note that IDEALEM preserves reconstruction quality with the exception of measures affected by the number of peaks/jumps. [CONTINUE] Considering its very high compression ratios, the results shown here confirms the effectiveness of IDEALEM as a new lossy compression method."
    },
    {
        "id": "1015",
        "table": {
            "header": [
                "[ITALIC] t",
                "join",
                "index/filtering filtering",
                "index/filtering serialization",
                "verification",
                "|| [ITALIC] C||"
            ],
            "rows": [
                [
                    "0.95",
                    "233",
                    "134",
                    "96",
                    "92",
                    "72.7GB"
                ],
                [
                    "0.9",
                    "2815",
                    "1892",
                    "921",
                    "698",
                    "0.56TB"
                ],
                [
                    "0.85",
                    "11367",
                    "8935",
                    "2430",
                    "2311",
                    "1.8TB"
                ]
            ],
            "title": "TABLE IV: GPU join time decomposition for processing the complete DBLP dataset (in secs)"
        },
        "insight": "Further, we drill-down on the GPU join time, as shown in Table 4, where it is shown that the GPU join time is solely attributed to the index/filtering time; the join time is roughly equal to the sum of filtering, index building and serialization."
    },
    {
        "id": "1016",
        "table": {
            "header": [
                "[EMPTY]",
                "0.95",
                "0.90",
                "0.85",
                "0.80",
                "0.75",
                "0.7",
                "0.65",
                "0.6",
                "0.55",
                "0.5"
            ],
            "rows": [
                [
                    "ALL",
                    "1",
                    "1",
                    "0",
                    "1",
                    "2",
                    "3",
                    "4",
                    "4",
                    "5",
                    "3"
                ],
                [
                    "PPJ",
                    "2",
                    "2",
                    "2",
                    "4",
                    "4",
                    "3",
                    "2",
                    "2",
                    "1",
                    "3"
                ],
                [
                    "GRP",
                    "4",
                    "4",
                    "5",
                    "2",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1",
                    "1"
                ]
            ],
            "title": "TABLE V: Number of datasets in which each algorithm is the best per threshold"
        },
        "insight": "Table 5 shows which algorithm exhibited the best performance on the GPU. We can observe, that as in the CPU comparison in , there is no algorithm that dominates the others. However, ALL favors low thresholds, PPJ mid-range and GRP the high ones,"
    },
    {
        "id": "1017",
        "table": {
            "header": [
                "[BOLD] Systems",
                "[BOLD] Precision@K  [BOLD] (Start)",
                "[BOLD] Precision@K  [BOLD] (End)",
                "[BOLD] Training time"
            ],
            "rows": [
                [
                    "Lightor",
                    "0.906",
                    "0.719",
                    "1.06 sec"
                ],
                [
                    "Joint-LSTM",
                    "0.629",
                    "0.600",
                    ">3 days"
                ]
            ],
            "title": "TABLE I: An end-to-end comparison between Lightor and Joint-LSTM."
        },
        "insight": "Table [CONTINUE] reports their training time and Video Precision of Top-5 highlights (k=5). In terms of efficiency, LIGHTOR required 100000\u00d7 less training time compared to Joint-LSTM. In terms of effectiveness, LIGHTOR achieved a Video Precision@K (start) of 0.906 and a Video Precision@K (end) of 0.719, while the Video Precision@K (start) and the Video Precision@K (end) of Joint-LSTM are both round 0.6. This is because that LIGHTOR has a much better generalization than Joint-LSTM. [CONTINUE] TABLE I: An end-to-end comparison between LIGHTOR and JointLSTM."
    },
    {
        "id": "1018",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "Nationality",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "1 Portuguese",
                    "261",
                    "96.3"
                ],
                [
                    "Other",
                    "10",
                    "3.7"
                ],
                [
                    "Age",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "M (DP)",
                    "26.60 (8.628)",
                    "26.60 (8.628)"
                ],
                [
                    "Min-Max",
                    "18 \u2013 58",
                    "18 \u2013 58"
                ],
                [
                    "Gender",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Female",
                    "149",
                    "55.0"
                ],
                [
                    "Male",
                    "122",
                    "45.0"
                ],
                [
                    "School level",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "High school",
                    "77",
                    "28.4"
                ],
                [
                    "College degree",
                    "109",
                    "40.2"
                ],
                [
                    "Master\u2019s degree",
                    "79",
                    "29.2"
                ],
                [
                    "Ph.D.",
                    "6",
                    "2.2"
                ],
                [
                    "Occupation",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Student",
                    "178",
                    "65.7"
                ],
                [
                    "Student worker",
                    "50",
                    "18.5"
                ],
                [
                    "Worker",
                    "37",
                    "13.7"
                ],
                [
                    "Unemployed",
                    "6",
                    "2.2"
                ]
            ],
            "title": "Table 1: Respondents characterization"
        },
        "insight": "Table 1 shows the characterization of the respondents. The majority of them were Portuguese (96.3%), and the age varies between 18 and 58 years old, with M=26.60 (SD=8.628). Regarding their gender, 55% was female, and 45% was male. 40.2% has a college degree, 29.2% has a master's degree, 28.4% has a high school level, and 2.2% has a Ph.D. Additionally, 65.7% are students, 18.5% are student workers, 13.7% are workers and only 2.2% are unemployed."
    },
    {
        "id": "1019",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "Workers\u2019 occupation",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Technician",
                    "26",
                    "70.3"
                ],
                [
                    "Management",
                    "8",
                    "21.6"
                ],
                [
                    "Commercial",
                    "3",
                    "8.1"
                ],
                [
                    "Workers\u2019 department*",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Research",
                    "9",
                    "25.0"
                ],
                [
                    "Tourism",
                    "9",
                    "25.0"
                ],
                [
                    "Technician",
                    "8",
                    "22.2"
                ],
                [
                    "Health",
                    "5",
                    "13.9"
                ],
                [
                    "Financial management",
                    "3",
                    "8.3"
                ],
                [
                    "Commercial",
                    "2",
                    "5.6"
                ]
            ],
            "title": "Table 2: Workers\u2019 situation"
        },
        "insight": "When the workers were questioned (N=37), 70.3% of them affirmed to be technicians, 21.6% are managers, and 8.1% are commercials (Table 2). These are distributed by different departments: 25.0% in research, 25.0% in tourism, 22.2% in the Technical Department, 13.9% in Healthcare, 8.3% in Financial Management and 5.6% in the Commercial Department."
    },
    {
        "id": "1020",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "Students\u2019 living area",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Urban",
                    "178",
                    "78.1"
                ],
                [
                    "Rural",
                    "50",
                    "21.9"
                ],
                [
                    "Students\u2019 level objective",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "College degree",
                    "14",
                    "6.1"
                ],
                [
                    "Master\u2019s degree",
                    "169",
                    "74.1"
                ],
                [
                    "Ph.D.",
                    "45",
                    "19.7"
                ],
                [
                    "Students\u2019 study area*",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Engineering",
                    "110",
                    "48.5"
                ],
                [
                    "Health",
                    "53",
                    "23.3"
                ],
                [
                    "Sciences",
                    "33",
                    "14.5"
                ],
                [
                    "Economy and management",
                    "24",
                    "10.6"
                ],
                [
                    "Arts and architecture",
                    "7",
                    "3.1"
                ]
            ],
            "title": "Table 3: Students\u2019 situation"
        },
        "insight": "Regarding students (228 respondents), the majority lives in the urban area (78.1%), and 21.9% lives in rural area (Table 3). 74.1% of the students are studying to obtain the master's degree, 19.7% to obtain the [CONTINUE] Ph.D. and 6.1% to obtain the college degree. The study area of the students is engineering (48.5%), health (23.3%), sciences (14.5%), economics and management (10.6%), and arts and architecture (3.1%)."
    },
    {
        "id": "1021",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "Difficulty to find a pharmacy",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "No",
                    "171",
                    "63.1"
                ],
                [
                    "Yes",
                    "100",
                    "36.9"
                ],
                [
                    "Frequency of searching a pharmacy",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Less than 5 times per month",
                    "95",
                    "95.0"
                ],
                [
                    "Between 5 and 10 times per month",
                    "1",
                    "1.0"
                ],
                [
                    "More than 10 times per month",
                    "4",
                    "4.0"
                ],
                [
                    "Necessity for more information about in-service pharmacies or available drugs",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Never",
                    "11",
                    "11.0"
                ],
                [
                    "Rarely",
                    "40",
                    "40.0"
                ],
                [
                    "Sometimes",
                    "38",
                    "38.0"
                ],
                [
                    "Frequently",
                    "10",
                    "10.0"
                ],
                [
                    "Always",
                    "1",
                    "1.0"
                ],
                [
                    "Use of software to locate pharmacies",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "No",
                    "46",
                    "46.0"
                ],
                [
                    "Yes",
                    "54",
                    "54.0"
                ],
                [
                    "Which?",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "\u201cFarm\u00e1cias de servi\u00e7o\u201d",
                    "42",
                    "77.8"
                ],
                [
                    "\u201cFarm\u00e1cias portuguesas\u201d",
                    "34",
                    "63.0"
                ],
                [
                    "Other",
                    "3",
                    "5.6"
                ]
            ],
            "title": "Table 4: Searching for pharmacies"
        },
        "insight": "When asked about the past usage of applications for support in the contact/location of pharmacies (results in Table 4), 63.1% responded no, and 36.9% (100 respondents) answered yes. The respondents who said yes, 95.0% of them consider to search for a pharmacy-related subject less than five times per month, 1.0% between five and ten times per month, and 4.0% more than ten times per month. Additionally, these respondents were asked about the need for more information about in-service pharmacies or available drugs to achieve higher efficiency in obtaining their necessities. 11% of them said never, 40.0% rarely, 38.0% sometimes, 10% frequently, and 1% always. [CONTINUE] Despite these opinions, 54% of these respondents consider to know any software that allows the location of pharmacies in service: 77.8% knows \"Farm\u00b4acias de Servic\u00b8o,\" 63% knows \"Farm\u00b4acias Portuguesas\" and 5.6% indicates other software as Google."
    },
    {
        "id": "1022",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "Difficulty to find drugs in a single pharmacy?",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "No",
                    "49",
                    "49.0"
                ],
                [
                    "Yes",
                    "51",
                    "51.0"
                ],
                [
                    "Which?",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "sometimes the nearest or open pharmacy does not have all the necessary drugs available",
                    "60",
                    "60.0"
                ],
                [
                    "the need to order unavailable drugs and with it a considerable waiting time",
                    "25",
                    "25.0"
                ],
                [
                    "more difficult to get drugs in a weekend or holiday periods",
                    "24",
                    "24.0"
                ],
                [
                    "it takes a long time to find what the respondents want, and sometimes they have to go to several pharmacies",
                    "21",
                    "21.0"
                ],
                [
                    "software systems are not very explanatory of pharmacy search results or are outdated",
                    "15",
                    "15.0"
                ],
                [
                    "pharmacies unavailability",
                    "1",
                    "1.0"
                ],
                [
                    "Previous contact to a pharmacy?",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "No",
                    "90",
                    "90.0"
                ],
                [
                    "Yes",
                    "10",
                    "10.0"
                ]
            ],
            "title": "Table 5: Difficulties to find drugs"
        },
        "insight": "Based on Table 5, 51.0% of the respondents who used applications for support in the contact/location of pharmacies, find difficulties to find all the drugs in their prescription in a single pharmacy. The main difficulties mentioned are: 1. sometimes the nearest or open pharmacy does not have all the necessary drugs available (60%), 2. the need to order unavailable drugs and with it a considerable waiting time (25%), [CONTINUE] it is more difficult to get drugs in weekends or holiday periods (24%), 4. it takes a long time to find what the respondents want, and sometimes they have to go to several pharmacies (21%), 6. pharmacies unavailability (1%). 5. software systems are not very explanatory of pharmacy search results or are outdated (15%), [CONTINUE] To solve these difficulties, 10% has already previously contacted the pharmacy in service in order to check in advance if all the drugs in their prescription are available and do not waste time in traveling."
    },
    {
        "id": "1023",
        "table": {
            "header": [
                "[EMPTY]",
                "[BOLD] N",
                "[BOLD] %"
            ],
            "rows": [
                [
                    "How to search in applications for a pharmacy?*",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Less or equal to 5km search",
                    "27",
                    "27.3"
                ],
                [
                    "More than 5km search",
                    "8",
                    "8.1"
                ],
                [
                    "Search for all near me",
                    "31",
                    "31.3"
                ],
                [
                    "Less or equal to 3 near me",
                    "27",
                    "27.3"
                ],
                [
                    "More than 3 near me",
                    "6",
                    "6.1"
                ],
                [
                    "Imagine you go to a local pharmacy and, after arriving there, you verify that the same does not have all of their drugs available. What do you do?",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "search or inquire about nearby pharmacies to complete the prescription",
                    "59",
                    "59.0"
                ],
                [
                    "give up on going to a new pharmacy and order the necessary drugs, getting on waiting list",
                    "32",
                    "32.0"
                ],
                [
                    "call other nearby pharmacies to see if they have the necessary drugs",
                    "20",
                    "20.0"
                ],
                [
                    "try asking someone for advice on the subject or more information",
                    "16",
                    "16.0"
                ],
                [
                    "Interest in a new system",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Without interest",
                    "1",
                    "1.0"
                ],
                [
                    "Low interest",
                    "6",
                    "6.0"
                ],
                [
                    "Some interest",
                    "39",
                    "39.0"
                ],
                [
                    "High interest",
                    "54",
                    "54.0"
                ]
            ],
            "title": "Table 6: Behavior of respondents to find drugs"
        },
        "insight": "In case the respondent has to search in applications or on the internet about a particular local pharmacy (Table 6), 27.3% consider that they only search results less or equal than 5km away, 8.1% search results for more than 5km, 31.3% search all results near them, 27.3% search for 3 or fewer results, and 6.1% search more than 3 results near them. [CONTINUE] Considering the scenario of the respondent going to a local pharmacy and, after arriving there, verifying that the same does not have all of their medicines available: 1. 59.0% search or inquire about nearby pharmacies to complete their prescription, 2. 32% give up on going to a new pharmacy and order the necessary drugs, getting on the waiting list, 3. 20% call other nearby pharmacies to see if they have the necessary drugs, 4. 16.0% try asking someone for advice on the subject or more information. [CONTINUE] only 1.0% have no interest in a new system of contact with pharmacies, 2. 6.0% have low interest, 3. 39% have some interest in the proposed system, 4. moreover, 54% of them have high interest. After the descriptive analysis of the sample, it is essential to perceive the influence of some factors in the opinion of the respondents. Thus, a cross-tabulation analysis of the information was performed using the chi-square test, whose null hypothesis is that the variables are independent. Additionally, the chi-square test provides two statistics that indicate whether the variables are associated or independent: a chi-square statistic and a p-value. Thus, if the p-value is less than or equal to 0.05 (level of significance considered as acceptable), the null hypothesis is rejected, and the variables are associated. If the p-value is more significant than 0.05, the null hypothesis is not rejected, and it is possible to conclude that variables are independent."
    },
    {
        "id": "1024",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Age Less or equal to 23",
                "Age More than 23",
                "X2",
                "p"
            ],
            "rows": [
                [
                    "Difficulty in obtaining drugs",
                    "Yes",
                    "19",
                    "32",
                    "6.763",
                    "[BOLD] 0.009"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "31",
                    "18",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Interest in a new system",
                    "Some interest",
                    "27",
                    "19",
                    "2.576",
                    "0.108"
                ],
                [
                    "[EMPTY]",
                    "High interest",
                    "23",
                    "31",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Necessity of more information",
                    "Rarely",
                    "30",
                    "21",
                    "3.241",
                    "0.072"
                ],
                [
                    "[EMPTY]",
                    "Frequently",
                    "20",
                    "29",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Use of location software",
                    "Yes",
                    "24",
                    "30",
                    "1.449",
                    "0.229"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "26",
                    "20",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 7: Influence of age in the respondent opinion"
        },
        "insight": "Table 7 presents the main results of the influence of the age in some opinions of the respondents. Regarding the first comparison (age vs. difficulty to obtain drugs), the null hypothesis is rejected, and variables are associated. This means that young people have fewer difficulties to find drugs, and older people have more difficulties, which could be explained by the fact that old people have more health problems or have already lived more experiences."
    },
    {
        "id": "1025",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Necessity of more information Rarely",
                "Necessity of more information Frequently",
                "X2",
                "p"
            ],
            "rows": [
                [
                    "Difficulty in obtaining drugs",
                    "Yes",
                    "21",
                    "30",
                    "4.019",
                    "[BOLD] 0.045"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "30",
                    "19",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 8: Relation between necessity of more information and difficulty to obtain drugs"
        },
        "insight": "The comparison between the necessity of more information and the difficulty in obtaining drugs (Table 8) show that these variables are associated (p=0.045<0.05). Thus, people who had more difficulties to get drugs are precisely those who feel more necessity to have more information available. By the opposite, people with few difficulties to obtain their drugs are who rarely feel the necessity to have more information available."
    },
    {
        "id": "1026",
        "table": {
            "header": [
                "[EMPTY]",
                "[EMPTY]",
                "Interest in a new system Some interest",
                "Interest in a new system High interest",
                "X2",
                "p"
            ],
            "rows": [
                [
                    "Difficulty to obtain drugs",
                    "Yes",
                    "15",
                    "36",
                    "11.530",
                    "[BOLD] 0.001"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "31",
                    "18",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Waste of time to find the necessary drugs",
                    "Yes",
                    "5",
                    "16",
                    "5.270",
                    "[BOLD] 0.022"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "41",
                    "38",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Sometimes the open pharmacy does not have the needed drugs",
                    "Yes",
                    "21",
                    "39",
                    "7.307",
                    "[BOLD] 0.007"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "25",
                    "15",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "I search or inform myself of nearby pharmacies",
                    "Yes",
                    "20",
                    "39",
                    "8.484",
                    "[BOLD] 0.004"
                ],
                [
                    "[EMPTY]",
                    "No",
                    "26",
                    "15",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ],
            "title": "Table 9: Factors that influence the interest in a new system"
        },
        "insight": "Table 9 present some factor that could influence the interest in a new system. Regarding the difficulty to obtain drugs, it is possible to verify that people who had to feel that, are the most interested in a new system, and people with fewer difficulties does not feel the same interest. [CONTINUE] Additionally, people who had already a waste of time to find the necessary drugs have more interest in a new system. If they do not waste time to find drugs, they have less interest in a new system. [CONTINUE] Regarding the situation \"sometimes the open pharmacy does not have the needed drugs\" vs. \"interest in a new system,\" the null hypothesis is rejected and, then, both variables are associated. Similar to previous analysis, people who consider that sometimes the open pharmacy does not have the needed drugs have a high interest in a new system. [CONTINUE] Finally, people that search or inform of nearby pharmacies are the most interested in a new system (p=0.004), and the opposite also happens. [CONTINUE] To conclude, it turns out that the most active people in the search for pharmacies, drugs or even information are those who are most interested in a new system that easily makes all this information available."
    },
    {
        "id": "1027",
        "table": {
            "header": [
                "[BOLD] Agent",
                "[BOLD] KB Size",
                "[BOLD] F1 Score (std.)"
            ],
            "rows": [
                [
                    "[ITALIC] Dual Track Manager",
                    "[EMPTY]",
                    "0.79 (0.020)"
                ],
                [
                    "[ITALIC] Baseline I",
                    "17",
                    "0.59 (0.030)"
                ],
                [
                    "[ITALIC] Baseline II",
                    "17",
                    "0.61 (0.017)"
                ],
                [
                    "[ITALIC] Dual Track Manager",
                    "[EMPTY]",
                    "0.77 (0.025)"
                ],
                [
                    "[ITALIC] Baseline I",
                    "26",
                    "0.52 (0.016)"
                ],
                [
                    "[ITALIC] Baseline II",
                    "26",
                    "0.66 (0.007)"
                ],
                [
                    "[ITALIC] Dual Track Manager",
                    "[EMPTY]",
                    "0.62 (0.011)"
                ],
                [
                    "[ITALIC] Baseline I",
                    "37",
                    "0.47 (0.019)"
                ],
                [
                    "[ITALIC] Baseline II",
                    "37",
                    "0.46 (0.022)"
                ]
            ],
            "title": "TABLE I: F1 Score of KB Update Given Different KB Sizes"
        },
        "insight": "We further evaluated whether the entity added to KB matches with the human intention or not (Hypothesis-II). As presented in Table I, our algorithm consistently maintains higher F1 score in comparison to other baseline agents in the medium sized KB."
    },
    {
        "id": "1028",
        "table": {
            "header": [
                "[EMPTY]",
                "Q1",
                "Q2",
                "Q3",
                "Q4"
            ],
            "rows": [
                [
                    "Our dialog agent",
                    "3.42",
                    "2.50",
                    "[BOLD] 1.50",
                    "[BOLD] 2.50"
                ],
                [
                    "A baseline agent with static KB",
                    "3.33",
                    "1.83",
                    "2.17",
                    "1.75"
                ]
            ],
            "title": "TABLE II: Results of the human participant experiment."
        },
        "insight": "By the end of each dialog, each participant filled out a survey form that includes prompts: Q1, Task is easy to participants; Q2, Robot understood participant; Q3, Robot frustrated participant; Q4, Participant will use the robot in the future. The response choices range from 0 (Strongly disagree) to 4 (Strongly agree). [CONTINUE] Table II shows the average scores. At the 0.1 confidence level, our dialog agent performed significantly better in response for Q3 (frustrated) and Q4 (usefulness). There is no significance difference observed in responses to the other two questions."
    },
    {
        "id": "1029",
        "table": {
            "header": [
                "Hyperparameters",
                "Classification",
                "Regression"
            ],
            "rows": [
                [
                    "number of trees",
                    "1000",
                    "1000"
                ],
                [
                    "maximum depth of the trees",
                    "2",
                    "2"
                ],
                [
                    "best split max features",
                    "\u221anumber of features",
                    "13(number of features)"
                ]
            ],
            "title": "TABLE III: Hyperparameters of the Random Forest models"
        },
        "insight": "Person-specific models\u2014they were developed using Random Forest (RF) models (Table III). All [CONTINUE] Generic models\u2014they were also developed using Random Forest (RF) models (Table III). [CONTINUE] some folds. In our case, all folds achieved similar performance2. Secondly, all the models use a very simple RF model (Table III) that is less likely to overfit."
    },
    {
        "id": "1030",
        "table": {
            "header": [
                "[BOLD] Model",
                "[BOLD] Group 1  [BOLD] Male",
                "[BOLD] Group 1  [BOLD] Female",
                "[BOLD] Group 2  [BOLD] Male",
                "[BOLD] Group 2  [BOLD] Female",
                " [BOLD] Group 3   [BOLD] Male",
                " [BOLD] Group 3   [BOLD] Female"
            ],
            "rows": [
                [
                    "VGG-Face",
                    "7.99",
                    "9.38 (\u219117%)",
                    "12.03 (\u219150%)",
                    "13.95 (\u219176%)",
                    "18.43 (\u2191131%)",
                    "23.66 (\u2191196%)"
                ],
                [
                    "ResNet-50",
                    "1.60",
                    "1.96 (\u219122%)",
                    "2.15 (\u219134%)",
                    "3.61 (\u2191126%)",
                    "3.25 (\u2191103%)",
                    "5.07 (\u2191217%)"
                ]
            ],
            "title": "Table 1: Performance (False Match Rate in % @ False Non-Match Rate = 0.1%) of Face Recognition Models on the DiveFace dataset. We show in brackets the relative error growth rates with respect to the best class (Group 1 Male)."
        },
        "insight": "Results reported in Table 1 exhibit large gaps between performances obtained by different demographic groups, suggesting that both gender and ethnicity significantly affect the performance of biased models. These effects are particularly high for ethnicity, with a very large degradation of the results for the class less represented in the training data (Group 3 Female). This degradation produces a relative increment of the Equal Error Rate (EER) of 196% and 217% for VGGFace and ResNet-50, respectively, with regard to the best class (Group 1 Male)."
    },
    {
        "id": "1031",
        "table": {
            "header": [
                "DoH Server",
                "Location",
                "Ping (ms)",
                "Resolution (ms)"
            ],
            "rows": [
                [
                    "cloudflare-dns.com",
                    "anycast",
                    "3.14",
                    "300"
                ],
                [
                    "commons.host",
                    "anycast",
                    "3.99",
                    "310"
                ],
                [
                    "dns.google",
                    "anycast",
                    "2.93",
                    "312"
                ],
                [
                    "mozilla.cloudflare-dns.com",
                    "anycast",
                    "3.19",
                    "314"
                ],
                [
                    "doh.dnswarden.com",
                    "DE",
                    "87.50",
                    "327"
                ],
                [
                    "doh.powerdns.org",
                    "NL",
                    "77.90",
                    "331"
                ],
                [
                    "doh.securedns.eu",
                    "NL",
                    "85.30",
                    "335"
                ],
                [
                    "doh.li",
                    "UK",
                    "74.90",
                    "335"
                ],
                [
                    "doh.appliedprivacy.net",
                    "AT",
                    "96.60",
                    "342"
                ],
                [
                    "doh.42l.fr",
                    "FR",
                    "76.20",
                    "343"
                ],
                [
                    "dns.dnsoverhttps.net",
                    "US",
                    "73.00",
                    "343"
                ],
                [
                    "dns.aa.net.uk",
                    "UK",
                    "71.40",
                    "344"
                ],
                [
                    "doh.xfinity.com",
                    "US",
                    "23.53",
                    "347"
                ],
                [
                    "dns.containerpi.com",
                    "JP",
                    "162.10",
                    "347"
                ],
                [
                    "doh.opendns.com",
                    "anycast",
                    "3.90",
                    "355"
                ],
                [
                    "jcdns.fun",
                    "NL",
                    "85.23",
                    "358"
                ],
                [
                    "rdns.faelix.net",
                    "anycast",
                    "82.70",
                    "358"
                ],
                [
                    "dns.hostux.net",
                    "LU",
                    "81.60",
                    "364"
                ],
                [
                    "dohdot.coxlab.net",
                    "US",
                    "[EMPTY]",
                    "376"
                ],
                [
                    "doh-2.seby.io",
                    "AU",
                    "[EMPTY]",
                    "377"
                ],
                [
                    "dns.twnic.tw",
                    "TW",
                    "230.00",
                    "380"
                ],
                [
                    "doh.dns.sb",
                    "anycast",
                    "3.20",
                    "388"
                ],
                [
                    "ibksturm.synology.me",
                    "CH",
                    "105.80",
                    "416"
                ],
                [
                    "jp.tiarap.org",
                    "anycast",
                    "3.28",
                    "421"
                ],
                [
                    "ibuki.cgnat.net",
                    "BR",
                    "137.10",
                    "426"
                ],
                [
                    "dns.dns-over-https.com",
                    "JP",
                    "[EMPTY]",
                    "452"
                ]
            ],
            "title": "TABLE I: Geographical location information of the 26 DoH recursors and the median of their resolution time."
        },
        "insight": "Table [CONTINUE] shows the geographical location, the ping time, and the median DNS resolution time of the 26 recursors used in our experiments. Except for those recursors that do not respond to our ping (ICMP) packets (whose ping time is marked as N/A), there is a correlation between the geographical location and the ping time. Notably, most recursors that are hosted on anycast IP address(es) have the shortest ping time, while unicast recursors have a longer ping time. [CONTINUE] As shown in Table I, the top four anycast recursors with the best resolution time also have relatively small ping time. However, there are other four anycast recursors also have relatively small ping time, but are not among the top recursors with shortest resolution time."
    },
    {
        "id": "1032",
        "table": {
            "header": [
                "system parameters",
                "system parameters",
                "system parameters",
                "algorithm parameters",
                "algorithm parameters"
            ],
            "rows": [
                [
                    "maximum",
                    "minimum",
                    "churn",
                    "[ITALIC] join_ [ITALIC] bound",
                    "[ITALIC] rw_ [ITALIC] bound"
                ],
                [
                    "failures",
                    "system",
                    "rate",
                    "fraction",
                    "fraction"
                ],
                [
                    "( [ITALIC] f)",
                    "size ( [ITALIC] NSmin)",
                    "( [ITALIC] \u03b1)",
                    "( [ITALIC] \u03b3)",
                    "( [ITALIC] \u03b2)"
                ],
                [
                    "1",
                    "8",
                    "0",
                    "[EMPTY]",
                    "0.86"
                ],
                [
                    "1",
                    "10",
                    "0.01",
                    "0.82",
                    "0.84"
                ],
                [
                    "1",
                    "13",
                    "0.02",
                    "0.79",
                    "0.80"
                ],
                [
                    "1",
                    "190",
                    "0.05",
                    "0.79",
                    "0.80"
                ],
                [
                    "2",
                    "19",
                    "0.01",
                    "0.80",
                    "0.83"
                ],
                [
                    "2",
                    "24",
                    "0.02",
                    "0.81",
                    "0.82"
                ],
                [
                    "2",
                    "347",
                    "0.05",
                    "0.70",
                    "0.77"
                ],
                [
                    "5",
                    "44",
                    "0.01",
                    "0.80",
                    "0.83"
                ],
                [
                    "5",
                    "57",
                    "0.02",
                    "0.79",
                    "0.82"
                ],
                [
                    "5",
                    "826",
                    "0.05",
                    "0.79",
                    "0.82"
                ],
                [
                    "10",
                    "85",
                    "0.01",
                    "0.80",
                    "0.83"
                ],
                [
                    "10",
                    "113",
                    "0.02",
                    "0.79",
                    "0.82"
                ],
                [
                    "10",
                    "1630",
                    "0.05",
                    "0.79",
                    "0.82"
                ],
                [
                    "100",
                    "838",
                    "0.01",
                    "0.79",
                    "0.82"
                ],
                [
                    "100",
                    "1107",
                    "0.02",
                    "0.79",
                    "0.82"
                ],
                [
                    "100",
                    "16015",
                    "0.05",
                    "0.79",
                    "0.82"
                ],
                [
                    "1000",
                    "8360",
                    "0.01",
                    "0.79",
                    "0.82"
                ],
                [
                    "1000",
                    "11042",
                    "0.02",
                    "0.79",
                    "0.82"
                ],
                [
                    "1000",
                    "159935",
                    "0.05",
                    "0.79",
                    "0.82"
                ]
            ],
            "title": "TABLE I: Values for the parameters that satisfy constraints (1)\u2013(7)."
        },
        "insight": "Table [CONTINUE] gives a few sets of values for which the above constraints are satisfied. In all consistent sets of parameter values, the churn rate \u03b1 is never more than 0.05 and [CONTINUE] Smin > 8.5f . The algorithm can tolerate any size of f as long as [CONTINUE] Smin is proportionally big."
    },
    {
        "id": "1033",
        "table": {
            "header": [
                "Surface",
                "# vertices",
                "SCP\u00a0 Time (s)",
                "SCP\u00a0 mean(| [ITALIC] d|)",
                "CETM\u00a0 Time (s)",
                "CETM\u00a0 mean(| [ITALIC] d|)",
                "PGCP Time (s)",
                "PGCP mean(| [ITALIC] d|)"
            ],
            "rows": [
                [
                    "Sophie",
                    "21K",
                    "1.1",
                    "0.2",
                    "1.2",
                    "0.2",
                    "0.6",
                    "0.2"
                ],
                [
                    "Niccol\u00f2 da Uzzano",
                    "25K",
                    "1.3",
                    "0.6",
                    "Failed",
                    "Failed",
                    "0.7",
                    "0.6"
                ],
                [
                    "Mask",
                    "32K",
                    "1.6",
                    "0.2",
                    "7.3",
                    "0.2",
                    "0.9",
                    "0.2"
                ],
                [
                    "Max Planck",
                    "50K",
                    "2.6",
                    "0.5",
                    "5.5",
                    "0.5",
                    "1.5",
                    "0.5"
                ],
                [
                    "Bunny",
                    "85K",
                    "4.4",
                    "0.5",
                    "18.8",
                    "0.5",
                    "2.0",
                    "0.5"
                ],
                [
                    "Julius",
                    "220K",
                    "14.2",
                    "0.1",
                    "19.5",
                    "0.1",
                    "6.6",
                    "0.1"
                ],
                [
                    "Buddha",
                    "240K",
                    "13.7",
                    "0.6",
                    "49.0",
                    "0.6",
                    "9.2",
                    "0.6"
                ],
                [
                    "Face",
                    "1M",
                    "85.2",
                    "<0.1",
                    "98.1",
                    "<0.1",
                    "47.6",
                    "<0.1"
                ]
            ],
            "title": "Table 1: The performance of spectral conformal parameterization (SCP)\u00a0[17], conformal equivalence of triangle meshes (CETM)\u00a0[18] and PGCP for free-boundary conformal parameterization of simply-connected open surfaces."
        },
        "insight": "To assess the performance of our method, we compare it with the spectral conformal parameterization (SCP)  and conformal equivalence of triangle meshes (CETM)  in terms of the computation time and the angular distortion (see Table 1). [CONTINUE] The experimental results show that our proposed method is significantly faster than both SCP and CETM by over 40% and 70% respectively on average, while maintaining comparable accuracy in terms of the average angular distortion. This demonstrates the effectiveness of our method for free-boundary global conformal parameterization."
    },
    {
        "id": "1034",
        "table": {
            "header": [
                "Surface",
                "# vertices",
                "LDM\u00a0 Time (s)",
                "LDM\u00a0 mean(| [ITALIC] d|)",
                "CEM\u00a0 Time (s)",
                "CEM\u00a0 mean(| [ITALIC] d|)",
                "PGCP Time (s)",
                "PGCP mean(| [ITALIC] d|)"
            ],
            "rows": [
                [
                    "Ogre",
                    "20K",
                    "1.1",
                    "1.5",
                    "0.3",
                    "2.6",
                    "0.5",
                    "1.5"
                ],
                [
                    "Niccol\u00f2 da Uzzano",
                    "25K",
                    "1.6",
                    "0.8",
                    "1.4",
                    "1.3",
                    "0.8",
                    "0.8"
                ],
                [
                    "Brain",
                    "48K",
                    "2.9",
                    "1.6",
                    "2.9",
                    "1.5",
                    "1.3",
                    "1.5"
                ],
                [
                    "Gargoyle",
                    "50K",
                    "3.1",
                    "1.9",
                    "2.8",
                    "2.1",
                    "1.4",
                    "1.9"
                ],
                [
                    "Hand",
                    "53K",
                    "3.4",
                    "1.2",
                    "3.4",
                    "1.2",
                    "1.4",
                    "1.2"
                ],
                [
                    "Octopus",
                    "150K",
                    "15.4",
                    "7.2",
                    "10.4",
                    "24.0",
                    "8.9",
                    "5.6"
                ],
                [
                    "Buddha",
                    "240K",
                    "22.4",
                    "0.7",
                    "25.1",
                    "0.7",
                    "11.4",
                    "0.7"
                ],
                [
                    "Nefertiti",
                    "1M",
                    "87.9",
                    "2.9",
                    "83.2",
                    "4.2",
                    "52.7",
                    "2.9"
                ]
            ],
            "title": "Table 2: The performance of linear disk conformal map (LDM)\u00a0[25], conformal energy minimization (CEM)\u00a0[26] and PGCP for disk conformal parameterization of simply-connected open surfaces."
        },
        "insight": "To evaluate the performance of our method, we compare it with the state-of-the-art linear disk conformal map (LDM) method  and the conformal energy minimization (CEM) method  (see Table 2). [CONTINUE] It can be observed that our method is significantly faster than LDM and CEM by over 50% and 30% on average respectively. Also, our method achieves comparable or smaller angular distortion when compared to the two other methods. This shows that our method is advantageous for disk conformal parameterization."
    },
    {
        "id": "1035",
        "table": {
            "header": [
                "Surface",
                "# vertices",
                "FFGCM\u00a0 Time (s)",
                "FFGCM\u00a0 mean( [ITALIC] d)",
                "FLASH\u00a0 Time (s)",
                "FLASH\u00a0 mean( [ITALIC] d)",
                "PGCP Time (s)",
                "PGCP mean( [ITALIC] d)"
            ],
            "rows": [
                [
                    "Horse",
                    "20K",
                    "12.1",
                    "11.0",
                    "0.4",
                    "3.0",
                    "0.4",
                    "2.7"
                ],
                [
                    "Bulldog",
                    "50K",
                    "22.0",
                    "1.0",
                    "0.9",
                    "1.1",
                    "1.0",
                    "1.0"
                ],
                [
                    "Chinese Lion",
                    "50K",
                    "29.3",
                    "1.3",
                    "1.1",
                    "1.3",
                    "1.1",
                    "1.3"
                ],
                [
                    "Duck",
                    "100K",
                    "100.4",
                    "1.1",
                    "2.2",
                    "0.4",
                    "2.4",
                    "0.3"
                ],
                [
                    "David",
                    "130K",
                    "46.6",
                    "0.2",
                    "3.5",
                    "0.2",
                    "3.4",
                    "0.2"
                ],
                [
                    "Octopus",
                    "150K",
                    "112.3",
                    "37.2",
                    "10.1",
                    "6.9",
                    "7.1",
                    "2.6"
                ],
                [
                    "Lion Vase",
                    "210K",
                    "222.7",
                    "14.4",
                    "4.5",
                    "0.8",
                    "4.7",
                    "0.7"
                ],
                [
                    "Asian Dragon",
                    "1M",
                    "Failed",
                    "Failed",
                    "64.4",
                    "1.3",
                    "48.5",
                    "0.9"
                ]
            ],
            "title": "Table 3: The performance of folding-free global conformal mapping (FFGCM)\u00a0[30], FLASH\u00a0[31] and PGCP for spherical conformal parameterization of genus-0 closed surfaces."
        },
        "insight": "To evaluate the performance, we compare our proposed method with the state-of-the-art folding-free global conformal mapping (FFGCM) algorithm  and the FLASH algorithm  (see Table 3). The MATLAB version of FFGCM is kindly provided by its authors, and the MATLAB version of FLASH can be found at . Because of the \"divide-and-conquer\" nature of our method, our method is capable of producing spherical conformal parameterizations with a smaller angular distortion when compared to the two state-of-the-art algorithms. In particular, the FLASH algorithm involves puncturing a triangle from the input surface and flattening the punctured surface onto a big triangular domain. This step unavoidably creates squeezed regions and produces certain angular distortions. While the distortions are alleviated in the subsequent step using quasiconformal composition, the step again involves a domain where most vertices are squeezed at the interior, which leads to some distortions. By contrast, our proposed PGCP method flattens each submesh naturally, with the shape of the submesh boundary taken into consideration. This effectively reduces the angular distortions, thereby producing a spherical conformal parameterization with a better accuracy. Moreover, because of the ability of exploiting parallelism, our method achieves a significant reduction in computational time by over 90% on average when compared to FFGCM. When compared to FLASH, our method achieves comparable efficiency for moderate meshes and a notable reduction in computational time by around 25% for dense meshes. This shows the advantages of our method for spherical conformal parameterization."
    },
    {
        "id": "1036",
        "table": {
            "header": [
                "Parameterization",
                "Surface",
                "# vertices",
                "BFF\u00a0 Time (s)",
                "BFF\u00a0 mean(| [ITALIC] d|)",
                "PGCP Time (s)",
                "PGCP mean(| [ITALIC] d|)"
            ],
            "rows": [
                [
                    "Free-boundary",
                    "Sophie",
                    "21K",
                    "0.9",
                    "0.2",
                    "0.6",
                    "0.2"
                ],
                [
                    "Free-boundary",
                    "Niccol\u00f2 da Uzzano",
                    "25K",
                    "1.0",
                    "0.5",
                    "0.7",
                    "0.6"
                ],
                [
                    "Free-boundary",
                    "Mask",
                    "32K",
                    "1.3",
                    "0.2",
                    "0.9",
                    "0.2"
                ],
                [
                    "Free-boundary",
                    "Max Planck",
                    "50K",
                    "3.0",
                    "0.5",
                    "1.5",
                    "0.5"
                ],
                [
                    "Free-boundary",
                    "Bunny",
                    "85K",
                    "4.7",
                    "1.2",
                    "2.0",
                    "0.5"
                ],
                [
                    "Free-boundary",
                    "Julius",
                    "220K",
                    "16.8",
                    "0.1",
                    "6.6",
                    "0.1"
                ],
                [
                    "Free-boundary",
                    "Buddha",
                    "240K",
                    "14.4",
                    "0.6",
                    "9.2",
                    "0.6"
                ],
                [
                    "Free-boundary",
                    "Face",
                    "1M",
                    "Failed",
                    "Failed",
                    "47.6",
                    "<0.1"
                ],
                [
                    "Disk-boundary",
                    "Ogre",
                    "20K",
                    "0.7",
                    "1.5",
                    "0.5",
                    "1.5"
                ],
                [
                    "Disk-boundary",
                    "Niccol\u00f2 da Uzzano",
                    "25K",
                    "1.0",
                    "1.1",
                    "0.8",
                    "0.8"
                ],
                [
                    "Disk-boundary",
                    "Brain",
                    "48K",
                    "2.4",
                    "1.6",
                    "1.3",
                    "1.5"
                ],
                [
                    "Disk-boundary",
                    "Gargoyle",
                    "50K",
                    "2.1",
                    "1.9",
                    "1.4",
                    "1.9"
                ],
                [
                    "Disk-boundary",
                    "Hand",
                    "53K",
                    "2.6",
                    "1.6",
                    "1.4",
                    "1.2"
                ],
                [
                    "Disk-boundary",
                    "Octopus",
                    "150K",
                    "5.5",
                    "24.7",
                    "8.9",
                    "5.6"
                ],
                [
                    "Disk-boundary",
                    "Buddha",
                    "240K",
                    "14.5",
                    "0.9",
                    "11.4",
                    "0.7"
                ],
                [
                    "Disk-boundary",
                    "Nefertiti",
                    "1M",
                    "Failed",
                    "Failed",
                    "52.7",
                    "2.9"
                ],
                [
                    "Spherical",
                    "Horse",
                    "20K",
                    "1.0",
                    "74.52",
                    "0.4",
                    "2.7"
                ],
                [
                    "Spherical",
                    "Bulldog",
                    "50K",
                    "3.1",
                    "11.5",
                    "1.0",
                    "1.0"
                ],
                [
                    "Spherical",
                    "Chinese Lion",
                    "50K",
                    "3.0",
                    "4.4",
                    "1.1",
                    "1.3"
                ],
                [
                    "Spherical",
                    "Duck",
                    "100K",
                    "7.6",
                    "5.7",
                    "2.4",
                    "0.3"
                ],
                [
                    "Spherical",
                    "David",
                    "130K",
                    "15.9",
                    "2.6",
                    "3.4",
                    "0.2"
                ],
                [
                    "Spherical",
                    "Octopus",
                    "150K",
                    "10.2",
                    "74.3",
                    "7.1",
                    "2.6"
                ],
                [
                    "Spherical",
                    "Lion Vase",
                    "210K",
                    "14.2",
                    "4.9",
                    "4.7",
                    "0.7"
                ],
                [
                    "Spherical",
                    "Asian Dragon",
                    "1M",
                    "Failed",
                    "Failed",
                    "48.5",
                    "0.9"
                ]
            ],
            "title": "Table 4: Comparison between BFF\u00a0[56] and PGCP for free-boundary conformal parameterization for simply-connected open surfaces, disk conformal parameterization for simply-connected open surfaces, and spherical conformal parameterization for genus-0 closed surfaces."
        },
        "insight": "Table 4 shows the comparison between the two methods. For free-boundary and disk [CONTINUE] conformal parameterization, it can be observed that our method achieves at least comparable and sometimes better conformality, with a shorter computational time. For spherical conformal [CONTINUE] parameterization, our method is advantageous in both the conformality and efficiency. A possible reason is that BFF handles genus-0 closed surfaces by removing an arbitrary vertex star, flattening the punctured surface onto a disk conformally followed by a suitable rescaling, and finally mapping the disk onto the sphere using stereographic projection and filling the punctured vertex star at the pole. The choice of the vertex star greatly affects the overall shape of the disk parameterization of the punctured surface and the final angular distortion of the spherical parameterization. By contrast, our \"divide and conquer\" approach enables us to tackle the conformal flattening problem of subdomains which are obtained from a more natural partition of the surface, thereby achieving better conformality."
    },
    {
        "id": "1037",
        "table": {
            "header": [
                "Surface",
                "Cut paths",
                "mean(| [ITALIC] d|)",
                "sd(| [ITALIC] d|)",
                "median(| [ITALIC] d|)",
                "iqr(| [ITALIC] d|)"
            ],
            "rows": [
                [
                    "Face",
                    "Figure\u00a0 12  (top, leftmost)",
                    "0.25",
                    "0.44",
                    "0.13",
                    "0.23"
                ],
                [
                    "Face",
                    "Figure\u00a0 12  (top, second left)",
                    "0.26",
                    "0.44",
                    "0.16",
                    "0.24"
                ],
                [
                    "Face",
                    "Figure\u00a0 12  (top, second right)",
                    "0.24",
                    "0.44",
                    "0.12",
                    "0.24"
                ],
                [
                    "Face",
                    "Figure\u00a0 12  (top, rightmost)",
                    "0.26",
                    "0.44",
                    "0.15",
                    "0.25"
                ],
                [
                    "Bunny",
                    "Figure\u00a0 12  (middle, leftmost)",
                    "0.49",
                    "0.70",
                    "0.33",
                    "0.48"
                ],
                [
                    "Bunny",
                    "Figure\u00a0 12  (middle, second left)",
                    "0.49",
                    "0.70",
                    "0.33",
                    "0.48"
                ],
                [
                    "Bunny",
                    "Figure\u00a0 12  (middle, second right)",
                    "0.49",
                    "0.70",
                    "0.33",
                    "0.48"
                ],
                [
                    "Bunny",
                    "Figure\u00a0 12  (middle, rightmost)",
                    "0.49",
                    "0.70",
                    "0.33",
                    "0.49"
                ],
                [
                    "Duck",
                    "Figure\u00a0 12  (bottom, leftmost)",
                    "0.27",
                    "0.46",
                    "0.16",
                    "0.27"
                ],
                [
                    "Duck",
                    "Figure\u00a0 12  (bottom, second left)",
                    "0.27",
                    "0.46",
                    "0.16",
                    "0.27"
                ],
                [
                    "Duck",
                    "Figure\u00a0 12  (bottom, second right)",
                    "0.28",
                    "0.48",
                    "0.17",
                    "0.30"
                ],
                [
                    "Duck",
                    "Figure\u00a0 12  (bottom, rightmost)",
                    "0.27",
                    "0.47",
                    "0.16",
                    "0.27"
                ]
            ],
            "title": "Table 5: The performance of our proposed PGCP method with different choices of cut paths. The mean, standard deviation, median, and interquartile range of the absolute angular distortion |d| are evaluated."
        },
        "insight": "The accuracy of the resulting parameterizations is recorded in Table 5. It can be observed that the angular distortions produced by different choices of cut paths are highly consistent, which indicates that our method is robust to the choice of the cut paths."
    },
    {
        "id": "1038",
        "table": {
            "header": [
                "# of subdomains  [ITALIC] n",
                "2",
                "3",
                "4",
                "5",
                "6"
            ],
            "rows": [
                [
                    "Average mesh size |V|/ [ITALIC] n",
                    "42K",
                    "28K",
                    "21K",
                    "17K",
                    "14K"
                ],
                [
                    "Parallel speedup  [ITALIC] Sn",
                    "1",
                    "1.31",
                    "1.66",
                    "1.91",
                    "2.05"
                ],
                [
                    "[ITALIC] n/2",
                    "1",
                    "1.5",
                    "2",
                    "2.5",
                    "3"
                ],
                [
                    "Parallel efficiency  [ITALIC] En",
                    "1",
                    "0.87",
                    "0.83",
                    "0.76",
                    "0.68"
                ]
            ],
            "title": "Table 6: The performance of our proposed PGCP method for parameterizing the bunny model, with different number of subdomains used. Note that the welding algorithm requires at least two subdomains and hence we use n=2 as the baseline. For this reason, we define the parallel speedup to be Sn=T2Tn where Ti is the time taken with i subdomains used, and the parallel efficiency to be En=Snn/2."
        },
        "insight": "Table 6 shows the experimental results with 2, 3, 4, 5, 6 subdomains. Note that 2 is used as the baseline as the welding algorithm requires at least two subdomains. From the experimental result, we observe a speedup achieved by exploiting parallelization. To evaluate the parallel efficiency, we consider the ratio En = Sn n/2 , where Sn is the speedup achieved by n subdomains compared to the baseline and n/2 is the subdomain ratio. It can be observed that En is close to 1 for small n but shows a decreasing trend as n increases."
    }
]